{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_answer": "", "predicted_evidence": ["In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents.", "To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.", "MISSA is based on the generative pre-trained transformer BIBREF32. We use an Adam optimizer with a learning rate of 6.25e-5 and $L2$ weight decay of $0.01$, we set the coefficient of language modeling loss to be 2, the coefficient of intent and slot classifiers to be 1, and the coefficient of next-utterance classifier to be 1. We first pre-train the model on the PERSONA-CHAT dataset. When fine-tuning on the AntiScam and the PersuasionForGood datasets, we use $80\\%$ data for training, $10\\%$ data for validation, and $10\\%$ data for testing. Since the original PersuasionForGood dataset is annotated with intents, we separate the original on-task and off-task intents, which are shown in Table TABREF2. To deal with the words out of the vocabulary, we conduct delexicalization to replace slot values with corresponding slot tokens during the training phase, and replace the slot tokens with pre-defined information during testing.", "To explore the influence of the intent-based conditional response generation method and the designed response filter, we perform an ablation study. The results are shown in Table TABREF19. We find that MISSA has higher fluency score and coherence score than MISSA-con (4.18 vs 3.78 for fluency, and 3.75 vs 3.68 for coherence), which suggests that conditioning on the system intent to generate responses improves the quality of the generated sentences. Compared with MISSA-sel, MISSA achieves better performance on all the metrics. For example, the engagement score for MISSA is 3.69 while MISSA-sel only has 2.87. This is because the response filter removed all the incoherent responses, which makes the attacker more willing to keep chatting. The ablation study shows both the conditional language generation mechanism and the response filter are essential to MISSA's good performance.", "In these tasks, users often perform complex actions that are beyond a simple set of pre-defined intents. In order to reach a common state, the user and the system need to build rapport and trust which naturally involves off-task content. Previous work did not model off-task content BIBREF2, which may have led to less optimal results. For example, in the persuasion task BIBREF1, users would ask the system \u201cHow do you feel about war?\" An example of an on-task system response that the system could have made is \u201cDo you want to make a donation?\", which sticks to the task but neglects users' question. However, a better response to such an off-task question is \u201cWar is destructive and pitiless, but you can donate to help child victims of war.\" This response is better, as it has been found that users are more likely to end the conversation if the system neglects their questions BIBREF4."]}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_answer": "", "predicted_evidence": ["The authors also trained a classifier to distinguish the correct next-utterance appended to the input human utterances from a set of randomly selected utterance distractors. In addition, they introduced dialog state embeddings to indicate speaker role in the model. The model significantly outperformed previous baselines over both automatic evaluations and human evaluations in social conversations. Since the TransferTransfo framework performs well in open domain, we adapt it for non-collaborative settings. We keep all the embeddings in the framework and train the language model and next-utterance classification task in a multi-task fashion following TransferTransfo.", "We make two major changes: (1) To address the problem that TransferTransfo is originally designed for an open domain without explicit intents and regulations, we add two intent classifiers and two semantic slot classifiers to classify the intents and semantic slots for both human utterances and system responses as an effort to incorporate the proposed hierarchical intent and semantic slot annotation for non-collaborative tasks. (2) In dialog systems, multiple generated responses can be coherent under the current context. Generating diverse responses has proven to be an enduring challenge. To increase response diversity, we sample multiple generated responses and choose an appropriate one according to a set of pre-defined rules.", "Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.", "Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.", "Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system."]}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_answer": "", "predicted_evidence": ["To tackle the issue of incoherent system responses to off-task content, previous studies have built hybrid systems to interleave off-task and on-task content. BIBREF4 used a rule-based dialog manager for on-task content and a neural model for off-task content, and trained a reinforcement learning model to select between these two models based on the dialog context. However, such a method is difficult to train and struggles to generalize beyond the movie promotion task they considered. To tackle these problems, we propose a hierarchical intent annotation scheme that separates on-task and off-task information in order to provide detailed supervision. For on-task information, we directly use task-related intents for representation. Off-task information, on the other hand, is too general to categorize into specific intents, so we choose dialog acts that convey syntax information. These acts, such as \u201copen question\" are general to all tasks.", "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.", "Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance.", "MISSA is able to classify multiple intents and multiple semantic slots in a single utterance with these classifiers. Figure FIGREF6 shows how it works on the AntiScam dataset. Specifically, we set a special token $<$sep$>$ at the end of each sentence in an utterance (an utterance can consist of multiple sentences). Next, we pass the token's position information to the transformer architecture and obtain the representation of the position (represented as colored position at last layer in Figure FIGREF6). After that, we concatenate the embeddings at these position with the hidden states of last sentence. We pass these concatenated representations to the intent classifier and the slot classifier to obtain an intent and a semantic slot for each sentence in the utterance. As shown in Figure FIGREF6, the loss function ${\\mathcal {L}}$ for the model combines all the task losses:", "To decouple syntactic and semantic information in utterances and provide detailed supervision, we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. The advantage of this hierarchical annotation scheme is apparent when starting a new non-collaborative task: we only need to focus on designing the on-task categories and semantic slots which are the same as traditional task-oriented dialog systems. Consequently, we don't have to worry about the off-task annotation design since the off-task category is universal."]}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_answer": "", "predicted_evidence": ["To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.", "MISSA is able to classify multiple intents and multiple semantic slots in a single utterance with these classifiers. Figure FIGREF6 shows how it works on the AntiScam dataset. Specifically, we set a special token $<$sep$>$ at the end of each sentence in an utterance (an utterance can consist of multiple sentences). Next, we pass the token's position information to the transformer architecture and obtain the representation of the position (represented as colored position at last layer in Figure FIGREF6). After that, we concatenate the embeddings at these position with the hidden states of last sentence. We pass these concatenated representations to the intent classifier and the slot classifier to obtain an intent and a semantic slot for each sentence in the utterance. As shown in Figure FIGREF6, the loss function ${\\mathcal {L}}$ for the model combines all the task losses:", "Traditional task-oriented dialog systems BIBREF12 are usually composed of multiple independent modules, for example, natural language understanding, dialog state tracking BIBREF13, BIBREF14, dialog policy manager BIBREF15, and natural language generation BIBREF16. Conversational intent is adopted to capture the meaning of task content in these dialog systems BIBREF2, BIBREF17. In comparison to this work, we use a hierarchical intent scheme that includes off-task and on-task intents to capture utterance meaning. We also train the model in a multi-task fashion to predict decoupled intents and semantic slots. The major defect of a separately trained pipeline is the laborious dialog state design and annotation. In order to mitigate this problem, recent work has explored replacing independent modules with end-to-end neural networks BIBREF18, BIBREF19, BIBREF20. Our model also follows this end-to-end fashion.", "where $L^i_{t}$ is the intent or semantic label of $i$-th sentence at turn $t$. $h^l_{t-1}$ is the hidden states at the end of last sentence in turn $t-1$, $h^i_{t}$ is the last hidden states at the end of $i$-th sentence in turn $t$. $W_{2h}$ are weights learned during training.", "MISSA can generate multiple sentences in a single system turn. Therefore, we perform system generation conditioned on predicted system intents. More specifically, during the training phase, in addition to inserting a special $<$sep$>$ token at the end of each sentence, we also insert the intent of the system response as special tokens at the head of each sentence in the system response. For example, in Figure FIGREF6, we insert a $<$pos_ans$>$ token at the head of $S_t^1$, which is the system response in green. We then use a cross entropy loss function to calculate the loss between the predicted token and the ground truth intent token. During the testing phase, the model first generates a special intent token, then after being conditioned on this intent token, the model keeps generating a sentence until it generates a $<$sep$>$ token. After that, the model continues to generate another intent token and another sentence until it generates an $<$eos$>$ token."]}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_answer": "", "predicted_evidence": ["Our goal is to evaluate the usefulness of this crowdsourced structured data for entity linking. We will therefore refrain from augmenting it with any external data (such as phrases and topical information extracted from Wikipedia pages), as is generally done when working with DBpedia or YAGO. By avoiding a complex mash-up of data coming from disparate sources, our entity linking system is also simpler and easier to reproduce. Finally, it is possible keep OpenTapioca in real-time synchronization with the live version of Wikidata, with a lag of a few seconds only. This means that users are able to fix or improve the knowledge graph, for instance by adding a missing alias on an item, and immediately see the benefits on their entity linking task. This constrasts with all other systems we are aware of, where the user either cannot directly intervene on the underlying data, or there is a significant delay in propagating these updates to the entity linking system.", "Wikidata is a wiki itself, meaning that it can be edited by anyone, but differs from usual wikis by its data model: information about an entity can only be input as structured data, in a format that is similar to RDF.", "The weighted graph $G_d$ can be represented as an adjacency matrix. We transform it into a column-stochastic matrix $M_d$ by normalizing its columns to sum to one. This defines a Markov chain on the candidate entities, that we will use to propagate the local evidence.", "We review the main differences between Wikidata and static knowledge bases extracted from Wikipedia, and analyze their implactions for entity linking. We illustrate these differences by building a simple entity linker, OpenTapioca, which only uses data from Wikidata, and show that it is competitive with other systems with access to larger data sources for some tasks. OpenTapioca can be trained easily from a Wikidata dump only, and can be efficiently kept up to date in real time as Wikidata evolves. We also propose tools to adapt existing entity linking datasets to Wikidata, and offer a new entity linking dataset, consisting of affiliation strings extracted from research articles.", "The local compatibility is therefore represented by a vector of features $F(e,w)$ and the local compatibility is computed as follows, where $\\lambda $ is a weights vector: $\nF(e,w) &= ( -\\log p(d[s]), \\log p(e) , n_e, s_e, 1 ) \\\\\np(e|d[s]) &\\propto e^{F(e,w) \\cdot \\lambda }\n$"]}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_answer": "", "predicted_evidence": ["High volume streams demand highly efficient feature computation. This applies in particular to novelty based features since they can be computationally expensive. We explore two approaches to novelty computation: one based on vector proximity, the other on kterm hashing.", "In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours. Once we obtained the value for the PF feature, we compute its weight using the SVM. The combination of the weight for the PF feature with the weights for all other features, obtained in the initial trainings round, resembles the final model.", "On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score.", "Novelty based features revealed the highest impact on detection performance. In particular kterms formed from the top keywords contribute the most. This is interesting, as when kterm hashing was introduced (Wurzer et. al, 2015), all kterms were considered as equally important. We found that prioritising certain kterms yields increased performance.", "The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds as we require a model of all other features to identify 'pseudo' rumours. In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours."]}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_answer": "", "predicted_evidence": ["Novelty based Features", "Therefore, researchers like Liu et. al (2015), Wu et. al (2015), Zhao et. al (2015) and Zhou et. al (2015) focus on 'early rumour-detection' while allowing a delay up to 24 hours. Their focus on latency aware rumour detection makes their approaches conceptually related to ours. Zhao et. al (1015) found clustering tweets containing enquiry patterns as an indication of rumours. Also clustering tweets by keywords and subsequently judging rumours using an ensemble model that combine user, propagation and content-based features proved to be effective (Zhou et. al, 2015). Although the computation of their features is efficient, the need for repeated mentions in the form of response by other users results in increased latency between publication and detection. The approach with the lowest latency banks on the 'wisdom of the crowd' (Liu et.", "The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds as we require a model of all other features to identify 'pseudo' rumours. In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours.", "The PF feature describes the maximum similarity between a new document and those documents previously considered as rumour. Similarities are measured by vector proximity in term space. Conceptually, PF passes on evidence to repeated signals by increasing the rumour score of future documents if they are similar to a recently detected rumour. Note that this allows harnessing information from repeated signals without the need of operating retrospectively.", "We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages."]}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_answer": "", "predicted_evidence": ["On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score.", "Apart from novelty based features, we also apply a range of 51 context based features. The full list of features can be found in table 6 . The focus lies on features that can be computed instantly based only on the text of a message to keep the latency of our approach to a minimum. Most of these 51 features overlap with previous studies (Castillo et. al, 2011; Liu et. al, 2015; Qazvinian et. al, 2011; Yang et. al, 2012; Zhao et. al, 2015). This includes features based on the presence or number of URLs, hash-tags and user-names, POS tags, punctuation characters as well as 8 different categories of sentiment and emotions.", "Liu et. al (2015) report performance based on the first 5 messages which clearly outperforms Yang for early rumour detection. However, we find that when reducing the set from 5 to 1, their superiority is only marginal. In contrast, the combination of novelty and pseudo relevance based features performs significantly better (sign test with $p < 0.05$ ) than the baselines for instantaneous rumour detections. Novelty based features benefit from news articles as an external data source, which explains their superior performance. In particular for instantaneous rumour detection, where information can only be obtained from a single message, the use of external data proves to perform superior. Note that accuracy is a single value metric describing performance at an optimal threshold. Figure 1 compares the effectiveness of the three algorithms for the full range of rumour scores for instantaneous detection. Different applications require a different balance between miss and false alarm.", "Novelty based Features", "The second approach to compute novelty relies on kterm hashing (Wurzer et. al, 2015), a recent advance in novelty detection that improved the efficiency by an order of magnitude without sacrificing effectiveness. Kterm hashing computes novelty non-comparatively. Instead of measuring similarity between documents, a single representation of previously seen information is constructed. For each document, all possible kterms are formed and hashed onto a Bloom Filter. Novelty is computed by the fraction of unseen kterms. Kterm hashing has the interesting characteristic of forming a collective 'memory', able to span all trusted resources. We exhaustively form kterm for all news articles and store their corresponding hash positions in a Bloom Filter. This filter then captures the combined information of all trusted resources. A single representation allows computing novelty with a single step, instead of comparing each social media message individually with all trusted resources."]}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_answer": "", "predicted_evidence": ["Previous approaches to rumour detection rely on repeated signals to form propagation graphs or clustering methods. Beside causing a detection delay these methods are also blind to less popular rumours that don't go viral. In contrast, novelty based feature require only a single message enabling them to detect even the smallest rumours. Examples for such small rumours are shown in table 3 .", "In contrast, we introduce new features to detect rumours as early as possible - preferably instantly, allowing them to be debunked before they spread and cause harm.", "We ordered the rumours and non-rumours chronologically and divided them in half, forming a training and test set. We ensured that each of the sets consists of 50% rumours and non-rumours. This is important when effectiveness is measured by accuracy. All training and optimization use the trainings set. Performance is then reported based on a single run on the test set.", "Table 2 compares the performance of our features with the two classifiers on the 101 rumours and 101 non-rumours of the test set, when detecting rumour instantly after their publication. The table reveals comparable accuracy for Yang and Liu at around 60%. Our observed performance of Yang matches those by Liu et. al (2015). Surprisingly, the algorithm Liu does not perform significantly better than Yang when applied to instantaneous rumour detection although they claimed to operate in real-time. Liu et. al (2015) report performance based on the first 5 messages which clearly outperforms Yang for early rumour detection. However, we find that when reducing the set from 5 to 1, their superiority is only marginal. In contrast, the combination of novelty and pseudo relevance based features performs significantly better (sign test with $p < 0.05$ ) than the baselines for instantaneous rumour detections. Novelty based features benefit from news articles as an external data source, which explains their superior performance.", "We measure the throughput on an idle machine and average the observed performance over 5 runs. Figure 2 presents performance when processing more and more weibos. The average throughput of our system is around 7,000 weibos per second, which clearly exceeds the average volume of the full Twitter (5,700 tweets/sec.) and Sina Weibo (1,200 weibos/sec.) stream. Since the number of news articles is relatively small, we find no difference in terms of efficiency between computing novelty features based on kterm hashing and vector similarity. Figure 2 also illustrates that our proposed features can be computed in constant time with respect to the number of messages processed. This is crucial to keep operation in a true streaming environment feasible. Approaches, whose runtime depend on the number of documents processed become progressively slower, which is inapplicable when operating on data streams. Our experiments show that the proposed features perform effectively and their efficiency allows them to detect rumours instantly after their publication."]}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_answer": "", "predicted_evidence": ["The second approach to compute novelty relies on kterm hashing (Wurzer et. al, 2015), a recent advance in novelty detection that improved the efficiency by an order of magnitude without sacrificing effectiveness. Kterm hashing computes novelty non-comparatively. Instead of measuring similarity between documents, a single representation of previously seen information is constructed. For each document, all possible kterms are formed and hashed onto a Bloom Filter. Novelty is computed by the fraction of unseen kterms. Kterm hashing has the interesting characteristic of forming a collective 'memory', able to span all trusted resources. We exhaustively form kterm for all news articles and store their corresponding hash positions in a Bloom Filter. This filter then captures the combined information of all trusted resources. A single representation allows computing novelty with a single step, instead of comparing each social media message individually with all trusted resources.", "To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods.", "Computing novelty based on traditional vector proximity alone does not yield adequate performance due to the length discrepancy between news wire articles and social media messages. To make vector proximity applicable, we slide a term-level based window, whose length resembles the average social media message length, through each of the news articles. This results in sub-documents whose length resembles those of social media messages. Novelty is computed using term weighted tf-idf dot products between the social media message and all news sub-documents. The inverse of the minimum similarity to the nearest neighbour equates to the degree of novelty.", "When kterm hashing was introduced by Wurzer et. al (2015) for novelty detection on English tweets, they weighted all kterm uniformly. We found that treating all kterms as equally important, does not unlock the full potential of kterm hashing. Therefore, we additionally extract the top 10 keywords ranked by $tf.idf$ and build a separate set of kterms solely based on them. This allows us to compute a dedicated weight for kterms based on these top 10 keywords. The distinction in weights between kterms based on all versus keyword yields superior rumour detection quality, as described in section \"Feature analysis\" . This leaves us with a total of 6 novelty based features for kterm hashing - kterms of length 1 to 3 for all words and keywords.", "We frame the Real-time Rumour Detection task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass over the data."]}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_answer": "", "predicted_evidence": ["We introduced two new categories of features which significantly improve instantaneous rumour detection performance. Novelty based features consider the increased presence of unconfirmed information within a message with respect to trusted sources as an indication of being a rumour. Pseudo feedback features consider messages that are similar to previously detected rumours as more likely to also be a rumour. Pseudo feedback and its variant, recursive pseudo feedback, allow harnessing repeated signals without the need of operating retrospectively. Our evaluation showed that novelty and pseudo feedback based features perform significantly more effective than other real-time and early detection baselines, when detecting rumours instantly after their publication. This advantage vanishes when allowing an increased detection delay. We also showed that the proposed features can be computed efficiently enough to operate on the average Twitter and Sina Weibo stream while keeping time and space requirements constant.", "We measure the throughput on an idle machine and average the observed performance over 5 runs. Figure 2 presents performance when processing more and more weibos. The average throughput of our system is around 7,000 weibos per second, which clearly exceeds the average volume of the full Twitter (5,700 tweets/sec.) and Sina Weibo (1,200 weibos/sec.) stream. Since the number of news articles is relatively small, we find no difference in terms of efficiency between computing novelty features based on kterm hashing and vector similarity. Figure 2 also illustrates that our proposed features can be computed in constant time with respect to the number of messages processed. This is crucial to keep operation in a true streaming environment feasible. Approaches, whose runtime depend on the number of documents processed become progressively slower, which is inapplicable when operating on data streams. Our experiments show that the proposed features perform effectively and their efficiency allows them to detect rumours instantly after their publication.", "al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours. Liu is claimed to perform in real-time while, requiring a cluster of 5 repeated messages to judge them for rumours.", "We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.", "The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set."]}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_answer": "", "predicted_evidence": ["Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 have been proven to be effective in modeling textual documents. In these models, a word token in a document is assumed to be generated by a hidden mixture model, where the hidden variables are the topic indexes for each word and the topic assignments for words are related to document level topic weights. Due to the effectiveness and efficiency in modeling the document generation process, topic models are widely adopted in quite a lot of real world tasks such as sentiment classification BIBREF5 , social network analysis BIBREF6 , BIBREF5 , and recommendation systems BIBREF7 .", "All the classification results are shown in Table TABREF37 . From the table, we can see that SLRTM is the best model under each setting on both datasets. We can further find that the embedding based methods (Doc-NADE, GMNTM and SLRTM) generate better document representations than other models, demonstrating the representative power of neural networks based on distributed representations. In addition, when the training data is larger (i.e., with more sentences per document as Wiki10+), GMNTM generates worse topical information than Doc-NADE while our SLRTM outperforms Doc-NADE, showing that with sufficient data, SLRTM is more effective in topic modeling since topic coherence is further constrained for each sentence.", "Most topic models take the bag-of-words assumption, in which every document is treated as an unordered set of words and the word tokens in such a document are sampled independently with each other. The bag-of-words assumption brings computational convenience, however, it sacrifices the characterization of sequential properties of words in a document and the topic coherence between words belonging to the same language segment (e.g., sentence). As a result, people have observed many negative examples. Just list one for illustration BIBREF8 : the department chair couches offers and the chair department offers couches have very different topics, although they have exactly the same bag of words.", "We measure the performances of different topic models according to the perplexity per word on the test set, defined as INLINEFORM0 , where INLINEFORM1 is the number of words in document INLINEFORM2 . The experimental results are summarized in Table TABREF33 . Based on the table, we have the following discussions:", "There are some efforts trying to address the limitations of the bag-of-words assumption. For example, in BIBREF27 , both semantic (i.e., related with topics) and syntactic properties of words were modeled. After that, a hidden Markov transition model for topics was proposed BIBREF9 , in which all the words in a sentence were regarded as having the same topic. Such a one sentence, one topic assumption was also used by some other works, including BIBREF10 , BIBREF11 . Although these works have made some meaningful attempts on topic coherence and sequential dependency across sentences, they have not sufficiently model the sequential dependency of words within a sentence. To address this problem, the authors of BIBREF12 adopted the neural language model technology BIBREF13 to enhance topic model. In particular, they assume that every document, sentence, and word have their own topics and the topical information is conveyed by their embedding vectors through a Gaussian Mixture Model (GMM) as a prior."]}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_answer": "", "predicted_evidence": ["In this subsection, we demonstrate the capability of SLRTM in generating reasonable and understandable sentences given particular topics. In the experiment, we trained a larger SLRTM with 128 topics on a randomly sampled INLINEFORM0 Wikipedia documents in the year of 2010 with average 275 words per document. The dictionary is composed of roughly INLINEFORM1 most frequent words including common punctuation marks, with uppercase letters transformed into lowercases. The size of word embedding, topic embedding and RNN hidden layer are set to 512, 1024 and 1024, respectively.", "For the sake of fairness, similar to BIBREF12 , we set the word embedding size, topic embedding size, and LSTM hidden layer size to be 128, 128, and 600 respectively. In the experiment, we tested the performances of SLRTM and the baselines with respect to different number of topics INLINEFORM0 , i.e., INLINEFORM1 . In initialization (values of INLINEFORM2 and INLINEFORM3 ), the LSTM weight matrices were initialized as orthogonal matrices, the word/topic embeddings were randomly sampled from the uniform distribution INLINEFORM4 and are fined-tuned through the training process, INLINEFORM5 and INLINEFORM6 were both set to INLINEFORM7 . The mini-batch size in Algorithm SECREF15 was set as INLINEFORM8 , and we ran the E-Step of the algorithm for only one iteration for efficiently consideration, which leads to the final convergence after about 6 epochs for both datasets. Gradient clipping with a clip value of 20 was used during the optimization of LSTM weights.", "HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.", "", "One of the most representative topic models is Latent Dirichlet Allocation BIBREF2 , in which every word in a document has its topic drawn from document level topic weights. Several variants of LDA have been developed such as hierarchical topic models BIBREF22 and supervised topic models BIBREF3 . With the recent development of deep learning, there are also neural network based topic models such as BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , which use distributed representations of words to improve topic semantics."]}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "predicted_answer": "", "predicted_evidence": ["The relation between $d$, $k$ and $PE_t^TPE_{t+k}$ is displayed in Fig FIGREF18. The sinusoidal position embeddings are distance-aware but lacks directionality.", "", "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.", "To get $H_{d_k}\\in \\mathbb {R}^{l \\times d_k}$, we first split $H$ into $d/d_k$ partitions in the second dimension, then for each head we use one partition. $\\mathbf {u} \\in \\mathbb {R}^{d_k}$, $\\mathbf {v} \\in \\mathbb {R}^{d_k}$ are learnable parameters, $R_{t-j}$ is the relative positional encoding, and $R_{t-j} \\in \\mathbb {R}^{d_k}$, $i$ in Eq.() is in the range $[0, \\frac{d_k}{2}]$. $Q_t^TK_j$ in Eq.", "Therefore, to improve the Transformer with direction- and distance-aware characteristic, we calculate the attention scores using the equations below:"]}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "predicted_answer": "", "predicted_evidence": ["However, the property of distance-awareness also disappears when $PE_t$ is projected into the query and key space of self-attention. Since in vanilla Transformer the calculation between $PE_t$ and $PE_{t+k}$ is actually $PE_t^TW_q^TW_kPE_{t+k}$, where $W_q, W_k$ are parameters in Eq.(DISPLAY_FORM7). Mathematically, it can be viewed as $PE_t^TWPE_{t+k}$ with only one parameter $W$. The relation between $PE_t^TPE_{t+k}$ and $PE_t^TWPE_{t+k}$ is depicted in Fig FIGREF19.", "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.", "", "because $\\sin (-x)=-\\sin (x), \\cos (x)=\\cos (-x)$. This means for an offset $t$, the forward and backward relative positional encoding are the same with respect to the $\\cos (c_it)$ terms, but is the opposite with respect to the $\\sin (c_it)$ terms. Therefore, by using $R_{t-j}$, the attention score can distinguish different directions and distances.", "The relation between $d$, $k$ and $PE_t^TPE_{t+k}$ is displayed in Fig FIGREF18. The sinusoidal position embeddings are distance-aware but lacks directionality."]}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_answer": "", "predicted_evidence": ["The ID-CNN encoder is from BIBREF23, and we re-implement their model in PyTorch. For different combinations, we use random search to find its best hyper-parameters. Hyper-parameters for character encoders were fixed. The details can be found in the supplementary material.", "Their statistics are listed in Table TABREF28. For all datasets, we replace all digits with \u201c0\u201d, and use the BIOES tag schema. For English, we use the Glove 100d pre-trained embedding BIBREF25. For the character encoder, we use 30d randomly initialized character embeddings. More details on models' hyper-parameters can be found in the supplementary material. For Chinese, we used the character embedding and bigram embedding released by BIBREF33. All pre-trained embeddings are finetuned during training. In order to reduce the impact of randomness, we ran all of our experiments at least three times, and its average F1 score and standard deviation are reported.", "where $t$ is index of the target token, $j$ is the index of the context token, $Q_t, K_j$ is the query vector and key vector of token $t, j$ respectively, $W_q, W_v \\in \\mathbb {R}^{d \\times d_k}$. To get $H_{d_k}\\in \\mathbb {R}^{l \\times d_k}$, we first split $H$ into $d/d_k$ partitions in the second dimension, then for each head we use one partition. $\\mathbf {u} \\in \\mathbb {R}^{d_k}$, $\\mathbf {v} \\in \\mathbb {R}^{d_k}$ are learnable parameters, $R_{t-j}$ is the relative positional encoding, and $R_{t-j} \\in \\mathbb {R}^{d_k}$, $i$ in Eq.", "", "The hyper-parameters and search ranges for different encoders are presented in Table TABREF40, Table TABREF41 and Table TABREF42."]}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_answer": "", "predicted_evidence": ["Based on KALM, KALM-QA BIBREF6 is developed for question answering. KALM-QA shares the same components with KALM for syntactic parsing, frame-based parsing and role-filler disambiguation. Different from KALM, KALM-QA translates the questions to unique logical representation for queries (ULRQ), which are used to query the authored knowledge base.", "Constructing ULR. The extracted frame instances are translated into the corresponding logical representations, unique logical representation (ULR). Examples can be found in reference BIBREF5 .", "Role-filler Disambiguation. Based on the extracted frame instance, the role-filler disambiguation module disambiguates the meaning of each role-filler word for the corresponding frame role a BabelNet Synset ID. A complex algorithm BIBREF5 was proposed to measure the semantic similarity between a candidate BabelNet synset that contains the role-filler word and the frame-role synset. The algorithm also has optimizations that improve the efficiency of the algorithm e.g., priority-based search, caching, and so on. In addition to disambiguating the meaning of the role-fillers, this module is also used to prune the extracted frame instances where the role-filler word and the frame role are semantically incompatible.", "The first three arguments of an lvp-fact identify the lexical unit, its part of speech, and the frame. The fourth argument is a set of pattern-terms, each having three parts: the name of a role, a grammatical pattern, and the required/optional flag. The grammatical pattern determines the grammatical context in which the lexical unit, a role, and a role-filler word can appear in that frame. Each grammatical pattern is captured by a parsing rule (a Prolog rule) that can be used to extract appropriate role-filler words based on the APE parses.", "Controlled natural languages (CNLs) BIBREF0 were developed as a technology that achieves this goal. CNLs are designed based on natural languages (NLs) but with restricted syntax and interpretation rules that determine the unique meaning of the sentence. Representative CNLs include Attempto Controlled English BIBREF1 and PENG BIBREF2 . Each CNL is developed with a language parser which translates the English sentences into an intermediate structure, discourse representation structure (DRS) BIBREF3 . Based on the DRS structure, the language parsers further translate the DRS into the corresponding logical representations, e.g., Answer Set Programming (ASP) BIBREF4 programs. One main issue with the aforementioned CNLs is that the systems do not provide enough background knowledge to preserve semantic equivalences of sentences that represent the same meaning but are expressed via different linguistic structures."]}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_answer": "", "predicted_evidence": ["Time Reasoning. Time-related information is a crucial part of human knowledge, but semantic parsing that takes the time into account is rather hard. However, we can develop a CNL that would incorporate enough time related idioms to be useful in a number of domains of discourse (e.g., tax law). Time can then be added to DRSs and incorporated into our frame based approach down to the very level of the logical facts into which sentences will be translated. This time information can be represented either via special time-aware relations among events (e.g., before, after, causality, triggering) or using a reserved argument to represent time in each fluent.", "Authoring Rules from CNL. There are two research problems with rules. The first problem is the standardization of rules parses that express the same information but via different syntactic forms or using different expressions. Suppose the knowledge base contains sentences like: (1) if a person buys a car then the person owns the car, (2) every person who is a purchaser of a car is an owner of the car, (3) if a car is bought by a person then the person possesses the car. All the above sentences represent rules and express exactly the same meaning. However, KALM's current syntactic parser will represent them in different DRSs and therefore not being able to map them into the same logical form. The second problem involves the recognition and representation of different types of rules in logic. For instance, defeasible rules are very common in text. However, this type of rules cannot be handled by first order logic.", "pattern(Seller,verb->pp(from)->dep,optnl)]).", "Figure FIGREF1 shows the architecture of KALM which translates a CNL sentence to the corresponding logical representations, unique logical representations (ULR).", "This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems."]}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_answer": "", "predicted_evidence": ["Constructing ULR. The extracted frame instances are translated into the corresponding logical representations, unique logical representation (ULR). Examples can be found in reference BIBREF5 .", "pattern(Goods,verb->object,required),", "pattern(Seller,verb->pp(from)->dep,optnl)]).", "Based on KALM, KALM-QA BIBREF6 is developed for question answering. KALM-QA shares the same components with KALM for syntactic parsing, frame-based parsing and role-filler disambiguation. Different from KALM, KALM-QA translates the questions to unique logical representation for queries (ULRQ), which are used to query the authored knowledge base.", "Authoring Rules from CNL. There are two research problems with rules. The first problem is the standardization of rules parses that express the same information but via different syntactic forms or using different expressions. Suppose the knowledge base contains sentences like: (1) if a person buys a car then the person owns the car, (2) every person who is a purchaser of a car is an owner of the car, (3) if a car is bought by a person then the person possesses the car. All the above sentences represent rules and express exactly the same meaning. However, KALM's current syntactic parser will represent them in different DRSs and therefore not being able to map them into the same logical form. The second problem involves the recognition and representation of different types of rules in logic. For instance, defeasible rules are very common in text. However, this type of rules cannot be handled by first order logic. We believe defeasible logic BIBREF19 is a good fit."]}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_answer": "", "predicted_evidence": ["Current societies are exposed to a continuous flow of information that results in a large production of data (e.g. news articles, micro-blogs, social media posts, among others), at different moments in time. In addition to this, the consumption of information has dramatically changed: more and more people directly access information through social media platforms (e.g. Facebook and Twitter), and are less and less exposed to a diversity of perspectives and opinions. The combination of these factors may easily result in information overload and impenetrable \u201cfilter bubbles\u201d. Events, i.e. things that happen or hold as true in the world, are the basic components of such data stream. Being able to correctly identify and classify them plays a major role to develop robust solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic summarization and question answering (Q.A.).", "We follow the formulation of the task as specified in the EVENTI exercise: determine the extent and the class of event mentions in a text, according to the It-TimeML $<$ EVENT $>$ tag definition (Subtask B in EVENTI).", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality.", "In EVENTI, the tag $<$ EVENT $>$ is applied to every linguistic expression denoting a situation that happens or occurs, or a state in which something obtains or holds true, regardless of the specific parts-of-speech that may realize it. EVENTI distinguishes between single token and multi-tokens events, where the latter are restricted to specific cases of eventive multi-word expressions in lexicographic dictionaries (e.g. \u201cfare le valigie\u201d [to pack]), verbal periphrases (e.g. \u201c(essere) in grado di\u201d [(to be) able to]; \u201cc'\u00e8\u201d [there is]), and named events (e.g. \u201cla strage di Beslan\u201d [Beslan school siege]).", "casa (B-STATE $|$ I-STATE $|$ ... $|$ O) O"]}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_answer": "", "predicted_evidence": ["We approached the task in a single-step by detecting and classifying event mentions at once rather than in the standard two step approach, i.e. detection first and classification on top of the detected elements. The task is formulated as a seq2seq problem, by converting the original annotation format into an BIO scheme (Beginning, Inside, Outside), with the resulting alphabet being B-class_label, I-class_label and O. Example \"System and Experiments\" below illustrates a simplified version of the problem for a short sentence:", "Although FBK-HLT suffers in the classification subtask, it qualifies as a highly competitive system for the detection subtask. By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK-100, ILC-ItWack, Berardi2015_Glove) , almost equals one (Berardi2015_w2v) , and it is outperformed only by one (Fastext-It) . In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack. The results also indicates that word embeddings have a major contribution on Recall, supporting observations that distributed representations have better generalization capabilities than discrete feature vectors.", "input problem solution", "The network obtains the best F1 score, both for detection (F1 of 0.880 for strict evaluation and 0.903 for relaxed evaluation with Fastext-It embeddings) and for classification (F1-class of 0.756 for strict evaluation, and 0.751 for relaxed evaluation with Fastext-It embeddings). Although FBK-HLT suffers in the classification subtask, it qualifies as a highly competitive system for the detection subtask. By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK-100, ILC-ItWack, Berardi2015_Glove) , almost equals one (Berardi2015_w2v) , and it is outperformed only by one (Fastext-It) . In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack.", "Concerning the classification, we focused on the mismatches between correctly identified events (extent layer) and class assignment. The Fastext-It model wrongly assigns the class to only 557 event tokens compared to the 729 cases for FBK-HLT. The distribution of the class errors, in terms of absolute numbers, is the same between the two systems, with the top three wrong classes being, in both cases, OCCURRENCE, I_ACTION and STATE. OCCURRENCE, not surprisingly, is the class that tends to be assigned more often by both systems, being also the most frequent. However, if FBK-HLT largely overgeneralizes OCCURRENCE (59.53% of all class errors), this corresponds to only one third of the errors (37.70%) in the Bi-LSTM-CRF network. Other notable differences concern I_ACTION (27.82% of errors for the Bi-LSTM-CRF vs. 17.28% for FBK-HLT), STATE (8.79% for the Bi-LSTM-CRF vs."]}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_answer": "", "predicted_evidence": ["andare (B-STATE $|$ I-STATE $|$ ... $|$ O) B-OCCUR", "The author wants to thank all researchers and research groups who made available their word embeddings and their code. Sharing is caring.", "casa (B-STATE $|$ I-STATE $|$ ... $|$ O) O", "Each event is further assigned to one of 7 possible classes, namely: OCCURRENCE, ASPECTUAL, PERCEPTION, REPORTING, I(NTESIONAL)_STATE, I(NTENSIONAL)_ACTION, and STATE. These classes are derived from the English TimeML Annotation Guidelines BIBREF12 . The TimeML event classes distinguishes with respect to other classifications, such as ACE BIBREF1 or FrameNet BIBREF13 , because they expresses relationships the target event participates in (such as factual, evidential, reported, intensional) rather than semantic categories denoting the meaning of the event. This means that the EVENT classes are assigned by taking into account both the semantic and the syntactic context of occurrence of the target event. Readers are referred to the EVENTI Annotation Guidelines for more details.", "By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK-100, ILC-ItWack, Berardi2015_Glove) , almost equals one (Berardi2015_w2v) , and it is outperformed only by one (Fastext-It) . In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack. The results also indicates that word embeddings have a major contribution on Recall, supporting observations that distributed representations have better generalization capabilities than discrete feature vectors. This is further supported by the fact that these results are obtained using a single step approach, where the network has to deal with a total of 15 possible different labels."]}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_answer": "", "predicted_evidence": ["In recent decades, the study of gender and language has also attracted computational researchers. Echoing Lakoff's original claim, a popular strand of computational work focuses on differences in how women and men talk, analyzing key lexical traits BIBREF8, BIBREF9, BIBREF10 and predicting a person's gender from some text they have written BIBREF11, BIBREF12. There is also research studying how people talk to women and men BIBREF13, as well as how people talk about women and men, typically in specific domains such as sports journalism BIBREF14, fiction writing BIBREF15, movie scripts BIBREF16, and Wikipedia biographies BIBREF17, BIBREF18. Our work builds on this body by diving into two novel domains: celebrity news, which explores gender in pop culture, and student reviews of CS professors, which examines gender in academia and, particularly, the historically male-dominated field of CS.", "Our contributions include:", "Our first goal was to discover words that are significantly associated with men or women in a given domain. We employed an approach used by BIBREF10 in their work to analyze differences in how men and women write on Twitter.", "In recent decades, the study of gender and language has also attracted computational researchers. Echoing Lakoff's original claim, a popular strand of computational work focuses on differences in how women and men talk, analyzing key lexical traits BIBREF8, BIBREF9, BIBREF10 and predicting a person's gender from some text they have written BIBREF11, BIBREF12. There is also research studying how people talk to women and men BIBREF13, as well as how people talk about women and men, typically in specific domains such as sports journalism BIBREF14, fiction writing BIBREF15, movie scripts BIBREF16, and Wikipedia biographies BIBREF17, BIBREF18. Our work builds on this body by diving into two novel domains: celebrity news, which explores gender in pop culture, and student reviews of CS professors, which examines gender in academia and, particularly, the historically male-dominated field of CS. Furthermore, many of these works rely on manually constructed lexicons or topics to pinpoint gendered language, but our methods automatically infer gender-associated words and labeled clusters, thus reducing supervision and increasing the potential to discover subtleties in the data.", "Our first dataset contains articles from celebrity magazines People, UsWeekly, and E!News. We labeled each article for whether it was reporting on men, women, or neither/unknown. To do this, we first extracted the article's topic tags. Some of these tags referred to people, but others to non-people entities, such as \u201cGift Ideas\u201d or \u201cHealth.\u201d To distinguish between these types of tags, we queried each tag on Wikipedia and checked whether the top page result contained a \u201cBorn\u201d entry in its infobox \u2013 if so, we concluded that the tag referred to a person."]}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_answer": "", "predicted_evidence": ["Less studied in NLP is how gender norms manifest in everyday language \u2013 do people talk about women and men in different ways? These types of differences are far subtler than abusive language, but they can provide valuable insight into the roots of more extreme acts of discrimination. Subtle differences are difficult to observe because each case on its own could be attributed to circumstance, a passing comment or an accidental word. However, at the level of hundreds of thousands of data points, these patterns, if they do exist, become undeniable. Thus, in this work, we introduce new datasets and methods so that we can study subtle gender associations in language at the large-scale.", "First, to operationalize, we say that term $i$ is associated with gender $j$ if, when discussing individuals of gender $j$, $i$ is used with unusual frequency \u2013 which we can check with statistical hypothesis tests. Let $f_i$ represent the likelihood of $i$ appearing when discussing women or men. $f_i$ is unknown, but we can model the distribution of all possible $f_i$ using the corpus of texts that we have from the domain. We construct a gender-balanced version of the corpus by randomly undersampling the more prevalent gender until the proportions of each gender are equal. Assuming a non-informative prior distribution on $f_i$, the posterior distribution is Beta($k_i$, $N - k_i$), where $k_i$ is the count of $i$ in the gender-balanced corpus and $N$ is the total count of words in that corpus.", "We applied this method to discover gender-associated words in both domains. In Table TABREF9, we present a sample of the most gender-associated nouns from the celebrity domain. Several themes emerge: for example, female celebrities seem to be more associated with appearance (\u201cgown,\u201d \u201cphoto,\u201d \u201chair,\u201d \u201clook\u201d), while male celebrities are more associated with creating content (\u201cmovie,\u201d \u201cfilm,\u201d \u201chost,\u201d \u201cdirector\u201d). This echoes real-world trends: for instance, on the red carpet, actresses tend to be asked more questions about their appearance \u2013- what brands they are wearing, how long it took to get ready, etc. \u2013- while actors are asked questions about their careers and creative processes (as an example, see BIBREF31).", "As BIBREF10 discuss, \u201cthe distribution of the gender-specific counts can be described by an integral over all possible $f_i$. This integral defines the Beta-Binomial distribution BIBREF29, and has a closed form solution.\u201d We say that term $i$ is significantly associated with gender $j$ if the cumulative distribution at $k_{ij}$ (the count of $i$ in the $j$ portion of the gender-balanced corpus) is $p \\le 0.05$. As in the original work, we apply the Bonferroni correction BIBREF30 for multiple comparisons because we are computing statistical tests for thousands of hypotheses.", "To test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator with five words \u2013 four drawn from one cluster and one drawn randomly from the domain vocabulary \u2013 and we ask them to pick out the intruder. The intuition is that if the cluster is coherent, then an observer should be able to identify the out-of-cluster word as the intruder. For both domains, we report results on all clusters and on the top 8, ranked by ascending normalized sum of squared errors, which can be seen as a prediction of coherence. In the celebrity domain, annotators identified the out-of-cluster word 73% of the time in the top-8 and 53% overall. In the professor domain, annotators identified it 60% of the time in the top-8 and 49% overall."]}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_answer": "", "predicted_evidence": ["It is well-established that gender bias exists in language \u2013 for example, we see evidence of this given the prevalence of sexism in abusive language datasets BIBREF0, BIBREF1. However, these are extreme cases of gender norms in language, and only encompass a small proportion of speakers or texts.", "Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.", "The study of gender and language has a rich history in social science. Its roots are often attributed to Robin Lakoff, who argued that language is fundamental to gender inequality, \u201creflected in both the ways women are expected to speak, and the ways in which women are spoken of\u201d BIBREF2. Prominent scholars following Lakoff have included Deborah Tannen BIBREF3, Mary Bucholtz and Kira Hall BIBREF4, Janet Holmes BIBREF5, Penelope Eckert BIBREF6, and Deborah Cameron BIBREF7, along with many others.", "Modeling gender associations in language could also be instrumental to other NLP tasks. Abusive language is often founded in sexism BIBREF0, BIBREF1, so models of gender associations could help to improve detection in those cases. Gender bias also manifests in NLP pipelines: prior research has found that word embeddings preserve gender biases BIBREF19, BIBREF20, BIBREF21, and some have developed methods to reduce this bias BIBREF22, BIBREF23. Yet, the problem is far from solved; for example, BIBREF24 showed that it is still possible to recover gender bias from \u201cde-biased\u201d embeddings. These findings further motivate our research, since before we can fully reduce gender bias in embeddings, we need to develop a deeper understanding of how gender permeates through language in the first place.", "We also build on methods to cluster words in word embedding space and automatically label clusters. Clustering word embeddings has proven useful for discovering salient patterns in text corpora BIBREF25, BIBREF26. Once clusters are derived, we would like them to be interpretable. Much research simply considers the top-n words from each cluster, but this method can be subjective and time-consuming to interpret. Thus, there are efforts to design methods of automatic cluster labeling BIBREF27. We take a similar approach to BIBREF28, who leverage word embeddings and WordNet during labeling, and we extend their method with additional techniques and evaluations."]}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_answer": "", "predicted_evidence": ["In recent decades, the study of gender and language has also attracted computational researchers. Echoing Lakoff's original claim, a popular strand of computational work focuses on differences in how women and men talk, analyzing key lexical traits BIBREF8, BIBREF9, BIBREF10 and predicting a person's gender from some text they have written BIBREF11, BIBREF12. There is also research studying how people talk to women and men BIBREF13, as well as how people talk about women and men, typically in specific domains such as sports journalism BIBREF14, fiction writing BIBREF15, movie scripts BIBREF16, and Wikipedia biographies BIBREF17, BIBREF18. Our work builds on this body by diving into two novel domains: celebrity news, which explores gender in pop culture, and student reviews of CS professors, which examines gender in academia and, particularly, the historically male-dominated field of CS.", "The clusters we mentioned so far all lean heavily toward one gender association or the other, but some clusters are interesting precisely because they do not lean heavily \u2013 this allows us to see where semantic groupings do not align exactly with gender association. For example, in the celebrity domain, there is a cluster labeled lover that has a mix of female-associated words (\u201cboyfriend,\u201d \u201cbeau,\u201d \u201chubby\u201d) and male-associated words (\u201cwife,\u201d \u201cgirlfriend\u201d). Jointly leveraging cluster labels and gender associations allows us to see that in the semantic context of having a lover, women are typically associated with male figures and men with female figures, which reflects heteronormativity in society.", "Then, from the person's Wikipedia page, we determined their gender by checking whether the introductory paragraphs of the page contained more male or female pronouns. This method was simple but effective, since pronouns in the introduction almost always resolve to the subject of that page. In fact, on a sample of 80 tags that we manually annotated, we found that comparing pronoun counts predicted gender with perfect accuracy. Finally, if an article tagged at least one woman and did not tag any men, we labeled the article as Female; in the opposite case, we labeled it as Male.", "Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.", "At the word-level, we hypothesized that in the celebrity domain, women were more associated with appearance and men with creating content. Now, we can validate those hypotheses against labeled clusters \u2013 indeed, there is a cluster labeled clothing that is 100% female (i.e. 100% words are female-associated), and a 80% male cluster labeled movie. Likewise, in the professor domain, we had guessed that women are associated with communication and men with knowledge, and there is a 100% female cluster labeled communication and a 89% male cluster labeled cognition. Thus, cluster labeling proves to be very effective at pulling out the patterns that we believed we saw at the word-level, but could not formally validate."]}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_answer": "", "predicted_evidence": ["Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women.", "The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the \u201cdeflection\u201d strategy, such as \u201cWhy do you ask?\u201d. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this.", "NeuralConvo BIBREF13, a re-implementation of BIBREF14;", "We first gather abusive utterances from 600K conversations with US-based customers. We search for relevant utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11% BIBREF2 and 30% BIBREF6. Since we are not allowed to directly quote from our corpus in order to protect customer rights, we summarise the data to a total of 109 \u201cprototypical\" utterances - substantially extending the previous dataset of 35 utterances from Amanda:EthicsNLP2018 - and categorise these utterances based on the Linguistic Society's definition of sexual harassment BIBREF7:", "We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study. Following this methodology, we collected a total of 2441 system replies in July-August 2018 - 3.5 times more data than Amanda:EthicsNLP2018 - which 2 expert annotators manually annotated according to the categories in Table TABREF14 ($\\kappa =0.66$)."]}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_answer": "", "predicted_evidence": ["We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018.", "[noitemsep]", "Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them.", "Sexual Requests and Demands, e.g. \u201cWill you have sex with me?\u201d, \u201cTalk dirty to me.\u201d", "Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the \u201cdeflection\u201d strategy, such as \u201cWhy do you ask?\u201d. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. In turn, the \u201cclean\u201d seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses ( 16% of the time)."]}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_answer": "", "predicted_evidence": ["Gender and Sexuality, e.g. \u201cAre you gay?\u201d, \u201cHow do you have sex?\u201d", "Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them.", "Sexualised Comments, e.g. \u201cI love watching porn.\u201d, \u201cI'm horny.\u201d", "[noitemsep]", "Sexual Requests and Demands, e.g. \u201cWill you have sex with me?\u201d, \u201cTalk dirty to me.\u201d"]}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_answer": "", "predicted_evidence": ["The class court decision RS includes references to decisions. It does not have any subclasses, the coarsed and fine-grained versions are identical. In court decision, the name of the official decision-making collection, the volume and the numbered article are cited. Often mentioned are also the court, if necessary the decision type, date and file number. Example (UNKREF39) cites decisions of the Federal Constitutional Court (BVerfG) and the Federal Social Court (BSG). Decisions of the BVerfG are referenced with regard to pages, while decisions of the BSG are sorted according to paragraphs, numbers and marginal numbers.", "To evaluate and potentially improve the quality of the annotations, part of the dataset was annotated by a second linguist (using the annotation guidelines specifically prepared for its construction). We selected a small part that could be annotated in approx. two weeks. For the sentence extraction we paid special attention to the anonymised mentions of person, location or organization entities, because these are usually explained at their first mention. The resulting sample consisted of 2005 sentences with a broad variety of different entities (3 % of all sentences from each federal court). The agreement between the two annotators was measured using Kappa on a token basis. All class labels were taken into account in accordance with the IOB2 scheme BIBREF18. The inter-annotator agreement is 0.89, i. e., there is mostly very good agreement between the two annotators. Differences were in the identification of court decision and legal literature. Some unusual references of court decision (consisting only of decision type, court, date, file number) were not annotated such as `Urteil des Landgerichts Darmstadt vom 16.", "The dataset includes two different versions of annotations, one with a set of 19 fine-grained semantic classes and another one with a set of 7 coarse-grained classes (Table ). There are 53,632 annotated entities in total, the majority of which (74.34 %) are legal entities, the others are person, location and organization (25.66 %). Overall, the most frequent entities are law GS (34.53 %) and court decision RS (23.46 %). The other legal classes (ordinance VO, European legal norm EUN, regulation VS, contract VT, and legal literature LIT) are much less frequent (1\u20136 % each). Even less frequent (less than 1 %) are lawyer AN, street STR, landscape LDS, and brand MRK.", "The dataset was originally annotated by the first author. To evaluate and potentially improve the quality of the annotations, part of the dataset was annotated by a second linguist (using the annotation guidelines specifically prepared for its construction). We selected a small part that could be annotated in approx. two weeks. For the sentence extraction we paid special attention to the anonymised mentions of person, location or organization entities, because these are usually explained at their first mention. The resulting sample consisted of 2005 sentences with a broad variety of different entities (3 % of all sentences from each federal court). The agreement between the two annotators was measured using Kappa on a token basis. All class labels were taken into account in accordance with the IOB2 scheme BIBREF18. The inter-annotator agreement is 0.89, i. e., there is mostly very good agreement between the two annotators. Differences were in the identification of court decision and legal literature.", "Basically, legal entities are either designations or references. A designation (or name) is the title of a legal document. In law texts, the title is strictly standardised and consists of a long title, short title and an abbreviation BIBREF13. The title of the Act on the Federal Constitutional Court is: `Gesetz \u00fcber das Bundesverfassungsgericht (Bundesverfassungsgerichtsgesetz \u2013 BVerfGG)', where `Gesetz \u00fcber das Bundesverfassungsgericht' is the long title, `Bundesverfassungsgerichtsgesetz' is the short title, and `BVerfGG' is the abbreviation. A reference to a legal norm is also fixed with rules for short and full references BIBREF13. Designations or references of binding individual acts such as regulations or contracts, however, are not uniformly defined."]}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_answer": "", "predicted_evidence": ["Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set.", "As proposed by BIBREF0 , we treat words as a sequence of subword units. Then, the sequence is fed to a simple bi-LSTM network BIBREF15 , BIBREF16 and hidden states from each direction are weighted with a set of parameters which are also learned during training. Finally, the weighted vector is used as the word embedding given in Eq. 9 .", "G\u00f6zde G\u00fcl \u015eahin was a PhD student at Istanbul Technical University and a visiting research student at University of Edinburgh during this study. She was funded by T\u00fcbitak (The Scientific and Technological Research Council of Turkey) 2214-A scholarship during her visit to University of Edinburgh. She was granted access to CoNLL-09 Semantic Role Labeling Shared Task data by Linguistic Data Consortium (LDC). This work was supported by ERC H2020 Advanced Fellowship GA 742137 SEMANTAX and a Google Faculty award to Mark Steedman. We would like to thank Adam Lopez for fruitful discussions, guidance and support during the first author's visit.", "Character-level neural models are becoming the defacto standard for NLP problems due to their accessibility and ability to handle unseen data. In this work, we investigated how they compare to models with access to gold morphological analysis, on a sentence-level semantic task. We evaluated their quality on semantic role labeling in a number of agglutinative and fusional languages. Our results lead to the following conclusions:", "Formally, we generate a label sequence $\\vec{l}$ for each sentence and predicate pair: $(s,p)$ . Each $l_t\\in \\vec{l}$ is chosen from $\\mathcal {L}=\\lbrace  \\mathit {roles \\cup nonrole}\\rbrace $ , where $roles$ are language-specific semantic roles (mostly consistent with PropBank) and $nonrole$ is a symbol to present tokens that are not arguments. Given $\\theta $ as model parameters and $g_t$ as gold label for $t_{th}$ token, we find the parameters that minimize the negative log likelihood of the sequence:"]}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_answer": "", "predicted_evidence": ["To gain insights on this issue, we measure how models perform as the distance between the predicate and the argument increases. The unit of measure is number of tokens between the two; and argument is defined as the head of the argument phrase in accordance with dependency-based SRL task. For that purpose, we created bins of [0-4], [5-9], [10-14] and [15-19] distances. Then, we have calculate F1 scores for arguments in each bin. Due to low number of predicate-argument pairs in buckets, we could not analyze German and Turkish; and also the bin [15-19] is only used for Czech. Our results are shown in Fig. 3 . We observe that either char or char3 closely follows the oracle for all languages. The gap between the two does not increase with the distance, suggesting that the performance gap is not related to long range dependencies. In other words, both characters and the oracle handle long range dependencies equally well.", "$$p_{final} = f(p_0, p_1,..,p_n|\\phi )$$   (Eq. 36)", "For all experiments, we initialized weight parameters orthogonally and used one layer bi-LSTMs both for subword composition and argument labeling with hidden size of 200. Subword embedding size is chosen as 200. We used gradient clipping and early stopping to prevent overfitting. Stochastic gradient descent is used as the optimizer. The initial learning rate is set to 1 and reduced by half if scores on development set do not improve after 3 epochs. We use the provided splits and evaluate the results with the official evaluation script provided by CoNLL-09 shared task. In this work (and in most of the recent SRL works), only the scores for argument labeling are reported, which may cause confusions for the readers while comparing with older SRL studies. Most of the early SRL work report combined scores (argument labeling with predicate sense disambiguation (PSD)). However, PSD is considered a simpler task with higher F1 scores .", "One way to infer similarity is to measure diversity. Consider a set of baseline models that are not diverse, i.e., making similar errors with similar inputs. In such a case, combination of these models would not be able to overcome the biases of the learners, hence the combination would not achieve a better result. In order to test if character and morphological models are similar, we combine them and measure the performance of the ensemble. Suppose that a prediction $p_{i}$ is generated for each token by a model $m_i$ , $i \\in n$ , then the final prediction is calculated from these predictions by:", "Character-level neural models are becoming the defacto standard for NLP problems due to their accessibility and ability to handle unseen data. In this work, we investigated how they compare to models with access to gold morphological analysis, on a sentence-level semantic task. We evaluated their quality on semantic role labeling in a number of agglutinative and fusional languages. Our results lead to the following conclusions:"]}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_answer": "", "predicted_evidence": ["This assumption has two major shortcomings especially for languages with rich morphology: (1) inability to handle unseen or out-of-vocabulary (OOV) word-forms (2) inability to exploit the regularities among word parts. The limitations of word embeddings are particularly pronounced in sentence-level semantic tasks, especially in languages where word parts play a crucial role. Consider the Turkish sentences \u201cK\u00f6y+l\u00fc-ler (villagers) \u015fehr+e (to town) geldi (came)\u201d and \u201cSendika+l\u0131-lar (union members) meclis+e (to council) geldi (came)\u201d. Here the stems k\u00f6y (village) and sendika (union) function similarly in semantic terms with respect to the verb come (as the origin of the agents of the verb), where \u015fehir (town) and meclis (council) both function as the end point. These semantic similarities are determined by the common word parts shown in bold.", "Throughout this paper, our aim was to gain insights on how models perform on different languages rather than scoring the highest F1. For this reason, we used a model that can be considered small when compared to recent neural SRL models and avoided parameter search. However, we wonder how the models behave when given a larger network. To answer this question, we trained char3 and oracle models with more layers for two fusional languages (Spanish, Catalan), and two agglutinative languages (Finnish, Turkish). The results given in Table 6 clearly shows that model complexity provides relatively more benefit to morphological models. This indicates that morphological signals help to extract more complex linguistic features that have semantic clues.", "Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set.", "$$p_{final} = f(p_0, p_1,..,p_n|\\phi )$$   (Eq. 36)", "Finally, the label distribution is calculated via softmax function over the concatenated hidden states from both directions."]}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_answer": "", "predicted_evidence": ["We also estimated a notion of semantic specificity based on the concepts of a KG. For each visual feature, we aggregated the captions of the figures that most activate it and used Cogito to disambiguate the Sensigrafo concepts that appear in them. Then, we estimated how important each concept is to each feature by calculating its tf-idf. Finally, we averaged the resulting values to obtain a consolidated semantic specificity score per feature.", "We can see a marked division between the results obtained on natural images datasets (table TABREF20) and those focused on scientific figures (table TABREF21). In the former case, VSE++ and DSVE-loc clearly beat all the other approaches. In contrast, our model performs poorly on such datasets although results are ameliorated when we use pre-trained visual features from ImageNet (\"Oursvgg\" and \"Oursvgg-vec\"). Interestingly, the situation reverts with the scientific datasets. While the recall of DSVE-loc drops dramatically in SciGraph, and even more in SemScholar, our approach shows the opposite behavior in both figure and caption retrieval. Using visual features enriched with pre-trained semantic embeddings from Vecsigrafo during training of the FCC task further improves recall in the bidirectional retrieval task. Compared to natural images, the additional complexity of scientific figures and their caption texts, which in addition are considerably longer (see table TABREF19), seems to have a clear impact in this regard.", "We evaluate the language and visual representations emerging from FCC in the context of two classification tasks that aim to identify the scientific field an arbitrary text fragment (a caption) or a figure belong to, according to the SciGraph taxonomy. The latter is a particularly hard task due to the whimsical nature of the figures that appear in our corpus: figure and diagram layout is arbitrary; charts, e.g. bar and pie charts, are used to showcase data in any field from health to engineering; figures and natural images appear indistinctly, etc. Also, note that we only rely on the actual figure, not the text fragment where it is mentioned in the paper.", "A study of the complexity of figure-caption correspondence compared to classical image-sentence matching.", "For each word $w_k$, the FCC task learns a d-D embedding $\\vec{w}_k$, which can be combined with pre-trained word ($\\vec{w^{\\prime }}_k$), lemma ($\\vec{l}_k$) and concept ($\\vec{c}_k$) embeddings to produce a single vector $\\vec{t}_k$. If no pre-trained knowledge is transferred from an external source, then $\\vec{t}_k=\\vec{w}_k$. Note that we previously lemmatize and disambiguate $D$ against the KG in order to select the right pre-trained lemma and concept embeddings for each particular occurrence of $w_k$. Equation DISPLAY_FORM8 shows the different combinations of learnt and pre-trained embeddings we consider: (a) learnt word embeddings only, (b) learnt and pre-trained word embeddings and (c) learnt word embeddings and pre-trained semantic embeddings, including both lemmas and concepts, in line with our recent findings presented in BIBREF16."]}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_answer": "", "predicted_evidence": ["We focus on multiple-choice questions, 73% of the dataset. Table TABREF24 shows the performance of our model against the results reported in BIBREF23 for five TQA baselines: random, BiDAF (focused on text machine comprehension), text only ($TQA_1$, based on MemoryNet), text+image ($TQA_2$, VQA), and text+diagrams ($TQA_3$, DSDP-NET). We successfully reproduced the $TQA_1$ and $TQA_2$ architectures and adapted the latter. Then, we replaced the visual features in $TQA_2$ with those learnt by the FCC visual subnetwork both in a completely unsupervised way ($FCC_6$ in table TABREF15) and with pre-trained semantic embeddings ($FCC_7$), resulting in $TQA_4$ and $TQA_5$, respectively.", "In this paper, we provide empirical evidence of this and show that co-training text and visual features from a large corpus of scientific figures and their captions in a correspondence task (FCC) is an effective, flexible and elegant unsupervised means towards overcoming such complexity. We show how such features can be significantly improved by enriching them with additional knowledge sources and, particularly, structured KGs. We prove the benefits of our approach against supervised baselines and in different transfer learning tasks, including text and visual classification and multi-modal machine comprehension applied to question answering, with results generally beyond the state of the art. In the future, it will be interesting to further the study of the interplay between the semantic concepts explicitly represented in different KGs, contextualized embeddings e.g. from SciBERT BIBREF31, and the text and visual features learnt in the FCC task. We also plan to continue to charter the knowledge captured in such features and to pursue the optimization and practical application of our approach.", "Let $V$ be a vocabulary of words from a collection of documents $D$. Also, let $L$ be their lemmas, i.e. base forms without morphological or conjugational variations, and $C$ the concepts (or senses) in a KG. Each word $w_k$ in $V$, e.g. made, has one lemma $l_k$ (make) and may be linked to one or more concepts $c_k$ in $C$ (create or produce something).", "All the captions show a strong semantic correspondence with their associated figures. Figure FIGREF29 shows the activation heatmaps for two sample captions, calculated on the embeddings layer of the language subnetwork. The upper one corresponds to the fourth column left-right and third figure top-down in figure FIGREF28. Its caption reads: \"The Aliev-Panfilov model with $\\alpha =0.01$...The phase portrait depicts trajectories for distinct initial values $\\varphi _0$ and $r_0$...\". Below, (first column, fourth figure in figure FIGREF28): \"Relative protein levels of ubiquitin-protein conjugates in M. quadriceps...A representative immunoblot specific to ubiquitin...\". Consistently with our analysis, activation focuses on the most relevant tokens for each text feature: \"Aliev-Panfilov model\" and \"immunoblot\", respectively.", "Interestingly, it also seems to have learnt some type of is-a relations (western blot is a type of immunoblot). The second feature focuses on variations of the term radiograph, e.g. radiograph-y/s. The third feature specializes in text related to curve plots involving several statistic analysis, e.g. Real-time PCR, one-way ANOVA or Gaussian distribution. Sometimes (fourth figure from top) the caption does not mention the plot directly, but focuses on the analysis instead, e.g. \"the data presented here are mean values of duplicate experiments\", indicating transfer of knowledge from the visual part during training. The fourth feature extracts citations and models named after prominent scientists, e.g. Evans function (first and fourth figure), Manley (1992) (second), and Aliev-Panfilov model (third). The fifth feature extracts chromatography terminology, e.g. 3D surface plot, photomicrograph or color map and, finally, the right-most feature focuses on different types of named diagrams, like flow charts and state diagrams, e.g."]}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_answer": "", "predicted_evidence": ["In our experiments, concatenation proved optimal to combine the embeddings learnt by the network and the pre-trained embeddings, compared to other methods like summation, multiplication, average or learning a task-specific weighting of the different representations as in BIBREF17. Since some words may not have associated pre-trained word, lemma or concept embeddings, we pad these sequences with $\\varnothing _W$, $\\varnothing _L$ and $\\varnothing _C$, which are never included in the vocabulary. The dimensionality of $\\vec{t}_k$ is fixed to 300, i.e. the size of each sub-vector in configurations $(a)$, $(b)$ and $(c)$ is 300, 150 and 100, respectively. In doing so, we aimed at limiting the number of trainable parameters and balance the contribution of each information source.", "Vision features. The analysis was carried out on an unconstrained variety of charts, diagrams and natural images from SciGraph, without filtering by figure type or scientific field. To obtain a representative sample of what the FCC network learns, we focus on the 512-D vector resulting from the last convolutional block before the fusion subnetwork. We pick the features with the most significant activation over the whole dataset and select the figures that activate them most. To this purpose, we prioritize those with higher maximum activation against the average activation.", "The research reported in this paper is supported by the EU Horizon 2020 programme, under grants European Language Grid-825627 and Co-inform-770302.", "This architecture enables the FCC task to learn visual and text features from scratch in a completely unsupervised manner, just by observing the correspondence of figures and captions. Next, we extend it to enable the transfer of additional pre-trained information. Here, we focus on adding pre-trained embeddings on the language branch, and then back-propagate to the visual features during FCC training. Adding pre-trained visual features is also possible and indeed we also evaluate its impact in the FCC task in section SECREF14.", "We also estimated a notion of semantic specificity based on the concepts of a KG. For each visual feature, we aggregated the captions of the figures that most activate it and used Cogito to disambiguate the Sensigrafo concepts that appear in them. Then, we estimated how important each concept is to each feature by calculating its tf-idf. Finally, we averaged the resulting values to obtain a consolidated semantic specificity score per feature."]}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "predicted_answer": "", "predicted_evidence": ["It contains 4 blocks of conv+conv+pool layers, where inside each block the two convolutional layers have the same number of filters, while consecutive blocks have doubling number of filters (64, 128, 256, 512). The input layer receives 224x224x3 images. The final layer produces a 512-D vector after 28x28 max-pooling. Each convolutional layer is followed by batch normalization BIBREF14 and ReLU layers. Based on BIBREF15, the language subnetwork has 3 convolutional blocks, each with 512 filters and a 5-element window size with ReLU activation. Each convolutional layer is followed by a 5-max pooling layer, except for the final layer, which produces a 512-D vector after 35-max pooling. The language subnetwork has a 300-D embeddings layer at the input, with a maximum sequence length of 1,000 tokens. The fusion subnetwork calculates the element-wise product of the 512-D visual and text feature vectors into a single vector $r$ to produce a 2-way classification output (correspond or not).", "In its most basic form, i.e. configuration $(a)$, the FCC network has over 32M trainable parameters (28M in the language subnetwork, 4M in the vision subnetwork and 135K in the fusion subnetwork) and takes 12 hours to train on a single GPU Nvidia GeForce RTX 2080 Ti for a relatively small corpus (SN SciGraph, see section SECREF12). We used 10-fold cross validation, Adam optimization BIBREF18 with learning rate $10^{-4}$ and weight decay $10^{-5}$. The network was implemented in Keras and TensorFlow, with batch size 32. The number of positive and negative cases is balanced within the batches.", "In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific publication. To this purpose, we explore how multi-modal scientific knowledge can be learnt from the correspondence between figures and captions.", "A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar.", "Each convolutional layer is followed by batch normalization BIBREF14 and ReLU layers. Based on BIBREF15, the language subnetwork has 3 convolutional blocks, each with 512 filters and a 5-element window size with ReLU activation. Each convolutional layer is followed by a 5-max pooling layer, except for the final layer, which produces a 512-D vector after 35-max pooling. The language subnetwork has a 300-D embeddings layer at the input, with a maximum sequence length of 1,000 tokens. The fusion subnetwork calculates the element-wise product of the 512-D visual and text feature vectors into a single vector $r$ to produce a 2-way classification output (correspond or not). It has two fully connected layers, with ReLU and an intermediate feature size of 128-D. The probability of each choice is the softmax of $r$, i.e. $\\hat{y} = softmax(r) \\in \\mathbb {R}^{2}$."]}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_answer": "", "predicted_evidence": ["Flickr30K and COCO, as image-sentence matching benchmarks.", "We leverage the TQA dataset and the baselines in BIBREF23 to evaluate the features learnt by the FCC task in a multi-modal machine comprehension scenario. We study how our model, which was not originally trained for this task, performs against state of the art models specifically trained for diagram question answering and textual reading comprehension in a very challenging dataset. We also study how pre-trained semantic embeddings impact in the TQA task: first, by enriching the visual features learnt in the FCC task as shown in section SECREF6 and then by using pre-trained semantic embeddings to enrich word representations in the TQA corpus.", "Adding pre-trained knowledge at the input layer of the language subnetwork provides an additional boost, particularly with lemma and concept embeddings from Vecsigrafo ($FCC_5$). Vecsigrafo clearly outperformed HolE ($FCC_3$), which was also beaten by pre-trained fastText BIBREF24 word embeddings ($FCC_2$) trained on SemScholar.", "In our experiments, concatenation proved optimal to combine the embeddings learnt by the network and the pre-trained embeddings, compared to other methods like summation, multiplication, average or learning a task-specific weighting of the different representations as in BIBREF17. Since some words may not have associated pre-trained word, lemma or concept embeddings, we pad these sequences with $\\varnothing _W$, $\\varnothing _L$ and $\\varnothing _C$, which are never included in the vocabulary. The dimensionality of $\\vec{t}_k$ is fixed to 300, i.e. the size of each sub-vector in configurations $(a)$, $(b)$ and $(c)$ is 300, 150 and 100, respectively. In doing so, we aimed at limiting the number of trainable parameters and balance the contribution of each information source.", "For each word $w_k$, the FCC task learns a d-D embedding $\\vec{w}_k$, which can be combined with pre-trained word ($\\vec{w^{\\prime }}_k$), lemma ($\\vec{l}_k$) and concept ($\\vec{c}_k$) embeddings to produce a single vector $\\vec{t}_k$. If no pre-trained knowledge is transferred from an external source, then $\\vec{t}_k=\\vec{w}_k$. Note that we previously lemmatize and disambiguate $D$ against the KG in order to select the right pre-trained lemma and concept embeddings for each particular occurrence of $w_k$."]}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_answer": "", "predicted_evidence": ["All the captions show a strong semantic correspondence with their associated figures. Figure FIGREF29 shows the activation heatmaps for two sample captions, calculated on the embeddings layer of the language subnetwork. The upper one corresponds to the fourth column left-right and third figure top-down in figure FIGREF28. Its caption reads: \"The Aliev-Panfilov model with $\\alpha =0.01$...The phase portrait depicts trajectories for distinct initial values $\\varphi _0$ and $r_0$...\". Below, (first column, fourth figure in figure FIGREF28): \"Relative protein levels of ubiquitin-protein conjugates in M. quadriceps...A representative immunoblot specific to ubiquitin...\". Consistently with our analysis, activation focuses on the most relevant tokens for each text feature: \"Aliev-Panfilov model\" and \"immunoblot\", respectively.", "Vision features. The analysis was carried out on an unconstrained variety of charts, diagrams and natural images from SciGraph, without filtering by figure type or scientific field. To obtain a representative sample of what the FCC network learns, we focus on the 512-D vector resulting from the last convolutional block before the fusion subnetwork. We pick the features with the most significant activation over the whole dataset and select the figures that activate them most. To this purpose, we prioritize those with higher maximum activation against the average activation.", "There is a wealth of knowledge in scientific literature and only a fraction of it is text. However, understanding scientific figures is a challenging task for machines, which is beyond their ability to process natural images. In this paper, we provide empirical evidence of this and show that co-training text and visual features from a large corpus of scientific figures and their captions in a correspondence task (FCC) is an effective, flexible and elegant unsupervised means towards overcoming such complexity. We show how such features can be significantly improved by enriching them with additional knowledge sources and, particularly, structured KGs. We prove the benefits of our approach against supervised baselines and in different transfer learning tasks, including text and visual classification and multi-modal machine comprehension applied to question answering, with results generally beyond the state of the art. In the future, it will be interesting to further the study of the interplay between the semantic concepts explicitly represented in different KGs, contextualized embeddings e.g.", "In this paper, we provide empirical evidence of this and show that co-training text and visual features from a large corpus of scientific figures and their captions in a correspondence task (FCC) is an effective, flexible and elegant unsupervised means towards overcoming such complexity. We show how such features can be significantly improved by enriching them with additional knowledge sources and, particularly, structured KGs. We prove the benefits of our approach against supervised baselines and in different transfer learning tasks, including text and visual classification and multi-modal machine comprehension applied to question answering, with results generally beyond the state of the art. In the future, it will be interesting to further the study of the interplay between the semantic concepts explicitly represented in different KGs, contextualized embeddings e.g. from SciBERT BIBREF31, and the text and visual features learnt in the FCC task. We also plan to continue to charter the knowledge captured in such features and to pursue the optimization and practical application of our approach.", "For each word $w_k$, the FCC task learns a d-D embedding $\\vec{w}_k$, which can be combined with pre-trained word ($\\vec{w^{\\prime }}_k$), lemma ($\\vec{l}_k$) and concept ($\\vec{c}_k$) embeddings to produce a single vector $\\vec{t}_k$. If no pre-trained knowledge is transferred from an external source, then $\\vec{t}_k=\\vec{w}_k$. Note that we previously lemmatize and disambiguate $D$ against the KG in order to select the right pre-trained lemma and concept embeddings for each particular occurrence of $w_k$. Equation DISPLAY_FORM8 shows the different combinations of learnt and pre-trained embeddings we consider: (a) learnt word embeddings only, (b) learnt and pre-trained word embeddings and (c) learnt word embeddings and pre-trained semantic embeddings, including both lemmas and concepts, in line with our recent findings presented in BIBREF16."]}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_answer": "", "predicted_evidence": ["To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.", "For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data. We used exponential decay of ratio INLINEFORM0 and early stopping on the validation when there was no improvement after 1000 steps. Our code is available for download on GitHub .", "In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.", "On the other hand, our model also offers us very interesting insights on how the learning is performed, since we can inspect the attention weights that the neural network is assigning to each specific token when predicting the emotion intensity. By visualizing these weights we can have a clear notion about the parts of the sentence that the model considers are more important. As Figure FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset, where positive words are generally identified. However, we also see some examples where the lack of semantic information about the input words, specially for hashtags or user mentions, makes the model unable to identify some of these the most salient words to predict emotion intensity. Several pre-processing techniques can be implemented to alleviate this problem, which we intend to explore in the future.", "We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet."]}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "predicted_answer": "", "predicted_evidence": ["For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data. We used exponential decay of ratio INLINEFORM0 and early stopping on the validation when there was no improvement after 1000 steps. Our code is available for download on GitHub .", "In this paper we also only report results for LSTMs, which outperformed regular RNNs as well as GRUs and a batch normalized version of the LSTM in on preliminary experiments. The hidden size of the attentional component is set to match the size of the augmented hidden vectors on each case. Given this setting, we explored different hyper-parameter configurations, including context window sizes of 1, 3 and 5 as well as RNN hidden state sizes of 100, 200 and 300. We experimented with unidirectional and bidirectional versions of the RNNs.", "Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features.", "To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.", "To validate the usefulness of our binary features, we performed an ablation experiment and trained our best models for each corpus without them. Table TABREF15 summarizes our results in terms of Pearson correlation on the development portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of words missing in GloVe. In this sense, we think the usage of additional features, such as the ones derived from emotion or sentiment lexicons could indeed boost our model capabilities. This is proposed for future work."]}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": "", "predicted_evidence": ["In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.", "To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of INLINEFORM0 and INLINEFORM1 . We also added a weighed L2 regularization term to our loss function. We experimented with different values for weight INLINEFORM2 , with a minimum value of 0.01 and a maximum of 0.2.", "We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet.", "To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.", "Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features."]}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_answer": "", "predicted_evidence": ["Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect\u2014emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated.", "Where INLINEFORM0 can be regarded as a context window of ordered word embedding vectors around position INLINEFORM1 , with a total size of INLINEFORM2 . To further complement the context-aware token representations, we concatenate each hidden vector to a vector of binary features INLINEFORM3 , extracted from each tweet token, defining an augmented hidden state INLINEFORM4 . Finally, we combine our INLINEFORM5 augmented hidden states, compressing them into a single vector, using a global intra-sentence attentional component in a fashion similar to vinyalsgrammar2015. Formally, DISPLAYFORM0", "Where INLINEFORM0 is the vector that compresses the input sentence INLINEFORM1 , focusing on the relevant parts to estimate emotion intensity. We input this compressed sentence representation into a feed-forward neural network, INLINEFORM2 , where INLINEFORM3 is the final predicted emotion intensity. As a loss function we use the mini-batch negative Pearson correlation with the gold-standard.", "The task is specially challenging since tweets contain informal language, spelling errors and text referring to external content. Given the 140 character limit of tweets, it is also possible to find some phenomena such as the intensive usage of emoticons and of other special Twitter features, such as hashtags and usernames mentions \u2014used to call or notify other users. In this paper we describe our system designed for the WASSA-2017 Shared Task on Emotion Intensity, which we tackle based on the premise of representation learning without the usage of external information, such as lexicons. In particular, we use a Bi-LSTM model with intra-sentence attention on top of word embeddings to generate a tweet representation that is suitable for emotion intensity. Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external information sources.", "In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the Weka baseline for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors."]}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_answer": "", "predicted_evidence": ["Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect\u2014emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated.", "While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many for a sentence or passage. However, it is often useful for applications to know the degree to which an emotion is expressed in text. To this end, the WASSA-2017 Shared Task on Emotion Intensity BIBREF0 represents the first task where systems have to automatically determine the intensity of emotions in tweets. Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one.", "While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount of sadness.", "To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.", "Where INLINEFORM0 can be regarded as a context window of ordered word embedding vectors around position INLINEFORM1 , with a total size of INLINEFORM2 . To further complement the context-aware token representations, we concatenate each hidden vector to a vector of binary features INLINEFORM3 , extracted from each tweet token, defining an augmented hidden state INLINEFORM4 . Finally, we combine our INLINEFORM5 augmented hidden states, compressing them into a single vector, using a global intra-sentence attentional component in a fashion similar to vinyalsgrammar2015. Formally, DISPLAYFORM0"]}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "predicted_answer": "", "predicted_evidence": ["Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of micro-F1 and for most of the macro-F1 scores. This shows the power of the deep learning approach.", "We test different models, namely single task single language (STSL), single task multilingual (STML), and multitask multilingual models (MTML) on our dataset. In multilingual settings, we tested Babylon multilingual word embeddings BIBREF10 and MUSE BIBREF30 on the different tasks. We use Babylon embeddings since they appear to outperform MUSE on our data.", "After annotating the pilot dataset, we noticed common misconceptions regarding race, ethnicity, and nationality, therefore we merged these attributes into one label origin. Then, we asked the annotators to determine whether the tweet insults or discriminates against people based on their (1) origin, (2) religious affiliation, (3) gender, (4) sexual orientation, (5) special needs or (6) other. Table TABREF20 shows there are fewer tweets targeting disability in Arabic compared to English and French and no tweets insulting people based on their sexual orientation which may be due to the fact that the labels of gender, gender identity, and sexual orientation use almost the same wording. On the other hand, French contains a small number of tweets targeting people based on their gender in comparison to English and Arabic. We have observed significant differences in terms of target attributes in the three languages. More data may help us examine the problems affecting targets of different linguistic backgrounds.", "In this paper, we presented a multilingual hate speech dataset of English, French, and Arabic tweets. We analyzed in details the difficulties related to the collection and annotation of this dataset. We performed multilingual and multitask learning on our corpora and showed that deep learning models perform better than traditional BOW-based models in most of the multilabel classification tasks. Multilingual multitask learning also helped tasks where each label had less annotated data associated with it. Better tuned deep learning settings in our multilingual and multitask models would be expected to outperform the existing state-of-the-art embeddings and algorithms applied to our data. The different annotation labels and comparable corpora would help us perform transfer learning and investigate how multimodal information on the tweets, additional unlabeled data, label transformation, and label information sharing may boost the classification performance in the future.", "We claim that the choice of a suitable emotion representation model is key to this sub-task, given the subjective nature and social ground of the annotator's sentiment analysis. After collecting the annotation results of the pilot dataset regarding how people feel about the tweets, and observing the added categories, we adopted a range of sentiments that are in the negative and neutral scales of the hourglass of emotions introduced by BIBREF29. This model includes sentiments that are connected to objectively assessed natural language opinions, and excludes what is known as self-conscious or moral emotions such as shame and guilt. Our labels include shock, sadness, disgust, anger, fear, confusion in case of ambivalence, and indifference. This is the second multilabel task of our model."]}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_answer": "", "predicted_evidence": ["Sluice networks BIBREF8 learn the weights of the neural networks sharing parameters (sluices) jointly with the rest of the model and share an embedding layer, Babylon embeddings in our case, that associates the elements of an input sequence. We use a standard 1-layer BiLSTM partitioned into two subspaces, a shared subspace and a private one, forced to be orthogonal through a regularization penalty term in the loss function in order to enable the multitask network to learn both task-specific and shared representations. The hidden layer has a dimension of 200, the learning rate is initially set to 0.1 with a learning rate decay, and we use the DyNet BIBREF31 automatic minibatch function to speed-up the computation. We initialize the cross-stitch unit to imbalanced, set the standard deviation of the Gaussian noise to 2, and use simple stochastic gradient descent (SGD) as the optimizer.", "Non-English hate speech datasets include Italian, German, Dutch, and Arabic corpora. BIBREF6 present a dataset of Italian tweets, in which the annotations capture the degree of intensity of offensive and aggressive tweets, in addition to whether the tweets are ironic and contain stereotypes or not. BIBREF2 have collected more than 500 German tweets against refugees, and annotated them as hateful and not hateful. BIBREF23 detect bullies and victims among youngsters in Dutch comments on AskFM, and classify cyberbullying comments as insults or threats. Moreover, BIBREF5 provide a corpus of Arabic sectarian speech.", "For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech.", "For deep learning based models, we run bidirectional LSTM (biLSTM) models with one hidden layer on each of the classification tasks. Deeper BiLSTM models performed poorly due to the size of the tweets. We chose to use Sluice networks BIBREF8 since they are suitable for loosely related tasks such as the annotated aspects of our corpora.", "To fully exploit the collected annotations, we tested multitask learning on our dataset. Multitask learning BIBREF7 allows neural networks to share parameters with one another and, thus, learn from related tasks. It has been used in different NLP tasks such as parsing BIBREF9, dependency parsing BIBREF26, neural machine translation BIBREF27, sentiment analysis BIBREF28, and other tasks. Multitask learning architectures tackle challenges that include sharing the label space and the question of private and shared space for loosely related tasks BIBREF8, for which techniques may involve a massive space of potential parameter sharing architectures."]}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_answer": "", "predicted_evidence": ["We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task. Since in natural language processing, there is a peculiar interest in multitask learning, where different tasks can be used to help each other BIBREF7, BIBREF8, BIBREF9, we use a unified model to handle the annotated data in all three languages and five tasks. We adopt BIBREF8 as a learning algorithm adapted to loosely related tasks such as our five annotated aspects and, use the Babylon cross-lingual embeddings BIBREF10 to align the three languages. We compare the multilingual multitask learning settings with monolingual multitask, multilingual single-task, and monolingual single-task learning settings respectively. Then, we report the performance results of the different settings and discuss how each task affects the remaining ones. We release our dataset and code to the community to extend research work on multilingual hate speech detection and classification.", "Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments.", "Given the subjectivity and the complexity of such data, annotation schemes have rarely been made fine-grained. Table TABREF10 compares different labelsets that exist in the literature. For instance, BIBREF12 use racist, sexist, and normal as labels; BIBREF13 label their data as hateful, offensive (but not hateful), and neither, while BIBREF16 present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. BIBREF15 label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal. On the other hand, BIBREF20 have chosen to detect ideologies of hate speech counting 40 different hate ideologies among 13 extremist hate groups.", "We determined 16 common target groups tagged by the annotators after the first annotation step. The annotators had to decide on whether the tweet is aimed at women, people of African descent, Hispanics, gay people, Asians, Arabs, immigrants in general, refugees; people of different religious affiliations such as Hindu, Christian, Jewish people, and Muslims; or from political ideologies socialists, and others. We also provided the annotators with a category to cover hate directed towards one individual, which cannot be generalized. In case the tweet targets more than one group of people, the annotators should choose the group which would be the most affected by it according to them. Table TABREF10 shows the counts of the five categories out of 16 that commonly occur in the three languages. In fact, most of the tweets target individuals or fall into the \u201cother\u201d category. In the latter case, they may target people with different political views such as liberals or conservatives in English and French, or specific ethnic groups such as Kurdish people in Arabic.", "Moreover, although people of various linguistic backgrounds are exposed to hate speech BIBREF3, BIBREF2, English is still at the center of existing work on toxic language analysis. Recently, some research studies have been conducted on languages such as German BIBREF4, Arabic BIBREF5, and Italian BIBREF6. However, such studies usually use monolingual corpora and do not contrast, or examine the correlations between online hate speech in different languages. On the other hand, tasks involving more than one language such as the hatEval task, which covers English and Spanish, include only separate classification tasks, namely (a) women and immigrants as target groups, (b) individual or generic hate and, (c) aggressive or non-aggressive hate speech."]}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "predicted_answer": "", "predicted_evidence": ["One more challenge that the annotators and ourselves had to tackle, consisted of Arabic diglossia and switching between different Arabic dialects and Modern Standard Arabic (MSA). While MSA represents the standardized and literary variety of Arabic, there are several Arabic dialects spoken in North Africa and the Middle East in use on Twitter. Therefore, we searched for derogatory terms adapted to different circumstances, and acquired an Arabic corpus that combines tweets written in MSA and Arabic dialects. For instance, the tweet shown in Figure FIGREF5 contains a dialectal slur that means \u201cmaiden.\u201d", "For deep learning based models, we run bidirectional LSTM (biLSTM) models with one hidden layer on each of the classification tasks. Deeper BiLSTM models performed poorly due to the size of the tweets. We chose to use Sluice networks BIBREF8 since they are suitable for loosely related tasks such as the annotated aspects of our corpora.", "With the expanding amount of text data generated on different social media platforms, current filters are insufficient to prevent the spread of hate speech. Most internet users involved in a study conducted by the Pew Research Center report having been subjected to offensive name calling online or witnessed someone being physically threatened or harassed online. Additionally, Amnesty International within Element AI have lately reported that many women politicians and journalists are assaulted every 30 seconds on Twitter. This is despite the Twitter policy condemning the promotion of violence against people on the basis of race, ethnicity, national origin, sexual orientation, gender identity, religious affiliation, age, disability, or serious disease. Hate speech may not represent the general opinion, yet it promotes the dehumanization of people who are typically from minority groups BIBREF0, BIBREF1 and can incite hate crime BIBREF2.", "In order to inspect which tasks hurt or help one another, we trained multilingual models for pairwise tasks such as (group, target), (hostility, annotator's sentiment), (hostility, target), (hostility, group), (annotator's sentiment, target) and (annotator's sentiment, group). We noticed that when trained jointly, the target attribute slightly improves the performance of the tweet's hostility type classification by 0.03,0.05 and 0.01 better than the best reported scores in English, French, and Arabic, respectively. When target groups and attributes are trained jointly, the macro F-score of the target group classification in Arabic improves by 0.25 and when we train the tweet's hostility type within the annotator's sentiment, we improve the macro F-score of Arabic by 0.02. We believe that we can take advantage of the correlations between target attributes and groups along with other tasks, to set logic rules and develop better multilingual and multitask settings.", "We report and discuss the results of five classification tasks: (1) the directness of the speech, (2) the hostility type of the tweet, (3) the discriminating target attribute, (4) the target group, and (5) the annotator's sentiment."]}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_answer": "", "predicted_evidence": ["On the other hand, there is also a syntactic disambiguation problem which as yet lacks good solutions. For instance, the English language contains irregular verbs like \u201cset\u201d or \u201cput\u201d. Depending on the structure of a sentence, it is not possible to recognize their verbal tense, e.g., present or past tense. Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue.", "Besides C. Shi et al BIBREF11 , Ar\u010dan and Buitelaar BIBREF19 presented an approach to translate domain-specific expressions represented by English KBs in order to make the knowledge accessible for other languages. They claimed that KBs are mostly in English, therefore they cannot contribute to the problem of MT to other languages. Thus, they translated two KBs belonging to medical and financial domains, along with the English Wikipedia, to German. Once translated, the KBs were used as external resources in the translation of German-English. The results were quite appealing and the further research into this area should be undertaken. Recently, Moussallem et al BIBREF20 created THOTH, an approach which translates and enriches knowledge graphs across languages. Their approach relies on two different recurrent neural network models along with knowledge graph embeddings. The authors applied their approach on the German DBpedia with the German translation of the English DBpedia on two tasks: fact checking and entity linking.", "According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms.", "Translating KBs. According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al.", "SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation."]}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_answer": "", "predicted_evidence": ["Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue. For instance, the sentence \u201cAnna usually put her notebook on the table for studying\" may be annotated using a certain vocabulary and represented by triples. Thus, the verb \u201cput\", which is represented by a predicate that groups essential information about the verbal tense, may support the generation step of a given MT system. This sentence usually fails when translated to rich morphological languages, such as Brazilian-Portuguese and Arabic, for which the verb influences the translation of \u201cusually\" to the past tense.", "", "Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popovi\u0107 BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult.", "SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation.", "This work was supported by the German Federal Ministry of Transport and Digital Infrastructure (BMVI) in the projects LIMBO (no. 19F2029I) and OPAL (no. 19F2028A) as well as by the Brazilian National Council for Scientific and Technological Development (CNPq) (no. 206971/2014-1)"]}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_answer": "", "predicted_evidence": ["One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words.", "Non-standard speech. The non-standard language problem is a rather important one in the MT field. Many people use the colloquial form to speak and write to each other on social networks. Thus, when MT systems are applied on this context, the input text frequently contains slang, MWE, and unreasonable abbreviations such as \u201cIdr = I don't remember.\u201d and \u201ccya = see you\u201d. Additionally, idioms contribute to this problem, decreasing the translation quality. Idioms often have an entirely different meaning than their separated word meanings. Consequently, most translation outputs of such expressions contain errors. For a good translation, the MT system needs to recognize such slang and try to map it to the target language. Some SMT systems like Google or Bing have recognition patterns over non-standard speech from old translations through the Web using SMT approaches.", "Therefore, we suggest recognizing such entities before the translation process and first linking them to a reference knowledge base. Afterwards, the type of entities would be agglutinated along with their labels and their translations from a reference knowledge base. For instance, in NMT, the idea is to include in the training set for the aforementioned word \u201cKiwi\", \u201cKiwi.animal.link, Kiwi.person.link, Kiwi.food.link\" then finally to align them with the translations in the target text. For example, in SMT, the additional information can be included by XML or by an additional model. In contrast, in NMT, this additional information can be used as parameters in the training phase. This method would also contribute to OOV mistakes regarding names. This idea is supported by BIBREF11 where the authors encoded the types of entities along with the words to improve the translation of sentences between Chinese-English. Recently, Moussallem et al.", "The challenges above are clearly not independent, which means that addressing one of them can have an impact on the others. Since NMT has shown impressive results on reordering, the main problem turns out to be the disambiguation process (both syntactically and semantically) in SMT approaches BIBREF0 .", "For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms. Therefore, we suggest investigating an MT approach mainly based on SWT using NN for translating KBs."]}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_answer": "", "predicted_evidence": ["The recorded speech signal was sampled at 16KHz frequency. We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. The MFCC features were also sampled at 100Hz same as the sampling frequency of EEG features to avoid seq2seq problem.", "We used Brain Vision EEG recording hardware. Our EEG cap had 32 wet EEG electrodes including one electrode as ground as shown in Figure 1. We used EEGLab BIBREF17 to obtain the EEG sensor location mapping. It is based on standard 10-20 EEG sensor placement method for 32 electrodes.", "For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively.", "For data set B, the 8 subjects were asked to repeat the same previous experiment but this time we used background music played from our lab computer to generate a background noise of 65 dB. Here we had 24 speech EEG recording examples for each sentence.", "When we used the EEG features for ASR without dimension reduction, the ASR performance went down by 40 %. The non linear dimension reduction of EEG features significantly improved the performance of ASR."]}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_answer": "", "predicted_evidence": ["RNN encoder - decoder ASR model consists of a RNN encoder and a RNN decoder with attention mechanism BIBREF12 , BIBREF13 , BIBREF14 . The number of time steps of the encoder is equal to the product of sampling frequency of the input features and the length of input sequence. There is no fixed value for time steps in our case. We used dynamic RNN cell. We used a single layer GRU with 128 hidden units for both encoder and decoder. A dense layer followed by softmax activation is used after the decoder GRU to get the prediction probabilities. Dense layer performs an affine transformation. The number of time steps of the decoder GRU is same as the number of words present in the sentence for a given training example. Training objective is to maximize the log probability of the ordered conditionals, ie: INLINEFORM0 , where X is input feature vector, INLINEFORM1 's are the labels for the ordered words present in that training example and INLINEFORM2 is the length of the output label sentence for that example.", "An end-to-end ASR model maps input feature vectors to an output sequence of vectors of posterior probabilities of tokens without using separate acoustic model, pronunciation model and language model. In this work we implemented two different types of state of art end to end ASR models used for the task of continuous speech recognition and the input feature vectors can be EEG features or concatenation of acoustic and EEG features. We used Google's tensorflow and keras deep learning libraries for building our ASR models.", "In our work we used character based CTC ASR model. CTC assumes the conditional independence constraint that output predictions are independent given the entire input sequence.", "Electroencephalography (EEG) is a non invasive way of measuring electrical activity of human brain. In BIBREF0 we demonstrated deep learning based automatic speech recognition (ASR) using EEG signals for a limited English vocabulary of four words and five vowels. In this paper we extend our work for a much larger English vocabulary and we use state-of-art end-to-end continuous speech recognition models to perform recognition. In our prior work we predicted isolated words and vowels.", "We further plan to publish our speech EEG data base used in this work to help advancement of research in this area."]}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": "", "predicted_evidence": ["RNN encoder - decoder ASR model consists of a RNN encoder and a RNN decoder with attention mechanism BIBREF12 , BIBREF13 , BIBREF14 . The number of time steps of the encoder is equal to the product of sampling frequency of the input features and the length of input sequence. There is no fixed value for time steps in our case. We used dynamic RNN cell. We used a single layer GRU with 128 hidden units for both encoder and decoder. A dense layer followed by softmax activation is used after the decoder GRU to get the prediction probabilities. Dense layer performs an affine transformation. The number of time steps of the decoder GRU is same as the number of words present in the sentence for a given training example.", "We now explain the loss function used in our CTC model. Consider training data set INLINEFORM0 with training examples INLINEFORM1 and the corresponding label set INLINEFORM2 with target vectors INLINEFORM3 . Consider any training example, label pair ( INLINEFORM4 , INLINEFORM5 ). Let the number of time steps of the RNN encoder for ( INLINEFORM6 , INLINEFORM7 ) is INLINEFORM8 . In case of character based CTC model, the RNN predicts a character at every time step. Whereas in word based CTC model, the RNN predicts a word at every time step. For the sake of simplicity, let us assume that length of target vector INLINEFORM9 is equal to INLINEFORM10 . Let the probability vector output by the RNN at each time step INLINEFORM11 be INLINEFORM12 and let INLINEFORM13 value of INLINEFORM14 be denoted by INLINEFORM15 .", "In our work we used character based CTC ASR model. CTC assumes the conditional independence constraint that output predictions are independent given the entire input sequence.", "INLINEFORM0 can be intuitively seen as a measure of how much attention INLINEFORM1 must pay to INLINEFORM2 , INLINEFORM3 . INLINEFORM4 is mathematically defined as INLINEFORM5 , where INLINEFORM6 is hidden state of the decoder GRU at time step INLINEFORM7 .", "For data set A, the 10 subjects were asked to speak the first 30 sentences from the USC-TIMIT database BIBREF16 and their simultaneous speech and EEG signals were recorded. This data was recorded in presence of background noise of 40 dB (noise generated by room air conditioner fan). We then asked each subject to repeat the same experiment two more times, thus we had 30 speech EEG recording examples for each sentence."]}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_answer": "", "predicted_evidence": ["Knowledge distillation (KD) has been widely used to transfer knowledge from a large teacher model to a smaller student model. In other words, the student model mimics the behavior of the teacher model by minimize the knowledge distillation loss functions. Various types of knowledge distillation can be employed at different sub-layers. Generally, all types of knowledge distillation can be modeled as minimizing the following loss function:", "The overall pipeline of LadaBERT (Lightweight Adaptation of BERT) is illustrated in Figure FIGREF8. As shown in the figure, the pre-trained BERT model (e.g., BERT-Base) is served as the teacher as well as the initial status of the student model. Then, the student model is compressed towards smaller parameter size through a hybrid model compression framework in an iterative manner until the target compression ratio is reached. Concretely, in each iteration, the parameter size of student model is first reduced by $1-\\Delta $ based on weight pruning and matrix factorization, and then the parameters are fine-tuned by the loss function of knowledge distillation. The motivation behind is that matrix factorization and weight pruning are complementary with each other. Matrix factorization calculates the optimal approximation under a certain rank, while weight pruning introduces additional sparsity to the decomposed matrices.", "To combine the benefits of weight pruning with matrix factorization, we leverage a hybrid approach that applies weight pruning on the basis of decomposed matrices generated by SVD. Following Equation (DISPLAY_FORM12), SVD-based matrix factorization for any weight matrix ${W}$ can be written as: ${W}_{svd}={A}_{m\\times r}{B}_{n\\times r}^T$. Then, weight pruning is applied on the decomposed matrices ${A} \\in \\mathbb {R}^{m \\times r}$ and ${B} \\in \\mathbb {R}^{n \\times r}$ separately. The weight matrix after hybrid compression is denoted by:", "TinyBERT BIBREF3 instantiates a tiny student model, which has totally 14.5M parameters ($7.5\\times $ compression ratio) composed of 4 layers, 312 hidden units, 1200 intermediate size and 12 heads. For a fair comparison, we reproduce the TinyBERT pipeline without general distillation and data augmentation, which is time-exhaustive and resource-consuming.", "BERT-SMALL has the same model architecture as TinyBERT, but is directly pre-trained by the official BERT pipeline. The performance values are inherited from BIBREF3 for reference."]}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_answer": "", "predicted_evidence": ["The goal of matrix factorization is to decompose a matrix into the product of two matrices in lower dimensions, and Singular Value Decomposition (SVD) is a popular way of matrix factorization that generalizes the eigendecomposition of a square normal matrix to a $m \\times n$ matrix. It has been proved that SVD is the best approximation of a matrix given the rank $r$ under Frobenius norm BIBREF12. Matrix factorization was widely studied in the deep learning domain for model compression and acceleration BIBREF13, BIBREF14, BIBREF15. Sainath et al BIBREF13 explored a low-rank matrix factorization method of DNN layers for acoustic modeling. Xu et al. BIBREF14, BIBREF15 applied singular value decomposition to deep neural network acoustic models and achieved comparable performances with state-of-the-art models through much fewer parameters.", "This approach does not generate sparse connectivity patterns and brings much larger acceleration ratio with existing BLAS libraries for dense matrix multiplications. Ye et al. BIBREF8 argued that small weights are in fact important for preserving the performance of a model, and Hu et al. BIBREF6 alleviated this problem by a data-driven approach that pruned zero-activation neurons iteratively based on intermediate feature maps. Zhu and Gupta BIBREF7 empirically compared large-sparse models with smaller dense models of similar parameter sizes and found that large sparse models performed better consistently. In addition, sparsity-induced models BIBREF9, BIBREF10, BIBREF11 can be regarded as similar methods as pruning. For example, Wen et al. BIBREF9 applied group lasso as a regularizer at training time, and Louizos et al. BIBREF10 learned sparse neural networks through $l_0$ regularization.", "In this paper, we demonstrate that a combination of matrix factorization and weight pruning is better than single solutions for BERT-oriented model compression. Similar phenomena has been reported in the computer vision scenarios BIBREF28, which shows that low-rank and sparsity are complementary to each other. Here we provide another explanation to support this observation.", "To further demonstrate the efficiency of LadaBERT, we visualize the learning curves on MNLI-m and QQP datasets in Figure FIGREF42 and FIGREF42, where LadaBERT-3 is compared to the strongest baseline, TinyBERT, under $7.5 \\times $ compression ratio. As shown in the figures, LadaBERT-3 achieves good performances much faster and results in a better convergence point. After training $2 \\times 10^4$ steps (batches) on MNLI-m dataset, the performance of LadaBERT-3 is already comparable to TinyBERT after convergence (approximately $2 \\times 10^5$ steps), achieving nearly $10 \\times $ acceleration. And on QQP dataset, both performance improvement and training speed acceleration is very significant. This clearly shows the superiority of combining matrix factorization, weight pruning and knowledge distillation in a reinforce manner.", "As the memory and latency constraints vary in different scenarios, the pre-trained BERT model should be adaptive to different requirements with accuracy retained to the largest extent. Existing BERT-oriented model compression solutions largely depend on knowledge distillation BIBREF1, which is inefficient and resource-consuming because a large training corpus is required to learn the behaviors of a teacher. For example, DistilBERT BIBREF2 is re-trained on the same corpus as pre-training a vanilla BERT from scratch; and TinyBERT BIBREF3 utilizes expensive data augmentation to fit the distillation target. The costs of these model compression methods are as large as pre-training and unaffordable for low-resource settings. Therefore, it is straight-forward to ask, can we design a lightweight method to generate adaptive models with comparable accuracy using significantly less time and resource consumption? In this paper, we propose LadaBERT (Lightweight adaptation of BERT through hybrid model compression) to tackle the raised questions. Specifically, LadaBERT is based on an iterative hybrid model compression framework consisting of weighting pruning, matrix factorization and knowledge distillation."]}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "predicted_answer": "", "predicted_evidence": ["Therefore, for a target model compression ratio $P_{svd}$, the desired rank $r$ can be calculated by:", "Weight pruning BIBREF4 is an unstructured compression method that induces desirable sparsity for a neural network model. For a neural network $f({x; \\theta })$ with parameters $\\theta $, weight pruning finds a binary mask ${M} \\in \\lbrace 0, 1\\rbrace ^{|\\theta |}$ subject to a given sparsity ratio, $P_{weight}$. The neural network after pruning will be $f({x; M \\cdot \\theta })$, where the non-zero parameter size is $||{M}||_1 = P_{weight}\\cdot |\\theta |$, where $|\\theta |$ is the number of parameters in $\\theta $. For example, when $P_m = 0.3$, there are 70% zeros and 30% ones in the mask ${m}$. We adopt a simple pruning strategy in our implementation: the binary mask is generated by setting the smallest weights to zeros BIBREF36.", "As shown in the figure, the pre-trained BERT model (e.g., BERT-Base) is served as the teacher as well as the initial status of the student model. Then, the student model is compressed towards smaller parameter size through a hybrid model compression framework in an iterative manner until the target compression ratio is reached. Concretely, in each iteration, the parameter size of student model is first reduced by $1-\\Delta $ based on weight pruning and matrix factorization, and then the parameters are fine-tuned by the loss function of knowledge distillation. The motivation behind is that matrix factorization and weight pruning are complementary with each other. Matrix factorization calculates the optimal approximation under a certain rank, while weight pruning introduces additional sparsity to the decomposed matrices. Moreover, weight pruning and matrix factorization generates better initial and intermediate status of the student model, which improve the efficiency and effectiveness of knowledge distillation. In the following subsections, we will introduce the algorithms in detail.", "where ${M_A}$ and ${M_B}$ are binary masks derived by the weight pruning algorithm with compression ratio $P_{weight}$. The compression ratio of this hybrid approach can be calculated by:", "The goal of matrix factorization is to decompose a matrix into the product of two matrices in lower dimensions, and Singular Value Decomposition (SVD) is a popular way of matrix factorization that generalizes the eigendecomposition of a square normal matrix to a $m \\times n$ matrix. It has been proved that SVD is the best approximation of a matrix given the rank $r$ under Frobenius norm BIBREF12. Matrix factorization was widely studied in the deep learning domain for model compression and acceleration BIBREF13, BIBREF14, BIBREF15. Sainath et al BIBREF13 explored a low-rank matrix factorization method of DNN layers for acoustic modeling. Xu et al. BIBREF14, BIBREF15 applied singular value decomposition to deep neural network acoustic models and achieved comparable performances with state-of-the-art models through much fewer parameters. GroupReduce BIBREF16 focused on the compression of neural language models and applied low-rank matrix approximation to vocabulary-partition."]}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_answer": "", "predicted_evidence": ["The role of the reader is to derive the meaning representation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-over-time pooling operation BIBREF16 , BIBREF17 , BIBREF18 . Next, we build representations for documents using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the acquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below.", "At the document level, a recurrent neural network composes a sequence of sentence vectors into a document vector. Note that this is a somewhat simplistic attempt at capturing document organization at the level of sentence to sentence transitions. One might view the hidden states of the recurrent neural network as a list of partial representations with each focusing mostly on the corresponding input sentence given the previous context. These representations altogether constitute the document representation, which captures local and global sentential information with minimum compression.", "The nn-se outperforms the lead and lreg baselines with a significant margin, while performing slightly better than the ilp model. This is an encouraging result since our model has only access to embedding features obtained from raw text. In comparison, lreg uses a set of manually selected features, while the ilp system takes advantage of syntactic information and extracts summaries subject to well-engineered linguistic constraints, which are not available to our models. Overall, our sentence extraction model achieves performance comparable to the state of the art without sophisticated constraint optimization (ilp, tgraph) or sentence ranking mechanisms (urank). We visualize the sentence weights of the nn-se model in the top half of Figure 4 . As can be seen, the model is able to locate text portions which contribute most to the overall meaning of the document.", "Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length BIBREF0 , the words in the title, the presence of proper nouns, content features such as word frequency BIBREF1 , and event features such as action nouns BIBREF2 . Sentences are typically assigned a score indicating the strength of presence of these features. Several methods have been used in order to select the summary sentences ranging from binary classifiers BIBREF3 , to hidden Markov models BIBREF4 , graph-based algorithms BIBREF5 , BIBREF6 , and integer linear programming BIBREF7 .", "To create the training data for sentence extraction, we reverse approximated the gold standard label of each document sentence given the summary based on their semantic correspondence BIBREF7 . Specifically, we designed a rule-based system that determines whether a document sentence matches a highlight and should be labeled with 1 (must be in the summary), and 0 otherwise. The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, the number of entities appearing in the highlight and in the document sentence. We adjusted the weights of the rules on 9,000 documents with manual sentence labels created by woodsend2010automatic. The method obtained an accuracy of 85% when evaluated on a held-out set of 216 documents coming from the same dataset and was subsequently used to label 200K documents. Approximately 30% of the sentences in each document were deemed summary-worthy."]}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_answer": "", "predicted_evidence": ["The nn-se outperforms the lead and lreg baselines with a significant margin, while performing slightly better than the ilp model. This is an encouraging result since our model has only access to embedding features obtained from raw text. In comparison, lreg uses a set of manually selected features, while the ilp system takes advantage of syntactic information and extracts summaries subject to well-engineered linguistic constraints, which are not available to our models. Overall, our sentence extraction model achieves performance comparable to the state of the art without sophisticated constraint optimization (ilp, tgraph) or sentence ranking mechanisms (urank). We visualize the sentence weights of the nn-se model in the top half of Figure 4 . As can be seen, the model is able to locate text portions which contribute most to the overall meaning of the document.", "The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document.", "$$\\bar{\\mathbf {h}}_{t} = \\text{LSTM} ( p_{t-1} \\mathbf {s}_{t-1}, \\mathbf {\\bar{h}}_{t-1})$$   (Eq. 20)", "Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length BIBREF0 , the words in the title, the presence of proper nouns, content features such as word frequency BIBREF1 , and event features such as action nouns BIBREF2 . Sentences are typically assigned a score indicating the strength of presence of these features. Several methods have been used in order to select the summary sentences ranging from binary classifiers BIBREF3 , to hidden Markov models BIBREF4 , graph-based algorithms BIBREF5 , BIBREF6 , and integer linear programming BIBREF7 .", "rush2015neural propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article. In contrast, our model summarizes documents rather than individual sentences, producing multi-sentential discourse. A major architectural difference is that our decoder selects output symbols from the document of interest rather than the entire vocabulary. This effectively helps us sidestep the difficulty of searching for the next output symbol under a large vocabulary, with low-frequency words and named entities whose representations can be challenging to learn. Gu:ea:16 and gulcehre2016pointing propose a similar \u201ccopy\u201d mechanism in sentence compression and other tasks; their model can accommodate both generation and extraction by selecting which sub-sequences in the input sequence to copy in the output."]}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "predicted_answer": "", "predicted_evidence": ["where MLP is a multi-layer neural network with as input the concatenation of $\\mathbf {\\bar{h}}_t$ and $\\mathbf {h}_t$ . $p_{t-1}$ represents the degree to which the extractor believes the previous sentence should be extracted and memorized ( $p_{t-1}$ =1 if the system is certain; 0 otherwise).", "The results of our human evaluation study are shown in Table 2 . Specifically, we show, proportionally, how often our participants ranked each system 1st, 2nd, and so on. Perhaps unsurprisingly, the human-written descriptions were considered best and ranked 1st 27% of the time, however closely followed by our nn-se model which was ranked 1st 22% of the time. The ilp system was mostly ranked in 2nd place (38% of the time). The rest of the systems occupied lower ranks. We further converted the ranks to ratings on a scale of 1 to 6 (assigning ratings 6 $\\dots $ 1 to rank placements 1 $\\dots $ 6). This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable effect of system type. Specifically, post-hoc Tukey tests showed that nn-se and ilp are significantly ( $p < 0.01$ ) better than lead, nn-we, and nn-abs but do not differ significantly from each other or the human goldstandard.", "In this work we presented a data-driven summarization framework based on an encoder-extractor architecture. We developed two classes of models based on sentence and word extraction. Our models can be trained on large scale datasets and learn informativeness features based on continuous representations without recourse to linguistic annotations. Two important ideas behind our work are the creation of hierarchical neural structures that reflect the nature of the summarization task and generation by extraction. The later effectively enables us to sidestep the difficulties of generating under a large vocabulary, essentially covering the entire dataset, with many low-frequency words and named entities.", "$$\\bar{\\mathbf {h}}_{t} = \\text{LSTM} ( \\mathbf {w^{\\prime }}_{t-1},\n\\mathbf {\\bar{h}}_{t-1})\\footnote {We empirically found that feeding\nthe previous sentence-level attention vector as additional\ninput to the LSTM would lead to small performance improvements.\nThis is not shown in the equation.}$$   (Eq. 25)", "In the above equations, $\\mathbf {w}_i$ corresponds to the vector of the $i$ -th word in the input document, whereas $\\mathbf {z}$ , $\\mathbf {W}_e$ , $\\mathbf {W}_r$ , $\\mathbf {v}$ , $\\mathbf {W}_{e^{\\prime }}$ , and $\\mathbf {W}_{r^{\\prime }}$ are model weights. The model architecture is shown in Figure 3 ."]}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_answer": "", "predicted_evidence": ["w_2$ , the loss for join and meet learning can be written as the following: $\n& d_c(w_1,w_2,w_c) = \\left\\Vert  \\max (0,w_1 \\vee w_2-w_c) \\right\\Vert ^2 \\\\\n& d_p(w_1,w_2,w_p) = \\left\\Vert  \\max (0,w_p - w_1 \\wedge w_2) \\right\\Vert ^2 \\\\\n& {\\small L_\\text{join} = \\sum _{w_1,w_2,w_c}\\max (0, m+d_c(w_1,w_2,w_c))}\\\\\n& {\\small L_\\text{meet} = \\sum _{w_1,w_2,w_p}\\max (0, m+d_p(w_1,w_2,w_p))}\\\\\n& L = L_\\text{join} + L_\\text{meet}\n$", "In this work we presented two extensions to the order embedding model. The first incorporates unstructured text to improve performance on Is-A relations, while the second uses long-range constraints automatically derived from the ontology to provide the model with more useful global supervision. In future work we would like to explore embedding models for structured prediction that automatically incorporate additional forms of reasoning such as negation, joint learning of ontological and other commonsense relations, and the application of improved training methods to new models for ontology prediction such as Poincar\u00e9 embeddings.", "In both sets of experiments we train all models using the Adam optimizer BIBREF15 , using embeddings of dimension 50, with all hyperparameters tuned on a development set. When embedding multi-word phrases, we represent them as the average of the constituent word embeddings.", "In this experiment, we use the same dataset as BIBREF0 , created by taking 40,00 edges from the 838,073-edge transitive closure of the WordNet hierarchy for the dev set, 4,000 for the test set, and training on the rest of the transitive closure. We additionally add the long-range join and meet constraints (3,028,302 and 4,006 respectively) between different concepts and see that the inclusion of this additional supervision results in further improvement over the baseline order embedding model, as seen in Table 3.", "We can add many additional training examples to our data by enforcing that the vector join and meet operations satisfy the joins and meets found in the training lattice/DAG. If $w_c$ and $w_p$ are the nearest common child and parent for a pair $w_1, w_2$ , the loss for join and meet learning can be written as the following: $\n& d_c(w_1,w_2,w_c) = \\left\\Vert  \\max (0,w_1 \\vee w_2-w_c) \\right\\Vert ^2 \\\\\n& d_p(w_1,w_2,w_p) = \\left\\Vert  \\max (0,w_p - w_1 \\wedge w_2) \\right\\Vert ^2 \\\\\n& {\\small L_\\text{join} = \\sum _{w_1,w_2,w_c}\\max (0,"]}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_answer": "", "predicted_evidence": ["WordNet is a knowledge base (KB) of single words and relations between them such as hypernymy and meronymy. For our task, we use the hypernym relations only. ConceptNet is a KB of triples consisting of a left term $t_1$ , a relation $R$ , and a right term $t_2$ . The relations come from a fixed set of size 34. But unlike WordNet, terms in ConceptNet can be phrases. We focus on the Is-A relation in this work. MCG also consists of hierarchical relations between multi-word phrases, ranging from extremely general to specific. Examples from each dataset are shown in Table 1 .", "In this work we presented two extensions to the order embedding model. The first incorporates unstructured text to improve performance on Is-A relations, while the second uses long-range constraints automatically derived from the ontology to provide the model with more useful global supervision. In future work we would like to explore embedding models for structured prediction that automatically incorporate additional forms of reasoning such as negation, joint learning of ontological and other commonsense relations, and the application of improved training methods to new models for ontology prediction such as Poincar\u00e9 embeddings.", "The second extension uses the complex partial-order structure of real-world ontologies to find long-distance triplet constraints among embeddings which are poorly enforced by the standard pairwise training method. By adding our additional triplet constraints to the baseline order-embedding model, we find performance improves from 90.6 to 91.3 accuracy on the WordNet ontology dataset.", "We focus on the order-embedding model BIBREF0 which was proposed for general hierarchical prediction including multimodal problems such as image captioning. While the original work included results on ontology prediction on WordNet, we focus exclusively on the model's application to commonsense knowledge, with its unique characteristics including complex ordering structure, compositional, multi-word entities, and the wealth of commonsense knowledge to be found in large-scale unstructured text data.", "We find that order embeddings' ease of extension, both by incorporating non-ordered data, and additional training constraints derived from the structure of the problem, makes it a promising avenue for the development of further algorithms for automatic learning and jointly consistent prediction of ontologies."]}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "predicted_answer": "", "predicted_evidence": ["Model performance during train is presented in Table 5 . While all the models outperformed the baselines, not all of them did so with a significant margin due to the robustness of the baselines selected. The ones found to be significantly better than the baselines were models IIb (Domain-specific) and IIc (Twitter-only) (permutation test, $n = 10^5$ both $p < 0.05$ ). The difference in precision between model IIb and IIc points out to the former making the wrong predictions for news articles. These errors are most likely in selecting the wrong supporting segment. Moreover, even though models IIa-c only produce negative labels, they still achieve improved performance over the state-of-the-art systems, highlighting the highly skewed nature of the training dataset.", "We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation.", "The LORELEI program provides a framework for developing and testing systems for real-time humanitarian crises response in the context of low-resource languages. The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames \u2014 data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames.", "Social media has received a lot of attention as a way to understand what people communicate during disasters BIBREF16 , BIBREF11 . These communications typically center around collective sense-making BIBREF17 , supportive actions BIBREF18 , BIBREF19 , and social sharing of emotions and empathetic concerns for affected individuals BIBREF20 . To organize and make sense of the sentiment information found in social media, particularly those messages sent during the disaster, several works propose the use of machine learning models (e.g., Support Vector Machines, Naive Bayes, and Neural Networks) trained on a multitude of linguistic features. These features include bag of words, part-of-speech tags, n-grams, and word embeddings; as well as previously validated sentiment lexica such as Linguistic Inquiry and Word Count (LIWC) BIBREF22 , AFINN BIBREF23 , and SentiWordNet BIBREF24 .", "Neural networks have been shown to capture specific task related subtleties which can complement the manually constructed sentiment lexica described in the previous subsection. For this work, we learn sentiment representations using a bilateral Long Short-Term Memory model BIBREF41 trained on the Stanford Sentiment Treebank BIBREF42 . This model was selected because it provided a good trade off between simplicity and performance on a fine-grained sentiment task, and has been shown to achieve competitive results to the state-of-the-art BIBREF43 ."]}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_answer": "", "predicted_evidence": ["To organize and make sense of the sentiment information found in social media, particularly those messages sent during the disaster, several works propose the use of machine learning models (e.g., Support Vector Machines, Naive Bayes, and Neural Networks) trained on a multitude of linguistic features. These features include bag of words, part-of-speech tags, n-grams, and word embeddings; as well as previously validated sentiment lexica such as Linguistic Inquiry and Word Count (LIWC) BIBREF22 , AFINN BIBREF23 , and SentiWordNet BIBREF24 . Most of the work is centered around identifying messages expressing sentiment towards a particular situation as a way to distinguish crisis-related posts from irrelevant information BIBREF25 . Either in a binary fashion (positive vs. negative) (e.g., BIBREF25 ) or over fine-grained emotional classes (e.g., BIBREF16 ).", "To identify sentiment targeted towards an entity, we use the recently released Target-Based Sentiment Analysis (TBSA) model from BIBREF45 . In TBSA, two stacked LSTM cells are trained to predict both sentiment and target boundary tags (e.g., predicting S-POS to indicate the start of the target towards which the author is expressing positive sentiment, I-POS and E-POS to indicate intermediate and end of the target). In our submission, since input text documents can be arbitrarily long, we only consider sentences which include a known and relevant entity; these segments are then fed to the TBSA model to predict targeted sentiment. If the target predicted by this model matched with any of the known entities, the system would output the polarity and the target.", "The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames \u2014 data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames. While the task of identifying Situation Frames is similar to existing tasks in literature (e.g., slot filling), it is defined by the very limited availability of data BIBREF15 . This lack of data requires the use of simpler but more robust models and the utilization of transfer learning or data augmentation techniques.", "Initial experiments showed that our main source of error was not being able to correctly identify the supporting segment. Even if polarity, source and target were correctly identified, missing the correct segment was considered an error, and thus lowered our models' precision. To address this, we decided to use a model which only produced results for tweets given that these only contain one segment, making the segment identification sub-task trivial.", "The Sentiment, Emotion, and Cognitive State (SEC) evaluation task was a recent addition to the LORELEI program introduced in 2019, which aims to leverage sentiment information from the incoming documents. This in turn may be used in identifying severity of the crisis in different geographic locations for efficient distribution of the available resources. In this work, we describe our systems for targeted sentiment detection for the SEC task. Our systems are designed to identify authored expressions of sentiment and emotion towards a HADR crisis. To this end, our models are based on a combination of state-of-the-art sentiment classifiers and simple rule-based systems. We evaluate our systems as part of the NIST LoREHLT 2019 SEC pilot task."]}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_answer": "", "predicted_evidence": ["To identify sentiment targeted towards an entity, we use the recently released Target-Based Sentiment Analysis (TBSA) model from BIBREF45 . In TBSA, two stacked LSTM cells are trained to predict both sentiment and target boundary tags (e.g., predicting S-POS to indicate the start of the target towards which the author is expressing positive sentiment, I-POS and E-POS to indicate intermediate and end of the target). In our submission, since input text documents can be arbitrarily long, we only consider sentences which include a known and relevant entity; these segments are then fed to the TBSA model to predict targeted sentiment. If the target predicted by this model matched with any of the known entities, the system would output the polarity and the target.", "Given a dataset of text documents and manually annotated situation frames, the task is to automatically detect sentiment polarity relevant to existing frames and identify the source and target for each sentiment instance. The source is defined as a person or a group of people expressing the sentiment, and can be either a PER/ORG/GPE (person, organization or geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the frame or an entity in the document.", "Situation awareness information is encoded into situation frames in the LORELEI program BIBREF35 . Situation Frames (SF) are similar in nature to those used in Natural Language Understanding (NLU) systems: in essence they are data structures that record information corresponding to a single incident at a single location BIBREF15 . A SF frame includes a situation Type taken from a fixed inventory of 11 categories (e.g., medical need, shelter, infrastructure), Location where the situation exists (if a location is mentioned) and additional variables highlighting the Status of the situation (e.g., entities involved in resolution, time and urgency). An example of a SF can be found in table 1 . A list of situation frames and documents serve as input for our sentiment analysis systems.", "We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation.", "Understanding the expressed sentiment from an affected population during the on-set of a crisis is a particularly difficult task, especially in low-resource scenarios. There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information."]}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_answer": "", "predicted_evidence": ["Table 6 present the official evaluation results for English and Spanish. Some information is missing since at the time of submission only partial score had been made public. As previously mentioned, the pre-trained state-of-the-art models (model I) were directly applied to the evaluation data without any adaptation. These performed reasonably well for the English data. Among the submissions of the SEC Task pilot, our systems outperformed the other competitors for both languages.", "Understanding the expressed sentiment from an affected population during the on-set of a crisis is a particularly difficult task, especially in low-resource scenarios. There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information. Our results show that state-of-the-art sentiment classifiers can be leveraged out-of-the-box for a reasonable performance on English data.", "In this model we limit our focus on the task of correctly identifying those segments with sentiment towards a SF. That is, given a pair of SF and segment, we train models to identify if this segment contains any sentiment towards that SF. This allows us to expand our dataset from 123 documents into one with $\\sum _d |SF_d| \\times |d|$ number of samples, where $|d|$ is the length of the document (i.e., number of segments) and $|SF_d|$ is the number of SF annotations for document $d$ . Summary of the training dataset after augmentation is given in Table 3 .", "Automatic translations from Spanish to English were obtained from Microsoft Bing using their publicly available API. For the pilot evaluation, we translated all of the Spanish documents into English, and included them as additional training data. At this time we do not translate English to Spanish, but plan to explore this thread in future work.", "Opinion mining and sentiment analysis techniques offer a viable way of addressing these needs, with complementary insights to what keyword searches or topic and event extraction might offer BIBREF8 . Studies have shown that sentiment analysis of social media during crises can be useful to support response coordination BIBREF9 or provide information about which audiences might be affected by emerging risk events BIBREF10 . For example, identifying tweets labeled as \u201cfear\u201d might support responders on assessing mental health effects among the affected population BIBREF11 . Given the critical and global nature of the HADR events, tools must process information quickly, from a variety of sources and languages, making it easily accessible to first responders and decision makers for damage assessment and to launch relief efforts accordingly BIBREF12 , BIBREF13 . However, research efforts in these tasks are primarily focused on high resource languages such as English, even though such crises may happen anywhere in the world."]}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "predicted_answer": "", "predicted_evidence": ["For this task, the node features consist of the representations learned during node classification in Section SECREF24 These representations are extracted by feeding the features representing node position, size and shape to the graph neural network, which in both cases uses the GraphSAGE architecture BIBREF28, and recording the output of the final softmax activation. Compared to a one-hot encoding, representing node identity using a probability distribution from a softmax activation reduces the sparsity of the feature vector. This yields a 5-dimensional feature vector for each node.", "Table TABREF29 provides a baseline for graph classification from a dummy classifier, as well as results for random forest (RF) and support vector machine (SVM) classifiers trained on 850 and tested on 150 diagrams. The macro F1 scores show that the RF classifier with 100 decision trees offers competitive performance for all target classes and both AI2D and AI2D-RST, in some cases outperforming graph neural networks. It should be noted, however, that the RF classifier is trained with node features learned using GraphSAGE.", "Finally, there are many aspects of diagrammatic representation that were not explored in this study. To begin with, a comparison of representations for discourse structures using the question-answering set accompanying AI2D would be particularly interesting, especially if both AI2D and AI2D-RST graphs were enriched with features from state of the art semantic representations for natural language and graphic elements.", "In light of the results for graph classification, one should note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes, which ensures that features are propagated to neighbouring nodes, the crowd-sourced AI2D graphs contain both isolated nodes and subgraphs. To what extent these disconnections affect the performance for AI2D warrant a separate study. Additionally, more advanced techniques than mere averaging, such as pooling, should be explored in future work.", "Overall, the results nevertheless suggest that simple layout features can provide a foundation for representing diagrammatic structures, which use the layout space to organise the content and set up discourse relations between different elements. To what extent these layout features can support the prediction of actual discourse relations should be explored in future research."]}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "predicted_answer": "", "predicted_evidence": ["Generally, most architectures do not benefit from combining the grouping and connectivity graphs in AI2D-RST. This is an interesting finding, as many diagram types differ in terms of their connectivity structures (e.g. cycles and networks) BIBREF11. The edges introduced from the connectivity graph naturally increase the flow of information in the graph, but this does not seem to help learn distinctive features between diagram types. On the other hand, it should be noted that the nodes are not typed, that is, the model cannot distinguish between edges from the grouping and connectivity graphs.", "The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema.", "AI2D and AI2D-RST share most node types that represent different diagram elements, namely text, graphics, arrows and the image constant, which is a node that stands for the entire diagram. In AI2D, generic diagram elements such as titles describing the entire diagram are typically connected to the image constant. In AI2D-RST, the image constant acts as the root node of the tree in the grouping graph. In addition to text, graphics, arrows and the image constant, AI2D-RST features two additional node types for groups and discourse relations, whereas AI2D includes an additional node for arrowheads. To summarise, AI2D contains five distinct node types, whereas AI2D-RST has six. Note, however, that only grouping and connectivity graphs used in this study, which limits the number to five for AI2D-RST.", "This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4", "Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24."]}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "predicted_answer": "", "predicted_evidence": ["The purpose of the node classification task is to evaluate how well algorithms learn to classify the parts of a diagram using the graph-based representations in AI2D and AI2D-RST and node features representing the position, size and shape of the element, as described in Section SECREF11 Identifying the correct node type is a key step when populating a graph with candidate nodes from object detectors, particularly if the nodes will be processed further, for instance, to extract semantic representations from CNN features or word embeddings. Furthermore, the node representations learned during this task can be used as node features for graph classification, as will be shown shortly below in Section SECREF26", "Overall, the results nevertheless suggest that simple layout features can provide a foundation for representing diagrammatic structures, which use the layout space to organise the content and set up discourse relations between different elements. To what extent these layout features can support the prediction of actual discourse relations should be explored in future research.", "The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema.", "AI2D-RST models discourse relations using nodes, which have a 25-dimensional, one-hot encoded feature vector to represent the type of discourse relation, which are drawn from Rhetorical Structure Theory BIBREF21. In AI2D, the discourse relations derived from engelhardt2002 are represented using a 10-dimensional one-hot encoded vector, which is associated with edges connecting diagram elements participating in the relation. Because the two resources draw on different theories and represent discourse relations differently, I use the grouping and connectivity graph for AI2D-RST representations and ignore the edge features in AI2D, as these descriptions attempt to describe roughly the same multimodal structures. A comparison of discourse relations is left for a follow-up study focusing on representing the discourse structure of diagrams.", "In this article, I compared graph-based representations of diagrams representing primary school science topics from two datasets that contain the same diagrams, which have been annotated by either crowd-sourced workers or trained experts. The comparison involved two tasks, graph and node classification, using four different architectures for graph neural networks, which were compared to baselines from dummy, random forest and support vector machine classifiers."]}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "predicted_answer": "", "predicted_evidence": ["The results for graph classification using graph neural networks presented in Table TABREF27 show certain differences between AI2D and AI2D-RST. When classifying diagrams into the original semantic categories defined in AI2D ($N = 17$), the AI2D graphs significantly outperform AI2D-RST when using the GraphSAGE architecture. For all other graph neural networks, the differences between AI2D and AI2D-RST are not statistically significant. This is not surprising as the AI2D graphs were tailored for the original classes, yet the AI2D-RST graphs seem to capture generic properties that help to classify diagrams into semantic categories nearly as accurately as AI2D graphs designed specifically for this purpose, although no semantic features apart from the layout structure are provided to the classifier.", "I evaluated the following graph neural network architectures for both graph and node classification tasks:", "This task compares the performance of graph-based representations in AI2D and AI2D-RST for classifying entire diagrams. Here the aim is to evaluate to what extent graph neural networks can learn about the generic structure of primary school science diagrams from the graph-based representations in AI2D and AI2D-RST. Correctly identifying what the diagram attempts to communicate and how carries implications for tasks such as visual question answering, as the type of a diagram constrains the interpretation of key diagrammatic elements, such as the meaning of lines and arrows BIBREF1, BIBREF17.", "The grouping graph, which is initially populated by diagram elements from the AI2D layout segmentation, provides a foundation for describing connectivity and discourse structure by adding nodes to the grouping graph that stand for groups of diagram elements, as shown in the upper part of Figure FIGREF1. In addition, the grouping graph includes annotations for 11 different diagram types identified in the data (e.g. cycles, cross-sections and networks), which may be used as target labels during training, as explained in Section SECREF26 The coarse and fine-grained diagram types identified in the data are shown in Figure FIGREF8.", "This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks."]}
{"question_id": "fff1ed2435ba622d884ecde377ff2de127167638", "predicted_answer": "", "predicted_evidence": ["I evaluated the following graph neural network architectures for both graph and node classification tasks:", "Overall, the results nevertheless suggest that simple layout features can provide a foundation for representing diagrammatic structures, which use the layout space to organise the content and set up discourse relations between different elements. To what extent these layout features can support the prediction of actual discourse relations should be explored in future research.", "I implemented all graph neural networks using Deep Graph Library 0.4 BIBREF29 on the PyTorch 1.3 backend BIBREF30. For GCN, GAT and SAGE, each network consists of two of the aforementioned layers with a Rectified Linear Unit (ReLU) activation, followed by a dense layer and a final softmax function for predicting class membership probabilities. For SGC, the network consists of a single SGC layer without an activation function. The implementations for each network are available in the repository associated with this article.", "This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks.", "The grouping graph, which is initially populated by diagram elements from the AI2D layout segmentation, provides a foundation for describing connectivity and discourse structure by adding nodes to the grouping graph that stand for groups of diagram elements, as shown in the upper part of Figure FIGREF1. In addition, the grouping graph includes annotations for 11 different diagram types identified in the data (e.g. cycles, cross-sections and networks), which may be used as target labels during training, as explained in Section SECREF26 The coarse and fine-grained diagram types identified in the data are shown in Figure FIGREF8."]}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "predicted_answer": "", "predicted_evidence": ["Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance.", "Table TABREF13 shows the relative performance of the neural models compared to previous controversy detection methods, evaluated on the Clueweb09 derived dataset of BIBREF6 and trained on the Wikipedia data from the same time frame. The TILE-Clique matching model outperforms all other models on Precision although this difference is not significant compared to the neural approaches. Similarly, the language model trained on the DBPedia dataset outperforms other models on Recall but shows no significant difference compared to the CNN model. Notably, the neural approaches show comparable results to the TILE-Clique model in terms of F1, demonstrating a balanced performance in terms of Precision and Recall. Furthermore, the CNN model shows a significant improvement compared to the other non neural baselines in terms of the AUC value (p < 0.05).", "Controversy is expected to change over time. Some issues become controversial, others cease to be so. To investigate robustness of controversy detection models with respect to changes over time, we evaluate model performance in two variants: trained and tested on 2018, or trained on the 2009 Wikipedia data and tested on the 2018 Wikipedia data. Table 3 shows the results for each of the text-based detection models.", "To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique model from BIBREF1 and the sentiment analysis baseline (using the state-of-the-art Polyglot library for python) from BIBREF6 in the comparison with previous work.", "For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 ."]}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "predicted_answer": "", "predicted_evidence": ["Controversy detection is an increasingly important task. Controversial content can signal the need for moderation on social platforms, either to prevent conflict between users or limit the spread of misinformation. More generally, controversies provide insight into societies BIBREF0 . Often, the controversial content is outside the direct control of a platform on which it is shared, mentioned or discussed. This raises the requirement of generally applicable methods to gauge controversial content on the web for moderation purposes. Unfortunately, what is controversial changes, and may lie more in the way topics are discussed rather than what is discussed, making it difficult to detect controversies in a robust fashion. We take the task of controversy detection and evaluate robustness of different methodologies with respect to the varying nature of controversies.", "Table TABREF13 shows the relative performance of the neural models compared to previous controversy detection methods, evaluated on the Clueweb09 derived dataset of BIBREF6 and trained on the Wikipedia data from the same time frame. The TILE-Clique matching model outperforms all other models on Precision although this difference is not significant compared to the neural approaches. Similarly, the language model trained on the DBPedia dataset outperforms other models on Recall but shows no significant difference compared to the CNN model. Notably, the neural approaches show comparable results to the TILE-Clique model in terms of F1, demonstrating a balanced performance in terms of Precision and Recall. Furthermore, the CNN model shows a significant improvement compared to the other non neural baselines in terms of the AUC value (p < 0.05).", "A proven approach in modelling text with neural networks is to use Recurrent Neural Networks (RNNs) which enjoy weight sharing capabilities to model words irrespective of their sequence location. A specific type, the Hierarchical Attention Network (HAN) proposed by BIBREF10 makes use of attention to build document representations in a hierarchical manner. It uses bi-directional Gated Recurrent Units (GRUs) BIBREF12 to selectively update representations of both words and sentences. This allows the network to both capture the hierarchy from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference.", "Within year, the hierarchical attention model (HAN) outperforms all other models on Recall, F1 and AUC, losing Precision to the CNN and SVM models. However, our main interest is the robustness when a model is trained on a different year (2009) than the test set (2018). These between year experiments show a superior score for the HAN model compared to the non-neural models on Recall, and show significant improvements on F1 (p < 0.05) and AUC (p < 0.05), losing only to the SVM model on Precision (non significantly). In terms of robustness, we can also take the percentage change between the within year and between year experiment into account (were smaller absolute changes are preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural models to illustrate the overall increase in robustness by neural approaches.", "To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique model from BIBREF1 and the sentiment analysis baseline (using the state-of-the-art Polyglot library for python) from BIBREF6 in the comparison with previous work."]}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "predicted_answer": "", "predicted_evidence": ["To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique model from BIBREF1 and the sentiment analysis baseline (using the state-of-the-art Polyglot library for python) from BIBREF6 in the comparison with previous work.", "We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset.", "To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics.", "Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks.", "We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts."]}
{"question_id": "6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "predicted_answer": "", "predicted_evidence": ["Controversy detection is an increasingly important task. Controversial content can signal the need for moderation on social platforms, either to prevent conflict between users or limit the spread of misinformation. More generally, controversies provide insight into societies BIBREF0 . Often, the controversial content is outside the direct control of a platform on which it is shared, mentioned or discussed. This raises the requirement of generally applicable methods to gauge controversial content on the web for moderation purposes. Unfortunately, what is controversial changes, and may lie more in the way topics are discussed rather than what is discussed, making it difficult to detect controversies in a robust fashion. We take the task of controversy detection and evaluate robustness of different methodologies with respect to the varying nature of controversies.", "Prior work on detecting controversies has taken three kinds of approaches: 1) lexical approaches, which seek to detect controversies through signal terms, either through bag-of-word classifiers, lexicons, or lexicon based language models BIBREF1 . 2) explicit modeling of controversy through platform-specific features, often in Wikipedia or social-media settings. Features such as mutual reverts BIBREF2 , user-provided flags BIBREF3 , interaction networks BIBREF4 or stance-distributions BIBREF5 have been used as platform-specific indicators of controversies. The downside of these approaches is the lack of generalizability due to their platform-specific nature. 3) matching models that combine lexical and explicit modelling approaches by looking at lexical similarities between a given text and a set of texts in a domain that provides explicit features BIBREF1 , BIBREF6 , BIBREF7 .", "Controversy is expected to change over time. Some issues become controversial, others cease to be so. To investigate robustness of controversy detection models with respect to changes over time, we evaluate model performance in two variants: trained and tested on 2018, or trained on the 2009 Wikipedia data and tested on the 2018 Wikipedia data. Table 3 shows the results for each of the text-based detection models.", "Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific features and reducing sensitivity to an exact lexical representation is paramount to robust controversy detection.", "First, we have demonstrated that neural methods perform as state-of-the-art tools in controversy detection on the ClueWeb09 BIBREF0 based testset, even beating matching models. Second, we investigated temporal stability, and demonstrated neural -and especially CNN- robustness in terms of Recall, F1 and AUC performance and stability with train and test sets that are 9 years apart. Thirdly, we show that CNN and HAN models outperform the SVM and LM baselines on Precision, F1 and AUC when tested on held-out-topics. Fourthly, we show that neural methods are better able to generalize from Wikipedia pages to unseen general web pages in terms of Precision, F1 and AUC. Lastly, neural methods seem better in line with human annotators with regard to certainty and disagreement."]}
{"question_id": "60e6296ca2a697892bd67558a21a83ef01a38177", "predicted_answer": "", "predicted_evidence": ["Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time. We analysed the performance of language model, SVM, CNN and HAN models on different tasks.", "Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific features and reducing sensitivity to an exact lexical representation is paramount to robust controversy detection.", "Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance.", "Within year, the hierarchical attention model (HAN) outperforms all other models on Recall, F1 and AUC, losing Precision to the CNN and SVM models. However, our main interest is the robustness when a model is trained on a different year (2009) than the test set (2018). These between year experiments show a superior score for the HAN model compared to the non-neural models on Recall, and show significant improvements on F1 (p < 0.05) and AUC (p < 0.05), losing only to the SVM model on Precision (non significantly). In terms of robustness, we can also take the percentage change between the within year and between year experiment into account (were smaller absolute changes are preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics.", "Certainty is the distance of human annotations to the midpoint of the four-point controversy scale, i.e. a score between 0 and 2.5 that expresses how sure annotators are of document (non)controversy. Here, the HAN shows errors most strongly negatively correlated to the certainty of annotators. Finally, annotators disagree on the controversy of some documents, expressed as the standard deviation of their controversy annotations. Again, the HAN model seems preferable, as it's errors are most strongly correlated to annotator disagreement. Overall, the neural methods have less biased performance in relation to (non)controversial documents, correlate more strongly with the certainty of human annotators and are susceptible to errors in similar conditions as when annotators disagree."]}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "predicted_answer": "", "predicted_evidence": ["Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances.", "Our work is distinguishable from previous work with respect to three dimensions:", "In addition to concatenating the word and visual embedding, we explore two variants of our model that allow for a finer-grained integration of the two modalities:", "For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table .", "There are several options for combining the text and video modalities. We opt for the simplest strategy, which concatenates the representations. For a word embedding INLINEFORM0 and corresponding visual representation INLINEFORM1 , the input to our RNNLM will be the concatenated vector INLINEFORM2 . For the examples where we were unable to compute visual features (see Section \u00a7 SECREF3 ), we set INLINEFORM3 to be a zero-vector."]}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "predicted_answer": "", "predicted_evidence": ["Our work is distinguishable from previous work with respect to three dimensions:", "For incorporating additional modalities, the NLP community has typically used datasets such as MS COCO BIBREF1 and Flickr BIBREF2 for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started investigating language grounding in images BIBREF8 , BIBREF9 , BIBREF10 and to lesser extent in videos BIBREF11 , BIBREF1 . However, language grounding has focused more on obtaining better word and sentence representations or other downstream tasks, and to lesser extent on language modeling.", "For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames: INLINEFORM4", "In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1", "In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs."]}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 are learned matrices.", "A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1", "There are several options for combining the text and video modalities. We opt for the simplest strategy, which concatenates the representations. For a word embedding INLINEFORM0 and corresponding visual representation INLINEFORM1 , the input to our RNNLM will be the concatenated vector INLINEFORM2 . For the examples where we were unable to compute visual features (see Section \u00a7 SECREF3 ), we set INLINEFORM3 to be a zero-vector.", "For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames: INLINEFORM4", "In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs."]}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "predicted_answer": "", "predicted_evidence": ["Thirdly, there is an idea of integrating neural rewriting architectures into the larger systems for automated reasoning. This can be motivated by the interesting contrast between some simpler ILP systems suffering for combinatorial explosion in presence of a large number of examples and neural methods which definitely benefit form large data sets.", "Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact structure of the expression, which in case of formal logic is always unambiguous and easy to extract. Perhaps this way the model could learn more efficiently from examples (and achieve higher results even on the small AIM data sets). The authors have a positive experience of applying TreeNNs to learn remainders of arithmetical expressions modulo small natural numbers \u2013 TreeNNs outperformed here neural models based on LSTM cells, giving almost perfect accuracy.", "Then experiments on more challenging but also much larger data sets for polynomial normalization were performed. Depending on the difficulty of the data, accuracy on the test sets achieved in our experiments varied between $70\\%$ and $99\\%$. The results in terms of accuracy are shown in Table TABREF13.", "Our work is also an inquiry into the capabilities of NNs as such, in the spirit of works like BIBREF7.", "An examination of the examples wrongly rewritten by the model was done. It turns out that the wrong outputs almost always parse (in $97 - 99 \\%$ of cases they are legal polynomial terms). Notably, depending on the difficulty of the data set, as much as $18 - 64 \\%$ of incorrect outputs are wrong only with respect to the constants in the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming."]}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "predicted_answer": "", "predicted_evidence": ["Thirdly, there is an idea of integrating neural rewriting architectures into the larger systems for automated reasoning. This can be motivated by the interesting contrast between some simpler ILP systems suffering for combinatorial explosion in presence of a large number of examples and neural methods which definitely benefit form large data sets.", "Piotrowski was supported by the grant of National Science Center, Poland, no. 2018/29/N/ST6/02903, and by the European Agency COST action CA15123. Urban and Brown were supported by the ERC Consolidator grant no. 649043 AI4REASON and by the Czech project AI&Reasoning CZ.02.1.01/0.0/0.0/15_003/0000466 and the European Regional Development Fund. Kaliszyk was supported by ERC Starting grant no. 714034 SMART.", "We hope that this work will inspire and trigger a discussion on the above (and other) ideas.", "We also run an experiment on the joint set of all rewrite rules (consisting of 41396 examples). Here the task was more difficult as a model needed not only to apply rewriting correctly, but also choose \u201cthe right\u201d rewrite rule applicable for a given term. Nevertheless, the performance was also very good, reaching $83\\%$ of accuracy.", "In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting? In particular: Can NMT models learn symbolic rewriting?"]}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "predicted_answer": "", "predicted_evidence": ["After a small grid search we decided to inherit most of the hyperparameters of the model from the best results achieved in BIBREF3 where -to-Mizar translation is learned. We used relatively small LSTM cells consisting of 2 layers with 128 units. The \u201cscaled Luong\u201d version of the attention mechanism was used, as well as dropout with rate equal $0.2$. The number of training steps was 10000. (This setting was used for all our experiments described below.)", "An examination of the examples wrongly rewritten by the model was done. It turns out that the wrong outputs almost always parse (in $97 - 99 \\%$ of cases they are legal polynomial terms). Notably, depending on the difficulty of the data set, as much as $18 - 64 \\%$ of incorrect outputs are wrong only with respect to the constants in the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming.", "Thirdly, there is an idea of integrating neural rewriting architectures into the larger systems for automated reasoning. This can be motivated by the interesting contrast between some simpler ILP systems suffering for combinatorial explosion in presence of a large number of examples and neural methods which definitely benefit form large data sets.", "In Table TABREF4 and Table TABREF5 there are presented examples of pairs of AIM terms in TPTP BIBREF9 format, before and after rewriting with, respectively, ground and nonground rewrite rules.", "Neural networks (NNs) turned out to be very useful in several domains. In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data."]}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "predicted_answer": "", "predicted_evidence": ["In the second iteration (second row), we add the common name of each of the 16 diseases as an anchor to one factor (16 total). Adding obesity as an anchor produces a clear Obesity topic, including several medications known to cause weight gain (e.g., acebutolol, klonopin). The anchored OSA topic, however, is quite poor and in fact resembles the rather generic topic to which obstructive sleep apnea is assigned by Unsupervised CorEx. It includes many spurious or non-specific terms like drug.", "This is likely due to the fact that obesity is a major risk factor of OSA, and so OSA symptoms are highly correlated with obesity and its other symptoms. Thus, the total correlation objective will attempt to group obesity and OSA-related terms together under a single latent factor. The sparse connection constraint mentioned in sec:methods prevents them from being connected to multiple factors. Indeed, sleep apnea appears in the obesity topic, suggesting the two topics are competing to explain OSA terms.", "This work was partially supported by DARPA award HR0011-15-C-0115. David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship.", "tab:class compares the classification performance of Unsupervised and Anchored CorEx on the soc.religion.christianity category from 20 Newsgroups for different choices of anchors. For both types of CorEx, the topic containing the corresponding terms is used as the classifier, but for Anchored CorEx those terms are also used as anchors when estimating the latent factor. Unsupervised CorEx does a reasonable job of discovering a coherent religion topic that already contains the terms God, Christian, and Jesus. However, using the terms Jesus and Christian as anchors yields a topic that better predicts the actual soc.religion.christianity category.", "To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics."]}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "predicted_answer": "", "predicted_evidence": ["tab:obesity:class shows the Macro-AUC and F1 scores (averaged across all diseases) on the Obesity Challenge data for the final anchored CorEx model and a Naive Bayes (NB) baseline, in which we train a separate classifier for each disease. Surprisingly, Anchored CorEx outperforms Naive Bayes (NB) by a large margin. Of course, Anchored CorEx is not a replacement for supervised learning: NB beats Anchored CorEx on 20 Newsgroups and does not represent a \u201cstrong\u201d baseline for Obesity 2008 (teams scored above 0.7 in Macro-F1 during the competition). It is nonetheless remarkable that Anchored CorEx performs as well as it does given that it is fundamentally unsupervised.", "We have introduced a simple information theoretic approach to topic modeling that can leverage domain knowledge specified informally as anchors. Our framework uses a novel combination of CorEx and the information bottleneck. Preliminary results suggest it can extract more precise, interpretable topics through a lightweight interactive process. We next plan to perform further empirical evaluations and to extend the algorithm to handle complex latent structures present in health care data.", "This is likely due to the fact that obesity is a major risk factor of OSA, and so OSA symptoms are highly correlated with obesity and its other symptoms. Thus, the total correlation objective will attempt to group obesity and OSA-related terms together under a single latent factor. The sparse connection constraint mentioned in sec:methods prevents them from being connected to multiple factors. Indeed, sleep apnea appears in the obesity topic, suggesting the two topics are competing to explain OSA terms.", "We preprocessed each document with a standard biomedical text pipeline that extracts common medical terms and phrases (grouping neighboring words where appropriate) and detecting negation (\u201cnot\u201d is prepended to negated terms) BIBREF23 , BIBREF24 . We converted each document to a binary bag-of-words with a vocabulary of 4114 (possibly negated) medical phrases. We used the 60/40 training/test split from the competition.", "In the third iteration, we correct this by adding sleep apnea as a second anchor to the OSA topic, and the resulting topic is clearly associated with OSA, including terms related to respiratory problems and medications used to treat (or believed to increase risk for) OSA. There is no noticeable reduction in quality in the Obesity topic."]}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "predicted_answer": "", "predicted_evidence": ["With respect to interpretable machine learning, our contributions are twofold. First, our framework provides a way for human users to share domain knowledge with a statistical learning algorithm that is both convenient for the human user and easily digestible by the machine. Second, our experimental results confirm that the introduction of simple anchor words can improve the coherence and human interpretability of topics discovered from data. Both are essential to successful and interactive collaboration between machine learning and human users.", "Anchored Correlation Explanation can be understood as a combination of Total Correlation Explanation (CorEx) BIBREF3 , BIBREF7 and the multivariate information bottleneck BIBREF4 , BIBREF8 . We search for a set of probabilistic functions of the inputs INLINEFORM0 for INLINEFORM1 that optimize the following information theoretic objective: INLINEFORM2", "Anchors allow us to both seed CorEx and impose semantics on latent factors: when analyzing medical documents, for example, we can anchor a diabetes latent factor to the word \u201cdiabetes.\u201d The INLINEFORM0 objective then discovers other words associated with \u201cdiabetes\u201d and includes them in this topic.", "In the second iteration (second row), we add the common name of each of the 16 diseases as an anchor to one factor (16 total). Adding obesity as an anchor produces a clear Obesity topic, including several medications known to cause weight gain (e.g., acebutolol, klonopin). The anchored OSA topic, however, is quite poor and in fact resembles the rather generic topic to which obstructive sleep apnea is assigned by Unsupervised CorEx. It includes many spurious or non-specific terms like drug.", "We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics."]}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the length of setence INLINEFORM1 , INLINEFORM2 is a discount parameter, INLINEFORM3 a given correct label sequence and INLINEFORM4 a predicted label sequence. For a given training instance INLINEFORM5 , our predicted label sequence is the label sequence with highest score: INLINEFORM6", "where INLINEFORM0 indicates the probability of label INLINEFORM1 at position INLINEFORM2 by the network with parameters INLINEFORM3 , INLINEFORM4 indicates the matrix of transition probability. In our model, INLINEFORM5 is computed as: DISPLAYFORM0", "We construct a semi-supervised model which is based on B-LSTM neural network and combine transition probability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy.", "In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The margin loss INLINEFORM2 serves as a trigger function to conduct the training process of B-LSTM based MMNN. We can introduce a new trigger function to guide the training process of neural network.", "To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media."]}
{"question_id": "2e4688205c8e344cded7a053b6014cce04ef1bd5", "predicted_answer": "", "predicted_evidence": ["To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media.", "F-Score and Label Accuracy Trigger Function The F-Score can be quite unstable in some situation. For instance, if there is no named entity in a sentence, F-Score will be always 0 regardless of the predicted label sequence. To take advantage of meaningful information provided by label accuracy, we introduce an integrated trigger function as follows: DISPLAYFORM0", "The results of our experiments also suggest directions for future work. We can observe all models in Table TABREF23 achieve a much lower recall than precision BIBREF25 . So we need to design some methods to solve the problem.", "where INLINEFORM0 are the transformation parameters, INLINEFORM1 the hidden vector and INLINEFORM2 the bias parameter. For a input sentence INLINEFORM3 with a label sequence INLINEFORM4 , a sentence-level score is then given as: DISPLAYFORM0", "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 . However, B-LSTM cannot learn sentence level label information. Huang et al. huang2015bidirectional combine CRF to use sentence level label information. We combine transition probability into our model to gain sentence level label information. To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) BIBREF19 based on B-LSTM. The prediction of label in position INLINEFORM0 is given as: DISPLAYFORM0"]}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "predicted_answer": "", "predicted_evidence": ["The results of our experiments also suggest directions for future work. We can observe all models in Table TABREF23 achieve a much lower recall than precision BIBREF25 . So we need to design some methods to solve the problem.", "To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media.", "Word segmentation takes an important part in Chinese text processing. Both Peng and Dredze peng-dredze:2015:EMNLP and Peng and Dredze peng-dredze:2016:P16-2 show the value of word segmentation to Chinese NER in social media. We present two methods to use word segmentation information in neural network model.", "Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.", "We adopt positional character embeddings in our next four models. Our first model is a B-LSTM neural network (baseline). To take advantage of traditional model BIBREF23 , BIBREF24 such as CRF, we combine transition probability in our B-LSTM based MMNN. We design a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The results of models are depicted as Figure UID11 . From the figure, we can know our models perfrom better with little loss in time."]}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "predicted_answer": "", "predicted_evidence": ["Character Embeddings and Word Segmentation Features We can treat word segmentation as discrete features in neural network model. The discrete features can be easily incorporated into neural network model BIBREF20 . We use word embeddings from a LSTM pretrained on MSRA 2006 corpus to initialize the word segmentation features.", "where INLINEFORM0 are the transformation parameters, INLINEFORM1 the hidden vector and INLINEFORM2 the bias parameter. For a input sentence INLINEFORM3 with a label sequence INLINEFORM4 , a sentence-level score is then given as: DISPLAYFORM0", "The natural language processing tasks on social media are more challenging which draw attention of many researchers BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . As the foundation of many downstream applications BIBREF4 , BIBREF5 , BIBREF6 such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . It is the informality of social media that discourages accuracy of NER systems. While efforts in English have narrowed the gap between social media and formal domains BIBREF3 , the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging BIBREF11 , BIBREF12 , BIBREF13 .", "We construct a semi-supervised model which is based on B-LSTM neural network and combine transition probability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy.", "F-Score Trigger Function The main criterion of NER task is F-score. However, high label accuracy does not mean high F-score. For instance, if every named entity's last character is labeledas O, the label accuracy can be quite high, but the precision, recall and F-score are 0. We use the F-Score between corrected label sequence and predicted label sequence as trigger function, which can conduct the training process to optimize the F-Score of training examples. Our new structured margin loss can be described as: DISPLAYFORM0"]}
{"question_id": "b70e4c49300dc3eab18e907ab903afd2a0c6075a", "predicted_answer": "", "predicted_evidence": ["We briefly review widely used approaches in cross-lingual transfer learning and some of the recent work in learning contextual word representations (CWR).", "For cross-lingual transfer, the most widely studied approach is to use multilingual word embeddings as features in neural network models. Several recent efforts have explored methods that align vector spaces for words in different languages BIBREF23, BIBREF24, BIBREF25.", "The encoder from an NMT model has been used as yet another effective way to contextualize word vectors BIBREF32. Additionally, recent progress in NMT has enabled one to train multilingual NMT systems that support translation from multiple source languages into multiple target languages within a single model BIBREF3. Our work is more closely related to two very recent works which explore the encoder from multilingual NMT model for cross-lingual transfer learning BIBREF4, BIBREF5. While BIBREF4 also consider multilingual systems, they do so on a much smaller scale, training it on only 2 languages. BIBREF5 uses a large scale model comparable to ours with 93 languages but they constrain the model by pooling encoder representations and therefore only obtain a single vector per sequence. Neither of these approaches have been used on token level sequence tagging tasks.", "However, when fine-tuning on downstream tasks, we do not use this token. We believe this creates a mismatch between the pre-training and fine-tuning steps. To investigate this further, we perform a small scale study where we train an mNMT model on 4 languages to and from English in two different settings: 1) where we prepend the $<$2xx$>$ token, and 2) where we don't prepend the $<$2xx$>$ token but instead encode it separately. The decoder jointly attends over both the source sentence encoder and the $<$2xx$>$ token encoding. The BLEU scores on the translation tasks are comparable using both these approaches. The results on cross-lingual zero-shot transfer in both settings are provided in Table TABREF39. Removing the $<$2xx$>$ token from the source sentence during mNMT training improves cross-lingual effectiveness on both POS tagging and XNLI task.", "XNLI is a popularly used corpus for evaluating cross-lingual sentence classification. It contains data in 15 languages BIBREF17. Evaluation is based on classification accuracy for pairs of sentences as one of entailment, neutral, or contradiction. We feed the text pair separated by a special token into MMTE and add a small network on top of it to build a classifier. This small network consists of a pre-pool feed-forward layer with 64 units, a max-pool layer which pools word level representations to get the sentence representation, and a post-pool feed-forward layer with 64 units. The optimizer used is Adafactor with a learning rate schedule of (0.2, 90k). The classifier is trained on English only and evaluated on all the 15 languages. Results are reported in Table TABREF21. Please refer to Appedix Table 1 for language names associated with the codes."]}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "predicted_answer": "", "predicted_evidence": ["While zero-shot transfer is a good measure of a model's natural cross-lingual effectiveness, the more practical setting is the few-shot transfer scenario as we almost always have access to, or can cheaply acquire, a small amount of data in the target language. We report the few-shot transfer results of mBERT and MMTE on the POS tagging dataset in TABREF33. To simulate the few-shot setting, in addition to using English data, we use 10 examples from each language (upsampled to 1000). MMTE outperforms mBERT in few-shot setting by 0.6 points averaged over 48 languages. Once again, we see that the gains are more pronounced in low resource languages.", "We use representations from a Massively Multilingual Translation Encoder (MMTE) that can handle 103 languages to achieve cross-lingual transfer on 5 classification and sequence tagging tasks spanning more than 50 languages.", "However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.", "During the pre-training step, when we perform the translation task using the mNMT system, we prepend a $<$2xx$>$ token to the source sentence, where xx indicates the target language. The encoder therefore has always seen a $<$2en$>$ token in front of non-English sentences and variety of different tokens depending on the target language in front of English sentence. However, when fine-tuning on downstream tasks, we do not use this token. We believe this creates a mismatch between the pre-training and fine-tuning steps. To investigate this further, we perform a small scale study where we train an mNMT model on 4 languages to and from English in two different settings: 1) where we prepend the $<$2xx$>$ token, and 2) where we don't prepend the $<$2xx$>$ token but instead encode it separately.", "More recent work has shown that CWRs obtained using unsupervised generative pre-training techniques such as language modeling or cloze task BIBREF26 have led to state-of-the-art results beyond what was achieved with traditional word type representations on many monolingual NLP tasks BIBREF27, BIBREF1, BIBREF28, BIBREF29 such as sentence classification, sequence tagging, and question answering. Subsequently, these contextual methods have been extended to produce multilingual representations by training a single model on text from multiple languages which have proven to be very effective for cross lingual transfer BIBREF18, BIBREF30, BIBREF31. BIBREF19 show that adding a translation language modeling (TLM) objective to mBERT's MLM objective utilizes both monolingual and parallel data to further improve the cross-lingual effectiveness."]}
{"question_id": "8599d6d14ac157169920c73b98a79737c7a68cf5", "predicted_answer": "", "predicted_evidence": ["In this paper we scale up the number of translation directions used in the NMT model to include 102 languages to and from English. Unlike BIBREF5, we do not apply any restricting operations such as pooling while training mNMT which allows us to obtain token level representations making it possible to transfer them to sequence tagging tasks as well. We find that mNMT models trained using plain translation losses can out of the box emerge as competitive alternatives to other methods at the forefront of cross-lingual transfer learning BIBREF1, BIBREF5", "It should be noted that fine-tuning is relatively inexpensive and fast. All of the results can be obtained within a few thousand gradient steps. The individual task-specific modeling details are described in detail in section SECREF3. It is also important to note that while the encoder, the attention mechanism, and the decoder of the model are trained in the pre-training phase, only the encoder is used during fine-tuning.", "We point out some of the major difference between mBERT and MMTE are:", "mBERT uses two unsupervised pre-training objectives called masked language modeling (MLM) and next sentence prediction (NSP) which are both trained on monolingual data in 104 languages. MMTE on the other hand uses parallel data in 103 languages (102 languages to and from English) for supervised training with negative log-likelihood as the loss. It should be noted that mBERT uses clean Wikipedia data while MMTE is pre-trained on noisy parallel data from the web.", "Given the wide distribution of data across language pairs, we used a temperature based data balancing strategy. For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs)."]}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "predicted_answer": "", "predicted_evidence": ["MLDoc is a balanced subset of the Reuters corpus covering 8 languages for document classification BIBREF8. This is a 4-way classification task of identifying topics between CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). Performance is evaluated based on classification accuracy. We split the document using the sentence-piece model and feed the first 200 tokens into the encoder for classification. The task-specific network and the optimizer used is same as the one used for XNLI. Learning rate schedule is (0.2,5k). We perform both in-language and zero-shot evaluation. The in-language setting has training, development and test sets from the language. In the zero-shot setting, the train and dev sets contain only English examples but we test on all the languages. The results of both the experiments are reported in Table TABREF23.", "Fine-tuning involves taking the encoder of our mNMT model, named Massively Multilingual Translation Encoder (MMTE), and adapting it to the downstream task. For tasks which involve single input, the text is directly fed into the encoder. For tasks such as entailment which involve input pairs, we concatenate the two inputs using a separator token and pass this through the encoder. For each downstream task, the inputs and outputs are passed through the encoder and we fine-tune all the parameters end-to-end. The encoder encodes the input through the stack of Transformer layers and produces representations for each token at the output. For sequence tagging tasks, these token level representations are individually fed into a task-specific output layer. For classification or entailment tasks, we apply max-pooling on the token level representations and feed this into the task-specific output layer.", "We train our multilingual NMT system on a massive scale, using an in-house corpus generated by crawling and extracting parallel sentences from the web BIBREF14. This corpus contains parallel documents for 102 languages, to and from English, comprising a total of 25 billion sentence pairs. The number of parallel sentences per language in our corpus ranges from around 35 thousand to almost 2 billion. Figure FIGREF10 illustrates the data distribution for all 204 language pairs used to train the NMT model. Language ids for all the languages are also provided in supplementary material.", "For NER, we use the dataset from the CoNLL 2002 and 2003 NER shared tasks, which when combined have 4 languages BIBREF21, BIBREF22. The labeling scheme is IOB with 4 types of named entities. The task-specific network, optimizer, and the learning rate schedule is the same as in the setup for POS tagging. The evaluation metric is span-based F1. Table TABREF29 reports the results of both in-language and zero-shot settings.", "We use BLEU score BIBREF15 to evaluate the quality of our translation model(s). Our mNMT model performs worse than the bilingual baseline on high resource language pairs but improves upon it on low resource language pairs. The average drop in BLEU score on 204 language pairs as compared to bilingual baselines is just 0.25 BLEU. This is impressive considering we are comparing one multilingual model to 204 different bilingual models. Table TABREF14 compares the BLEU scores achieved by mNMT to that of the bilingual baselines on 10 representative language pairs. These scores are obtained on an internal evaluation set which contains around 5k examples per language pair."]}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "predicted_answer": "", "predicted_evidence": ["Who would you talk to for a long conversation?", "in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert", "The proposed XPersona dataset is an extension of the persona-chat dataset BIBREF0, BIBREF1. Specifically, we extend the ConvAI2 BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. Since the test set of ConvAI2 is hidden, we split the original validation set into a new validation set and test sets. Then, we firstly automatically translate the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native speaker annotators with a fluent level of English and asked them to revise the machine-translated dialogues and persona sentences in the validation set and test set according to original English dialogues. The main goal of human annotation is to ensure the resulting conversations are coherent and fluent despite the cultural differences in target languages.", "We observe that both the monolingual and multilingual models can generate fluent responses. Compared to Bert2Bert and M-Bert2Bert, CausalBert and M-CausalBert can generate more on-topic responses but sometimes repeat through turns. CausalBert and M-CausalBert are on par with each other in monolingual conversational tasks, while M-CausalBert shows the advantage of handling a mixed-language context. For multilingual speakers, the conversation may involve multiple languages. Therefore, we experiment on M-CausalBert with two settings: 1) many-to-one, in which users converse with the model in 6 languages, and the model generate responses in English, 2) one-to-many, in which users converse with the model using English, and the model generates responses in 6 languages using language embedding and corresponding persona sentences.", "Figure FIGREF9 shows the conceptual differences between the encoder-decoder and casual decoder. Note that in both multilingual models, the dialogue history encoding process is language-agnostic, while decoding language is controlled by the language embedding. Such design allows the model to understand mixed-language dialogue contexts and to responds in the desired language (details in Section SECREF44)."]}
{"question_id": "6c8dc31a199b155e73c84173816c1e252137a0af", "predicted_answer": "", "predicted_evidence": ["To model the response generation, we use a Transformer BIBREF61 based encoder-decoder BIBREF10. As illustrated in Figure FIGREF9, we concatenate the system persona $\\mathcal {P}_s$ with the dialogue history $\\mathcal {D}_t$. Then we use the embedding layer $E$ to finally pass it to the encoder. In short, we have:", "in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert", "We report more the mixed-language samples generated by M-CausalBert in Table TABREF61 and TABREF62.", "If you had to say one of these speakers is interesting and one is boring, who would you say is more interesting?", "Cross-lingual adaptation learns the inter-connections among languages and circumvents the requirement of extensive training data in target languages BIBREF50, BIBREF51, BIBREF52. Cross-lingual transfer learning methods have been applied to multiple NLP tasks, such as named entity recognition BIBREF53, BIBREF54, natural language understanding BIBREF39, dialogue state tracking BIBREF55, part-of-speech tagging BIBREF50, BIBREF51, BIBREF56, and dependency parsing BIBREF57, BIBREF58. Meanwhile, BIBREF59 and BIBREF60 proposed pre-trained cross-lingual language models to align multiple language representations, achieving state-of-the-art results in many cross-lingual classification tasks."]}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "predicted_answer": "", "predicted_evidence": ["in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert", "in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert", "CausalBert and M-CausalBert are on par with each other in monolingual conversational tasks, while M-CausalBert shows the advantage of handling a mixed-language context. For multilingual speakers, the conversation may involve multiple languages. Therefore, we experiment on M-CausalBert with two settings: 1) many-to-one, in which users converse with the model in 6 languages, and the model generate responses in English, 2) one-to-many, in which users converse with the model using English, and the model generates responses in 6 languages using language embedding and corresponding persona sentences. Table TABREF42 and table TABREF43 illustrate the generation examples under these settings (more examples reported in Appendix C.1). Most of the time, M-CausalBert can understand the mixed-language context, and decode coherent response in different languages. Understanding the mixed-language dialogue context is a desirable skill for end-to-end chit-chat systems, and a systematic study of this research question is needed in future.", "Who would you talk to for a long conversation?", "To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages."]}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "predicted_answer": "", "predicted_evidence": ["Which speaker sounds more human?", "in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert", "If you had to say one of these speakers is interesting and one is boring, who would you say is more interesting?", "To model the response generation, we use a Transformer BIBREF61 based encoder-decoder BIBREF10. As illustrated in Figure FIGREF9, we concatenate the system persona $\\mathcal {P}_s$ with the dialogue history $\\mathcal {D}_t$. Then we use the embedding layer $E$ to finally pass it to the encoder. In short, we have:", "in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert"]}
{"question_id": "ae2142ee9e093ce485025168f4bcb3da4602739d", "predicted_answer": "", "predicted_evidence": ["As an example, consider the sentences in Figure FIGREF1 . In both languages, there is a pronoun in the second sentence that refers to the European Central Bank. When the second sentence is translated from English to German, the translation of the pronoun it is ambiguous. This ambiguity can only be resolved with context awareness: if a translation system has access to the previous English sentence, the previous German translation, or both, it can determine the antecedent the pronoun refers to. In this German sentence, the antecedent Europ\u00e4ische Zentralbank dictates the feminine gender of the pronoun sie.", "To provide a basis for filtering with constraints, we tokenize the whole data set with the Moses tokenizer, generate symmetric word alignments with fast_align BIBREF23 , parse the English text with CoreNLP BIBREF24 , parse the German text with ParZu BIBREF25 and perform coreference resolution on both sides. The coreference chains are obtained with the neural model of CoreNLP for English, and with CorZu for German BIBREF26 , respectively.", "To address this issue, we present an alternative way of evaluating larger-context models on a test set that allows to specifically measure a model's capability to correctly translate pronouns. The test suite consists of pairs of source and target sentences, in combination with contrastive translation variants (for evaluation by model scoring) and additional linguistic and contextual information (for further analysis). The resource is freely available. Additionally, we evaluate several context-aware models that have recently been proposed in the literature on this test set, and extend existing models with parameter tying.", "If the model score of the reference is indeed higher, we refer to this outcome as a \u201ccorrect decision\u201d by the model. The model's decision is only correct if the reference translation has a higher score than any contrastive translation. In our evaluation, we aggregate model decisions on the whole test set and report the overall percentage of correct decisions as accuracy.", "The BLEU scores in Table TABREF30 show a moderate improvement for most context-aware systems. This suggests that the architectural changes for the context-aware models do not degrade overall translation quality. The contrastive evaluation on our test set on the other hand shows a clear increase in the accuracy of pronoun translation: The best model s-hier-to-2.tied achieves a total of +16 percentage points accuracy on the test set over the baseline, see Table TABREF31 ."]}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_answer": "", "predicted_evidence": ["both pronouns are in a coreference chain;", "s-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.", "To evaluate pronoun translation, we perform contrastive evaluation and report the accuracy of models on our contrastive test set.", "Conceptually, our test set is most similar to the \u201ccross-lingual pronoun prediction\u201d task held at DiscoMT and WMT in recent years BIBREF15 , BIBREF16 , BIBREF17 : participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns.", "Regarding the comparison of different context-aware architectures, our results demonstrate the effectiveness of parameter sharing between the main encoder (or decoder) and the contextual encoder. We observe an improvement of 5 percentage points from s-hier-to-2 to s-hier-to-2.tied, and 4 percentage points from s-t-hier to s-t-hier.tied. Context encoders introduce a large number of extra parameters, while inter-sentential context is only relevant for a relatively small number of predictions. We hypothesize that the training signal is thus too weak to train a strong contextual encoder in an end-to-end fashion without parameter sharing. Our results also confirm the finding by BIBREF9 that multi-encoder architectures, specifically s-hier-to-2(.tied), can outperform a simple concatenation system in the translation of coreferential pronouns."]}
{"question_id": "cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "predicted_answer": "", "predicted_evidence": ["Table TABREF32 shows that context-aware models perform better than the baseline when the antecedent is outside the current sentence. In our experiments, all context-aware models consider one preceding sentence as context. The evaluation according to the distance of the antecedent in Table TABREF35 confirms that the subset of sentences with antecedent distance 1 benefits most from the tested context-aware models (up to +20 percentage points accuracy). However, we note two surprising patterns:", "We attribute the second observation to the existence of coreference chains where the preceding sentence contains a pronoun that refers to the same nominal antecedent as the pronoun in the current sentence. Consider the example in Table TABREF36 : The nominal antecedent of it in the current sentence is door, T\u00fcr in German with feminine gender. The nominal antecedent occurs two sentences before the current sentence, but the German sentence in between contains the pronoun sie, which is a useful signal for the context-aware models, even though they cannot know the nominal antecedent.", "Our experiments are based on models from BIBREF9 , who have released their source code. We extend their models with parameter sharing, which was shown to be beneficial by BIBREF8 . Additionally, we consider a concatenative baseline, similar to BIBREF5 , and Transformer-based models BIBREF8 .", "We present a large-scale test suite to specifically test the capacity of NMT models to translate pronouns correctly. The test set contains 12,000 difficult cases of pronoun translations from English it to its German counterparts er, sie and es, extracted automatically from OpenSubtitles BIBREF22 .", "The first edition of the task focused on English INLINEFORM0 French, and it was found that local context (such as the verb group) was a strong signal for pronoun prediction. Hence, future editions only provided target-side lemmas instead of fully inflected forms, which makes the task less suitable to evaluate end-to-end neural machine translation systems, although such systems have been trained on the task BIBREF18 ."]}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "predicted_answer": "", "predicted_evidence": ["For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from BIBREF2 , BIBREF8 . Systems are trained on lowercased text that was encoded using BPE (32k merge operations). Models consist of 6 encoder and decoder layers with 8 attention heads. The hidden state size is 512, the size of feedforward layers is 2048.", "However, this does not mean that systems actually produce the reference translation when given the source sentence for translation. An entirely different target sequence might rank higher in the system's beam during decoding. The only conclusion permitted by contrastive evaluation is whether or not the reference translation is more probable than a contrastive variant.", "All remaining models are based on the Transformer architecture BIBREF2 . A Transformer avoids recurrence completely: it follows an encoder-decoder architecture using stacked self-attention and fully connected layers for both the encoder and decoder.", "The Transformer-based models perform strongest on pronouns with intra-segmental antecedent, outperforming the recurrent baseline by 9\u201318 percentage points. This is likely an effect of increased model depth and the self-attentional architecture in this set of experiments. The model by BIBREF8 only uses source context, and outperforms the most comparable RNN system, s-hier.tied. However, the Transformer-based concat22 slightly underperforms the RNN-based concat22, and we consider it future research how to better exploit target context with Transformer-based models.", "We present a large-scale test suite to specifically test the capacity of NMT models to translate pronouns correctly. The test set contains 12,000 difficult cases of pronoun translations from English it to its German counterparts er, sie and es, extracted automatically from OpenSubtitles BIBREF22 ."]}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "predicted_answer": "", "predicted_evidence": ["We also tried other more advanced EL methods in our experiments. However, they do not improve the final performance of our model. Experimental results of using the EL system proposed in BIBREF19 is provided in Section SECREF4.", "We propose a deep neural model to improve fine-grained entity typing with entity linking. The problem of overfitting the weakly labeled training data is addressed by using a variant of the hinge loss and introducing noise during training. We conduct experiments on two commonly used dataset. The experimental results demonstrates the effectiveness of our approach.", "Ours (LocAttEL), which uses a more advanced EL system, does not achieve better performance than Ours (Full), which uses our own EL approach. After manually checking the results of the two EL approaches and the predictions of our model on FIGER (GOLD), we think this is mainly because: 1) Our model also uses the context while making predictions. Sometimes, if it \u201cthinks\u201d that the type information provided by EL is incorrect, it may not use it. 2) The performances of different EL approaches also depends on the dataset and the types of entities used for evaluation. We find that on FIGER (GOLD), the approach in BIBREF19 is better at distinguishing locations and sports teams, but it may also make some mistakes that our simple EL method does not. For example, it may incorrectly link \u201cMarch,\u201d the month, to an entity whose Wikipedia description fits the context better.", "Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted BIBREF0, BIBREF1 or learned from neural network models BIBREF8, BIBREF9, BIBREF10. Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies BIBREF11, BIBREF12, BIBREF9 address these problems by separating clean mentions and noisy mentions, modeling type correction BIBREF3, using a hierarchy-aware loss BIBREF9, etc.", "We use strict accuracy, Macro F1, and Micro F1 to evaluate fine-grained typing performance BIBREF0."]}
{"question_id": "6295951fda0cfa2eb4259d544b00bc7dade7c01e", "predicted_answer": "", "predicted_evidence": ["To make it more flexible, we also propose to use a variant of the hinge loss used by BIBREF16 to train our model:", "We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\mathbf {t}_1,...,\\mathbf {t}_k$. Then the score of the mention $m$ having the type $t_i\\in T$ is calculated as the dot product of $\\mathbf {u}_m$ and $\\mathbf {t}_i$:", "We use strict accuracy, Macro F1, and Micro F1 to evaluate fine-grained typing performance BIBREF0.", "This task is challenging because it usually uses a relatively large tag set, and some mentions may require the understanding of the context to be correctly labeled. Moreover, since manual annotation is very labor-intensive, existing approaches have to rely on distant supervision to train models BIBREF0, BIBREF2.", "Following existing studies, we also generate training data by using the anchor links in Wikipedia. Each anchor link can be used as a mention. These mentions are labeled by mapping the Freebase types of the target entries to the tag set $T$ BIBREF0."]}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_answer": "", "predicted_evidence": ["Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted BIBREF0, BIBREF1 or learned from neural network models BIBREF8, BIBREF9, BIBREF10. Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies BIBREF11, BIBREF12, BIBREF9 address these problems by separating clean mentions and noisy mentions, modeling type correction BIBREF3, using a hierarchy-aware loss BIBREF9, etc.", "BIBREF13 and BIBREF14 are two studies that are most related to this paper. BIBREF13 propose an unsupervised FET system where EL is an importat component. But they use EL to help with clustering and type name selection, which is very different from how we use it to improve the performance of a supervised FET model. BIBREF14 finds related entities based on the context instead of directly applying EL. The types of these entities are then used for inferring the type of the mention.", "Since the KB type representations we use in our FET model are also obtained through mapping Freebase types, they will perfectly match the automatically generated labels for the mentions that are correctly linked (i.e., when the entity returned by the EL algorithm and the target entry of the anchor link are the same). For example, in Figure FIGREF4, suppose the example sentence is a training sample obtained from Wikipedia, where \u201cDonald Trump\u201d is an anchor link points to the Wikipedia page of Donald Trump. After mapping the Freebase types of Donald Trump to the target tag set, this sample will be weakly annotated as /person/politician, /person/tv_personality, and /person/business, which is exactly the same as the type information (the \u201cTypes From KB\u201d in Figure FIGREF4) obtained through EL.", "Ours (LocAttEL), which uses a more advanced EL system, does not achieve better performance than Ours (Full), which uses our own EL approach. After manually checking the results of the two EL approaches and the predictions of our model on FIGER (GOLD), we think this is mainly because: 1) Our model also uses the context while making predictions. Sometimes, if it \u201cthinks\u201d that the type information provided by EL is incorrect, it may not use it. 2) The performances of different EL approaches also depends on the dataset and the types of entities used for evaluation. We find that on FIGER (GOLD), the approach in BIBREF19 is better at distinguishing locations and sports teams, but it may also make some mistakes that our simple EL method does not. For example, it may incorrectly link \u201cMarch,\u201d the month, to an entity whose Wikipedia description fits the context better. 3) For some mentions, although the EL system links it to an incorrect entity, the type of this entity is the same with the correct entity.", "Following existing studies, we also generate training data by using the anchor links in Wikipedia. Each anchor link can be used as a mention. These mentions are labeled by mapping the Freebase types of the target entries to the tag set $T$ BIBREF0."]}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "predicted_answer": "", "predicted_evidence": ["Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo.", "", "The definition document consists of 241 SE definitions and their descriptions. We iteratively construct entities in increasing order of number of words in the definitions with the help of their parts-of-speech tags. This helps in creating subset-of relation between a lower-word entity and a higher-word entity. Each root entity is lemmatized such that entities like processes and process appear only once.", "", "with at least one verb, can extract relation-like phrases from the phrase that links two concepts. An example is shown in Figure FIGREF27. Further investigation of relation extraction from SE handbook is left as future work."]}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "predicted_answer": "", "predicted_evidence": ["loc: represents location-like entities such as component facilities or centralized facility.", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "r\"\\([ ]*[A-Z][A-Za-z]*[ ]*\\)\"", "mea: represents measures, features, or behaviors such as cost, risk, or feasibility.", "Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings."]}
{"question_id": "77c3416578b52994227bae7f2529600f02183e12", "predicted_answer": "", "predicted_evidence": ["", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "r\"\\([ ]*[A-Z][A-Za-z]*[ ]*\\)\"", "Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is performed on a few selected entity tags such as opcon and syscon. For example, consider the sentence `SE functions should be performed'. `SE' has tag NNP and `functions' has tag NNS. We create a relation called subset-of between `SE functions' and `SE'.", "<TO>)*<VB.*>+(<MD>|<R.*>|<I.*>|<VB.*>|"]}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "predicted_answer": "", "predicted_evidence": ["", "Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from definitions. Only those phrases that connect two entities are selected for verb phrase extraction. Using NLTK's regex parser and chunker, a grammar such as", "Definition: Acceptable Risk", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results."]}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "predicted_answer": "", "predicted_evidence": ["The Systems Engineer's Virtual Assistant (SEVA) BIBREF0 was introduced with the goal to assist systems engineers (SE) in their problem-solving abilities by keeping track of large amounts of information of a NASA-specific project and using the information to answer queries from the user. In this work, we address a system element by constructing a common-knowledge concept recognition system for improving the performance of SEVA, using the static knowledge collected from the Systems Engineering Handbook BIBREF1 that is widely used in projects across the organization as domain-specific commonsense knowledge. At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook.", "Description: The risk that is understood and agreed to by the program/project, governing authority, mission directorate, and other customer(s) such that no further specific mitigating action is required.", "Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is performed on a few selected entity tags such as opcon and syscon. For example, consider the sentence `SE functions should be performed'. `SE' has tag NNP and `functions' has tag NNS. We create a relation called subset-of between `SE functions' and `SE'.", "In this work, we address a system element by constructing a common-knowledge concept recognition system for improving the performance of SEVA, using the static knowledge collected from the Systems Engineering Handbook BIBREF1 that is widely used in projects across the organization as domain-specific commonsense knowledge. At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain.", "The Systems Engineer's Virtual Assistant (SEVA) BIBREF0 was introduced with the goal to assist systems engineers (SE) in their problem-solving abilities by keeping track of large amounts of information of a NASA-specific project and using the information to answer queries from the user. In this work, we address a system element by constructing a common-knowledge concept recognition system for improving the performance of SEVA, using the static knowledge collected from the Systems Engineering Handbook BIBREF1 that is widely used in projects across the organization as domain-specific commonsense knowledge. At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts."]}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "predicted_answer": "", "predicted_evidence": ["Abbreviations are used frequently in SE text. We automatically extract abbreviations using simple pattern-matching around parentheses. Given below is a sample regex that matches most abbreviations in the SE handbook.", "Definition: Acceptable Risk", "", "mea: represents measures, features, or behaviors such as cost, risk, or feasibility.", "VP: {(<MD>|<R.*>|<I.*>|<VB.*>|<JJ.*>|"]}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "predicted_answer": "", "predicted_evidence": ["", "Definition: Acceptable Risk", "Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo.", "VP: {(<MD>|<R.*>|<I.*>|<VB.*>|<JJ.*>|", "Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from definitions. Only those phrases that connect two entities are selected for verb phrase extraction. Using NLTK's regex parser and chunker, a grammar such as"]}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "predicted_answer": "", "predicted_evidence": ["Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al.", "The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual.", "Document summarization from social media and news circles has received much attention for the past decades. Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task.", "The second model, INLINEFORM0 , is aiming at training paragraph vectors. It is also called distributed memory model of paragraph vectors (PV-DM) BIBREF26 , which is an extension of word2vec. In comparison with the word2vec framework, the only change in PV-DM is in the equation (3), where INLINEFORM1 is constructed from INLINEFORM2 and INLINEFORM3 , where matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 .", "Analyzing the rhetorical status of sentences manually requires huge amount of efforts, especially for structuring information from multiple documents. Fortunately, computer algorithms have been introduced to solve this problem. With the development of artificial intelligence, machine learning and computational linguistics, Natural Language Processing (NLP) has become a popular research area BIBREF4 , BIBREF5 . NLP covers the applications from document retrieval, text categorization BIBREF6 , document summarization BIBREF7 to sentiment analysis BIBREF8 , BIBREF9 . Those applications are targeting different types of text resources, such as articles from social media BIBREF10 and scientific publications BIBREF2 . There are several approaches to tackle these tasks. From machine learning prospective, text can be analysed via supervised BIBREF2 , semi-supervised BIBREF11 and unsupervised BIBREF12 algorithms."]}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "predicted_answer": "", "predicted_evidence": ["Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community. There are plenty of cases where word embedding and sentence representations have been applied to short text classification BIBREF17 and paraphrase detection BIBREF18 . However, the effectiveness of this technique on AZ needs further study. The research question is, is it possible to extract word embeddings as features to classify sentences into the seven categories mentioned above using supervised machine learning approach?", "The tool of word2vec proposed by Mikolov et al. BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling.", "BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized.", "where INLINEFORM0 is the word embedding for word INLINEFORM1 , which is learned by the classical word2vec algorithm BIBREF16 .", "The second model, INLINEFORM0 , is aiming at training paragraph vectors. It is also called distributed memory model of paragraph vectors (PV-DM) BIBREF26 , which is an extension of word2vec. In comparison with the word2vec framework, the only change in PV-DM is in the equation (3), where INLINEFORM1 is constructed from INLINEFORM2 and INLINEFORM3 , where matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 ."]}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "predicted_answer": "", "predicted_evidence": ["Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically.", "Le and Mikolov BIBREF26 introduced the concept of word vector representation in a formal way:", "As the growth rate of scientific publication has been rising dramatically, researchers are overwhelmed by the explosive information. It is almost impossible to digest the ideas contained in the documents emerged everyday. Therefore, computer assisted technologies such as document summarization are expected to play a role in condensing information and providing readers with more relevant short texts. Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences.", "Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et.", "The tool of word2vec proposed by Mikolov et al. BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling."]}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "predicted_answer": "", "predicted_evidence": ["INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 (4)", "The second model, INLINEFORM0 , is aiming at training paragraph vectors. It is also called distributed memory model of paragraph vectors (PV-DM) BIBREF26 , which is an extension of word2vec. In comparison with the word2vec framework, the only change in PV-DM is in the equation (3), where INLINEFORM1 is constructed from INLINEFORM2 and INLINEFORM3 , where matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 .", "PARAVEC and AVGWVEC+BSWE for BAS category only). Looking at the F-measure, AVGWVEC performs better than PARAVEC, but PARAVEC gave a better precision results on several categories, such as AIM, CTR, TXT and OWN. The results showed that PARAVEC model is not robust, for example, it performs badly for the category of BAS. For specific category classification, take the BAS category for example, the BSWE model outperforms others in terms of F-measure.", "When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories, 300-dimension features perform better than the 100-dimension ones, for example, CTR and BKG, but they are not as good as 100-dimension features for some categories, such as BAS.", "Finally, the results were compared between word embeddings and the methods of cuewords, Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is better than the method using cuewords matching. It also outperforms Teufel 2002 for most of the cases, except AIM, BAS and OWN. It won baseline for most of the categories, except OWN."]}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "predicted_answer": "", "predicted_evidence": ["Analyzing the rhetorical status of sentences manually requires huge amount of efforts, especially for structuring information from multiple documents. Fortunately, computer algorithms have been introduced to solve this problem. With the development of artificial intelligence, machine learning and computational linguistics, Natural Language Processing (NLP) has become a popular research area BIBREF4 , BIBREF5 . NLP covers the applications from document retrieval, text categorization BIBREF6 , document summarization BIBREF7 to sentiment analysis BIBREF8 , BIBREF9 . Those applications are targeting different types of text resources, such as articles from social media BIBREF10 and scientific publications BIBREF2 . There are several approaches to tackle these tasks. From machine learning prospective, text can be analysed via supervised BIBREF2 , semi-supervised BIBREF11 and unsupervised BIBREF12 algorithms.", "BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized.", "The tool of word2vec proposed by Mikolov et al. BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling.", "The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized. Usually, this optimizing process is done using stochastic gradient descent method. It doesn't need labels when training the models, which makes word2vec algorithm more valuable compared with traditional supervised machine learning methods that require a big amount of annotated data.", "Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community. There are plenty of cases where word embedding and sentence representations have been applied to short text classification BIBREF17 and paraphrase detection BIBREF18 . However, the effectiveness of this technique on AZ needs further study. The research question is, is it possible to extract word embeddings as features to classify sentences into the seven categories mentioned above using supervised machine learning approach?"]}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "predicted_answer": "", "predicted_evidence": ["When the feature dimension is set to 100 and the training corpus is ACL, the results generated by different models were compared (AVGWVEC,", "Table. TABREF19 and TABREF20 show the classification performance of different methods.", "When the model is set to AVGWVEC and the feature dimension is 100, the results computed from different training corpus were compared (ACL+AZ, MixedAbs and Brown corpus). ACL+AZ outperforms others and brown corpus is better than MixedAbs for most of the categories, but brown corpus is not as good as MixedAbs for the category of OWN.", "Finally, the results were compared between word embeddings and the methods of cuewords, Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is better than the method using cuewords matching. It also outperforms Teufel 2002 for most of the cases, except AIM, BAS and OWN. It won baseline for most of the categories, except OWN.", "The results were examined from the following aspects:"]}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "predicted_answer": "", "predicted_evidence": ["Secondary Input: The secondary task takes as input the vector of tweet content features, which is a 12-dimensional vector, as described above.", "INLINEFORM0 : Number of noun words in the tweet", "As shown in Table TABREF29 , we observe that the multitask learning based model which uses the Tweet2Vec encoding and the content features as inputs to two separate tasks outperforms all the baselines, achieving an F1-score of 0.89 for classification of tweets as Blackmarket or Genuine. The best baseline is Spam Detector 2 which achieves an F1-score of 0.77.", "We use the Tweet2Vec model BIBREF5 to generate a vector-space representation of each of the tweets. Tweet2Vec is a character-level deep learning based encoder for social media posts trained on the task of predicting the associated hashtags. It considers the assumption that posts with the same hashtags should have similar representation. It uses a bi-directional Gated Recurrent Unit (Bi-GRU) for learning the tweet representation. To get the representation for a particular tweet, the model combines the final GRU states by going through a forward and backward pass over the entire sequence.", "As shown in Figure FIGREF21 , the inputs are fed into separate fully connected (FC) layers with cross-stitch units stacked between successive layers. The cross-stitch units find the best shared representations using linear combinations, and learn the optimal linear combinations for a given set of tasks. The cross-stitch units essentially allow us to unify two separate networks for two separate tasks into a single network wherein each layer of the network shares the parameters from the other network using linear combinations. The network also employs batch-normalization and dropout to avoid overfitting."]}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "predicted_answer": "", "predicted_evidence": ["We show that our multitask learning approach outperforms Twitter spam detection approaches, as well as state-of-the-art classifiers by 14.1% (in terms of F1-score), achieving an F1-score of 0.89 on our dataset. In short, the contributions of the paper are threefold: a new dataset, characterization of blackmarket tweets, and a novel multitask learning framework to detect tweets posted on blackmarket services.", "blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to gain appraisals for their content), and (iii) auto-time retweet services (customers need to provide their Twitter access tokens, upon which their content is retweeted 10-20 times for each 15-minute window).", "INLINEFORM0 : Count of media content in the tweet", "Table TABREF1 shows a sample tweet that was posted on a blackmarket service and another sample tweet that was not. In this paper, we make the first attempt to detect tweets that are posted on blackmarket services. Our aim is to build a system that can flag tweets soon after they are posted, which is why we do not consider temporal features such as the number of retweets or likes that a tweet keeps gaining over time. Instead, we only rely on the features and representations extracted from the content of the tweets.", "The output layer of the first task classifies tweets as blackmarket or genuine using a cross entropy loss function. The output layer of the second task predicts the numerical values for the number of retweets and likes that a tweet will gain after five days of being posted by using a Mean Squared Error (MSE) loss. Note that the performance of the secondary task is not of importance to us, however, the secondary task helps the primary task. Therefore, we focus on the performance of the model in the primary task during training and evaluation."]}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "predicted_answer": "", "predicted_evidence": ["INLINEFORM0 : Number of special characters (non alpha-numeric) in the tweet", "The architecture of our model is shown in Figure FIGREF21 . We adopt multitask learning to develop our model. The primary task is set as a binary classification problem, wherein the tweets are classified as blackmarket or genuine. The secondary task is set as a regression problem, wherein the number of likes and retweets that a tweet will gain after five days of being posted is predicted.", "INLINEFORM0 : Count of media content in the tweet", "This section describes the features and tweet representation methodology, and the proposed model to solve the problem.", "We curate a novel dataset of tweets that have been posted to blackmarket services, and a corresponding set of tweets that haven't. We propose a multitask learning approach to combine properties from the characterization of blackmarket tweets via traditional feature extraction, with a deep learning based feature representation of the tweets. We train a neural network which takes as input both the traditional feature representation as well as the deep learning based representation generated using the Tweet2Vec model BIBREF5 , and utilizes cross-stitch units BIBREF6 to learn an optimal combination of shared and task-specific knowledge via soft parameter sharing."]}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "predicted_answer": "", "predicted_evidence": ["The model takes a different input feature vector for each of the tasks.", "The work was partially funded by DST (ECR/2017/00l691, DST/INT/UK/P158/2017), Ramanujan Fellowship, and the Infosys Centre of AI, IIIT-Delhi, India.", "We generate a combined feature vector by concatenating the tweet content features and the encoding generated by Tweet2Vec. This feature vector is then fed to state-of-the-art machine learning classifiers - Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM).", "blackWe analyse the false negatives generated by our model to find which type of tweets the model finds difficult to classify. The percentage of each class in the false negatives is as follows: Promotional - 23.29%, Politics - 10.96%, Entertainment - 21.92%, News - 9.59%, Spam - 5.48%, and Others - 28.77%. We observe that the tweets belonging to the category Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the following categories \u2013 News, Spam and Politics.", "Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset."]}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "predicted_answer": "", "predicted_evidence": ["Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.", "As shown in Figure FIGREF42, we present some examples generated by Xnlg and the baselines in four directions (En-En, En-Zh, Zh-En, and Zh-Zh). When decoding on an unseen language, Xlm tends to generate random output, because it is not designed for cross-lingual NLG. In terms of the pipeline model, we can observe that it suffers from the error propagation issue, especially when the source and target languages are all different from the training data. For example, when the pipeline model performs Zh-Zh-QG, keywords are translated twice, increasing the risk of mistranslation. In the second example, \u201catomic bomb\u201d is mistranslated to \u201cnuclear bomb\u201d, resulting in its low correctness. On the contrary, by directly transferring English supervision signals to the other generation directions, the generated questions of Xnlg match the references better than baselines.", "Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.", "We conduct experiments on the zero-shot Chinese-Chinese QG task to evaluate the cross-lingual transfer ability. In this task, models are trained with English QG data but evaluated with Chinese QG examples. We include the following models as our baselines:", "As shown in Figure FIGREF6(b), we propose a two-stage pre-training protocol for Xnlg. The first stage pretrains the encoding components, where the model learns to encode multilingual sentences to a shared embedding space. We consider using MLM and XMLM as the pre-training tasks. The objective of the first stage is to minimize: 1= (x,y) p XMLM(x,y) + x m MLM(x) where ${_{\\textnormal {p}}}$ indicates the parallel corpus, and ${_{\\textnormal {m}}}$ is the monolingual corpus."]}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "predicted_answer": "", "predicted_evidence": ["We conduct experiments over two cross-lingual NLG downstream tasks, i.e., cross-lingual question generation, and cross-lingual abstractive summarization. We compare Xnlg with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines.", "We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29.", "Specifically, Xnlg shares the same sequence-to-sequence model across languages, and is pre-trained with both monolingual and cross-lingual objectives. The model not only learns to understand multilingual input, but also is able to generate specific languages by conditioning on the encoded semantics. Figure FIGREF2 demonstrates how to use Xnlg to perform cross-lingual transfer for downstream tasks. The proposed model enables us to fine-tune the pre-trained model on monolingual NLG training data, and then evaluate it beyond a single language, including zero-shot cross-lingual generation. Besides, we explore several fine-tuning strategies to make a compromise between cross-lingual ability and task ability. In addition, we introduce two cross-lingual NLG datasets (i.e., question generation, and abstractive summarization) for evaluation, which includes three languages, namely English, Chinese, and French. Experimental results on the NLG tasks show that Xnlg achieves competitive performance compared with the machine-translation-based pipeline model in zero-shot cross-lingual settings.", "In this paper, we propose a cross-lingual pre-trained model (named as Xnlg) in order to transfer monolingual NLG supervision to other pre-trained languages by fine-tuning. Specifically, Xnlg shares the same sequence-to-sequence model across languages, and is pre-trained with both monolingual and cross-lingual objectives. The model not only learns to understand multilingual input, but also is able to generate specific languages by conditioning on the encoded semantics. Figure FIGREF2 demonstrates how to use Xnlg to perform cross-lingual transfer for downstream tasks. The proposed model enables us to fine-tune the pre-trained model on monolingual NLG training data, and then evaluate it beyond a single language, including zero-shot cross-lingual generation. Besides, we explore several fine-tuning strategies to make a compromise between cross-lingual ability and task ability. In addition, we introduce two cross-lingual NLG datasets (i.e., question generation, and abstractive summarization) for evaluation, which includes three languages, namely English, Chinese, and French.", "As shown in Table TABREF41, we use the En-En-QG and Zh-Zh-QG tasks to analyze the effects of using different fine-tuning strategies. It can be observed that fine-tuning encoder parameters, our model obtain an impressive performance for both English and Chinese QG, which shows the strong cross-lingual transfer ability of our model. When fine-tuning all the parameters, the model achieves the best score for English QG, but it suffers a performance drop when evaluating on Chinese QG. We find that fine-tuning decoder hurts cross-lingual decoding, and the model learns to only decodes English words. For only fine-tuning decoder, the performance degrades by a large margin for both languages because of the underfitting issue, which indicates the necessity of fine-tuning encoder."]}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "predicted_answer": "", "predicted_evidence": ["As shown in Figure FIGREF6(b), we propose a two-stage pre-training protocol for Xnlg. The first stage pretrains the encoding components, where the model learns to encode multilingual sentences to a shared embedding space. We consider using MLM and XMLM as the pre-training tasks. The objective of the first stage is to minimize: 1= (x,y) p XMLM(x,y) + x m MLM(x) where ${_{\\textnormal {p}}}$ indicates the parallel corpus, and ${_{\\textnormal {m}}}$ is the monolingual corpus.", "For fine-tuning on downstream NLG tasks, we use Adam optimizer with a learning rate of $5\\times 10^{-6}$. We set the batch size as 16 and 32 for question generation and abstractive summarization, respectively. When the target language is the same as the language of training data, we fine-tune all parameters. When the target language is different from the language of training data, we fine-tune the Transformer layers of the encoder. We truncate the input sentences to the first 256 tokens. During decoding, we use beam search with beam size of 3, and limit the length of the target sequence to 80 tokens.", "We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively.", "Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.", "Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM."]}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "predicted_answer": "", "predicted_evidence": ["In this paper, we propose a cross-lingual pre-trained model (named as Xnlg) in order to transfer monolingual NLG supervision to other pre-trained languages by fine-tuning. Specifically, Xnlg shares the same sequence-to-sequence model across languages, and is pre-trained with both monolingual and cross-lingual objectives. The model not only learns to understand multilingual input, but also is able to generate specific languages by conditioning on the encoded semantics. Figure FIGREF2 demonstrates how to use Xnlg to perform cross-lingual transfer for downstream tasks. The proposed model enables us to fine-tune the pre-trained model on monolingual NLG training data, and then evaluate it beyond a single language, including zero-shot cross-lingual generation. Besides, we explore several fine-tuning strategies to make a compromise between cross-lingual ability and task ability. In addition, we introduce two cross-lingual NLG datasets (i.e., question generation, and abstractive summarization) for evaluation, which includes three languages, namely English, Chinese, and French.", "We use the denoising auto-encoding (DAE) objective BIBREF24 to pretrain the encoder-decoder attention mechanism. Given sentence $x$ from the monolingual corpus, we use three types of noise to obtain the randomly perturbed text $\\hat{x}$. First, the word order is locally shuffled. Second, we randomly drop tokens of the sentence with a probability of $0.1$. Third, we substitute tokens with the special padding token P with a probability of $0.1$. The pre-training objective is to recover the original sentence $x$ by conditioning on $\\hat{x}$. The DAE loss is computed via: DAE(x) = -p(x|x) = -i = 1|x|p(xi | x, x<i) where $x_{<i}$ represents the tokens of previous time steps $x_1,\\cdots ,x_{i-1}$.", "We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively.", "For the Any-to-English NLG transfer, the decoder always generates English. So we can freeze the encoder parameters, and update the decoder parameters to retain the cross-lingual ability. As an alternative way, we can also fine-tune all the parameters to obtain the best results on the English dataset while having a slight drop in performance.", "The masked language modeling (MLM) BIBREF13 task, also known as the Cloze task BIBREF23, aims at predicting the randomly masked words according to their context. The objective pretrains the bidirectional encoder to obtain contextual representations. Following BIBREF13, we randomly mask 15% of the tokens in a monolingual sentence. For each masked token, we substitute it with a special token M, a random token, or the unchanged token with a probability of 0.8, 0.1, and 0.1, respectively. Let $x$ denote a sentence from the monolingual training corpus, and $M_{x}$ the set of randomly masked positions. The monolingual MLM loss is defined as: MLM(x) = -i Mxp( xi | xMx) where $x_{\\setminus M_{x}}$ is the masked version of input $x$. Notice that language tags are fed into the model for all pre-training tasks."]}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "predicted_answer": "", "predicted_evidence": ["The cold fusion mechanism of BIBREF3 pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure FIGREF13 ): DISPLAYFORM0", "Unlike tasks such as translation, where the semantics of the target are fully specified by the source, the generation of stories from prompts is far more open-ended. We find that seq2seq models ignore the prompt and focus solely on modeling the stories, because the local dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story.", "3 layers in encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 8 layers in the decoder with hidden unit sizes INLINEFORM2 with convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 nomalization INLINEFORM4 , 4 decoder self-attention heads.", "We have collected the first dataset for creative text generation based on short writing prompts. This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise. Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories.", "To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation."]}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "predicted_answer": "", "predicted_evidence": ["We have collected the first dataset for creative text generation based on short writing prompts. This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise. Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories.", "Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation BIBREF0 .", "(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search. The retrieved story from the training set is limited to 150 words to match the length of generated stories.", "Figure FIGREF27 shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a language model producing common words and punctuation. The second seq2seq model learns to focus on rare words, such as horned and robe.", "To train the fusion model, we first pretrain a Conv seq2seq with self-attention model on the WritingPrompts dataset. This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time. The two models are integrated with the fusion mechanism described in Section SECREF11 ."]}
{"question_id": "d66550f65484696c1284903708b87809ea705786", "predicted_answer": "", "predicted_evidence": ["High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.", "The challenges of WritingPrompts are primarily in modeling long-range dependencies and conditioning on an abstract, high-level prompt. Recurrent and convolutional networks have successfully modeled sentences BIBREF5 , BIBREF4 , but accurately modeling several paragraphs is an open problem. While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts. We will evaluate strategies to address these challenges in the following sections.", "Gated Attention: Similar to BIBREF9 , we use multi-head attention to allow each head to attend to information at different positions. However, the queries, keys and values are not given by linear projections but by more expressive gated deep neural nets with Gated Linear Unit BIBREF4 activations. We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections.", "We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of standard sequence models to drift off topic.", "We have collected the first dataset for creative text generation based on short writing prompts. This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise. Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories."]}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "predicted_answer": "", "predicted_evidence": ["We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of standard sequence models to drift off topic.", "The challenges of WritingPrompts are primarily in modeling long-range dependencies and conditioning on an abstract, high-level prompt. Recurrent and convolutional networks have successfully modeled sentences BIBREF5 , BIBREF4 , but accurately modeling several paragraphs is an open problem. While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts. We will evaluate strategies to address these challenges in the following sections.", "Previous work has proposed decomposing the challenge of generating long sequences of text into a hierarchical generation task. For instance, BIBREF19 use an LSTM to hierarchically learn word, then sentence, then paragraph embeddings, then transform the paragraph embeddings into text. BIBREF20 generate a discrete latent variable based on the context, then generates text conditioned upon it.", "Sequence-to-sequence neural networks BIBREF1 have achieved state of the art performance on a variety of text generation tasks, such as machine translation BIBREF1 and summarization BIBREF11 . Recent work has applied these models to more open-ended generation tasks, including writing Wikipedia articles BIBREF12 and poetry BIBREF13 .", "CNNs can only model a bounded context window, preventing the modeling of long-range dependencies within the output story. To enable modeling of unbounded context, we supplement the decoder with a self-attention mechanism BIBREF8 , BIBREF9 , which allows the model to refer to any previously generated words. The self-attention mechanism improves the model's ability to extract long-range context with limited computational impact due to parallelism."]}
{"question_id": "804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "predicted_answer": "", "predicted_evidence": ["Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.", "5 layers in the encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 5 layers in the decoder with hidden unit sizes INLINEFORM2 and convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 normalization INLINEFORM4 , 4 decoder self-attention heads.", "However, the fusion model has limitations. Using random sampling to generate can produce errors. For example, can't is tokenized to ca n't, and the model occasionally produces the first token but misses the second. A similar error is after one line of dialogue, the model may move to another line of dialogue without generating a newline token. A further obstacle is repetition. The model focuses frequently on what it has recently produced, which leads to the generation of similar text multiple times.", "(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.", "For our experiments, we limit the length of the stories to 1000 words maximum and limit the vocabulary size for the prompts and the stories to words appearing more than 10 times each. We model an unknown word token and an end of document token. This leads to a vocabulary size of 19,025 for the prompts and 104,960 for the stories. As the dataset is scraped from an online forum, the number of rare words and misspellings is quite large, so modeling the full vocabulary is challenging and computationally intensive."]}
{"question_id": "f2dba5bf75967407cce5d0a9c2618269225081f5", "predicted_answer": "", "predicted_evidence": ["The pretrained seq2seq model is the model in Section SECREF37 . The additional fused model has the following architecture:", "In the generation of prompts using the GCNN language model, we find that prompts are fairly generic compared to human prompts. Language models often struggle to model rare words accurately, as the probability distribution over the next word is dominated by more common words. This tends to produce similar prompts, particularly at the start \u2014 we see many prompts that start with the man. In contrast, many of the human prompts are very unique (e.g. prompting stories in fantasy worlds such as Harry Potter and Game of Thrones) and the language model rarely produces the specific vocabulary required by these settings.", "However, the fusion model has limitations. Using random sampling to generate can produce errors. For example, can't is tokenized to ca n't, and the model occasionally produces the first token but misses the second. A similar error is after one line of dialogue, the model may move to another line of dialogue without generating a newline token. A further obstacle is repetition. The model focuses frequently on what it has recently produced, which leads to the generation of similar text multiple times.", "For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.", "We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example."]}
{"question_id": "b783ec5cb9ad595da7db2c0ddf871152ae382c5f", "predicted_answer": "", "predicted_evidence": ["3 layers in encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 8 layers in the decoder with hidden unit sizes INLINEFORM2 with convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 nomalization INLINEFORM4 , 4 decoder self-attention heads.", "We analyze the effect of our modeling improvements on the WritingPrompts dataset.", "5 layers in the encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 5 layers in the decoder with hidden unit sizes INLINEFORM2 and convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 normalization INLINEFORM4 , 4 decoder self-attention heads.", "For prompt generation, we use a self-attentive GCNN language model trained with the same prompt-side vocabulary as the sequence-to-sequence story generation models. The language model to generate prompts has a validation perplexity of 63.06. Prompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is completed when the language model generates the end of prompt token.", "To train the fusion model, we first pretrain a Conv seq2seq with self-attention model on the WritingPrompts dataset. This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time. The two models are integrated with the fusion mechanism described in Section SECREF11 ."]}
{"question_id": "3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "predicted_answer": "", "predicted_evidence": ["A sparse target distribution INLINEFORM0 which satisfies INLINEFORM1 is computed as: DISPLAYFORM0", "BIBREF19 design a Dependency Tree-Structured LSTM for modeling sentences. This model outperforms the linear chain LSTM in STS tasks. Convolutional neural network (CNN) has recently been applied efficiently for semantic composition BIBREF0 , BIBREF20 , BIBREF5 . This technique uses convolutional filters to capture local dependencies in term of context windows and applies a pooling layer to extract global features. BIBREF21 use CNN to extract features at multiple level of granularity. The authors then compare their sentence representations via multiple similarity metrics at several granularities. BIBREF22 propose a hierarchical CNN-LSTM architecture for modeling sentences. In this approach, CNN is used as an encoder to encode an sentence into a continuous representation, and LSTM is used as a decoder. BIBREF23 train a sentence encoder on a textual entailment recognition database using a BiLSTM-Maxpooling network. This encoder achieves competitive results on a wide range of transfer tasks.", "As a result, we have a sentence-sentence similarity vector INLINEFORM0 as follows: DISPLAYFORM0", "We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower. In STSB, the prior top performance methods use ensemble approaches mixing hand-crafted features (word alignment, syntactic features, N-gram overlaps) and neural sentence representations, while our approach is only based on a neural sentence modeling architecture. In addition, we observed that InferSent shows the strong performance on SICK-R but quite low on STSB while our model consistently obtains the strong performances on both of the datasets. InferSent uses transfer knowledge on textual entailment data, consequently it obtains the strong performance on this entailment task.", "for INLINEFORM0 , and INLINEFORM1 is the similarity score."]}
{"question_id": "47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 respectively denote a logistic sigmoid function and element-wise multiplication; INLINEFORM1 are respectively two weights matrices and a bias vector for input gate INLINEFORM2 . The denotation is similar to forget gate INLINEFORM3 , output gate INLINEFORM4 , tanh layer INLINEFORM5 , memory cell INLINEFORM6 and hidden state INLINEFORM7 .", "Given two input sentences INLINEFORM0 and INLINEFORM1 , we encode them into two sequences of multi-aspect word embeddings INLINEFORM2 and INLINEFORM3 (Section 3.2). We then compute a word-word similarity vector INLINEFORM4 as follows: DISPLAYFORM0", "Neural similarity layers: the dimension of INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are respectively 50, 5, 5 and 100.", "In this section, we describe the process for evaluating the similarity/relation between two sentences. We compare two sentences via three levels: word-word, sentence-sentence and word-sentence.", "GloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens)."]}
{"question_id": "67cb001f8ca122ea859724804b41529fea5faeef", "predicted_answer": "", "predicted_evidence": ["for INLINEFORM0 , and INLINEFORM1 is the similarity score.", "Our main contributions are as follows:", "We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.", "This section describes two experiments: i) compare our model against recent systems; ii) evaluate the efficiency of using multiple pre-trained word embeddings.", "SL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 ."]}
{"question_id": "42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "predicted_answer": "", "predicted_evidence": ["Using our new dataset, we experiment with existing NLP features and compare results with a newly-proposed set of features. We designed these features to encode the dynamic relationship between a potential bully and victim, using comparative measures from their relative linguistic and social network profiles. Additionally, our features have low computational complexity, so they can scale to internet-scale datasets, unlike expensive network centrality and clustering measurements.", "The machine learning community has not reached a unanimous definition of cyberbullying either. They have instead echoed the uncertainty of the social scientists. Moreover, some authors have neglected to publish any objective cyberbullying criteria or even a working definition for their annotators, and among those who do, the formulation varies. This disagreement has slowed progress in the field, since classifiers and datasets cannot be as easily compared. Upon review, however, we found that all available definitions contained a strict subset of the following criteria: aggression (aggr), repetition (rep), harmful intent (harm), visibility among peers (peer), and power imbalance (power). The datasets built from these definitions are outlined in Table TABREF1.", "Internet users depend on content moderators to flag abusive text and ban cyberbullies from participating in online communities. However, due to the overwhelming volume of social media data produced every day, manual human moderation is often unfeasible. For this reason, social media platforms are beginning to rely instead on machine learning classifiers for automatic cyberbullying detection BIBREF7.", "The research community has developed increasingly competitive classifiers to detect harmful or aggressive content in text. Despite significant progress in recent years, however, existing models remain unfit for real-world applications. This is due, in part, to shortcomings in the training and testing data BIBREF8, BIBREF9, BIBREF10. Most annotation schemes have ignored the importance of social context, and researchers have neglected to provide annotators with objective criteria for distinguishing cyberbullying from other crude messages.", "After asking workers to review these examples, we gave them a short 7-question quiz to test their knowledge. Workers were given only one quiz attempt, and they were expected to score at least 6 out of 7 questions correctly before they could proceed to the paid HIT. Workers were then paid $\\$0.12$ for each thread that they annotated."]}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "predicted_answer": "", "predicted_evidence": ["In this study, we produced an original dataset for cyberbullying detection research and an approach that leverages this dataset to more accurately detect cyberbullying. Our labeling scheme was designed to accommodate the cyberbullying definitions that have been proposed throughout the literature. In order to more accurately represent the nature of cyberbullying, we decomposed this complex issue into five representative characteristics. Our classes distinguish cyberbullying from other related behaviors, such as isolated aggression or crude joking. To help annotators infer these distinctions, we provided them with the full context of each message's reply thread, along with a list of the author's most recent mentions. In this way, we secured a new set of labels for more reliable cyberbullying representations.", "Next, we consider inward and outward overlap. When the inward overlap is high, the author and target could have more common visibility. Similarly, if the outward overlap is high, then the author and target both follow similar accounts, so they might have similar interests or belong to the same social circles. Both inward and outward overlaps are expected to be higher when a post is visible among peers. This is true of both distributions in Figure FIGREF26. The difference in outward overlap is significant ($D=0.04$, $p=0.03$), and the difference for inward overlap is short of significant ($D=0.04$, $p=0.08$).", "Harmful intent is difficult to measure in isolated messages because social context determines pragmatic meaning. We attempt to approximate the author's harmful intent by measuring the linguistic \u201csurprise\u201d of a given message relative to the author's timeline history. We do this in two ways: through a simple ratio of new words, and through the use of language models.", "Mention overlap: Let $M_a$ be the set of all accounts mentioned by author $a$, and let $M_t$ be the set of all accounts mentioned by target $t$. We compute the ratio $\\frac{|M_a \\cap M_t|}{|M_a \\cup M_t|}$.", "We collected a sample of 1.3 million unlabeled tweets from the Twitter Filter API. Since cyberbullying is a social phenomenon, we chose to filter for tweets containing at least one \u201c@\u201d mention. To restrict our investigation to original English content, we removed all non-English posts and retweets (RTs), narrowing the size of our sample to 280,301 tweets."]}
{"question_id": "b857f3e3f1dad5df55f69d062978967fe023ac6f", "predicted_answer": "", "predicted_evidence": ["Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.", "Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.", "Since aggressive language is a key component of cyberbullying BIBREF12, we ran the pre-trained classifier of BIBREF35 over our dataset to identify hate speech and aggressive language and increase the prevalence of cyberbullying examples . This gave us a filtered set of 9,803 aggressive tweets.", "Our study focuses on the Twitter ecosystem and a small part of its network. The initial sampling of tweets was based on a machine learning classifier of aggressive English language. This classifier has an F1 score of 0.90 BIBREF35. Even with this filter, only 0.7% of tweets were deemed by a majority of MTurk workers as cyberbullying (Table TABREF17). This extreme class imbalance can disadvantage a wide range of machine learning models. Moreover, the MTurk workers exhibited only moderate inter-annotator agreement (Table TABREF17). We also acknowledge that notions of harmful intent and power imbalance can be subjective, since they may depend on the particular conventions or social structure of a given community. For these reasons, we recognize that cyberbullying still has not been unambiguously defined. Moreover, their underlying constructs are difficult to identify.", "The main contribution of our paper is not that we solved the problem of cyberbullying detection. Instead, we have exposed the challenge of defining and measuring cyberbullying activity, which has been historically overlooked in the research community."]}
{"question_id": "5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "predicted_answer": "", "predicted_evidence": ["Downward overlap measures the number of two-hop paths from the author to the target along following relationships; upward overlap measures two-hop paths in the opposite direction. Inward overlap measures the similarity between the two users' follower sets, and outward overlap measures the similarity between their sets of friends. Bidirectional overlap then is a more generalized measure of social network similarity. We provide a graphical depiction for each of these features on the right side of Figure FIGREF18.", "High downward overlap likely indicates that the target is socially relevant to the author, as high upward overlap indicates the author is relevant to the target. Therefore, when the author is more powerful, downward overlap is expected to be lower and upward overlap is expected be higher. This trend is slight but visible in the cumulative distribution functions of Figure FIGREF26 (a): downward overlap is indeed lower when the author is more powerful than when the users are equals ($D=0.143$). However, there is not a significant difference for upward overlap ($p=0.85$). We also observe that, when the target is more powerful, downward and upward overlap are both significantly lower ($D=0.516$ and $D=0.540$ respectively). It is reasonable to assume that messages can be sent to celebrities and other powerful figures without the need for common social connections.", "Harmful intent is difficult to measure in isolated messages because social context determines pragmatic meaning. We attempt to approximate the author's harmful intent by measuring the linguistic \u201csurprise\u201d of a given message relative to the author's timeline history. We do this in two ways: through a simple ratio of new words, and through the use of language models.", "The direct mention count measures the history of repeated communication between the author and the target. For harmful messages, downward overlap is higher ($D=0.178$) and upward overlap is lower ($D=0.374$) than for harmless messages, as shown in Figure FIGREF38. This means malicious authors tend to address the target repeatedly while the target responds with relatively few messages.", "To detect repeated aggression, we again employ the hate speech and offensive language classifier of BIBREF35. Each message is given a binary label according to the classifier-assigned class: aggressive (classified as hate speech or offensive language) or non-aggressive (classified as neither hate speech nor offensive language). From these labels, we derive the following features."]}
{"question_id": "235c7c7ca719068136928b18e19f9661e0f72806", "predicted_answer": "", "predicted_evidence": ["A cosine similarity of 1 means that users' timelines had identical counts across all weighted terms; a cosine similarity of 0 means that their timelines did not contain any words in common. We expect higher similarity scores between friends and associates.", "From these ground truth labels, we designed a new set of features to quantify each of the five cyberbullying criteria. Unlike previous text-based or user-based features, our features measure the relationship between a message author and target. We show that these features improve the performance of standard text-based models. These results demonstrate the relevance of social-network and language-based measurements to account for the nuanced social characteristics of cyberbullying.", "Maximum author retweets: The largest number of retweets the author received on a message in the thread.", "For the sake of comparison, we provide precision, recall, and $F_1$ scores for five different machine learning models: $k$-nearest neighbors (KNN), random forest, support vector machine (SVM), AdaBoost, and Multilayer Perceptron (MLP). Then we provide feature weights for our logistic regression model trained on each of the five cyberbullying criteria.", "Furthermore, because we lack the authority to define cyberbullying, we cannot assert a two-way implication between cyberbullying and the five criteria outlined here. It may be possible for cyberbullying to exist with only one criterion present, such as harmful intent. Our five criteria also might not span all of the dimensions of cyberbullying. However, they are representative of the literature in both the social science and machine learning communities, and they can be used in weighted combinations to accommodate new definitions."]}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "predicted_answer": "", "predicted_evidence": ["Using our proposed features from the previous section and ground truth labels from our annotation task, we trained a separate Logistic Regression classifier for each of the five cyberbullying criteria, and we report precision, recall, and $F_1$ measures over each binary label independently. We averaged results using five-fold cross-validation, with 80% of the data allocated for training and 20% of the data allocated for testing at each iteration. To account for the class imbalance in the training data, we used the synthetic minority over-sampling technique (SMOTE) BIBREF39. We did not over-sample testing sets, however, to ensure that our tests better match the class distributions obtained as we did by pre-filtering for aggressive directed Twitter messages.", "We compare our results across the five different feature combinations given in Table TABREF58. Note that because we do not include thread features in the User set, it can be used for cyberbullying prediction and early intervention. The Proposed set can be used for detection, sinct it is a collection of all newly proposed features, including thread features. The Combined adds these to the baseline text features.", "Here, we provide an original annotation framework and a new dataset for cyberbullying research, built to unify existing methods of ground truth annotation. In this dataset, we decompose the complex issue of cyberbullying into five key criteria, which were drawn from the social science and machine learning communities. These criteria can be combined and adapted for revised definitions of cyberbullying.", "Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).", "Internet users depend on content moderators to flag abusive text and ban cyberbullies from participating in online communities. However, due to the overwhelming volume of social media data produced every day, manual human moderation is often unfeasible. For this reason, social media platforms are beginning to rely instead on machine learning classifiers for automatic cyberbullying detection BIBREF7."]}
{"question_id": "c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "predicted_answer": "", "predicted_evidence": ["Finally, we trained our model with three categories of data namely: jokes, quotes, and tweets. We show that the network is able to generate texts belonging to the specific category when we pass the category as input along with the seed text.", "We combined multiple sources and de-duplicated them to arrive at a large corpus for training. The two sources for jokes are CrowdTruth and Subreddits. After cleaning, we ended up with 96910 jokes and a vocabulary size of 8922 words. The two sources for quotes are Quotables and the TheWebMiner. After cleaning, we ended up with 43383 quotes and a vocabulary size of 8916 words. We downloaded the scraped tweets from kaggle and ended up with 130250 tweets with a vocabulary size of 10805 words after cleaning. We constrained the vocabulary to about 10000 words in each case. Finally, we combined the jokes, quotes, and tweets along with their class labels (joke is 0, quote is 1, tweet is 2) into a single unified dataset. The combined dataset consists of 270543 sentences and a vocabulary size of 12614 words. Each sentence starts with a 'sos' tag and ends with a 'eos' tag to denote the start and end of sentences.", "The task of humor generation has been approached using deep neural networks with attention by BIBREF1 and unsupervised data mining to generate fixed-structure jokes by BIBREF0 . In the former work, an LSTM model with attention is used to generate jokes from a dataset consisting of 7699 jokes written by a single author giving the corpus a homogeneity of style. The jokes data is mixed with news data and a deep recurrent neural network is trained with weighted-pick strategy above the output layer to bring in randomness and a certain chance of producing funny sentences. This is the only work to the best of our knowledge which tried to mix jokes and non-jokes during training in order to bring in more information but it didn't train the network with a category tag so we have no control over what the model generates which makes judging it more subjective. It also means the network is trained in a kind of unsupervised manner when we could have trained it in a more supervised manner by telling it whether its a joke or not and later ask it to generate a joke specifically.", "The first experiment was training the model with just jokes.", "After cleaning, we ended up with 96910 jokes and a vocabulary size of 8922 words. The two sources for quotes are Quotables and the TheWebMiner. After cleaning, we ended up with 43383 quotes and a vocabulary size of 8916 words. We downloaded the scraped tweets from kaggle and ended up with 130250 tweets with a vocabulary size of 10805 words after cleaning. We constrained the vocabulary to about 10000 words in each case. Finally, we combined the jokes, quotes, and tweets along with their class labels (joke is 0, quote is 1, tweet is 2) into a single unified dataset. The combined dataset consists of 270543 sentences and a vocabulary size of 12614 words. Each sentence starts with a 'sos' tag and ends with a 'eos' tag to denote the start and end of sentences. The final datasets can be found on our github repository."]}
{"question_id": "9f1d81b2a6fe6835042a5229690e1951b97ff671", "predicted_answer": "", "predicted_evidence": ["For two sets INLINEFORM0 and INLINEFORM1 Jaccard score is defined as DISPLAYFORM0", "To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment as there is no proven system to measure the quality of content objectively.", "For example, the generated quote \"i love the music that i love because i love it . i can't tell you how to live without it\" and \"i am always training because i love it\" has a Phrase Overlap score of 0.66 because of the presence of the four-gram phrase \"because i love it\" and multiple occurrences of bigram phrase \"i love\" but these two texts are very different semantically. Overlap of bigrams, trigrams, and even 4-grams can be expected between texts with very different meaning but phrase overlap score heavily penalizes such matches.", "To evaluate the overall syntactic accuracy of our corpus we consider total percentage of sentences having at least one valid linkage at Null Count 0. We generated and randomly sampled 50 quotes, jokes, and tweets each and split them into sentences. From 150 generated texts we obtained 251 sentences and processed them adequately with capitalization. We have used exploration factor 0.1 while generating the texts. The results are presented in table TABREF42 . The accuracy is INLINEFORM0 i.e INLINEFORM1 of sentences were entirely correct and INLINEFORM2 were almost correct. Here we have to note that this numbers also includes sentences that were marked incorrect due to out of dictionary English words like iphone, dunno, gosh etc. and proper nouns.", "Finally, we trained our model with three categories of data namely: jokes, quotes, and tweets. We show that the network is able to generate texts belonging to the specific category when we pass the category as input along with the seed text."]}
{"question_id": "fae930129c2638ba6f9c9b3383e85aa130a73876", "predicted_answer": "", "predicted_evidence": ["Example: Life is all about the fact that I have to go to work today.", "In the correct order: sos what's the difference between being hungry and horny ? you can smell it . eos", "A variant of RNN called Long Short-Term Memory (LSTM) introduced by BIBREF7 have been shown to perform better than conventional RNNs by BIBREF8 which overcomes the above-mentioned modeling limitations of RNNs. LSTMs use three gates to regulate the hidden state variable of LSTM which functions as the memory unit.", "The lower the INLINEFORM0 value the novel the generated text which is indicative of the fact that our model is able to generalize beyond the training instances. For our experiment, we randomly sampled 100 instances from the training data and split the instances into two halves. We use the prefix half as the seed text to generate the categorical text (control tag set to 0,1 and 2). We then average the maximums of the Phrase Overlap metric between the generated texts (excluding the seed text) and all remaining training examples in our corpus to arrive at our final aggregated similarity score.", "The two sources for quotes are Quotables and the TheWebMiner. After cleaning, we ended up with 43383 quotes and a vocabulary size of 8916 words. We downloaded the scraped tweets from kaggle and ended up with 130250 tweets with a vocabulary size of 10805 words after cleaning. We constrained the vocabulary to about 10000 words in each case. Finally, we combined the jokes, quotes, and tweets along with their class labels (joke is 0, quote is 1, tweet is 2) into a single unified dataset. The combined dataset consists of 270543 sentences and a vocabulary size of 12614 words. Each sentence starts with a 'sos' tag and ends with a 'eos' tag to denote the start and end of sentences. The final datasets can be found on our github repository. When we train the controlled LSTM with the combined data, we use weighted sample strategy so that the three categories contribute equally to loss even though their numbers are different."]}
{"question_id": "1acfbdc34669cf19a778aceca941543f11b9a861", "predicted_answer": "", "predicted_evidence": ["The first capsule layer consists of INLINEFORM0 capsules, for which each capsule INLINEFORM1 has a vector output INLINEFORM2 . Vector outputs INLINEFORM3 are multiplied by weight matrices INLINEFORM4 to produce vectors INLINEFORM5 which are summed to produce a vector input INLINEFORM6 to the capsule in the second layer. The capsule then performs the non-linear squashing function to produce a vector output INLINEFORM7 : DISPLAYFORM0", "Given a user, a submitted query and the documents returned by a search system for that query, our approach is to re-rank the returned documents so that the more relevant documents should be ranked higher. Following BIBREF12 , we represent the relationship between the submitted query, the user and the returned document as a (s, r, o)-like triple (query, user, document). The triple captures how much interest a user puts on a document given a query. Thus, we can evaluate the effectiveness of our CapsE for the search personalization task.", "That historical information can be used to build the user profile, which is crucial to an effective search personalization system. Widely used approaches consist of two separated steps: (1) building the user profile from the interactions between the user and the search system; and then (2) learning a ranking function to re-rank the search results using the user profile BIBREF9 , BIBREF33 , BIBREF10 , BIBREF11 . The general goal is to re-rank the documents returned by the search system in such a way that the more relevant documents are ranked higher. In this case, apart from the user profile, dozens of other features have been proposed as the input of a learning-to-rank algorithm BIBREF9 , BIBREF33 . Alternatively, BIBREF12 modeled the potential user-oriented relationship between the submitted query and the returned document by applying TransE to reward higher scores for more relevant documents (e.g., clicked documents).", "We denote INLINEFORM0 , INLINEFORM1 and INLINEFORM2 as the INLINEFORM3 -dimensional embeddings of INLINEFORM4 , INLINEFORM5 and INLINEFORM6 , respectively. In our proposed CapsE, we follow BIBREF15 to view each embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] as a matrix INLINEFORM10 , and denote INLINEFORM11 as the INLINEFORM12 -th row of INLINEFORM13 . We use a filter INLINEFORM14 operated on the convolution layer. This filter INLINEFORM15 is repeatedly operated over every row of INLINEFORM16 to generate a feature map INLINEFORM17 , in which INLINEFORM18 where INLINEFORM19 denotes a dot product, INLINEFORM20 is a bias term and INLINEFORM21 is a non-linear activation function such as ReLU. Our model uses multiple filters INLINEFORM22 to generate feature maps. We denote INLINEFORM23 as the set of filters and INLINEFORM24 as the number of filters, thus we have INLINEFORM25 INLINEFORM26 -dimensional feature maps, for which each feature map can capture one single characteristic among entries at the same dimension.", "For search tasks, unlike classical methods, personalized search systems utilize the historical interactions between the user and the search system, such as submitted queries and clicked documents to tailor returned results to the need of that user BIBREF7 , BIBREF8 . That historical information can be used to build the user profile, which is crucial to an effective search personalization system. Widely used approaches consist of two separated steps: (1) building the user profile from the interactions between the user and the search system; and then (2) learning a ranking function to re-rank the search results using the user profile BIBREF9 , BIBREF33 , BIBREF10 , BIBREF11 . The general goal is to re-rank the documents returned by the search system in such a way that the more relevant documents are ranked higher."]}
{"question_id": "864295caceb1e15144c1746ab5671d085d7ff7a1", "predicted_answer": "", "predicted_evidence": ["That historical information can be used to build the user profile, which is crucial to an effective search personalization system. Widely used approaches consist of two separated steps: (1) building the user profile from the interactions between the user and the search system; and then (2) learning a ranking function to re-rank the search results using the user profile BIBREF9 , BIBREF33 , BIBREF10 , BIBREF11 . The general goal is to re-rank the documents returned by the search system in such a way that the more relevant documents are ranked higher. In this case, apart from the user profile, dozens of other features have been proposed as the input of a learning-to-rank algorithm BIBREF9 , BIBREF33 . Alternatively, BIBREF12 modeled the potential user-oriented relationship between the submitted query and the returned document by applying TransE to reward higher scores for more relevant documents (e.g., clicked documents).", "We see that the length and orientation of each capsule in the first layer can also help to model the important entries in the corresponding dimension, thus CapsE can work well on the \u201cside M\u201d of triples where entities often appear less frequently than others appearing in the \u201cside 1\u201d of triples. Additionally, existing models such as DISTMULT, ComplEx and ConvE can perform well for entities with high frequency, but may not for rare entities with low frequency. These are reasons why our CapsE can be considered as the best one on FB15k-237 and it outperforms most existing models on WN18RR.", "Datasets: We use two recent benchmark datasets WN18RR BIBREF17 and FB15k-237 BIBREF18 . These two datasets are created to avoid reversible relation problems, thus the prediction task becomes more realistic and hence more challenging BIBREF18 . Table TABREF7 presents the statistics of WN18RR and FB15k-237.", "Table TABREF17 presents the experimental results of the baselines and our model. Embedding models TranE, ConvKB and CapsE produce better ranking performances than traditional learning-to-rank search personalization models CI and SP. This indicates a prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. In particular, our MRR and Hits@1 scores are higher than those of TransE (with relative improvements of 14.5% and 22% over TransE, respectively). Specifically, our CapsE achieves the highest performances in both MRR and Hits@1 (our improvements over all five baselines are statistically significant with INLINEFORM0 using the paired t-test).", "To illustrate our training progress, we plot performances of CapsE on the validation set over epochs in Figure FIGREF18 . We observe that the performance is improved with the increase in the number of filters since capsules can encode more useful properties for a large embedding size."]}
{"question_id": "79e61134a6e29141cd19252571ffc92a0b4bc97f", "predicted_answer": "", "predicted_evidence": ["The contribution of this paper is two-fold. First, we propose a new strategy to improve the fine-tune-only strategy proposed by Peter et al. BIBREF0 , this allows us to achieve better results, at least on the selected tasks. More importantly, the results of this study demonstrate the importance of neural networks design, even in the presence of all-powerful pre-trained language models. Second, during the experiment, we have found that although simply using the proposed training strategy can result in higher accuracies compared to that of Peter et al. BIBREF0 , it is still a challenging task to find the appropriate methods to design and to utilize pre-trained networks. In this regard, we find that pre-trained models differ significantly from word embeddings in terms of their training strategies.", "As is shown in Table 2, even without modifying the networks to specifically adapt to the pre-trained model, our training strategy still brought improvement towards overall accuracy of 0.99% for the accuracy and 0.068 on the F1 score, proving the success of our proposed methods.", "In this aspect, however, there is a simple yet crucial question that needs to be addressed. That is, whether it is possible to top BERT with the commonly used or task specific layers, and if this is possible, how to best utilize the pre-trained language models in this situation. In this regards, Peters et al. BIBREF0 investigated how to best adapt the pre-trained model to a specific task, and focused on two different adaptation method,feature extraction and directly fine-tuning the pre-trained model, which corresponding to the strategy finetune-only and the strategy stack-only in Table TABREF1 . On this regard, Peters et al.", "However, the most successful ones are BERT BIBREF1 and Open-GPT BIBREF2 . Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer BIBREF16 structures, instead of LSTM BIBREF17 or GRU BIBREF18 . The difference between BERT and Open-GPT is that BERT uses bi-directional self-attentions while Open-GPT uses only unidirectional ones, as shown in Figure FIGREF2 . The transformer structures differ from the LSTM's in the two important aspects. First, it allows for stacking of multiple layers with residual connections and batch normalizations, which allows for free gradient flow. Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU BIBREF19 . After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD BIBREF3 , CoQA BIBREF4 , named entity recognition BIBREF5 , Glue BIBREF6 , machine translation BIBREF7 .", "In classical embedding + training networks, the general training method is to fix the word-embeddings, then train the top model until it converges, and finally fine-tuning the word-embeddings for a few epochs. This training strategy does not work when we replace pre-trained language models with word-embeddings. In our experiment, we first fix the pre-trained language models, and then we train the top neural networks only for a few epochs, until it reaches a reasonable accuracy, while closely monitoring the discrepancy between training accuracy and testing accuracy. After that, we fine-tune the pre-trained language model as well as our models on top together. This allows us to achieve better results on the experimentation. However, it is not yet clear to us when to stop the training of top neural networks. This poses an even more essential question for Auto ML researchers in the following sense."]}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "predicted_answer": "", "predicted_evidence": ["Under the strategy finetune-only, we use only single BERT.In order to adapt to different tasks, we will add a fully connected layer upon BERT. In the sequence labeling task, the BERT word embedding of each word passes through two fully connected layers, and the prediction probability of named entity can be obtained. In the next two verification tasks, we use \u201c[CLS]\u201d for prediction and add two fully connected layers subsequently. Under our strategy stack-and-finetune, we set different learning rates for the two phases. We tried to set the learning rate of the first stage to INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 , and set it to a smaller number in the latter stage, such as INLINEFORM5 , INLINEFORM6 , INLINEFORM7 and INLINEFORM8 .", "That is, whether it is possible to top BERT with the commonly used or task specific layers, and if this is possible, how to best utilize the pre-trained language models in this situation. In this regards, Peters et al. BIBREF0 investigated how to best adapt the pre-trained model to a specific task, and focused on two different adaptation method,feature extraction and directly fine-tuning the pre-trained model, which corresponding to the strategy finetune-only and the strategy stack-only in Table TABREF1 . On this regard, Peters et al. BIBREF0 performs five experiments, including: (1) named entity recognition BIBREF5 ; (2) sentiment analysis BIBREF24 ; (3) natural language inference BIBREF25 ; (4) paraphrase detection BIBREF26 ; (5) semantic textual similarity BIBREF27 . By the results of these tasks, Peters et al.", "Some early attempts include pre-trained models includes, CoVe BIBREF11 , CVT BIBREF12 , BIBREF13 , ELMo BIBREF14 and ULMFiT BIBREF15 . However, the most successful ones are BERT BIBREF1 and Open-GPT BIBREF2 . Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer BIBREF16 structures, instead of LSTM BIBREF17 or GRU BIBREF18 . The difference between BERT and Open-GPT is that BERT uses bi-directional self-attentions while Open-GPT uses only unidirectional ones, as shown in Figure FIGREF2 . The transformer structures differ from the LSTM's in the two important aspects. First, it allows for stacking of multiple layers with residual connections and batch normalizations, which allows for free gradient flow.", "We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score.", "The aforementioned deficiencies prompt researchers to propose deep neural networks that are able to be trained in an unsupervised fashion while being able to capture the contextual meaning of the words presented in the texts. Some early attempts include pre-trained models includes, CoVe BIBREF11 , CVT BIBREF12 , BIBREF13 , ELMo BIBREF14 and ULMFiT BIBREF15 . However, the most successful ones are BERT BIBREF1 and Open-GPT BIBREF2 . Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer BIBREF16 structures, instead of LSTM BIBREF17 or GRU BIBREF18 . The difference between BERT and Open-GPT is that BERT uses bi-directional self-attentions while Open-GPT uses only unidirectional ones, as shown in Figure FIGREF2 . The transformer structures differ from the LSTM's in the two important aspects."]}
{"question_id": "5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "predicted_answer": "", "predicted_evidence": ["In this aspect, however, there is a simple yet crucial question that needs to be addressed. That is, whether it is possible to top BERT with the commonly used or task specific layers, and if this is possible, how to best utilize the pre-trained language models in this situation. In this regards, Peters et al. BIBREF0 investigated how to best adapt the pre-trained model to a specific task, and focused on two different adaptation method,feature extraction and directly fine-tuning the pre-trained model, which corresponding to the strategy finetune-only and the strategy stack-only in Table TABREF1 . On this regard, Peters et al. BIBREF0 performs five experiments, including: (1) named entity recognition BIBREF5 ; (2) sentiment analysis BIBREF24 ; (3) natural language inference BIBREF25 ; (4) paraphrase detection BIBREF26 ; (5) semantic textual similarity BIBREF27 .", "We choose this strategy for the following reasons. Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. BIBREF1 , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization BIBREF28 . If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded. Therefore, in the first phase we fix the weights in the pre-training language models, and only train the model on top of it.", "Another aspect that is worth commenting in the first phase is that it is most beneficial that one does not train the top model until it reaches the highest accuracy on the training or validation data sets, but rather only up to a point where the prediction accuracy of the training and validation data sets do not differ much. This is intuitively reasonable for the following reasons. Unlike word embeddings, the pre-trained language models possess a large number of parameters compared to the task-specific models we build on top them. Therefore, if one were to train the top models until they reach the highest prediction accuracy in the training or validation data sets, it would likely cause the models to over-fit. Therefore, in our experiment, we found that this leads to the highest performance increase in the fine-tuning stage.", "Under our strategy stack-and-finetune, the model training process is divided into two phases, which are described in detail below. In the first phase, the parameters of the pre-training model are fixed, and only the upper-level models added for a specific task is learned. In the second phase, we fine-tune the upper-level models together with the pre-trained language models. We choose this strategy for the following reasons. Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. BIBREF1 , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization BIBREF28 .", "That is, whether it is possible to top BERT with the commonly used or task specific layers, and if this is possible, how to best utilize the pre-trained language models in this situation. In this regards, Peters et al. BIBREF0 investigated how to best adapt the pre-trained model to a specific task, and focused on two different adaptation method,feature extraction and directly fine-tuning the pre-trained model, which corresponding to the strategy finetune-only and the strategy stack-only in Table TABREF1 . On this regard, Peters et al. BIBREF0 performs five experiments, including: (1) named entity recognition BIBREF5 ; (2) sentiment analysis BIBREF24 ; (3) natural language inference BIBREF25 ; (4) paraphrase detection BIBREF26 ; (5) semantic textual similarity BIBREF27 . By the results of these tasks, Peters et al. BIBREF0 concludes that adding a light task-specific head and performing fine-tuning on BERT is better than building a complex network on top without BERT fine-tuning."]}
{"question_id": "7d539258b948cd5b5ad1230a15e4b739f29ed947", "predicted_answer": "", "predicted_evidence": [". You would have to say to:RecipientGoal them: \u201cI saw a house that costs $$20,000$.\u201d (en_lpp_1943.172).", "DurationInstrument", "Of these new construals, BeneficiaryExperiencer has the highest frequency in the corpus. The novelty of this construal lies in the possibility of Experiencer as function in Chinese, shown by the parallel examples in eg:enbenibeni,eg:zhbeniexpe, where 4 receives the construal annotation BeneficiaryExperiencer.", "The 2 adpositions annotated with RecipientDirection are 4 and 4, both meaning `towards' in Chinese. In eg:enrecipient,eg:zhrecipientdirection, both English to and Chinese 4 have Recipient as the scene role. In eg:enrecipient, Goal is labelled as the function of to because it indicates the completion of the \u201csaying\u201d event. In Chinese, 4 has the function label Direction provided that 4 highlights the orientation of the message uttered by the speaker as in eg:zhrecipientdirection. Even though they express the same scene role in the parallel corpus, their lexical semantics still requires them to have different functions in English versus Chinese.", "CircumstanceTime"]}
{"question_id": "9c1f70affc87024b4280f0876839309b8dddd579", "predicted_answer": "", "predicted_evidence": ["Similar to the distinction between RecipientGoal and RecipientDirection in English versus Chinese, language-specific lexical semantics contribute to unique construals in Chinese, i.e. semantic uses of adpositions that are unattested in the STREUSLE corpus. Six construals are newly attested in the Chinese corpus:", "children P:to adults should lenient comp", "Hence, further SNACS annotation and disambiguation efforts on Chinese adpositions cannot rely on the StanfordNLP adp category to identify annotation targets. Since adpositions mostly belong to a closed set of tokens, we apply a simple rule to identify all attested adpositions which are not functioning as the main predicate of a sentence, i.e., not the root of the dependency tree. As shown in Table TABREF43, our heuristic results in an $F_1$ of 82.4%, outperforming the strategy of using the StanfordNLP POS tagger.", "2sg must P:to 3pl say 1sg see asp one CL $10,000$ franc de house", "The 2 adpositions annotated with RecipientDirection are 4 and 4, both meaning `towards' in Chinese. In eg:enrecipient,eg:zhrecipientdirection, both English to and Chinese 4 have Recipient as the scene role. In eg:enrecipient, Goal is labelled as the function of to because it indicates the completion of the \u201csaying\u201d event. In Chinese, 4 has the function label Direction provided that 4 highlights the orientation of the message uttered by the speaker as in eg:zhrecipientdirection. Even though they express the same scene role in the parallel corpus, their lexical semantics still requires them to have different functions in English versus Chinese."]}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "predicted_answer": "", "predicted_evidence": ["There are a few observations in these distributions that are of particular interest. For some of the examples, we use an annotated subset of the English Little Prince corpus for qualitative comparisons, whereas all quantitative results in English refer to the larger STREUSLE corpus of English Web Treebank reviews BIBREF0.", "The distribution of scene role and function types in Chinese and English reflects the differences and similarities of adposition semantics in both languages. In tab:statssupersensezhen we compare this corpus with the largest English adposition supersense corpus, STREUSLE version 4.1 BIBREF0, which consists of web reviews. We note that the Chinese corpus is proportionally smaller than the English one in terms of token and adposition counts. Moreover, there are fewer scene role, function and construal types attested in Chinese. The proportion of construals in which the scene role differs from the function (scene$\\ne $fxn) is also halved in Chinese. In this section, we delve into comparisons regarding scene roles, functions, and full construals between the two corpora both quantitatively and qualitatively.", "For this corpus we have adapted schneider-etal-2018-comprehensive Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see sec:snacs) to Chinese. Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (subsec:adpositioncriteria), we apply the SNACS supersenses to a translation of The Little Prince (3 2 3), finding the supersenses to be robust and achieving high inter-annotator agreement (sec:corpus-annotation). We analyze the distribution of adpositions and supersenses in the corpus, and compare to adposition behavior in a separate English corpus (see sec:corpus-analysis). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (sec:adpositionidentification). The annotated corpus and the Chinese guidelines for SNACS will be made freely available online.", "`The sheep you wanted is in the box.' (zh_lpp_1943.92)", ". [4 de] 4 [[4 2 32] 44 12 [4 1 4 34]]"]}
{"question_id": "65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "predicted_answer": "", "predicted_evidence": ["Armand Nasseri: Project Planning, Dataset search, Code for SVM, Data Cleanup (StopWord Cleanup), ROC model running, PowerPoint Development, Report Analysis about SVM", "", "http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf", "Bigrams", "LSTM Loss and Accuracy"]}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "predicted_answer": "", "predicted_evidence": ["LSTM Loss and Accuracy", "[4] Liang Wu and Huan Liu, \u201cTracing Fake-News Footprints: Characterizing Social Media Messages by How They Propagate\u201d, February 2018,", "Zhiyuan Guo: Project Planning, DataSet Search, Polarity Graphs, Code for LSTM, RandomForest, Adding Functionality and Readability in each of the scripts, Code Integration, Grid Search model running, ROC model running, PowerPoint Development, Report Analysis for TFIDF and LSTM, Report Analysis for the Abstract, the Discussion, Conclusion, Pipeline Diagram, Report editing", "Although TF-IDF is an old algorithm, it is simple and effective to be used in the phase of pre-training$^{11}$. The computation of TfidfVectorizer involves computing the product of term frequency and inverse document frequency. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in$^{12}$.", ""]}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent.", "In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem.", "In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers."]}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_answer": "", "predicted_evidence": ["In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "In this section I present the extracted features partitioned in six groups and detail each of them separately.", "Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers."]}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "predicted_answer": "", "predicted_evidence": ["In this section I present the extracted features partitioned in six groups and detail each of them separately.", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.", "As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent.", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0"]}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "predicted_answer": "", "predicted_evidence": ["While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.", "In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "In this section I present the extracted features partitioned in six groups and detail each of them separately.", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics."]}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "In this section I present the extracted features partitioned in six groups and detail each of them separately.", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.", "As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent."]}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "predicted_answer": "", "predicted_evidence": ["In collecting and preprocessing the English texts we followed the same procedure as for the source language corpus, i.e., we manually created queries containing metadata descriptions of English books (e.g. author names) corresponding to German books which then were scraped. The spaCy model for sentence segmentation used a large English web corpus. See Section sourcecorpus. for more information.", "Bilingual text-to-text alignments", "Our corpus is structured in following folders:", "alignment maps produced by aeneas", "Table TABREF54 shows the results of our manual evaluation. The audio-text alignment was rated as in general as high quality. The text-text alignment rating increases corresponding to increasing hunalign confidence score which shows that the latter can be safely used to find a threshold for corpus filtering. Overall, the audio-text and text-text alignment scores are very similar to those reported by KocabiyikogluETAL:18."]}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "predicted_answer": "", "predicted_evidence": ["alignment maps produced by aeneas", "We presented a corpus of aligned triples of German audio, German text, and English translations for speech translation from German to English. The audio data in our corpus are read speech, based on German audio books, ensuring a low amount of speech disfluencies. The audio-text alignment and text-to-text sentence alignment was done with state-of-the-art alignment tools and checked to be of high quality in a manual evaluation. The audio-text alignment was generally rated very high. The text-text sentence alignment quality is comparable to widely used corpora such as that of KocabiyikogluETAL:18. A cutoff on a sentence alignment quality score allows to filter the text alignments further for speech translation, resulting in a clean corpus of $50,427$ German-English sentence pairs aligned to 110 hours of German speech. A larger version of the corpus, comprising 133 hours of German speech and high-quality alignments to German transcriptions is available for speech recognition.", "What has happened ? he asked.", "Data filtering based on hunalign alignment scores", "Und h\u00e4tten drei\u00dfigtausend Helfer sich ersehn."]}
{"question_id": "b569827ecd04ae8757dc3c9523ab97e3f47a6e00", "predicted_answer": "", "predicted_evidence": ["Given a bag of sentences $B^k = \\lbrace s^k_1, \\dots , s^k_{m^k}\\rbrace $ where each sentence contains common entity pair (i.e., head entity $e^k_h,$ and tail entity $e^k_t$), the target of relation extraction is to predict the relation $y^k$ between the two entities. For a clear demonstration, we omit indices of example and sentence in remainder if no confusion caused. Each sentence is a sequence of tokens, i.e., $s = [w_1, \\dots , w_n]$, where $n$ is the length of the sentence.", "We compare our proposed approach with extensive previous ones, including feature-engineering, competitive and state-of-the-art approaches, which are briefly summarized in the following.", "To maintain efficiency of proposed approach, we adopt the recently-promoted self-attention mechanism BIBREF16, BIBREF10, BIBREF17, BIBREF18, BIBREF19 for compressing a sequence of token representations into a sentence-level vector representation by exploiting global dependency, rather than computation-consuming pairwise ones BIBREF13. It is used to measure the contribution or importance of each token to relation extraction task w.r.t. the global dependency. Formally, given the entity-aware embedding $\\mathbf {X}$, we first calculate attention probabilities by a parameterized compatibility function, i.e.,", "To further verify the effectiveness of each module in the proposed framework, we conduct an extensive ablation study in this section. In particular, SeG w/o Ent denotes removing entity-aware embedding, SeG w/o Gate denotes removing selective gate and concatenating two representations from PCNN and self-attention, SeG w/o Gate w/o Self-Attn denotes removing self-attention enhanced selective gate. In addition, we also replace the some parts of the proposed framework with baseline module for an in-depth comparison. SeG+ATT denotes replacing mean-pooing with selective attention, and SeG w/ stack denotes using stacked PCNN and self-attention rather than in parallel.", "MIML BIBREF24 is a multi-instance, multi-label learning framework that jointly models both multiple instances and multiple relations."]}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "predicted_answer": "", "predicted_evidence": ["In particular, $\\mathbf {H}^{(1)}$, $\\mathbf {H}^{(2)}$ and $\\mathbf {H}^{(3)}$ are three consecutive parts of $\\mathbf {H}$, obtained by dividing $\\mathbf {H}$ according to the positions of head and tail entities. Consequently, $\\mathbf {s} \\in \\mathbb {R}^{3d_c}$ is the resulting sentence vector representation.", "Given a sentence bag $B = [s_1, \\dots , s_m]$ with common entity pair, where $m$ is the number of sentences. As elaborated in Section SECREF6, we can obtain $\\mathbf {S} = [\\mathbf {s}_1, \\dots , \\mathbf {s}_m]$ and $\\mathbf {U} = [\\mathbf {u}_1, \\dots , \\mathbf {u}_m]$ for each sentence in the bag, which are derived from PCNN and self-attention respectively.", "Given a bag of sentences $B^k = \\lbrace s^k_1, \\dots , s^k_{m^k}\\rbrace $ where each sentence contains common entity pair (i.e., head entity $e^k_h,$ and tail entity $e^k_t$), the target of relation extraction is to predict the relation $y^k$ between the two entities. For a clear demonstration, we omit indices of example and sentence in remainder if no confusion caused. Each sentence is a sequence of tokens, i.e., $s = [w_1, \\dots , w_n]$, where $n$ is the length of the sentence.", "Moreover, for proposed approach and comparative ones, we also show AUC curves and available numerical values in Figure FIGREF31 and Table TABREF32 respectively. The empirical results for AUC are coherent with those of P@N, which shows that, our proposed approach can significantly improve previous ones and reach a new state-of-the-art performance by handling wrongly labeled problem using context-aware selective gate mechanism. Specifically, our approach substantially improves both PCNN+HATT and PCNN+BAG-ATT by 21.4% in aspect of AUC for precision-recall.", "Unlike previous works under multi-instance framework that frequently use a selective attention module to aggregate sentence-level representations into bag-level one, we propose a innovative selective gate mechanism to perform this aggregation. The selective gate can mitigate problems existing in distantly supervised relation extraction and achieve a satisfactory empirical effectiveness. Specifically, when handling the noisy instance problem, selective attention tries to produce a distribution over all sentence in a bag; but if there is only one sentence in the bag, even the only sentence is wrongly labeled, the selective attention mechanism will be low-effective or even completely useless. Note that almost $80\\%$ of bags from popular relation extraction benchmark consist of only one sentence, and many of them suffer from the wrong label problem. In contrast, our proposed gate mechanism is competent to tackle such case by directly and dynamically aligning low gating value to the wrongly labeled instances and thus preventing noise representation being propagated."]}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "predicted_answer": "", "predicted_evidence": ["Compared to the baseline framework (i.e., selective attention for multi-instance learning), SeG is able to produce entity-aware embeddings and rich-contextual representations to facilitate downstream aggregation modules that stably learn from noisy training data. Moreover, SeG uses gate mechanism with pooling to overcome problem occurring in selective attention, which is caused by one-sentence bags. In addition, it still keeps a light-weight structure to ensure the scalability of this model.", "As illustrated in Figure FIGREF2, we propose a novel neural network, i.e., SeG, for distantly supervised relation extraction, which is composed of following neural components.", "Given a bag of sentences $B^k = \\lbrace s^k_1, \\dots , s^k_{m^k}\\rbrace $ where each sentence contains common entity pair (i.e., head entity $e^k_h,$ and tail entity $e^k_t$), the target of relation extraction is to predict the relation $y^k$ between the two entities. For a clear demonstration, we omit indices of example and sentence in remainder if no confusion caused. Each sentence is a sequence of tokens, i.e., $s = [w_1, \\dots , w_n]$, where $n$ is the length of the sentence. In addition, each token has a low-dimensional dense-vector representation, i.e., $[\\mathbf {v}_1, \\cdots , \\mathbf {v}_n] \\in \\mathbb {R}^{d_w \\times n}$, where $d_w$ denotes the dimension of word embedding.", "Given a bag of sentences $B^k = \\lbrace s^k_1, \\dots , s^k_{m^k}\\rbrace $ where each sentence contains common entity pair (i.e., head entity $e^k_h,$ and tail entity $e^k_t$), the target of relation extraction is to predict the relation $y^k$ between the two entities. For a clear demonstration, we omit indices of example and sentence in remainder if no confusion caused. Each sentence is a sequence of tokens, i.e., $s = [w_1, \\dots , w_n]$, where $n$ is the length of the sentence.", "In addition to the typical word embedding, relative position is a crucial feature for relation extraction, which can provide downstream neural model with rich positional information BIBREF8, BIBREF3. Relative positions explicitly describe the relative distances between each word $w_i$ and the two targeted entities $e_h$ and $e_t$. For $i$-th word, a randomly initialized weight matrix projects the relative position features into a two dense-vector representations w.r.t the head and tail entities, i.e., $\\mathbf {r}^{e_h}_i$ and $\\mathbf {r}^{e_t}_i\\in \\mathbb {R}^{d_r}$ respectively."]}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "predicted_answer": "", "predicted_evidence": ["Traditional approaches BIBREF0 , BIBREF1 , BIBREF2 for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning, there has been much interest in applying deep neural network based techniques to further improve the prediction performances BIBREF3 , BIBREF4 , BIBREF5 .", "where INLINEFORM0 is the number of training pairs and the superscript INLINEFORM1 indicates the INLINEFORM2 -th sentence pair BIBREF10 .", "One shortcoming of conventional RNNs is that they are only able to make use of previous context. In text entailment, the decision is made after the whole sentence pair is digested. Therefore, exploring future context would be better for sequence meaning representation. Bidirectional RNNs architecture BIBREF13 proposed a solution of making prediction based on future words. At each time step INLINEFORM0 , the model maintains two hidden states, one for the left-to-right propagation INLINEFORM1 and the other for the right-to-left propagation INLINEFORM2 . The hidden state of the Bidirectional LSTM is the concatenation of the forward and backward hidden states. The following equations illustrate the main ideas: DISPLAYFORM0", "where INLINEFORM0 and INLINEFORM1 are the element-wise sigmoid and hyperbolic tangent functions, INLINEFORM2 is the element-wise multiplication operator.", "The LSTM architecture BIBREF15 addresses the problem of learning long range dependencies by introducing a memory cell that is able to preserve state over long periods of time. Concretely, at each time step INLINEFORM0 , the LSTM unit can be defined as a collection of vectors in INLINEFORM1 : an input gate INLINEFORM2 , a forget gate INLINEFORM3 , an output gate INLINEFORM4 , a memory cell INLINEFORM5 and a hidden state INLINEFORM6 . We refer to INLINEFORM7 as the memory dimensionality of the LSTM. One step of an LSTM takes as input INLINEFORM8 , INLINEFORM9 , INLINEFORM10 and produces INLINEFORM11 , INLINEFORM12 via the following transition equations: DISPLAYFORM0"]}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_answer": "", "predicted_evidence": ["Referring to textual entailment recognition task, we want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label INLINEFORM0 given the inputs INLINEFORM1 is predicted by a softmax classifier that takes the hidden state INLINEFORM2 at the node as input: DISPLAYFORM0", "A key component of deep neural network is word embedding which serve as an lookup table to get word representations. From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling BIBREF6 , BIBREF7 , to high level tasks such as machine translation, information retrieval and semantic analysis BIBREF8 , BIBREF9 , BIBREF10 . Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. On the other hand, some researchers have found character-level convolutional networks BIBREF11 , BIBREF12 are useful in extracting information from raw signals for the task such as language modeling or text classification.", "We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) BIBREF6 , Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) BIBREF13 to learn sentence representations. We propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to represent the meaning sentences. More specifically, our new approach first generates two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We then inject the two sequence representations into bidirectional LSTM, which means forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept auxiliary character CNN embedding output. The final sentence representation is the concatenation of the two direction. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations.", "We propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to represent the meaning sentences. More specifically, our new approach first generates two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We then inject the two sequence representations into bidirectional LSTM, which means forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept auxiliary character CNN embedding output. The final sentence representation is the concatenation of the two direction. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Figure FIGREF1 shows the neural network architecture for general sentence relation modeling.", "where INLINEFORM0 . The objective function then can be defined as the regularized KL-divergence between INLINEFORM1 and INLINEFORM2 : DISPLAYFORM0"]}
{"question_id": "ad8411edf11d3429c9bdd08b3e07ee671464d73c", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the number of training pairs and the superscript INLINEFORM1 indicates the INLINEFORM2 th sentence pair.", "It's quite important to design a good topology for CNN to learn hidden features from heterogeneous feature planes. After several experiments, we found two topological graphs can be deployed in the architecture. Figure FIGREF20 and Figure FIGREF20 show the two CNN graphs. In Topology i@, we stack temporal convolution with kernel width as 1 and tanh activation on top of each feature plane. After that, we deploy another temporal convolution and tanh activation operation with kernel width as 2. In Topology ii@, however, we first stack temporal convolution and tanh activation with kernel width as 2. Then we deploy another temporal convolution and tanh activation operation with kernel width as 1. Experiment results demonstrate that the Topology i@ is slightly better than the Topology ii@. This conclusion is reasonable. The feature planes are heterogeneous. After conducting convolution and tanh activation transformation, it makes sense to compare values across different feature planes.", "We first initialize our word representations using publicly available 300-dimensional Glove word vectors . LSTM memory dimension is 100, the number of layers is 2. On the other hand, for CharCNN model we use threshold activation function on top of each temporal convolution and max pooling pairs . The CharCNN input frame size equals alphabet size, output frame size is 100. The maximum sentence length is 37. The kernel width of each temporal convolution is set to 3, the step is 1, the hidden units of HighwayMLP is 50. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule BIBREF16 . The learning rate is set to 0.05. The mini-batch size is 25. The model parameters were regularized with a per-minibatch L2 regularization strength of INLINEFORM0 . Note that word embeddings were fixed during training.", "In this experiment, we will compare tree LSTM with sequential LSTM. A limitation of the sequence LSTM architectures is that they only allow for strictly sequential information propagation. However, tree LSTMs allow richer network topologies where each LSTM unit is able to incorporate information from multiple child units. As in standard LSTM units, each Tree-LSTM unit (indexed by INLINEFORM0 ) contains input and output gates INLINEFORM1 and INLINEFORM2 , a memory cell INLINEFORM3 and hidden state INLINEFORM4 . The difference between the standard LSTM unit and tree LSTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units. Additionally, instead of a single forget gate, the tree LSTM unit contains one forget gate INLINEFORM5 for each child INLINEFORM6 . This allows the tree LSTM unit to selectively incorporate information from each child.", "The task of semantic relatedness prediction tries to measure the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related). More formally, given a sentence pair, we wish to predict a real-valued similarity score in a range of INLINEFORM0 , where INLINEFORM1 is an integer. The sequence INLINEFORM2 is the ordinal scale of similarity, where higher scores indicate greater degrees of similarity. We can predict the similarity score INLINEFORM3 by predicting the probability that the learned hidden representation INLINEFORM4 belongs to the ordinal scale. This is done by projecting an input representation onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input will located in corresponding scale."]}
{"question_id": "11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "predicted_answer": "", "predicted_evidence": ["Only the first TDNN layer of the x-vector network is shared with the ASR network. The phonetic classification is done at the frame level, while the speaker labels are classified at the segment level.", "The multitask part of c-vector has the same architecture as in the above section SECREF12 ASR Acoustic Model of c-vector is a 5-layer TDNN network. The slicing parameter is { t - 2; t - 1; t; t + 1; t + 2 }, { t - 1; t; t + 1 }, { t - 1; t; t + 1 }, { t - 3; t; t + 3}, { t - 6; t - 3; t}. The 5-th layer is the BN layer containing 128 nodes and other layers have 650 nodes.", "The ASR network has no statistics pooling component. The frame-level part of the x-vector network is a 7-layer TDNN. The input of each layer is the sliced output of the previous layer. The slicing parameter is: {t - 2; t - 1; t; t + 1; t + 2}, {t - 2; t; t + 2}, {t - 3; t; t + 3}, {t}, {t}, {t}, {t}. It has 512 nodes in layer 1 to 7.", "Etdnn/ams system is an extended version of tdnn with the additive margin softmax loss BIBREF1. Etdnn is used in speaker verification in BIBREF2. Compared with the traditional tdnn in BIBREF3, it has wider context and interleaving dense layers between each two tdnn layers. The architecture of our etdnn network is shown in table TABREF6. It is the same as the etdnn architecture in BIBREF2, except that the context of layer 5 of our system is t-3:t+3 instead of t-3, t, t+3. The x-vector is extracted from layer 12 prior to the ReLU non-linearity. For the loss, we use additive margin softmax with $m=0.15$ instead of traditional softmax loss or angular softmax loss.", "C-vector architecture is also one of our proposed systems in paper BIBREF7. As shown in Fig. FIGREF15, it is an extension of multitask architecture. It combines multitask architecture with an extra ASR Acoustic Model. The output of ASR Acoustic Model is concatenated with x-vector's frame-level output as the input of statistics pooling. Refer to BIBREF7 for more details."]}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "predicted_answer": "", "predicted_evidence": ["Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019.", "The frame-level part of the x-vector network is a 10-layer TDNN. The input of each layer is the sliced output of the previous layer. The slicing parameter is: {t - 2; t - 1; t; t + 1; t + 2}, { t }, { t - 2; t; t + 2 }, {t}, { t - 3; t; t + 3 }, {t }, {t - 4; t; t + 4 }, { t }, { t } , { t }. It has 512 nodes in layer 1 to 9, and the 10-th layer has 1500 nodes. The segment-level part of x-vector network is a 2-layer fully-connected network with 512 nodes per layer. The output is predicted by softmax and the size is the same as the number of speakers.", "Test on Intel Xeon Platinum 8168 for ftdnn and eftdnn system. Extracting embedding cost about 0.103RT for etdnn_ams, 0.089RT for multitask, 0.092RT for c-vector, 0.132RT for eftdnn, 0.0639RT for ftdnn, and 0.112RT for ResNet. Single trial cost around 1.2ms for etdnn_ams, 0.9ms for multitask, 0.9ms for c-vector, 0.059s for eftdnn, 0.0288s for ftdnn, 1.0ms for ResNet. The memory cost about 1G for an embedding extraction and a single trial. In the inference, we just use CPU.", "C-vector architecture is also one of our proposed systems in paper BIBREF7. As shown in Fig. FIGREF15, it is an extension of multitask architecture. It combines multitask architecture with an extra ASR Acoustic Model. The output of ASR Acoustic Model is concatenated with x-vector's frame-level output as the input of statistics pooling. Refer to BIBREF7 for more details.", "The multitask part of c-vector has the same architecture as in the above section SECREF12 ASR Acoustic Model of c-vector is a 5-layer TDNN network. The slicing parameter is { t - 2; t - 1; t; t + 1; t + 2 }, { t - 1; t; t + 1 }, { t - 1; t; t + 1 }, { t - 3; t; t + 3}, { t - 6; t - 3; t}. The 5-th layer is the BN layer containing 128 nodes and other layers have 650 nodes."]}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "predicted_answer": "", "predicted_evidence": ["C-vector architecture is also one of our proposed systems in paper BIBREF7. As shown in Fig. FIGREF15, it is an extension of multitask architecture. It combines multitask architecture with an extra ASR Acoustic Model. The output of ASR Acoustic Model is concatenated with x-vector's frame-level output as the input of statistics pooling. Refer to BIBREF7 for more details.", "The ASR network has no statistics pooling component. The frame-level part of the x-vector network is a 7-layer TDNN. The input of each layer is the sliced output of the previous layer. The slicing parameter is: {t - 2; t - 1; t; t + 1; t + 2}, {t - 2; t; t + 2}, {t - 3; t; t + 3}, {t}, {t}, {t}, {t}. It has 512 nodes in layer 1 to 7.", "Factorized TDNN (ftdnn) architecture is listed in table TABREF8. It is the same to BIBREF2 except that we use 1024 nodes instead of 512 nodes in layer 12 and 13. The x-vector is extracted from layer 12 prior to the ReLU non-linearity. So our x-vector is 1024 dimensional. More details about the architecture can be found in BIBREF2.", "After the embeddings are extracted, they are then transformed to 150 dimension using LDA. Then, embeddings are projected into unit sphere. At last, adapted PLDA with no dimension reduction is applied.", "ResNet architecture is also based on tdnn x-vector BIBREF3. The five frame level tdnn layers in BIBREF3 are replaced by ResNet34 (512 nodes) + DNN(512 nodes) + DNN(1000 nodes). Further details about ResNet34 can be found in BIBREF5. In our realization, acoustic features are regarded as a single channel picture and feed into the ResNet34. If the dimensions in the residual network don't match, zeros are added. The statistic pooling and segment level network stay the same. For the loss function, we use angular softmax with $m=4$. The x-vector is extracted from first DNN layer in segment level prior to the ReLU non-linearity. It has 512 dimensions."]}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "predicted_answer": "", "predicted_evidence": ["To estimate the quality of topic models, we use two main automatic measures: topic coherence and kernel uniqueness. For human content analysis, measures of topic coherence and kernel uniqueness are both important and complement each other. Topics can be coherent but have a lot of repetitions. On the other hand, generated topics can be very diverse, but incoherent within each topic.", "LDA-SIM algorithm", "The thesaurus topics seem to convey the contents in the most concentrated way. In the Syrian topic general word country is absent; instead of UN (United Nations), it contains word rebel, which is closer to the Syrian situation. In the Orthodox church topic, the unigram variant contains extra word year, relations of words Moscow and Kirill to other words in the topic can be inferred only from the encyclopedic knowledge.", "For evaluating topics with automatic quality measures, we used several English text collections and one Russian collection (Table TABREF7 ). We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 .", "[ht!] collection INLINEFORM0 , vocabulary INLINEFORM1 , number of topics INLINEFORM2 , initial INLINEFORM3 and INLINEFORM4 , sets of similar expressions INLINEFORM5 , hyperparameters INLINEFORM6 and INLINEFORM7 , INLINEFORM8 is the frequency of INLINEFORM9 in the document INLINEFORM10 distributions INLINEFORM11 and INLINEFORM12 not meet the stop criterion INLINEFORM13 INLINEFORM14"]}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "predicted_answer": "", "predicted_evidence": ["Introducing information from domain-specific thesaurus EuroVoc led to improving the initial model without the additional assumption, which can be explained by the absence of general abstract words in such information-retrieval thesauri.", "Currently, probabilistic topic models are important tools for improving automatic text processing including information retrieval, text categorization, summarization, etc. Besides, they can be useful in supporting expert analysis of document collections, news flows, or large volumes of messages in social networks BIBREF0 , BIBREF1 , BIBREF2 . To facilitate this analysis, such approaches as automatic topic labeling and various visualization techniques have been proposed BIBREF1 , BIBREF3 .", "Boyd-Graber et al. BIBREF4 indicate that to be understandable by humans, topics should be specific, coherent, and informative. Relationships between the topic components can be inferred. In BIBREF1 four topic visualization approaches are compared. The authors of the experiment concluded that manual topic labels include a considerable number of phrases; users prefer shorter labels with more general words and tend to incorporate phrases and more generic terminology when using more complex network graph. Blei and Lafferty BIBREF3 visualize topics with ngrams consisting of words mentioned in these topics. These works show that phrases and knowledge about hyponyms/hypernyms are important for topic representation.", "To combine knowledge with a topic model, we used RuThes thesaurus together with the additional block of the Islam thesaurus. The Islam thesaurus contains more than 5 thousand Islam-related terms including single words and expressions.", "In this paper we describe an approach to integrate large manual lexical resources such as WordNet or EuroVoc into probabilistic topic models, as well as automatically extracted n-grams to improve coherence and informativeness of generated topics. The structure of the paper is as follows. In Section 2 we consider related works. Section 3 describes the proposed approach. Section 4 enumerates automatic quality measures used in experiments. Section 5 presents the results obtained on several text collections according to automatic measures. Section 6 describes the results of manual evaluation of combined topic models for Islam Internet-site thematic analysis."]}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "predicted_answer": "", "predicted_evidence": ["We can compare this approach with the approaches applying the generalized Polya urn model BIBREF8 , BIBREF9 , BIBREF10 . To add prior knowledge, those approaches change topic distributions for related words globally in the collection. We modify topic probabilities for related words and phrases locally, in specific texts, only when related words (phrases) co-occur in these texts.", "LDA-SIM algorithm", "INLINEFORM0", "It is worth noting that adding ngrams sometimes worsens the TC-NPMI measure, especially on the JRC collection. This is due to the fact that in these evaluation frameworks, the topics' top elements contain a lot of multiword expressions, which rarely occur in Wikipedia, used for the coherence calculation, therefore the utilized automatic coherence measures can have insufficient evidence for correct estimates.", "Newman et al. BIBREF6 asked users to score topics on a 3-point scale, where 3=\u201cuseful\u201d (coherent) and 1=\u201cuseless\u201d (less coherent). They instructed the users that one indicator of usefulness is the ease by which one could think of a short label to describe a topic. Then several automatic measures, including WordNet-based measures and corpus co-occurrence measures, were compared. It was found that the best automatic measure having the largest correlation with human evaluation is word co-occurrence calculated as point-wise mutual information (PMI) on Wikipedia articles. Later Lau et al. BIBREF17 showed that normalized poinwise mutual information (NPMI) BIBREF18 calculated on Wikipedia articles correlates even more strongly with human scores."]}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "predicted_answer": "", "predicted_evidence": ["Hate speech is a difficult phenomenon to define and is not monolithic. Our classifications of hate speech tend to reflect our own subjective biases. People identify racist and homophobic slurs as hateful but tend to see sexist language as merely offensive. While our results show that people perform well at identifying some of the more egregious instances of hate speech, particularly anti-black racism and homophobia, it is important that we are cognizant of the social biases that enter into our algorithms and future work should aim to identify and correct these biases.", "Our results also illustrate how hate speech can be used in different ways: it can be directly send to a person or group of people targeted, it can be espoused to nobody in particular, and it can be used in conversation between people. Future work should distinguish between these different uses and look more closely at the social contexts and conversations in which hate speech occurs. We must also study more closely the people who use hate speech, focusing both on their individual characteristics and motivations and on the social structures they are embedded in.", "Consistent with previous work, we find that certain terms are particularly useful for distinguishing between hate speech and offensive language. While f*g, b*tch, and n*gga are used in both hate speech and offensive language, the terms f*ggot and n*gger are generally associated with hate speech. Many of the tweets considered most hateful contain multiple racial and homophobic slurs. While this allows us to easily identify some of the more egregious instances of hate speech it means that we are more likely to misclassify hate speech if it doesn't contain any curse words or offensive terms. To more accurately classify such cases we should find sources of training data that are hateful without necessarily using particular keywords or offensive language.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people.", "Syntactic features have been leveraged to better identify the targets and intensity of hate speech, for example sentences where a relevant noun and verb occur (e.g. kill and Jews) BIBREF6 , the POS trigram DT jewish NN BIBREF2 , and the syntactic structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 ."]}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "predicted_answer": "", "predicted_evidence": ["What constitutes hate speech and when does it differ from offensive language? No formal definition exists but there is a consensus that it is speech that targets disadvantaged social groups in a manner that is potentially harmful to them BIBREF0 , BIBREF1 . In the United States, hate speech is protected under the free speech provisions of the First Amendment, but it has been extensively debated in the legal sphere and with regards to speech codes on college campuses. In many countries, including the United Kingdom, Canada, and France, there are laws prohibiting hate speech, which tends to be defined as speech that targets minority groups in a way that could promote violence or social disorder. People convicted of using hate speech can often face large fines and even imprisonment. These laws extend to the internet and social media, leading many sites to create their own provisions against hate speech. Both Facebook and Twitter have responded to criticism for not doing enough to prevent hate speech on their sites by instituting policies to prohibit the use of their platforms for attacks on people based on characteristics like race, ethnicity, gender, and sexual orientation, or threats of violence towards others.", "We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech.", "Consistent with previous work, we find that certain terms are particularly useful for distinguishing between hate speech and offensive language. While f*g, b*tch, and n*gga are used in both hate speech and offensive language, the terms f*ggot and n*gger are generally associated with hate speech. Many of the tweets considered most hateful contain multiple racial and homophobic slurs. While this allows us to easily identify some of the more egregious instances of hate speech it means that we are more likely to misclassify hate speech if it doesn't contain any curse words or offensive terms. To more accurately classify such cases we should find sources of training data that are hateful without necessarily using particular keywords or offensive language.", "Hate speech is a difficult phenomenon to define and is not monolithic. Our classifications of hate speech tend to reflect our own subjective biases. People identify racist and homophobic slurs as hateful but tend to see sexist language as merely offensive. While our results show that people perform well at identifying some of the more egregious instances of hate speech, particularly anti-black racism and homophobia, it is important that we are cognizant of the social biases that enter into our algorithms and future work should aim to identify and correct these biases.", "Turning to borderline cases, where the probability of being offensive is marginally higher than hate speech, it appears that the majority are hate speech, both directed towards other Twitter users, @MDreyfus @NatFascist88 Sh*t your ass your moms p*ssy u Jew b*stard. Ur times coming. Heil Hitler! and general hateful statements like My advice of the day: If your a tranny...go f*ck your self!. These tweets fit our definition of hate speech but were likely misclassified because they do not contain any of the terms most strongly associated with hate speech. Finally, the hateful tweets incorrectly labeled as neither tend not to contain hate or curse words, for example If some one isn't an Anglo-Saxon Protestant, they have no right to be alive in the US. None at all, they are foreign filth contains a negative term, filth but no slur against a particular group. We also see that rarer types of hate speech, for example this anti-Chinese statement Every slant in #LA should be deported."]}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "predicted_answer": "", "predicted_evidence": ["Turning to borderline cases, where the probability of being offensive is marginally higher than hate speech, it appears that the majority are hate speech, both directed towards other Twitter users, @MDreyfus @NatFascist88 Sh*t your ass your moms p*ssy u Jew b*stard. Ur times coming. Heil Hitler! and general hateful statements like My advice of the day: If your a tranny...go f*ck your self!. These tweets fit our definition of hate speech but were likely misclassified because they do not contain any of the terms most strongly associated with hate speech. Finally, the hateful tweets incorrectly labeled as neither tend not to contain hate or curse words, for example If some one isn't an Anglo-Saxon Protestant, they have no right to be alive in the US. None at all, they are foreign filth contains a negative term, filth but no slur against a particular group. We also see that rarer types of hate speech, for example this anti-Chinese statement Every slant in #LA should be deported.", "It is likely that coders skimmed these tweets too quickly, picking out words or phrases that appeared to be hateful without considering the context. Turning to borderline cases, where the probability of being offensive is marginally higher than hate speech, it appears that the majority are hate speech, both directed towards other Twitter users, @MDreyfus @NatFascist88 Sh*t your ass your moms p*ssy u Jew b*stard. Ur times coming. Heil Hitler! and general hateful statements like My advice of the day: If your a tranny...go f*ck your self!. These tweets fit our definition of hate speech but were likely misclassified because they do not contain any of the terms most strongly associated with hate speech. Finally, the hateful tweets incorrectly labeled as neither tend not to contain hate or curse words, for example If some one isn't an Anglo-Saxon Protestant, they have no right to be alive in the US. None at all, they are foreign filth contains a negative term, filth but no slur against a particular group.", "Our results also illustrate how hate speech can be used in different ways: it can be directly send to a person or group of people targeted, it can be espoused to nobody in particular, and it can be used in conversation between people. Future work should distinguish between these different uses and look more closely at the social contexts and conversations in which hate speech occurs. We must also study more closely the people who use hate speech, focusing both on their individual characteristics and motivations and on the social structures they are embedded in.", "Hate speech is a difficult phenomenon to define and is not monolithic. Our classifications of hate speech tend to reflect our own subjective biases. People identify racist and homophobic slurs as hateful but tend to see sexist language as merely offensive. While our results show that people perform well at identifying some of the more egregious instances of hate speech, particularly anti-black racism and homophobia, it is important that we are cognizant of the social biases that enter into our algorithms and future work should aim to identify and correct these biases.", "Only 5% of tweets were coded as hate speech by the majority of coders and only 1.3% were coded unanimously, demonstrating the imprecision of the Hatebase lexicon. This is much lower than a comparable study using Twitter, where 11.6% of tweets were flagged as hate speech BIBREF5 , likely because we use a stricter criteria for hate speech. The majority of the tweets were considered to be offensive language (76% at 2/3, 53% at 3/3) and the remainder were considered to be non-offensive (16.6% at 2/3, 11.8% at 3/3). We then constructed features from these tweets and used them to train a classifier."]}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "predicted_answer": "", "predicted_evidence": ["Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases this may also be language that threatens or incites violence, but limiting our definition only to such cases would exclude a large proportion of hate speech. Importantly, our definition does not include all instances of offensive language because people often use terms that are highly offensive to certain groups but in a qualitatively different manner. For example some African Americans often use the term n*gga in everyday language online BIBREF2 , people use terms like h*e and b*tch when quoting rap lyrics, and teenagers use homophobic slurs like f*g as they play video games. Such language is prevalent on social media BIBREF3 , making this boundary condition crucial for any usable hate speech detection system .", "From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.", "If we conflate hate speech and offensive language then we erroneously consider many people to be hate speakers (errors in the lower triangle of Figure 1) and fail differentiate between commonplace offensive language and serious hate speech (errors in the upper triangle of Figure 1). Given the legal and moral implications of hate speech it is important that we are able to accurately distinguish between the two. Lexical methods are effective ways to identify potentially offensive terms but are inaccurate at identifying hate speech; only a small percentage of tweets flagged by the Hatebase lexicon were considered hate speech by human coders. While automated classification methods can achieve relatively high accuracy at differentiating between these different classes, close analysis of the results shows that the presence or absence of particular offensive or hateful terms can both help and hinder accurate classification.", "Tweets with overall positive sentiment and higher readability scores are more likely to belong to this class. The tweets in this category that have been misclassified as hate or offensive tend to mention race, sexuality, and other social categories that are targeted by hate speakers. Most appear to be misclassifications appear to be caused by on the presence of potentially offensive language, for example He's a damn good actor. As a gay man it's awesome to see an openly queer actor given the lead role for a major film contains the potentially the offensive terms gay and queer but uses them in a positive sense. This problem has been encountered in previous research BIBREF2 and illustrates the importance of taking context into account. We also found a small number of cases where the coders appear to have missed hate speech that was correctly identified by our model, e.g.", "Finally, turning to the neither class, we see that tweets with the highest predicted probability of belonging to this class all appear to be innocuous and were included in the sample because they contained terms included in the Hatebase lexicon such as charlie and bird that are generally not used in a hateful manner. Tweets with overall positive sentiment and higher readability scores are more likely to belong to this class. The tweets in this category that have been misclassified as hate or offensive tend to mention race, sexuality, and other social categories that are targeted by hate speakers. Most appear to be misclassifications appear to be caused by on the presence of potentially offensive language, for example He's a damn good actor. As a gay man it's awesome to see an openly queer actor given the lead role for a major film contains the potentially the offensive terms gay and queer but uses them in a positive sense."]}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "predicted_answer": "", "predicted_evidence": ["Finally, for the large variants of BERT and RoBERTa on SST-2 (second subfigure from both the top and the left), we observe a surprisingly consistent increase in quality when freezing 12\u201316 layers. This finding suggests that these models may be overparameterized for SST-2.", "Our research contribution is a comprehensive evaluation, across multiple pretrained transformers and datasets, of the number of final layers needed for fine-tuning. We show that, on most tasks, we need to fine-tune only one fourth of the final layers to achieve within 10% parity with the full model. Surprisingly, on SST-2, a sentiment classification dataset, we find that not fine-tuning all of the layers leads to improved quality.", "Furthering this approach with more data and improved modeling, BIBREF0 pretrain deep 12- and 24-layer bidirectional transformers BIBREF8 on the entirety of Wikipedia and BooksCorpus BIBREF9. Their approach, called BERT, achieves state of the art across all tasks in the General Language Understanding Evaluation (GLUE) benchmark BIBREF10, as well as the Stanford Question Answering Dataset (BIBREF11).", "The prevailing evidence in the neural network literature suggests that earlier layers extract universal features, while later ones perform task-specific modeling. BIBREF12 visualize the per-layer activations in image classification networks, finding that the first few layers function as corner and edge detectors, and the final layers as class-specific feature extractors. BIBREF13 demonstrate that the low- and high-level notions of content and style are separable in convolutional neural networks, with lower layers capturing content and higher layers style.", "In the pretrained language modeling paradigm, a language model (LM) is trained on vast amounts of text, then fine-tuned on a specific downstream task. BIBREF6 are one of the first to successfully apply this idea, outperforming state of the art in question answering, textual entailment, and sentiment classification. Their model, dubbed ELMo, comprises a two-layer BiLSTM pretrained on the Billion Word Corpus BIBREF7."]}
{"question_id": "441be93e2830cc0fc65afad6959db92754c9f5a8", "predicted_answer": "", "predicted_evidence": ["For our datasets, we use the GLUE benchmark, which comprises the tasks in natural language inference, sentiment classification, linguistic acceptability, and semantic similarity. Specifically, for natural language inference (NLI), it provides the Multigenre NLI (MNLI; BIBREF16), Question NLI (QNLI; BIBREF10), Recognizing Textual Entailment (RTE; BIBREF17), and Winograd NLI BIBREF18 datasets. For semantic textual similarity and paraphrasing, it contains the Microsoft Research Paraphrase Corpus (MRPC; BIBREF19), the Semantic Textual Similarity Benchmark (STS-B; BIBREF20), and Quora Question Pairs (QQP; BIBREF21). Finally, its single-sentence tasks consist of the binary-polarity Stanford Sentiment Treebank (SST-2; BIBREF22) and the Corpus of Linguistic Acceptability (CoLA; BIBREF23).", "Our fine-tuning procedure closely resembles those of BERT and RoBERTa. We choose the Adam optimizer BIBREF24 with a batch size of 16 and fine-tune BERT for 3 epochs and RoBERTa for 10, following the original papers. For hyperparameter tuning, the best learning rate is different for each task, and all of the original authors choose one between $1 \\times 10^{-5}$ and $5 \\times 10^{-5}$; thus, we perform line search over the interval with a step size of $1 \\times 10^{-5}$. We report the best results in Table TABREF5.", "As a result of this development, a flurry of recent papers has followed this more-data-plus-better-models principle. Two prominent examples include XLNet BIBREF1 and RoBERTa BIBREF2, both of which contest the present state of the art. XLNet proposes to pretrain two-stream attention-augmented transformers on an autoregressive LM objective, instead of the original cloze and next sentence prediction (NSP) tasks from BERT. RoBERTa primarily argues for pretraining longer, using more data, and removing the NSP task for BERT.", "Pretrained transformers. In the NLP literature, similar observations have been made for pretrained language models. BIBREF14 analyze BERT's attention and observe that the bottom layers attend broadly, while the top layers capture linguistic syntax. BIBREF4 find that the last few layers of BERT change the most after task-specific fine-tuning. Similar to our work, BIBREF5 fine-tune the top layers of BERT, as part of their baseline comparison for their model compression approach. However, none of the studies comprehensively examine the number of necessary final layers across multiple pretrained transformers and datasets.", "Our research contribution is a comprehensive evaluation, across multiple pretrained transformers and datasets, of the number of final layers needed for fine-tuning. We show that, on most tasks, we need to fine-tune only one fourth of the final layers to achieve within 10% parity with the full model. Surprisingly, on SST-2, a sentiment classification dataset, we find that not fine-tuning all of the layers leads to improved quality."]}
{"question_id": "7f11f128fd39b8060f5810fa84102f000d94ea33", "predicted_answer": "", "predicted_evidence": ["With the above derivation, we can prove the equivalence like following,", "SNLI and MultiNLI are prepared by Human Elicited, in which workers are given a context and asked to produce hypotheses corresponding to labels. SICK and JOCI are created by Human Judged, referring that hypotheses and premises are automatically paired while labels are generated by humans BIBREF6. In order to maximumly mitigate the impacts of annotation artifacts during evaluations, we train and validate models respectively on SNLI and MultiNLI and test on both SICK and JOCI. We also report models' performances on SNLI and MultiNLI.", "We make a few assumptions about an artifact-balanced distribution and how the biased datasets are generated from it, and demonstrate that we can train models fitting the artifact-balanced distribution using only the biased datasets.", "As to SNLI, we use the same partition as BIBREF0. For MultiNLI, we separately use two origin validation sets (Matched and Mismatched) as the testing sets for convenience, and refer them as MMatch and MMismatch. We randomly select 10000 samples out of the origin training set for validation and use the rest for training. As to JOCI, we use the whole \u201cB\u201d subsets for testing, whose premises are from SNLI-train while hypotheses are generated based on world knowledge BIBREF13, and convert the score to NLI labels following BIBREF6. As to SICK, we use the whole dataset for testing.", "We refer models trained only with hypotheses as hypothesis-only-model (Hyp), and models that utilize both premises and hypotheses as normal-model (Norm). We implement a simple LSTM model for Hyp and use DIIN BIBREF5 as Norm. We report AUC for Hyp and ACC for Norm. More details can be seen in Appendix SECREF15"]}
{"question_id": "2a55076a66795793d79a3edfae1041098404fbc3", "predicted_answer": "", "predicted_evidence": ["Furthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions. Norm trained with SNLI exceed baseline in JOCI with smooth terms $0.01$ and $0.1$. With larger smooth terms, Norm trained with both SNLI and MultiNLI exceeds baseline in SICK. Given the fact that JOCI is almost neutral to artifacts in SNLI, and the bias pattern of both SNLI and MultiNLI are even predictive in SICK, we owe these promotions to that our method improves models' semantic learning ability.", "We also make some assumptions about the artifact-balanced distribution. The first one is that the label is independent with the artifact in the hypothesis, defined as follows,", "In this paper, we take a close look at the annotation artifacts in NLI datasets. We find that the bias pattern could be predictive or misleading in cross-dataset testing. Furthermore, we propose a debiasing framework and experiments demonstrate that it can effectively mitigate the impacts of the bias pattern and improve the cross-dataset generalization ability of models. However, it remains an open problem that how we should treat the annotation artifacts. We cannot assert whether the bias pattern should not exist at all or it is actually some kind of nature. We hope that our findings will encourage more explorations on reliable evaluation protocols for NLI models.", "From the results of Hyp, we can find a trend that the larger the smooth value is, the lower the level of debiasing is, while with a very small or even no smooth value, the AUC may be lower than $0.5$. As mentioned before, we owe this to the imperfect estimation of $P(y|h)$, and we can conclude that a proper smooth value is a prerequisite for the best debiasing effect.", "We make a few assumptions about an artifact-balanced distribution and how the biased datasets are generated from it, and demonstrate that we can train models fitting the artifact-balanced distribution using only the biased datasets."]}
{"question_id": "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a", "predicted_answer": "", "predicted_evidence": ["In this paper, we take a close look at the annotation artifacts in NLI datasets. We find that the bias pattern could be predictive or misleading in cross-dataset testing. Furthermore, we propose a debiasing framework and experiments demonstrate that it can effectively mitigate the impacts of the bias pattern and improve the cross-dataset generalization ability of models. However, it remains an open problem that how we should treat the annotation artifacts. We cannot assert whether the bias pattern should not exist at all or it is actually some kind of nature. We hope that our findings will encourage more explorations on reliable evaluation protocols for NLI models.", "We consider the domain of the artifact-balanced distribution ${D}$ as $\\mathcal {X} \\times \\mathcal {A} \\times \\mathcal {Y} \\times \\mathcal {S}$, in which $\\mathcal {X}$ is the input variable space, $\\mathcal {Y}$ is the label space, $\\mathcal {A}$ is the feature space of annotation artifacts in hypotheses, $\\mathcal {S}$ is the selection intention space. We assume that the biased distribution $\\widehat{{D}}$ of origin datasets can be generated from the artifact-balanced distribution by selecting samples with $S = Y$, i.e., the selection intention matches with the label. We use $P(\\cdot )$ to represent the probability on $\\widehat{{D}}$ and use $Q(\\cdot )$ for ${D}$.", "Annotation Artifacts of SNLI are nearly neutral in JOCI, while MultiNLI is misleading. We find that AUC of Hyp baseline trained with SNLI is very close to $0.5$ on JOCI, indicating that JOCI is nearly neutral to artifacts in SNLI. However, when it comes to training with MultiNLI, the AUC of Hyp baseline is lower than $0.5$, indicating that the artifacts are misleading in JOCI.", "Anotation Artifacts of SNLI and MultiNLI can be generalized to SICK. Unexpectedly, it is shown that Hyp baseline can get $0.6250$ (AUC) trained with SNLI and $0.6079$ (AUC) with MultiNLI when tested on SICK, indicating that the bias pattern of SNLI and MultiNLI are predictive on SICK. The results imply that the bias pattern can even be generalized across datasets prepared by different methods.", "In this section, we present the experimental results for cross-dataset testing of artifacts and artifact-balanced learning. We show that cross-dataset testing is less affected by annotation artifacts, while there are still some influences more or less in different datasets. We also demonstrate that our proposed framework can mitigate the bias and improve the generalization ability of models."]}
{"question_id": "8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "predicted_answer": "", "predicted_evidence": ["From the results of Hyp, we can find a trend that the larger the smooth value is, the lower the level of debiasing is, while with a very small or even no smooth value, the AUC may be lower than $0.5$. As mentioned before, we owe this to the imperfect estimation of $P(y|h)$, and we can conclude that a proper smooth value is a prerequisite for the best debiasing effect.", "", "Annotation Artifacts of SNLI are nearly neutral in JOCI, while MultiNLI is misleading. We find that AUC of Hyp baseline trained with SNLI is very close to $0.5$ on JOCI, indicating that JOCI is nearly neutral to artifacts in SNLI. However, when it comes to training with MultiNLI, the AUC of Hyp baseline is lower than $0.5$, indicating that the artifacts are misleading in JOCI.", "For SNLI, we use Hard released by BIBREF2. For MMatch, we manually partition the set using fastText BIBREF18. And we summarize the size of the datasets used in Hard-Easy Testing below.", "We make a few assumptions about an artifact-balanced distribution and how the biased datasets are generated from it, and demonstrate that we can train models fitting the artifact-balanced distribution using only the biased datasets."]}
{"question_id": "0aca0a208a1e28857fab44e397dc7880e010dbca", "predicted_answer": "", "predicted_evidence": ["We randomly selected 7,220 tweets from our Twitter data based on keyword distributions and had those tweets annotated using workers recruited through Amazon MTurk. Each tweet was also annotated by an expert annotator (i.e., one of the authors). We treated the consensus answer of the crowdsourcing workers (i.e., at least 5 annotators for each tweet assignment) and the expert annotator as the gold-standard. Using control tweets is a common strategy to identify workers who cheat (e.g., randomly select an answer without reading the instructions and/or tweets) on annotation tasks. We introduced two control tweets in each annotation assignment, where each annotation assignment contains a total of 12 tweets (including the 2 control tweets). Only responses with the two control tweets answered corrected were considered valid responses and the worker would receive the 10 cents incentive.", "We first collected tweets based on a list of job loss-related keywords. We then randomly selected a set of sample tweets and had these tweets annotated (i.e., whether the tweet is a job loss event) using the Amazon MTurk platform. With these annotated tweets, we then evaluated 4 different active learning strategies (i.e., least confi-dent, entropy, vote entropy, and Kullback-Leibler (KL) divergence) through simulations.", "One of the most popular platforms is Twitter, which allows users to broadcast short texts (i.e., 140 characters initially, and 280 characters in a recent platform update) in real time with almost no restrictions on content. Twitter is a source of people\u2019s attitudes, opinions, and thoughts toward the things that happen in their daily life. Twitter data are publicly accessible through Twitter application programming interface (API); and there are several tools to download and process these data. Twitter is being increasingly used as a valuable instrument for surveillance research and predictive analytics in many fields including epidemiology, psychology, and social sciences. For example, Bian et al. explored the relation between promotional information and laypeople\u2019s discussion on Twitter by using topic modeling and sentiment analysis BIBREF0. Zhao et al. assessed the mental health signals among sexual and gender minorities using Twitter data BIBREF1. Twitter data can be used to study and predict population-level targets, such as disease incidence BIBREF2, political trends BIBREF3, earthquake detection BIBREF4, and crime perdition BIBREF5, and individual-level outcomes or life events, such as job loss BIBREF6, depression BIBREF7, and adverse events BIBREF8.", "Machine learning and deep learning have been wildly used in classification of tweets tasks. We evaluated 8 different classifiers: 4 traditional machine learning models (i.e., logistic regress [LR], Na\u00efve Bayes [NB], random forest [RF], and support vector machine [SVM]) and 4 deep learning models (i.e., convolutional neural network [CNN], recurrent neural network [RNN], long short-term memory [LSTM] RNN, and gated recurrent unit [GRU] RNN). 3,000 tweets out of 7,220 Amazon MTurk annotated dataset was used for classifier training (n = 2,000) and testing (n = 1,000). The rest of MTurk annotated dataset were used for the subsequent active learning experiments. Each classifier was trained 10 times and 95 confidence intervals (CI) for mean value were reported.", "Twitter is being increasingly used as a valuable instrument for surveillance research and predictive analytics in many fields including epidemiology, psychology, and social sciences. For example, Bian et al. explored the relation between promotional information and laypeople\u2019s discussion on Twitter by using topic modeling and sentiment analysis BIBREF0. Zhao et al. assessed the mental health signals among sexual and gender minorities using Twitter data BIBREF1. Twitter data can be used to study and predict population-level targets, such as disease incidence BIBREF2, political trends BIBREF3, earthquake detection BIBREF4, and crime perdition BIBREF5, and individual-level outcomes or life events, such as job loss BIBREF6, depression BIBREF7, and adverse events BIBREF8. Since tweets are unstructured textual data, natural language processing (NLP) and machine learning, especially deep learning nowadays, are often used for preprocessing and analytics. However, for many studiesBIBREF9, BIBREF10, BIBREF11, especially those that analyze individual-level targets, manual annotations of several thousands of tweets, often by experts, is needed to create gold-standard training datasets, to be fed to the NLP and machine learning tools for subsequent, reliable automated processing of millions of tweets."]}
{"question_id": "471683ba6251b631f38a24d42b6dba6f52dee429", "predicted_answer": "", "predicted_evidence": ["In active learning, the learning algorithm is set to proactively select a subset of available examples to be manually labeled next from a pool of yet unlabeled instances. The fundamental idea behind the concept is that a machine learning algorithm could potentially achieve a better accuracy quicker and using fewer training data if it were allowed to choose the most informative data it wants to learn from. In our experiment, we found that the entropy algorithm is the best way to build machine learning models fast and efficiently. Vote entropy and KL divergence, the query-by-committee active learning methods are helpful for the training of machine learning ensemble classifiers. However, all the active learning strategies we tested do not work well with deep learning model (i.e., CNN) or deep learning-based ensemble classifier.", "Query optimization techniques (e.g., active learning) can reduce the number of tweets that need to be labeled, while yielding comparable performance for the downstream machine learning tasks BIBREF17, BIBREF18, BIBREF19. Active learning algorithms have been widely applied in various areas including NLP BIBREF20 and image processing BIBREF21. In a pool-based active learning scenario, data samples for training a machine learning algorithm (e.g., a classifier for identifying job loss events) are drawn from a pool of unlabeled data according to some forms of informativeness measure (a.k.a. active learning strategies BIBREF22), and then the most informative instances are selected to be annotated. For a classification task, in essence, an active learning strategy should be able to pick the \u201cbest\u201d samples to be labelled that will improve the classification performance the most.", "In this study, we integrated active learning into a crowdsourcing pipeline for the classification of life events based on individual tweets. We analyzed the quality of crowdsourcing annotations and then experimented with different machine/deep learning classifiers combined with different active learning strategies to answer the following two research questions (RQs):", "In second RQ, we aimed to find which active learning strategy is most efficient and cost-effective to build event classification models using Twitter data. We started with selecting representative machine learning and deep learning classifiers. Among the 4 machine learning classifiers (i.e., LR, NB, RF, and SVM), LR and RF classifiers have the best performance on the task of identifying job loss events from tweets. Among the 4 deep learning methods (i.e., CNN, RNN, LSTM, LSTM with GRU), CNN has the best performance.", "In pool-based sampling for active learning, instances are drawn from a pool of samples according to some sort of informativeness measure, and then the most informative instances are selected to be annotated. This is the most common scenario in active learning studies BIBREF26. The informativeness measures of the pool instances are called active learning strategies (or query strategies). We evaluated 4 active learning strategies (i.e., least confident, entropy, vote entropy and KL divergence). Fig 1.C shows the workflow of our pool-based active learning experiments: for a given active learning strategy and classifiers trained with an initial set of training data (1) the classifiers make predictions of the remaining to-be-labelled dataset; (2) a set of samples is selected using the specific active learning strategy and annotated by human reviewers; (3) the classifiers are retrained with the newly annotated set of tweets. We repeated this process iteratively until the pool of data exhausts. For the least confident and entropy active learning strategies, we used the best performed machine learn-ing classifier and the best performed deep learning classifier plus the baseline classifier (LR)."]}
{"question_id": "5dfd58f91e7740899c23ebfe04b7176edce9ead2", "predicted_answer": "", "predicted_evidence": ["Next, we present the result of evaluating the baselines and other comparisons on the test set in Table TABREF28 . The INLINEFORM0 scores are averaged over the 5 cross-validation folds. We see that Major baseline performs very poorly compared to the Memo baseline, which surprisingly achieves over 90 INLINEFORM1 points. This result suggests that Memo is a more suitable baseline for this dataset in contrast with Major. The result also provides evidence to the usefulness of our evaluation metric which heavily penalizes a simple majority vote model. Furthermore, we notice that the rule-based tagger by Rashel et al. BIBREF7 performs worse than Memo, indicating that Memo is not just suitable but also quite a strong baseline. Moving on, we observe how CRF has 6 points advantage over Memo, signaling that incorporating contextual features and modeling tag-to-tag transitions are useful. Lastly, the biLSTM with CRF tagger performs the best with 97.47 INLINEFORM2 score.", "For the neural tagger, we set the size of the word, affix, and character embedding to 100, 20, and 30 respectively. We applied dropout regularization to the embedding layers. The max-pooled CNN has 30 filters for each filter width. We set the feedforward network and the biLSTM to have 100 hidden units. We put a dropout layer before the biLSTM input layer. We tuned the learning rate, dropout rate, context window size, and CNN filter width to the development set. As we said earlier, we experimented with different configurations in the embedding, encoding, and prediction step. We evaluated each configuration on the development set as well.", "To understand how each feature in the embedding step affects the neural tagger, we performed feature ablation on the development set and put the result in Table TABREF29 . We see that with only words as features (first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the performance further by 0.08 and 0.10 points respectively. From this result, we see that it is the character features that positively affect the neural tagger the most.", "FIGREF30 . The figure shows that the model can predict most tags almost perfectly, except for X and WH tag. The X tag is described as \"a word or part of a sentence which its category is unknown or uncertain\". The X tag is rather rare, as it only appears 397 times out of over 250K tokens. Some words annotated as X are typos and slang words. Some foreign terms and abbreviations are also annotated with X. The model might get confused as such words are usually tagged with a noun tag (NN or NNP). We also see that the model seems to confuse question words (WH) such as apa (what) or siapa (who) as SC since these words may be used in subordinate clauses as well. Looking at the data closely, we found that the tagging of such words are inconsistent. This inconsistency contributes to the inability of the model to distinguish the two tags well.", "In the prediction step, the tagger predicts the POS tag of the INLINEFORM0 -th word based on the output vector INLINEFORM1 . We tested two approaches: a softmax layer with greedy decoding and a CRF layer with Viterbi decoding. With a softmax layer, the tagger simply normalizes INLINEFORM2 and predicts using greedy decoding, i.e. picking the tag with the highest probability. In contrast, with a CRF layer, the tagger treats INLINEFORM3 as emission probability scores, models the tag-to-tag transition probability scores, and uses Viterbi algorithm to select the most probable tag sequence as the prediction. We refer readers to BIBREF17 to read more about how the CRF layer and Viterbi decoding work. We want to note that when we only embed words, encode using feedforward network, and predict using greedy decoding, the tagger is effectively the same as that in BIBREF8 . Also, when only the word and character features are used, with a biLSTM and CRF layer, the tagger is effectively the same as that in BIBREF9 ."]}
{"question_id": "c09bceea67273c10a0621da1a83b409f53342fd9", "predicted_answer": "", "predicted_evidence": ["To understand the performance of the neural model for each tag, we plot the confusion matrix from the development set of the first fold in Fig. FIGREF30 . The figure shows that the model can predict most tags almost perfectly, except for X and WH tag. The X tag is described as \"a word or part of a sentence which its category is unknown or uncertain\". The X tag is rather rare, as it only appears 397 times out of over 250K tokens. Some words annotated as X are typos and slang words. Some foreign terms and abbreviations are also annotated with X. The model might get confused as such words are usually tagged with a noun tag (NN or NNP). We also see that the model seems to confuse question words (WH) such as apa (what) or siapa (who) as SC since these words may be used in subordinate clauses as well.", "Pisceldo et al. BIBREF5 built an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on PANL10N dataset tagged manually using their proposed tagset. The dataset consists of 15K sentences. Another work used a hidden Markov model enhanced with an affix tree to better handle out-of-vocabulary (OOV) words BIBREF6 . They evaluated their models on the same PANL10N dataset and achieved more than 90% overall accuracy and roughly 70% accuracy for the OOV cases. We note that while the datasets are the same, the split could be different. Thus, making a fair comparison between them is difficult.", "We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons. Firstly, the tagger tags named entities and multi-word expressions based on a dictionary. Then, it uses MorphInd BIBREF15 to tag the rest of the words. Finally, they employ 15 hand-crafted rules to resolve ambiguous tags in the post-processing step. We want to note that we did not use their provided tokenizer since the IDN Tagged Corpus dataset is already tokenized. Their implementation is available online.", "Next, we present the result of evaluating the baselines and other comparisons on the test set in Table TABREF28 . The INLINEFORM0 scores are averaged over the 5 cross-validation folds. We see that Major baseline performs very poorly compared to the Memo baseline, which surprisingly achieves over 90 INLINEFORM1 points. This result suggests that Memo is a more suitable baseline for this dataset in contrast with Major. The result also provides evidence to the usefulness of our evaluation metric which heavily penalizes a simple majority vote model. Furthermore, we notice that the rule-based tagger by Rashel et al. BIBREF7 performs worse than Memo, indicating that Memo is not just suitable but also quite a strong baseline. Moving on, we observe how CRF has 6 points advantage over Memo, signaling that incorporating contextual features and modeling tag-to-tag transitions are useful. Lastly, the biLSTM with CRF tagger performs the best with 97.47 INLINEFORM2 score.", "We used CRF BIBREF12 as another comparison since it is the most common non-neural model for sequence labeling tasks. We employed contextual words as well as affixes as features. For some context window size INLINEFORM0 , the complete list of features is:"]}
{"question_id": "732bd97ae34541f215c436e2a1b98db1649cba27", "predicted_answer": "", "predicted_evidence": ["Since there are multiple tags, there are two flavors to compute an overall INLINEFORM0 score: micro and macro average. For POS tagging task where the tags do not span multiple words, micro-average INLINEFORM1 score is exactly the same as accuracy score. Thus, macro-average INLINEFORM2 score is our only option. However, there is still an issue. Macro-average INLINEFORM3 score computes the overall INLINEFORM4 score by averaging the INLINEFORM5 score of each tag. This approach means that when the model wrongly predicts a rarely occurring tag (e.g., foreign word), it is penalized as heavily as it does a frequent tag. To address this problem, we used weighted macro-average INLINEFORM6 score which takes into account the tag proportion imbalance. It computes the weighted average of the scores where each weight is equal to the corresponding tag's proportion in the dataset. This functionality is available in the scikit-learn library.", "Firstly, we report on our tuning experiments for the neural tagger. Table TABREF27 shows the evaluation results of the many configurations of our neural tagger on the development set. We group the results by the encoding and prediction step configuration. For each group, we show the highest INLINEFORM0 score among many embedding configurations. As we can see, biLSTM with CRF layer achieves 97.60 INLINEFORM1 score, the best score on the development set. This result agrees with many previous work in neural sequence labeling that a bidirectional LSTM with CRF layer performs best BIBREF10 , BIBREF17 , BIBREF9 . Therefore, we will use this tagger to represent the neural model hereinafter.", "To understand the performance of the neural model for each tag, we plot the confusion matrix from the development set of the first fold in Fig. FIGREF30 . The figure shows that the model can predict most tags almost perfectly, except for X and WH tag. The X tag is described as \"a word or part of a sentence which its category is unknown or uncertain\". The X tag is rather rare, as it only appears 397 times out of over 250K tokens. Some words annotated as X are typos and slang words. Some foreign terms and abbreviations are also annotated with X. The model might get confused as such words are usually tagged with a noun tag (NN or NNP). We also see that the model seems to confuse question words (WH) such as apa (what) or siapa (who) as SC since these words may be used in subordinate clauses as well. Looking at the data closely, we found that the tagging of such words are inconsistent.", "Embedding. In the embedding step, the tagger obtains vector representations of each word and additional features. We experimented with several additional features: prefixes, suffixes, and characters. Prefix features are the first 2 and 3 characters of the word. Likewise, suffix features are the last 2 and 3 characters of the word. For the character features, we followed BIBREF9 by embedding each character and composing the resulting vectors with a max-pooled CNN. The final embedding of a word is then the concatenation of all these vectors. Fig. FIGREF17 shows an illustration of the process.", "Part-of-speech (POS) tagging is a process to tag tokens in a string with their corresponding part-of-speech (e.g., noun, verb, etc). POS tagging is considered as one of the most basic tasks in NLP, as it is usually the first component in an NLP pipeline. This is because POS tags are shown to be useful features in various NLP tasks, such as named entity recognition BIBREF0 , BIBREF1 , machine translation BIBREF2 , BIBREF3 and constituency parsing BIBREF4 . Therefore, for any language, building a successful NLP system usually requires a well-performing POS tagger."]}
{"question_id": "183b385fb59ff1e3f658d4555a08b67c005a8734", "predicted_answer": "", "predicted_evidence": ["We used the IDN Tagged Corpus proposed in BIBREF11 . The corpus contains 10K sentences and 250K tokens that are tagged manually. Due to the small size, we used 5-fold cross-validation to split the corpus into training, development, and test sets. We did not split multi-word expressions but treated them as if they are a single token. All 5 folds of the dataset are available publicly to serve as a benchmark for future work.", "For all models, we preprocessed the dataset by lowercasing all words, except when the characters were embedded. For the CRF model, we used L2 regularization whose coefficient was tuned to the development set. As we mentioned previously, we tuned the context window size INLINEFORM0 to the development set as well.", "Since the dataset is highly imbalanced (majority of words are nouns), using accuracy score as the evaluation metric is not appropriate as it gives a high score to a model that always predicts nouns regardless of input. Therefore, we decided to use INLINEFORM0 score which considers both precision and recall of the predictions.", "Firstly, we report on our tuning experiments for the neural tagger. Table TABREF27 shows the evaluation results of the many configurations of our neural tagger on the development set. We group the results by the encoding and prediction step configuration. For each group, we show the highest INLINEFORM0 score among many embedding configurations. As we can see, biLSTM with CRF layer achieves 97.60 INLINEFORM1 score, the best score on the development set. This result agrees with many previous work in neural sequence labeling that a bidirectional LSTM with CRF layer performs best BIBREF10 , BIBREF17 , BIBREF9 . Therefore, we will use this tagger to represent the neural model hereinafter.", "To understand how each feature in the embedding step affects the neural tagger, we performed feature ablation on the development set and put the result in Table TABREF29 . We see that with only words as features (first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the performance further by 0.08 and 0.10 points respectively. From this result, we see that it is the character features that positively affect the neural tagger the most."]}
{"question_id": "5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "predicted_answer": "", "predicted_evidence": ["Actually, it is not by chance that we get these results, because DWE has the advantage of distinguishing between morphologically related words, which can be verified by the results of the similarity task. Meanwhile, in the word analogy task, those words expressing family relations in Chinese are mostly compositional in their character glyphs. For example, in an analogy pair \u201c\u5144\u5f1f\" (brother) : \u201c\u59d0\u59b9\" (sister) = \u201c\u513f\u5b50\" (son) : \u201c\u5973\u513f\" (daughter), we can easily find that \u201c\u5144\u5f1f\" and \u201c\u513f\u5b50\" share an exactly common part of glyph \u201c\u513f\" (male relative of a junior generation) while \u201c\u59d0\u59b9\" and \u201c\u5973\u513f\" share an exactly common part of glyph \u201c\u5973\" (female), and this kind of morphological pattern can be accurately captured by our model.", "Meanwhile, in the word analogy task, those words expressing family relations in Chinese are mostly compositional in their character glyphs. For example, in an analogy pair \u201c\u5144\u5f1f\" (brother) : \u201c\u59d0\u59b9\" (sister) = \u201c\u513f\u5b50\" (son) : \u201c\u5973\u513f\" (daughter), we can easily find that \u201c\u5144\u5f1f\" and \u201c\u513f\u5b50\" share an exactly common part of glyph \u201c\u513f\" (male relative of a junior generation) while \u201c\u59d0\u59b9\" and \u201c\u5973\u513f\" share an exactly common part of glyph \u201c\u5973\" (female), and this kind of morphological pattern can be accurately captured by our model. However, most of the names of countries, capitals and cities are transliterated words, and the relationship between the morphology and semantics of words is minimal, which is consistent with the findings reported in BIBREF4 . For instance, in an analogy pair \u201c\u897f\u73ed\u7259\" (Spain) : \u201c\u9a6c\u5fb7\u91cc\" (Madrid) = \u201c\u6cd5\u56fd\" (France) : \u201c\u5df4\u9ece\" (Paris), we cannot infer any relevance among these four words literally because they are all translated by pronunciation.", "As we mentioned earlier, it is reasonable and imperative to learn Chinese word embeddings from two channels, i.e., a sequential stroke n-gram channel and a spatial glyph channel. Inspired by the previous works BIBREF14 , BIBREF18 , BIBREF4 , BIBREF19 , we propose to combine the representation of Chinese words with the representation of characters to obtain finer-grained semantics, so that unknown words can be identified and their relationship with other known Chinese characters can be found by distinguishing the common stroke sequences or character glyph they share.", "We use gensim to implement both CBOW and Skipgram and apply the source codes pulished by the authors to implement CWE, JWE, GWE and GloVe. Since Cao et al. cao2018cw2vec did not publish their code, we follow their paper and reproduce cw2vec in mxnet which we also use to implement sisg BIBREF21 and our DWE. To encourage further research, we will publish our model and datasets.", "In summary, since different words that are morphologically similar tend to have similar semantics in Chinese, simultaneously modeling the sequential and spatial information of characters from both stroke n-grams and glyph features can indeed improve the modeling of Chinese word representations substantially."]}
{"question_id": "a79a23573d74ec62cbed5d5457a51419a66f6296", "predicted_answer": "", "predicted_evidence": ["UTF8gbsn Our DWE model is shown in Figure FIGREF9 . For an arbitrary Chinese word INLINEFORM0 , e.g., \u201c\u9a7e\u8f66\", it will be firstly decomposed into several characters, e.g., \u201c\u9a7e\" and \u201c\u8f66\", and each of the characters will be further processed in a dual-channel character embedding sub-module to refine its morphological information. In sequential channel, each character can be decomposed into a stroke sequence according to the criteria of Chinese writing system as shown in Figure FIGREF3 . After retrieving the stroke sequence, we add special boundary symbols INLINEFORM1 and INLINEFORM2 at the beginning and end of it and adopt an efficient approach by utilizing the stroke n-gram method BIBREF3 to extract strokes order information for each character. More precisely, we firstly scan each character throughout the training corpus and obtain a stroke n-gram dictionary INLINEFORM3 .", "where INLINEFORM0 and INLINEFORM1 are compositional operation. INLINEFORM6 is the word ID embedding and INLINEFORM7 is the number of characters in INLINEFORM8 .", "As we mentioned earlier, it is reasonable and imperative to learn Chinese word embeddings from two channels, i.e., a sequential stroke n-gram channel and a spatial glyph channel. Inspired by the previous works BIBREF14 , BIBREF18 , BIBREF4 , BIBREF19 , we propose to combine the representation of Chinese words with the representation of characters to obtain finer-grained semantics, so that unknown words can be identified and their relationship with other known Chinese characters can be found by distinguishing the common stroke sequences or character glyph they share.", "UTF8gbsn With the gradual exploration of the semantic features of Chinese, scholars have found that not only words and characters are important semantic carriers, but also stroke feature of Chinese characters is crucial for inferring semantics BIBREF3 . Actually, a Chinese word usually consists of several characters, and each character can be further decomposed into a stroke sequence which is certain and changeless, and this kind of stroke sequence is very similar to the construction of English words. In Chinese, a particular sequence of strokes can reflect the inherent semantics. As shown in the upper half of Figure FIGREF3 , the Chinese character \u201c\u9a7e\" (drive) can be decomposed into a sequence of eight strokes, where the last three strokes together correspond to a root character \u201c\u9a6c\" (horse) similar to the root \u201cclar\" of English word \u201cdeclare\" and \u201cclarify\".", "Moreover, Chinese is a language originated from Oracle Bone Inscriptions (a kind of hieroglyphics). Its character glyphs have a spatial structure similar to graphs which can convey abundant semantics BIBREF4 . Additionally, the critical reason why Chinese characters are so rich in morphological information is that they are composed of basic strokes in a 2-D spatial order. However, different spatial configurations of strokes may lead to different semantics. As shown in the lower half of Figure 1, three Chinese characters \u201c\u5165\" (enter), \u201c\u516b\" (eight) and \u201c\u4eba\" (man) share exactly a common stroke sequence, but they have completely different semantics because of their different spatial configurations."]}
{"question_id": "d427e9d181434078c78b7ee33a26b269f160f6d2", "predicted_answer": "", "predicted_evidence": ["Moreover, Chinese is a language originated from Oracle Bone Inscriptions (a kind of hieroglyphics). Its character glyphs have a spatial structure similar to graphs which can convey abundant semantics BIBREF4 . Additionally, the critical reason why Chinese characters are so rich in morphological information is that they are composed of basic strokes in a 2-D spatial order. However, different spatial configurations of strokes may lead to different semantics. As shown in the lower half of Figure 1, three Chinese characters \u201c\u5165\" (enter), \u201c\u516b\" (eight) and \u201c\u4eba\" (man) share exactly a common stroke sequence, but they have completely different semantics because of their different spatial configurations.", "We use gensim to implement both CBOW and Skipgram and apply the source codes pulished by the authors to implement CWE, JWE, GWE and GloVe. Since Cao et al. cao2018cw2vec did not publish their code, we follow their paper and reproduce cw2vec in mxnet which we also use to implement sisg BIBREF21 and our DWE. To encourage further research, we will publish our model and datasets.", "In addition, some biological investigations have confirmed that there are actually two processing channels for Chinese language. Specifically, Chinese readers not only activate the left brain which is a dominant hemisphere in processing alphabetic languages BIBREF5 , BIBREF6 , BIBREF7 , but also activate the areas of the right brain that are responsible for image processing and spatial information at the same time BIBREF8 . Therefore, we argue that the morphological information of characters in Chinese consists of two parts, i.e., the sequential information hidden in root-like strokes order, and the spatial information hidden in graph-like character glyphs. Along this line, we propose a novel Dual-channel Word Embedding (DWE) model for Chinese to realize the joint learning of sequential and spatial information in characters. Finally, we evaluate DWE on two representative tasks, where the experimental results exactly validate the superiority of DWE in capturing the morphological information of Chinese.", "Word embeddings are fixed-length vector representations for words BIBREF0 , BIBREF1 . In recent years, the morphology of words is drawing more and more attention BIBREF2 , especially for Chinese whose writing system is based on logograms.", "In summary, since different words that are morphologically similar tend to have similar semantics in Chinese, simultaneously modeling the sequential and spatial information of characters from both stroke n-grams and glyph features can indeed improve the modeling of Chinese word representations substantially."]}
{"question_id": "0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "predicted_answer": "", "predicted_evidence": ["At this stage a vocabulary with hashed values and their vectors exist in the model. For the exploitation of adjacent vectors in the state of encoding, values pass through the Convolutional Neural Network (CNN) and get merged with their context. The result of the encoding process is a matrix of vectors that represents information. Before the prediction of an ID, the matrix has to be passed through the Attention Layer of the CNN, using a query vector to summarize the input.", "One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4.", "Both datasets were fed into the library in proper format for training. In training process, the entity recognizer had the same configuration with the POS tagger, using the same percentages for train, validation and test sets. It must be noted that all the models used the Common Crawl pretrained vectors for a vocabulary. The results are compared using the macro F1 score.", "In the \u201cMakedonia\u201d dataset information about named entities is organized with the index of the character the named entity starts, the index of the character the named entity ends and the class of the named entity. The dataset was parsed and the named entities were added into the keyword list, with every record representing the token (or the set of tokens) and its class. Noise was removed from the list and the records were sorted by the length of the entity. The keyword list had an average of 72.000 records.", "In the second experiment, the datasets were compared to a common test set that followed the desired set of rules."]}
{"question_id": "5d03a82a70f7b1ab9829891403ec31607828cbd5", "predicted_answer": "", "predicted_evidence": ["For the creation of a Named Entity Recognizer in the Greek Language a number of steps was followed. The entities from the \u201cMakedonia\u201d dataset were extracted and annotated, forming a set of keywords that matched a specific set of rules the entities had to follow. These keywords were used to reform the dataset and also to find entities from a larger dataset, like Wikipedia. The spaCy model was trained using both datasets and their results are compared to a test set. Additionally, the spaCy model was trained using as a feature the POS tags of the tokens. All results are presented in SECREF13.", "In an experiment worth mentioning the correlation of the part of speech with the performance of the recognizer was explored. In this experiment, both pipelines (part of speech, entity recognition) were used for training with 30 iterations and the model was trained twice: with and without the usage of the part of speech information for recognition.", "Again, the Makedonia corpus performed better, because of the proper annotation on the keyword list.", "It is evident that the recognizer did not gain knowledge from the part of speech tags of the tokens.", "At first the datasets from both sources (Makedonia, Wikipedia) were used for training with 10 iterations and testing from the model. The results can be viewed in the following table:"]}
{"question_id": "6cad6f074b0486210ffa4982c8d1632f5aa91d91", "predicted_answer": "", "predicted_evidence": ["In the second experiment, the datasets were compared to a common test set that followed the desired set of rules.", "Again, the Makedonia corpus performed better, because of the proper annotation on the keyword list.", "Both datasets were fed into the library in proper format for training. In training process, the entity recognizer had the same configuration with the POS tagger, using the same percentages for train, validation and test sets. It must be noted that all the models used the Common Crawl pretrained vectors for a vocabulary. The results are compared using the macro F1 score.", "At prediction, a Softmax function is used for the prediction of a super tag with part of speech and morphology information. Similarly for named entities, the available class is predicted. After the training process of the model, the CNN is able to be used for NLP tasks.", "One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4."]}
{"question_id": "d38b3e0896b105d171e69ce34c689e4a7e934522", "predicted_answer": "", "predicted_evidence": ["Again, the Makedonia corpus performed better, because of the proper annotation on the keyword list.", "Both sources had good results in non entity tokens, which affected the F1 score. Moreover, the model did not perform well for facilities, as polyglot's Greek recognizer does not support that class and FAC entities cover a small amount of the list.", "Part of Speech Tagging for highly inflective languages, such as Greek is quite a difficult task. In the Greek Language, words can have different morphological forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek POS Tagging BIBREF1.", "In order to gain more information about the context of the Greek entities, a percentage of Greek Wikipedia was used. After applying sentence and token segmentation on Wikipedia text and using a pretrained model from polyglot, the keyword list increased. The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC). A percentage of Greek Wikipedia was parsed and used for training in spaCy. The results from the training are presented in SECREF13.", "In the first experiment the model was trained using pretrained vectors extracted from two different sources, Common Crawl and Wikipedia and can be found at the official FastText web page BIBREF8. Both sources were trained on the same algorithm called FastText BIBREF9, an extension of Word2Vec that treats tokens as an average sum of sub-words and finds similarities of words based on their n-grams. The configuration of the FastText model for Wikipedia vectors is according to BIBREF10, whilst the model for CC vectors is a position-weight CBOW 5 length n-grams with a window size of 5 tokens and 10 negative words. The file with the Common Crawl vectors consists of 2.000.000 tokens with 300 dimension, whereas the file with the Wikipedia vectors consists of 300.000 tokens with 300 dimension.The results can be viewed in the following table, with the first part describing the Common Crawl results and the second one the Wikipedia results."]}
{"question_id": "4379a3ece3fdb93b71db43f62833f5f724c49842", "predicted_answer": "", "predicted_evidence": ["Topics. Considering that \"bot\" users may post tweets about a limited number of targeted topics, we used topic modeling to the measure the heterogeneity of topics in a user's tweets. We used Latent Dirichlet Allocation (LDA)BIBREF25 to extract the top five topics from all of the users' 1000 most recent tweets (or all the tweets if a user has posted less than 1000 tweets), and used the mean of the weights of each topic across all of a user's tweets.", "We used the 8262 \"bot\" and \"non-bot\" users in experiments to train and evaluate three classification systems. We split the users into $80\\%$ (training) and $20\\%$ (test) sets, stratified based on the distribution of \"bot\" and \"non-bot\" users. The training set includes $61,160,686$ tweets posted by 6610 users, and the held-out test set includes $15,703,735$ tweets posted by 1652 users. First, we evaluated Botometer on our held-out test set. Botometer is a publicly available bot detection system designed for political dot detection. It outputs a score between 0 and 1 for a user, representing the likelihood that a user is a bot. Second, we used the Botometer score for each user as a feature in training a gradient boosting classifier which is a decision tree-based ensemble machine learning algorithm with gradient boosting BIBREF23 and can be used to address class imbalance.", "Mean Daily Posts. Considering that \"bot\" users may post tweets more frequently than \"non-bot\" users, we measured the average and standard deviation of the number of tweets posted daily by a user. As Figure 1 illustrates, a subset of \"bot\" users post, on average, more tweets daily than \"non-bot\" users.", "This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\kappa $ = $0.93$ (Cohen\u2019s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22. Their IAA does not include disagreements resulting from the change of a user's status to or from \"unavailable\" in the time between the first and second annotations.", "User Name. Finally, we used a publicly available lexicon to detect the presence or absence of a person's name in a user name. As Figure 2 illustrates, the name of a person is present (1) in approximately half of \"non-bot\" user names, whereas the name of a person is absent (0) in the majority of \"bot\" user names."]}
{"question_id": "0abc2499195185c94837e0340d00cd3b83ee795e", "predicted_answer": "", "predicted_evidence": ["This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\kappa $ = $0.93$ (Cohen\u2019s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22. Their IAA does not include disagreements resulting from the change of a user's status to or from \"unavailable\" in the time between the first and second annotations.", "This study was funded in part by the National Library of Medicine (NLM) (grant number: R01LM011176) and the National Institute on Drug Abuse (NIDA) (grant number: R01DA046619) of the National Institutes of Health (NIH). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.", "To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\kappa $ = $0.93$ (Cohen\u2019s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22.", "A brief error analysis of the 25 false negatives users (in the held-out test set of 1652 users) from the classifier with the extended feature set reveals that, while only one of the users is an account that automatically re-posts other users' tweets, the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting personal information. These users are particularly challenging to automatically identify as \"bot\" users because, with humans posting on behalf of an online maternity store, or to a pregnancy forum, for example, their tweets resemble those posted by \"non-bot\" users. In future work, we will focus on deriving features for modeling the nuances that distinguish such \"bot\" users.", "Profile Picture. In addition to tweet-related features, we used features based on information in users' profiles. Considering that a \"non-bot\" user's profile picture may be more likely to contain a face, we used a publicly available system to detect the number of faces in a profile picture. As Figure 2, illustrates a face was not detected in the profile picture of the majority of \"non-bot\" users (in the training set), whereas at least one face was detected in the profile picture of the majority of \"bot\" users."]}
{"question_id": "138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "predicted_answer": "", "predicted_evidence": ["Table 1 presents the precision, recall, and F$_1$-scores for the three bot detection systems evaluated on the held-out test set. The F$_1$-score for the \"bot\" class indicates that Botometer ($0.361$), designed for political bot detection, does not generalize well for detecting \"bot\" users in health-related data. Although the classifier with only the Botometer score as a feature ($0.286$) performs even worse than the default Botometer system, our extended feature set significantly improves performance ($0.700$). For imbalanced data, a higher F$_1$-score for the majority class is typical; in this case, it reflects that we have modeled the detection of \"bot\" users based on their natural distribution in health-related data.", "Profile Picture. In addition to tweet-related features, we used features based on information in users' profiles. Considering that a \"non-bot\" user's profile picture may be more likely to contain a face, we used a publicly available system to detect the number of faces in a profile picture. As Figure 2, illustrates a face was not detected in the profile picture of the majority of \"non-bot\" users (in the training set), whereas at least one face was detected in the profile picture of the majority of \"bot\" users.", "User Name. Finally, we used a publicly available lexicon to detect the presence or absence of a person's name in a user name. As Figure 2 illustrates, the name of a person is present (1) in approximately half of \"non-bot\" user names, whereas the name of a person is absent (0) in the majority of \"bot\" user names.", "Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\kappa $ = $0.93$ (Cohen\u2019s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22. Their IAA does not include disagreements resulting from the change of a user's status to or from \"unavailable\" in the time between the first and second annotations. Upon resolving the disagreements, 413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\".", "Topics. Considering that \"bot\" users may post tweets about a limited number of targeted topics, we used topic modeling to the measure the heterogeneity of topics in a user's tweets. We used Latent Dirichlet Allocation (LDA)BIBREF25 to extract the top five topics from all of the users' 1000 most recent tweets (or all the tweets if a user has posted less than 1000 tweets), and used the mean of the weights of each topic across all of a user's tweets."]}
{"question_id": "7e906dc00e92088a25df3719104d1750e5a27485", "predicted_answer": "", "predicted_evidence": ["Our results demonstrate that (i) a publicly available bot detection system, designed for political bot detection, underperforms when applied to health-related data, and (ii) extending the system with simple features derived from health-related data significantly improves performance. An F$_1$-score of $0.700$ for the \"bot\" class represents a promising benchmark for automatic classification of highly imbalanced Twitter data and, in this case, for detecting users who are not reporting information about their own pregnancy on Twitter. Detecting such users is particularly important in the process of automatically selecting cohortsBIBREF26 from a population of social media users for user-level observational studiesBIBREF27.", "This study was funded in part by the National Library of Medicine (NLM) (grant number: R01LM011176) and the National Institute on Drug Abuse (NIDA) (grant number: R01DA046619) of the National Institutes of Health (NIH). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.", "A brief error analysis of the 25 false negatives users (in the held-out test set of 1652 users) from the classifier with the extended feature set reveals that, while only one of the users is an account that automatically re-posts other users' tweets, the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting personal information. These users are particularly challenging to automatically identify as \"bot\" users because, with humans posting on behalf of an online maternity store, or to a pregnancy forum, for example, their tweets resemble those posted by \"non-bot\" users. In future work, we will focus on deriving features for modeling the nuances that distinguish such \"bot\" users.", "As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while existing systems have been successful in detecting bots in other domains, they do not perform as well for detecting health-related bots. Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. Introducing more features would likely contribute to further improving performance, which we will explore in future work.", "Table 1 presents the precision, recall, and F$_1$-scores for the three bot detection systems evaluated on the held-out test set. The F$_1$-score for the \"bot\" class indicates that Botometer ($0.361$), designed for political bot detection, does not generalize well for detecting \"bot\" users in health-related data. Although the classifier with only the Botometer score as a feature ($0.286$) performs even worse than the default Botometer system, our extended feature set significantly improves performance ($0.700$). For imbalanced data, a higher F$_1$-score for the majority class is typical; in this case, it reflects that we have modeled the detection of \"bot\" users based on their natural distribution in health-related data."]}
{"question_id": "0d9241e904bd2bbf5b9a6ed7b5fc929651d3e28e", "predicted_answer": "", "predicted_evidence": ["The baseline correctly picks 12.7% of all candidate responses, out of 100. Given that the dataset is focussed on support questions and multiple responses are likely to be relevant, this baseline already performs admirable. For reference, a BERT model on the OpenSubtitles dataset BIBREF22 achieves a 1-of-100 accuracy between 12.2% and 17.5%, depending on the model size BIBREF26.", "The research on conversational AI has been focussing on various aspects, including building high-quality datasets BIBREF3, BIBREF25, BIBREF22, BIBREF23, BIBREF26, BIBREF27, adding customizable personalities BIBREF23, BIBREF28, BIBREF29 or conjoining the efforts with regard to different datasets, models and evaluation practices BIBREF26. With these combined efforts, businesses and the general public quickly began developing ambitious use-cases, like customer support agents on social networks.", "We investigated the impact of emoji for conversational models and one could argue that these conclusions\u2014or even the BERT model\u2014can be generalized. We didn't investigate whether other tasks also benefited from our fine-tuned BERT model with the additional emoji tokens.", "Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric.", "During evaluation, we also observed utterances with only emoji characters. Even with our model that supports emoji, it could still be difficult to extract information like the subject of a conversation. Some of these utterances\u2014but not all\u2014were part of a larger conversation, so an interesting question could be how additional information affects the model."]}
{"question_id": "95646d0ac798dcfc15b43fa97a1908df9f7b9681", "predicted_answer": "", "predicted_evidence": ["Section SECREF2 will start with an overview of work on emoji representations, emoji-based models and analysis of emoji usage. A brief introduction in conversational systems will also be given. Section SECREF3 will then look into popular datasets with and without emoji and then introduce the dataset we used.", "The same shared task was tackled by BIBREF13 and a year later by BIBREF14, which made use of a pre-processor and tokenizer from BIBREF15. This tokenizer replaces some emoji and emoticons by tokens related to their meaning. So is \\o/ replaced with <happy>. Naturally, this approach suffers from the same issues as described before. And even though it's really useful to have some basic, out-of-the-box support for emoticons thanks to this work, we think that this strategy is too reducing to capture subtle nuances.", "The prevalent use of emoji\u2014and their text-based precursors\u2014is mostly unaddressed in current natural language processing (NLP) tasks. The support of the Unicode Standard BIBREF0 for emoji characters in 2010 ushered in a wide-spread, international adoption of these graphical elements in casual contexts. Interpreting the meaning of these characters has been challenging however, since they take on multiple semantic roles BIBREF1.", "We add new tokens to the BERT tokenizer for 2740 emoji from the Unicode Full Emoji List BIBREF0, as well as some aliases (in the form of :happy: as is a common notation for emoji). In total, 3627 emoji tokens are added to the vocabulary.", "Table TABREF1 gives an overview of frequently used and interesting conversational datasets. The lacuna of emoji-rich reference datasets was already mentioned in Section SECREF1 and is in our opinion one of the factors that emoji remain fairly underutilized."]}
{"question_id": "12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "predicted_answer": "", "predicted_evidence": ["We converted all UTF-8 encoded emoji to a textual alias for two reasons. First, this mitigates potential issues with text encodings that could drop the emoji. Second, this is also a common notation format for custom emoji, so we have one uniform token format. Aside from this attention to emoji, we use WordPiece embeddings BIBREF34 in the same manner as BIBREF5.", "the original word: 10% chance.", "An analysis on the use of emoji on a global scale is done by BIBREF16. For this, the authors used geo-tagged tweets, which also allowed them to correlate the popularity of certain emoji with development indicators. This shows that the information encoded by emoji\u2014and of course the accompanying tweet\u2014is not limited to sentiment or emotion. Also BIBREF17 analyze the uses of emoji on social networks. Their approach consists of finding information networks between emoji and English words with LINE BIBREF18.", "Inspired by the work on word representations, BIBREF8 presented Emoji2vec. This system generates a vector representation that's even compatible with the Word2vec representations, so they can be used together. This compatibility makes it easy to quickly incorporate Emoji2vec in existing systems that use Word2vec.", "Our model will then be discussed in Section SECREF4, including the tokenization in Subsection SECREF4, training setup in Subsection SECREF6 and evaluation in Subsection SECREF10. This brings us to the results of our experiment, which is discussed in Section SECREF5 and finally our conclusion and future work are presented in Section SECREF6."]}
{"question_id": "647f6e6b168ec38fcdb737d3b276f78402282f9d", "predicted_answer": "", "predicted_evidence": ["", "In this paper we discussed the current state of emoji usage for conversational systems, which mainly lacks large baseline datasets. When looking at public datasets, conversational AI makers have to choose between dataset size and emoji support, with some datasets at least containing a few textual emoticons. We argued that this duality results in systems that fail to capture some information encoded in those emoji and in turn fail to respond adequately.", "Based on this premise, we investigated how a response selection system based on BERT can be modified to support emoji. We proposed a format and tokenization method that's indifferent to current Unicode specifications, and thus also works for datasets containing custom emoji.", "Take for example the Ubuntu Dialog Corpus by BIBREF3, a commonly used corpus for multi-turn systems. This dataset was collected from an Internet Relay Chat (IRC) room casually discussing the operating system Ubuntu. IRC nodes usually support the ASCII text encoding, so there's no support for graphical emoji. However, in the 7,189,051 utterances, there are only 9946 happy emoticons (i.e. :-) and the cruelly denosed :) version) and 2125 sad emoticons.", "This work was supported by the Research Foundation - Flanders under EOS No. 30992574."]}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "predicted_answer": "", "predicted_evidence": ["Clinical questions are formed using the PIO framework, where clinical issues are broken down into four components: Population/Problem (P), Intervention (I), Comparator (C), and Outcome (O). We will refer to these categories as PIO elements, by using the common practice of merging the C and I categories. In BIBREF3 a literature screening performed in 10 systematic reviews was studied. It was found that using the PIO framework can significantly improve literature screening efficacy. Therefore, efficient extraction of PIO elements is a key feature of many EBM applications and could be thought of as a multi-label sentence classification problem.", "In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 .", "In order to quantify the performance of the classification model, we computed the precision and recall scores. On average, it was found that the model leads to better results when trained using the BioBERT embedding. In addition, the performance of the PIO classifier was measured by averaging the three Area Under Receiver Operating Characteristic Curve (ROC_AUC) scores for P, I, and O. The ROC_AUC score of 0.9951 was obtained by the model using the general BERT embedding. This score was improved to 0.9971 when using the BioBERT model pre-trained on medical context. The results are illustrated in Figure FIGREF9 .", "Biomedical papers, describing randomized controlled trials in medical intervention, are published at a high rate every year. The volume of these publications makes it very challenging for physicians to find the best medical intervention for a given patient group and condition BIBREF2 . Computational methods and natural language processing (NLP) could be adopted in order to expedite the process of biomedical evidence synthesis. Specifically, NLP tasks applied to well structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment.", "Evidence-based medicine (EBM) is of primary importance in the medical field. Its goal is to present statistical analyses of issues of clinical focus based on retrieving and analyzing numerous papers in the medical literature BIBREF0 . The PubMed database is one of the most commonly used databases in EBM BIBREF1 ."]}
{"question_id": "ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "predicted_answer": "", "predicted_evidence": ["Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 .", "The present work resulted in the creation of a PIO elements dataset, PICONET, and a classification tool. These constitute an important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.", "In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 .", "Evidence-based medicine (EBM) is of primary importance in the medical field. Its goal is to present statistical analyses of issues of clinical focus based on retrieving and analyzing numerous papers in the medical literature BIBREF0 . The PubMed database is one of the most commonly used databases in EBM BIBREF1 .", "For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by taking sentences from sections with headings such as aim. Furthermore, we cleaned the remaining data with various approaches including, but not limited to, language identification, removal of missing values, cleaning unicode characters, and filtering for sequences between 5 and 200 words, inclusive."]}
{"question_id": "afd1c482c311e25fc42b9dd59cdc32ac542f5752", "predicted_answer": "", "predicted_evidence": ["Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 .", "In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 .", "More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts.", "BERT (Bidirectional Encoder Representations from Transformers) is a deep bidirectional attention text embedding model. The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer BIBREF13 , BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks.", "Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 . These models are shallow and limited in terms of modeling capacity. Furthermore, most of these classifiers are trained to extract PIO elements one by one which is sub-optimal since this approach does not allow the use of shared structure among the individual classifiers."]}
{"question_id": "ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "predicted_answer": "", "predicted_evidence": ["The classification model is built on top of the BERT representation by adding a dense layer corresponding to the multi-label classifier with three output neurons corresponding to PIO labels. In order to insure that independent probabilities are assigned to the labels, as a loss function we have chosen the binary cross entropy with logits (BCEWithLogits) defined by DISPLAYFORM0", "In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 .", "Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects.", "Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative.", "In order to enhance the accuracy of the model, we investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to optimize the classification by learning a linear combination of the predicted probabilities, for the three classes, with the TF-IDF and QIEF scores. The results indicate that these text features were adequate for boosting the contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in the latter case."]}
{"question_id": "7066f33c373115b1ead905fe70a1e966f77ebeee", "predicted_answer": "", "predicted_evidence": ["Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label.", "In order to quantify the performance of the classification model, we computed the precision and recall scores. On average, it was found that the model leads to better results when trained using the BioBERT embedding. In addition, the performance of the PIO classifier was measured by averaging the three Area Under Receiver Operating Characteristic Curve (ROC_AUC) scores for P, I, and O. The ROC_AUC score of 0.9951 was obtained by the model using the general BERT embedding. This score was improved to 0.9971 when using the BioBERT model pre-trained on medical context. The results are illustrated in Figure FIGREF9 .", "Since its release, BERT has been pre-trained on a multitude of corpora. In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release BIBREF9 . This model is pre-trained on the BooksCorpus (800M words) BIBREF14 and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT BIBREF15 , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words).", "For the original BERT model, we have chosen the smallest uncased model, Bert-Base. The model has 12 attention layers and all texts are converted to lowercase by the tokenizer BIBREF9 . The architecture of the model is illustrated in Figure FIGREF7 .", "For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by taking sentences from sections with headings such as aim. Furthermore, we cleaned the remaining data with various approaches including, but not limited to, language identification, removal of missing values, cleaning unicode characters, and filtering for sequences between 5 and 200 words, inclusive."]}
{"question_id": "018b81f810a39b3f437a85573d24531efccd835f", "predicted_answer": "", "predicted_evidence": ["Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 .", "BERT (Bidirectional Encoder Representations from Transformers) is a deep bidirectional attention text embedding model. The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer BIBREF13 , BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks.", "We train the base classifier using the original training dataset, using INLINEFORM0 of the whole data as training dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on the excluded one and repeat the process for all five folds.", "Since its release, BERT has been pre-trained on a multitude of corpora. In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release BIBREF9 . This model is pre-trained on the BooksCorpus (800M words) BIBREF14 and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT BIBREF15 , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words).", "In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 ."]}
{"question_id": "e2c8d7f3ef5913582503e50244ca7158d0a62c42", "predicted_answer": "", "predicted_evidence": ["In our work, we looked exclusively at the predictions BERT makes at the word level. BIBREF24 and BIBREF26 examined the internal representations of BERT to find that syntactic concepts are learned at lower levels than semantic concepts. BIBREF23 are also interested in syntactic knowledge and propose a method to evaluate whether entire syntax trees are embedded in a linear transformation of a model's word representation space, finding that BERT does capture such information. As a complementary approach, BIBREF27 studied the attention mechanism of BERT, finding clear correlates with interpretable linguistic structures such as direct objects, and suggest that BERT's success is due in part to its syntactic awareness. However, by subjecting it to existing psycholinguistic tasks, BIBREF32 found that BERT fails in its ability to understand negation. In concurrent work, BIBREF33 show that BERT does not consistently outperform LSTM-based models on English subject-verb agreement tasks.", "Learning general-purpose sentence representations which accurately model sentential semantic content is a current goal of natural language processing research BIBREF0, BIBREF1, BIBREF2, BIBREF3. A prominent and successful approach is to pre-train neural networks to encode sentences into fixed length vectors BIBREF4, BIBREF5, with common architecture choices based on recurrent neural networks BIBREF6, BIBREF7, convolutional neural networks, or transformers BIBREF8. Many core linguistic phenomena that one would like to model in general-purpose sentence representations depend on syntactic structure BIBREF9, BIBREF10. Despite the fact that none of the aforementioned architectures have explicit syntactic structural representations, there is some evidence that these models can approximate such structure-dependent phenomena under certain conditions BIBREF11, BIBREF12, BIBREF13, BIBREF14, in addition to their widespread success in practical tasks.", "Following BIBREF21, we call the syntactically dependent word the \u201ctarget\u201d of the agreement relation, and the word with which it agrees we call the \u201ccontroller\u201d. An example of an agreement relation in English is given in (UNKREF2), in which the inflected form of the verb be (are) reflects the plural number of its syntactic head keys. In all examples in this section, the controller and target are given in bold. In this example, keys is the controller and are is the target of the agreement relation.", "Agreement phenomena are an important and cross-linguistically common property of natural languages, and as such have been extensively studied in syntax and morphology BIBREF19. Languages often express grammatical features, such as number and gender, through inflectional morphology. An agreement relation is a morphophonologically overt co-variance in feature values between two words in a syntactic relationship BIBREF20. In other words, agreement refers to when the morphosyntactic features of one word are reflected in its syntactic dependents. In this way, agreement relations are overt markers of covert syntactic structure. Thus, evaluating a model's ability to capture agreement relations is also an evaluation of its ability to capture syntactic structure.", "We sourced our cloze data from version 2.4 of the Universal Dependencies treebanks BIBREF28. The UD treebanks use a consistent schema across all languages to annotate naturally occurring sentences at the word level with rich grammatical information. We used the part-of-speech and dependency information to identify potential agreement relations. Specifically, we identified all instances of subject-verb, noun-determiner, noun-attributive adjective and subject-predicate adjective word pairs. We then used the morphosyntactic annotations for number, gender, case and person to filter out word pairs that disagree due to errors in the underlying data source (e.g. one is annotated as plural while the other is singular) or that are not annotated for any of the four features."]}
{"question_id": "654fe0109502f2ed2dc8dad359dbbce4393e03dc", "predicted_answer": "", "predicted_evidence": ["The agreement relation in (UNKREF2) is between a subject and its verb, but there are other types of agreement relations. In addition to subject-verb agreement, three other types of agreement relations are cross-linguistically common: agreement of noun with i) determiner, ii) attributive adjective and iii) predicate adjective BIBREF22. The latter two types are distinguished by whether the adjective modifies the noun within a noun phrase or whether it is predicated of the subject of a clause. The first two types are sometimes categorized as nominal concord rather than agreement, but for our purposes this is merely a difference in terminology.", "The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information. Even when one word from an agreement relation is masked out, other cues remain in the sentence (e.g. when masking out the noun for a French attributive adjective agreement relation, number information is still available from the determiner). To counter this in future work, we plan to run our experiment twice, masking out the controller and then the target. Second, we used a different evaluation scheme than previous work BIBREF17 by averaging BERT's predictions over many word types and plan to compare both schemes in future work.", "Given the success of large pre-trained language representation models on downstream tasks, it is not surprising that that the field wants to understand the extent of their linguistic knowledge. In our work, we looked exclusively at the predictions BERT makes at the word level. BIBREF24 and BIBREF26 examined the internal representations of BERT to find that syntactic concepts are learned at lower levels than semantic concepts. BIBREF23 are also interested in syntactic knowledge and propose a method to evaluate whether entire syntax trees are embedded in a linear transformation of a model's word representation space, finding that BERT does capture such information. As a complementary approach, BIBREF27 studied the attention mechanism of BERT, finding clear correlates with interpretable linguistic structures such as direct objects, and suggest that BERT's success is due in part to its syntactic awareness.", "In the design of our datasets, we followed two principles. First, we chose data sources that are available across multiple languages, because we are interested in cross-linguistic generality. The languages in this study are those with sufficiently large data sources that also appear in the multilingual BERT model. Second, we use naturally-occurring data (cf. BIBREF18).", "Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by BIBREF17 showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result."]}
{"question_id": "da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "predicted_answer": "", "predicted_evidence": ["BAE-I: Insert a token to the left or right of $t$", "BAE-R: Replace token $t$ (See Algorithm )", "To ensure semantic similarity on introducing perturbations in the input text, we filter the set of top K masked tokens (K is a pre-defined constant) predicted by BERT-MLM using a Universal Sentence Encoder (USE) BIBREF14 based sentence similarity scorer. For the R operations we add an additional check for grammatical correctness of the generated adversarial example by filtering out predicted tokens that do not form the same part of speech (POS) as the original token $t_i$ in the sentence.", "Some tokens in the input are more attended to by $C$ than others, and therefore contribute more towards the final prediction. Replacing these tokens or inserting a new token adjacent to them can thus have a stronger effect on altering the classifier prediction. We estimate the token importance $I_i$ of each token $t_i \\in \\mathbb {S}=[t_1, \\dots , t_n]$, by deleting $t_i$ from $\\mathbb {S}$ and computing the decrease in probability of predicting the correct label $y$, similar to BIBREF11.", "The recent advent of powerful language models BIBREF12, BIBREF13 in NLP has paved the way for using them in various downstream applications. In this paper, we present a simple yet novel technique: BAE (BERT-based Adversarial Examples), which uses a language model (LM) for token replacement to best fit the overall context. We perturb an input sentence by either replacing a token or inserting a new token in the sentence, by means of masking a part of the input and using a LM to fill in the mask (See Figure FIGREF1). BAE relies on the powerful BERT masked LM for ensuring grammatical correctness of the adversarial examples. Our attack beats the previous baselines by a large margin and confirms the inherent vulnerabilities of modern text classification models to adversarial attacks. Moreover, BAE produces more richer and natural looking adversarial examples as it uses the semantics learned by a LM."]}
{"question_id": "363a24ecb8ab45215134935e7e8165fff72ff90f", "predicted_answer": "", "predicted_evidence": ["Effectiveness We study the effectiveness of BAE on limiting the number of R/I operations permitted on the original text. We plot the attack performance as a function of maximum $\\%$ perturbation (ratio of number of word replacements and insertions to the length of the original text) for the TREC dataset. From Figure , we clearly observe that the BAE attacks are consistently stronger than TextFooler. The classifier models are relatively robust to perturbations up to 20$\\%$, while the effectiveness saturates at 40-50$\\%$. Surprisingly, a 50$\\%$ perturbation for the TREC dataset translates to replacing or inserting just 3-4 words, due to the short text lengths.", "Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.", "Replace a token $t \\in \\mathbb {S}$ with another", "Recent studies have shown the vulnerability of ML models to adversarial attacks, small perturbations which lead to misclassification of inputs. Adversarial example generation in NLP BIBREF0 is more challenging than in common computer vision tasks BIBREF1, BIBREF2, BIBREF3 due to two main reasons: the discrete nature of input space and ensuring semantic coherence with the original sentence. A major bottleneck in applying gradient based BIBREF4 or generator model BIBREF5 based approaches to generate adversarial examples in NLP is the backward propagation of the perturbations from the continuous embedding space to the discrete token space.", "BAE-I: Insert a token to the left or right of $t$"]}
{"question_id": "74396ead9f88a9efc7626240ce128582ab69ef2b", "predicted_answer": "", "predicted_evidence": ["Most research on automatic sarcasm detection to date has focused on the Twitter domain, which boasts an ample source of publicly-available data, some of which is already self-labeled by users for the presence of sarcasm (e.g., with #sarcasm). However, Twitter is highly informal, space-restricted, and subject to frequent topic fluctuations from one post to the next due to the ebb and flow of current events\u2014in short, it is not broadly representative of most text domains. Thus, sarcasm detectors trained using features designed for maximum Twitter performance are not necessarily transferable to other domains. Despite this, it is desirable to develop approaches that can harness the more generalizable information present in the abundance of Twitter data.", "Twitter-specific features are based on the work of BIBREF15 maynard2014cares and BIBREF4 RiloffSarcasm. Maynard and Greenwood detect sarcastic tweets by checking for the presence of learned hashtags that correspond with sarcastic tweets, as well as sarcasm-indicator phrases and emoticons. We construct binary features based on their work, and on Riloff et al.'s work RiloffSarcasm, which determined whether or not a tweet was sarcastic by checking for positive sentiment phrases contrasting with negative situations (both of which were learned from other sarcastic tweets). We also add a feature indicating the presence of laughter terms. Amazon-based features are primarily borrowed from BIBREF12 's buschmeier-cimiano-klinger:2014:W14-26 earlier work on the Amazon dataset. [4]Individual binary features for each of the sarcasm hashtags (5 features) and laughter tokens (9 features) were also included.", "The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276.", "Three feature sets were developed (one general, and two targeted toward Twitter and Amazon, respectively). Resources used to develop the features are described in Table TABREF9 . Five classifiers (Na\u00efve Bayes, J48, Bagging, DecisionTable, and SVM), all from the Weka library, were tested using five-fold cross-validation on the training sets, and the highest-scoring (Na\u00efve Bayes) was selected for use on the test set.", "Each model was tested on the Amazon test data (the model trained only on Twitter was also tested on the Twitter test set). Amazon reviews were selected as the target domain since the Twitter dataset was much larger than the Amazon dataset; this scenario is more consistent with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for a domain-general approach. [6]Part-of-speech is considered in MPQA; Amazon and Twitter data was tagged using Stanford CoreNLP BIBREF20 and the Twitter POS-tagger BIBREF21 , respectively."]}
{"question_id": "8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "predicted_answer": "", "predicted_evidence": ["In this work, we develop a set of domain-independent features for sarcasm detection and show that the features generally perform well across text domains. Further, we validate that domain adaptation can be applied to sarcasm detection to leverage patterns in out-of-domain training data, even when results from training only on that source domain data are extremely bad (far below baseline results), to improve over training on only the target data or over training on the simply combined dataset. Finally, we make a new dataset of sarcastic and non-sarcastic tweets available online as a resource to other researchers.", "Most research on automatic sarcasm detection to date has focused on the Twitter domain, which boasts an ample source of publicly-available data, some of which is already self-labeled by users for the presence of sarcasm (e.g., with #sarcasm). However, Twitter is highly informal, space-restricted, and subject to frequent topic fluctuations from one post to the next due to the ebb and flow of current events\u2014in short, it is not broadly representative of most text domains. Thus, sarcasm detectors trained using features designed for maximum Twitter performance are not necessarily transferable to other domains. Despite this, it is desirable to develop approaches that can harness the more generalizable information present in the abundance of Twitter data.", "The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset).", "Requiring that the specified hashtag trailed the rest of the tweet (it could only be followed by other hashtags) was done based on the observation that when sarcastic or emotional hashtags occur in the main tweet body, the tweet generally discusses sarcasm or the specified emotion, rather than actually expressing sarcasm or the specified emotion. Finally, requiring that only one of the specified hashtags trailed the tweet eliminated cases of ambiguity between sarcastic and non-sarcastic tweets. All trailing \u201c#sarcasm\u201d or emotion hashtags were removed from the data before training and testing, and both datasets were randomly divided into training (80%) and testing (20%) sets. Further details are shown in Table TABREF6 .", "Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags \u201c#sarcasm,\u201d \u201c#happiness,\u201d \u201c#sadness,\u201d \u201c#anger,\u201d \u201c#surprise,\u201d \u201c#fear,\u201d and \u201c#disgust\u201d were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way."]}
{"question_id": "eaed0b721cc3137b964f5265c7ecf76f565053e9", "predicted_answer": "", "predicted_evidence": ["Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags \u201c#sarcasm,\u201d \u201c#happiness,\u201d \u201c#sadness,\u201d \u201c#anger,\u201d \u201c#surprise,\u201d \u201c#fear,\u201d and \u201c#disgust\u201d were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way.", "Results are reported for models trained only on Twitter, only on Amazon, on both training sets, and on both training sets when Daum\u00e9's daumeiii:2007:ACLMain EasyAdapt technique is applied, employing Twitter as the algorithm's source domain and Amazon as its target domain. EasyAdapt works by modifying the feature space so that it contains three mappings of the original features: a general (source + target) version, a source-only version, and a target-only version. More specifically, assuming an input feature set INLINEFORM0 for some INLINEFORM1 , where INLINEFORM2 is the number of features in the set, EasyAdapt transforms INLINEFORM3 to the augmented set, INLINEFORM4 . The mappings INLINEFORM5 for the source and target domain data, respectively, are defined as INLINEFORM6 and INLINEFORM7 , where INLINEFORM8 is the zero vector.", "Most research on automatic sarcasm detection to date has focused on the Twitter domain, which boasts an ample source of publicly-available data, some of which is already self-labeled by users for the presence of sarcasm (e.g., with #sarcasm). However, Twitter is highly informal, space-restricted, and subject to frequent topic fluctuations from one post to the next due to the ebb and flow of current events\u2014in short, it is not broadly representative of most text domains. Thus, sarcasm detectors trained using features designed for maximum Twitter performance are not necessarily transferable to other domains. Despite this, it is desirable to develop approaches that can harness the more generalizable information present in the abundance of Twitter data.", "This work develops a set of domain-independent features and demonstrates their usefulness for general sarcasm detection. Moreover, it shows that by applying a domain adaptation step to the extracted features, even a surplus of \u201cbad\u201d training data can be used to improve the performance of the classifier on target domain data, reducing error by 14% relative to prior work. The Twitter corpus described in this paper is publicly available for research purposes,[2] and represents a substantial contribution to multiple NLP sub-communities. This shared corpus of tweets annotated for sarcasm will hasten the advancement of further research. In the future, we plan to extend our approach to detect sarcasm in a completely novel domain, literature, eventually integrating the work into an application to support reading comprehension.", "Three feature sets were developed (one general, and two targeted toward Twitter and Amazon, respectively). Resources used to develop the features are described in Table TABREF9 . Five classifiers (Na\u00efve Bayes, J48, Bagging, DecisionTable, and SVM), all from the Weka library, were tested using five-fold cross-validation on the training sets, and the highest-scoring (Na\u00efve Bayes) was selected for use on the test set."]}
{"question_id": "ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "predicted_answer": "", "predicted_evidence": ["Requiring that the specified hashtag trailed the rest of the tweet (it could only be followed by other hashtags) was done based on the observation that when sarcastic or emotional hashtags occur in the main tweet body, the tweet generally discusses sarcasm or the specified emotion, rather than actually expressing sarcasm or the specified emotion. Finally, requiring that only one of the specified hashtags trailed the tweet eliminated cases of ambiguity between sarcastic and non-sarcastic tweets. All trailing \u201c#sarcasm\u201d or emotion hashtags were removed from the data before training and testing, and both datasets were randomly divided into training (80%) and testing (20%) sets. Further details are shown in Table TABREF6 .", "However, combining all of that same Twitter data with a much smaller amount of Amazon data (3998 Twitter training instances relative to 1003 Amazon training instances) and applying EasyAdapt to the combined dataset performed quite well ( INLINEFORM0 =0.780). The classifier was able to take advantage of a wealth of additional Twitter samples that had led to terrible performance on their own ( INLINEFORM1 =0.276). Thus, the high performance demonstrated when the EasyAdapt algorithm is applied to the training data from the two domains is particularly impressive. It shows that more data is indeed better data\u2014provided that the proper features are selected and the classifier is properly guided in handling it.", "Each model was tested on the Amazon test data (the model trained only on Twitter was also tested on the Twitter test set). Amazon reviews were selected as the target domain since the Twitter dataset was much larger than the Amazon dataset; this scenario is more consistent with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for a domain-general approach. [6]Part-of-speech is considered in MPQA; Amazon and Twitter data was tagged using Stanford CoreNLP BIBREF20 and the Twitter POS-tagger BIBREF21 , respectively.", "Since tweets are self-labeled, users' own varying definitions of sarcasm lead to some extreme idiosyncrasies in the kinds of tweets labeled as sarcastic. Sarcastic tweets were also often dependent upon outside context. Some examples include (#sarcasm tags were removed in the actual dataset): \u201cMy daughter's 5th grade play went over as professional, flawless, and well rehearsed as a Trump speech. #sarcasm,\u201d \u201c#MilanAlessandria Mario Balotelli scored the fifth goal in the 5-0 win. He should play for the #Azzurri at #EURO2016. #sarcasm,\u201d and \u201cGood morning #sarcasm.\u201d", "Twitter-specific features are based on the work of BIBREF15 maynard2014cares and BIBREF4 RiloffSarcasm. Maynard and Greenwood detect sarcastic tweets by checking for the presence of learned hashtags that correspond with sarcastic tweets, as well as sarcasm-indicator phrases and emoticons. We construct binary features based on their work, and on Riloff et al.'s work RiloffSarcasm, which determined whether or not a tweet was sarcastic by checking for positive sentiment phrases contrasting with negative situations (both of which were learned from other sarcastic tweets). We also add a feature indicating the presence of laughter terms. Amazon-based features are primarily borrowed from BIBREF12 's buschmeier-cimiano-klinger:2014:W14-26 earlier work on the Amazon dataset. [4]Individual binary features for each of the sarcasm hashtags (5 features) and laughter tokens (9 features) were also included."]}
{"question_id": "38af3f25c36c3725a31304ab96e2c200c55792b4", "predicted_answer": "", "predicted_evidence": ["Twitter is a micro-blogging service that allows users to post short \u201ctweets\u201d to share content or describe their feelings or opinions in 140 characters or less. For researchers, it boasts a low cost of annotation and plentiful supply of data (users often self-label their tweets using the \u201c#\u201d symbol\u2014many explicitly label their sarcastic tweets using the hashtag \u201c#sarcasm\u201d). A variety of approaches have been taken toward automatically detecting sarcasm on Twitter, including explicitly using the information present in a tweet's hashtag(s); Maynard and Greenwood maynard2014cares learned which hashtags characteristically corresponded with sarcastic tweets, and used the presence of those indicators to predict other sarcastic tweets, with high success. BIBREF2 liebrecht2013perfect detected sarcasm in Dutch tweets using unigram, bigram, and trigram features.", "Other researchers have had success identifying sarcasm by a tweet's use of positive sentiment to describe a negative situation BIBREF4 , employing contextual BIBREF5 or pragmatic BIBREF6 features, and observing the writing style and emotional scenario of a tweet BIBREF7 . An underlying theme among these methods is that the features are generally designed specifically for use with tweets. A major challenge in developing a more general approach for sarcasm detection lies in developing features that are present across many domains, yet still specific enough to reliably capture the differences between sarcastic and non-sarcastic text.", "Finally, some past research has found that it is more difficult to discriminate between sarcastic and non-sarcastic texts when the non-sarcastic texts contain sentiment BIBREF6 , BIBREF8 . Since our non-sarcastic tweets are emotionally-charged, our classifier may have exhibited lower performance than it would have with only neutral non-sarcastic tweets. Since distinguishing between literal and sarcastic sentiment is useful for real-world applications of sarcasm detection, we consider the presence of sentiment in our dataset to be a worthwhile challenge.", "In this work, we develop a set of domain-independent features for sarcasm detection and show that the features generally perform well across text domains. Further, we validate that domain adaptation can be applied to sarcasm detection to leverage patterns in out-of-domain training data, even when results from training only on that source domain data are extremely bad (far below baseline results), to improve over training on only the target data or over training on the simply combined dataset. Finally, we make a new dataset of sarcastic and non-sarcastic tweets available online as a resource to other researchers.", "Results are reported for models trained only on Twitter, only on Amazon, on both training sets, and on both training sets when Daum\u00e9's daumeiii:2007:ACLMain EasyAdapt technique is applied, employing Twitter as the algorithm's source domain and Amazon as its target domain. EasyAdapt works by modifying the feature space so that it contains three mappings of the original features: a general (source + target) version, a source-only version, and a target-only version. More specifically, assuming an input feature set INLINEFORM0 for some INLINEFORM1 , where INLINEFORM2 is the number of features in the set, EasyAdapt transforms INLINEFORM3 to the augmented set, INLINEFORM4 . The mappings INLINEFORM5 for the source and target domain data, respectively, are defined as INLINEFORM6 and INLINEFORM7 , where INLINEFORM8 is the zero vector. Refer to Daum\u00e9 daumeiii:2007:ACLMain for an in-depth discussion of this technique."]}
{"question_id": "9465d96a1368299fd3662d91aa94ba85347b4ccd", "predicted_answer": "", "predicted_evidence": ["Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay BIBREF8 with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures BIBREF9. A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11. The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense BIBREF5.", "A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11. The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense BIBREF5. A CNN (Convolutional neural network) deep learning approach outperformed every model trained, with pre-trained FastText embeddings and updateable embeddings learned by the model as features. In OffensEval (SemEval-2019 Task 6), participants had the opportunity to use the OLID to train their own systems, with the top teams outperforming the original models trained on the dataset.", "Research on other languages includes datasets such as: A Dutch corpus of posts from the social networking site Ask.fm for the detection of cyberbullying BIBREF15, a German Twitter corpus exploring the issue of hate speech targeted to refugees BIBREF16, another Dutch corpus using data from two anti-Islamic groups in Facebook BIBREF17, a hate speech corpus in Italian BIBREF18, an abusive language corpus in Arabic BIBREF19, a corpus of offensive comments from Facebook and Reddit in Danish BIBREF20, another Twitter corpus in German BIBREF4 for GermEval2018, a second Italian corpus from Facebook and Twitter BIBREF21, an aggressive post corpus from Mexican Twitter in Spanish BIBREF2 and finally an aggressive comments corpus from Facebook in Hindi BIBREF3. SemEval 2019 presented a novel task: Multilingual detection of hate speech specifically against immigrants and women with a dataset from Twitter, in English and Spanish BIBREF22.", "attacking or containing abusive words) or okay BIBREF8 with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures BIBREF9. A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11. The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense BIBREF5. A CNN (Convolutional neural network) deep learning approach outperformed every model trained, with pre-trained FastText embeddings and updateable embeddings learned by the model as features.", "The final query for data collection was for tweets containing \u201c\u03b5\u03af\u03c3\u03b1\u03b9\u201d (eisai, \u201cyou are\u201d) as a keyword, inspired by BIBREF5. This particular keyword is considered a stop word as it is quite common and frequent in languages but was suspected to prove helpful for building the dataset for this particular project, as offensive language often follows the following structure: auxiliary verb (be) + noun/adjective. The immediacy of social media and specifically Twitter provides the opportunity for targeted insults to be investigated, following data mining of tweets including \u201cyou are\u201d as a keyword. In fact, many tweets present in the dataset showed users verbally insulting other users or famous people and TV personas, confirming that \u201c\u03b5\u03af\u03c3\u03b1\u03b9\u201d was a facilitating keyword for the task in question."]}
{"question_id": "e8c3f59313df20db0cdd49b84a37c44da849fe17", "predicted_answer": "", "predicted_evidence": ["Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay BIBREF8 with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures BIBREF9. A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11.", "Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay BIBREF8 with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures BIBREF9. A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11. The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense BIBREF5.", "We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID BIBREF5. Duplicate punctuation such as question and exclamation marks was normalized. After removing duplicate tweets, the dataset was comprised of 46,218 tweets of which 5,000 were randomly sampled for annotation. We used LightTag to annotate the dataset due to its simple and straightforward user interface and limitless annotations, provided by the software creators.", "Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset. Inter-annotator agreement was subsequently calculated and labels with 100% agreement were deemed acceptable annotations. In cases of disagreement, labels with majority agreement above 66% were selected as the actual annotations of the tweets in question. For labels with complete disagreement between annotators, one of the authors of this paper reviewed the tweets with two extra human judges, to get the desired majority agreement above 66%. Figure FIGREF6 is a confusion matrix that shows the inter-annotator agreement or reliability, statistically measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content.", "The posts in OGTD v1.0 were collected between May and June, 2019. We used the Twitter API initially collecting tweets from popular and trending hashtags in Greece, including television programs such as series, reality and entertainment shows. Due to the municipal, regional as well as the European Parliament election taking place at the time, many hashtags included tweets discussing the elections. The intuition behind this approach is that Twitter as a microblogging service often gathers complaints and profane comments on widely viewed television and politics, and as such, this period was a good opportunity for data collection."]}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "predicted_answer": "", "predicted_evidence": ["We would like to acknowledge Maria, Raphael and Anastasia, the team of volunteer annotators that provided their free time and efforts to help us produce v1.0 of the dataset of Greek tweets for offensive language detection, as well as Fotini and that helped review tweets with ambivalent labels. Additionally, we would like to express our sincere gratitude to the LightTag team and especially to Tal Perry for granting us free use for their annotation platform.", "Finally, both OGTD v1.0 and v2.0 provide the opportunity for researchers to test cross-lingual learning methods as it can be used in conjunction with the English OLID and other datasets annotated using the same guidelines such as the one by sigurbergsson2019offensive for Danish and by coltekikin2020 for Turkish while simultaneously facilitating the development of language resources for NLP in Greek.", "We have recently released OGTD v2.0 as training data for OffensEval 2020 (SemEval-2020 Task 12) BIBREF28. The reasoning behind the expansion of the dataset was to have a larger Greek dataset for the competition. New posts were collected in November 2019 following the same approach we used to compile v1.0 described in this paper. This second batch of tweets included tweets with trending hashtags, shows and topics from Greece at the time. Additionally, keywords that proved to retrieve interesting tweets in the first version were once again used in the search, along with new keywords like pejorative terms. When the collection was finished, 5,508 tweets were randomly sampled to be then annotated by a team of volunteers. The annotation guidelines were the same ones we used for v1.0. OGTD v2.0 combines the existing with the newly annotated tweets in a larger dataset of 10,287 instances.", "In the age of social media, offensive content online has become prevalent in recent years. There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types BIBREF0. BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources.", "Several experiments were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning models. These features were selected based on previous methodology used by researchers and taking the dataset size into consideration. The TF-IDF weighted features are often used for text classification and are useful for determining how important a word is to a post in a corpus. The threshold for corpus specific words was set to 80%, ignoring terms appearing in more than 80% of the documents while the minimum document frequency was set to 6, and both unigrams and bigrams were tested. Given the consistent use of linguistic features for training machine learning models and results from previous work for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features."]}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "predicted_answer": "", "predicted_evidence": ["For the first six deep learning models we used Greek word embeddings trained on a large Greek web corpus BIBREF23. Each Greek word can be represented with a 300 dimensional vector using the trained model. The vector then can be used to feed in to the deep learning models which will be described in section SECREF16. For the last deep learning architecture we wanted to use a BERT BIBREF24 model trained on Greek. However there was no BERT model available for Greek language. The model that came closest our requirement was multilingual BERT model trained on 108 languages BIBREF24 including Greek. Since training BERT is a very computationaly expensive task we used the available multilingual BERT cased model for the sixth deep learning architecture.", "The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD.", "The performance of individual classifiers for offensive language identification with TF/IDF unigram features is demonstrated in table TABREF8 below. We can see that both linear classifiers (SVM and SGDC) outperform the other classifiers in terms of macro-F1, which does not take label imbalance into account. The Linear SVM and SGDC perform almost identically, with the Linear SVM performing slightly better in recall score for the Not Offensive class and SGDC in recall score for the Offensive class. Bernoulli Na\u00efve Bayes performs better than all classifiers in recall score for the Offensive class but yields the lowest precision score of all classifiers. While the RBF SVM and Multinomial Na\u00efve Bayes yield better recall score for the Not Offensive class, their recall scores for the Offensive class are really low.", "TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning models. These features were selected based on previous methodology used by researchers and taking the dataset size into consideration. The TF-IDF weighted features are often used for text classification and are useful for determining how important a word is to a post in a corpus. The threshold for corpus specific words was set to 80%, ignoring terms appearing in more than 80% of the documents while the minimum document frequency was set to 6, and both unigrams and bigrams were tested. Given the consistent use of linguistic features for training machine learning models and results from previous work for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features. Using the spaCy pipeline for Greek, POS-tags and dependency relations were extracted for every token in a tweet, which were then transformed to count matrices. A sentiment lexicon was considered, but one suitable for this project is as of yet unavailable for Greek.", "Following the methodology described in BIBREF5 and others, including a recent comparable Danish dataset BIBREF20, we collected tweets using keywords such as sensitive or obscene language. Queries for tweets containing common curse words and expressions usually found in offensive messages in Greek as keywords (such as the well-known word for \u201casshole\u201d, \u201c\u03bc\u03b1\u03bb\u03ac\u03ba\u03b1\u03c2\u201d (malakas) or \u201cgo to hell\u201d, \u201c\u03c3\u03c4\u03bf \u03b4\u03b9\u03ac\u03bf\u03bb\u03bf\u201d (sto diaolo), etc.) returned a large number of tweets. Aiming to compile a dataset including offensive tweets of diverse types (sexist, racist, etc.) targeted at various social groups, the Twitter API was queried with expletives such as \u201c\u03c0\u03bf\u03c5\u03c4\u03ac\u03bd\u03b1\u201d (poutana, \u201cwhore\u201d), \u201c\u03ba\u03b1\u03c1\u03b9\u03cc\u03bb\u03b1\u201d (kariola, \u201cbitch\u201d), \u201c\u03c0\u03bf\u03cd\u03c3\u03c4\u03b7\u03c2\u201d (poustis, \u201cfaggot\u201d), etc."]}
{"question_id": "de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "predicted_answer": "", "predicted_evidence": ["In the age of social media, offensive content online has become prevalent in recent years. There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types BIBREF0. BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources.", "Models trained with TF/IDF bigram features performed worse, with scores of all evaluation metrics dropping with the exception of Multinomial Na\u00efve Bayes which improved in F1-score for the Not Offensive class. The full results are reported in table TABREF9 below. Three other approaches were opted for training the models with the implementation of POS and dependency relation tags via a transformation pipeline, also including TF/IDF unigram features, performing better than the addition of bigrams.", "In the age of social media, offensive content online has become prevalent in recent years. There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types BIBREF0. BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish BIBREF2, Hindi BIBREF3, and German BIBREF4, to name a few.", "We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID BIBREF5. Duplicate punctuation such as question and exclamation marks was normalized. After removing duplicate tweets, the dataset was comprised of 46,218 tweets of which 5,000 were randomly sampled for annotation. We used LightTag to annotate the dataset due to its simple and straightforward user interface and limitless annotations, provided by the software creators.", "Experiments with linguistic features were conducted, to inspect their efficiency for this task. For these experiments, the RBF SVM was not used due to data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all three features are shown in table TABREF10 below. While the Linear SVM model improved the recall score on the previous model trained with bigrams, the other models show a significant drop in their performance."]}
{"question_id": "5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "predicted_answer": "", "predicted_evidence": ["TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning models. These features were selected based on previous methodology used by researchers and taking the dataset size into consideration. The TF-IDF weighted features are often used for text classification and are useful for determining how important a word is to a post in a corpus. The threshold for corpus specific words was set to 80%, ignoring terms appearing in more than 80% of the documents while the minimum document frequency was set to 6, and both unigrams and bigrams were tested. Given the consistent use of linguistic features for training machine learning models and results from previous work for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features. Using the spaCy pipeline for Greek, POS-tags and dependency relations were extracted for every token in a tweet, which were then transformed to count matrices. A sentiment lexicon was considered, but one suitable for this project is as of yet unavailable for Greek.", "In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score.", "Several experiments were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning models. These features were selected based on previous methodology used by researchers and taking the dataset size into consideration. The TF-IDF weighted features are often used for text classification and are useful for determining how important a word is to a post in a corpus. The threshold for corpus specific words was set to 80%, ignoring terms appearing in more than 80% of the documents while the minimum document frequency was set to 6, and both unigrams and bigrams were tested. Given the consistent use of linguistic features for training machine learning models and results from previous work for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features.", "Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn.", "Queries for tweets containing common curse words and expressions usually found in offensive messages in Greek as keywords (such as the well-known word for \u201casshole\u201d, \u201c\u03bc\u03b1\u03bb\u03ac\u03ba\u03b1\u03c2\u201d (malakas) or \u201cgo to hell\u201d, \u201c\u03c3\u03c4\u03bf \u03b4\u03b9\u03ac\u03bf\u03bb\u03bf\u201d (sto diaolo), etc.) returned a large number of tweets. Aiming to compile a dataset including offensive tweets of diverse types (sexist, racist, etc.) targeted at various social groups, the Twitter API was queried with expletives such as \u201c\u03c0\u03bf\u03c5\u03c4\u03ac\u03bd\u03b1\u201d (poutana, \u201cwhore\u201d), \u201c\u03ba\u03b1\u03c1\u03b9\u03cc\u03bb\u03b1\u201d (kariola, \u201cbitch\u201d), \u201c\u03c0\u03bf\u03cd\u03c3\u03c4\u03b7\u03c2\u201d (poustis, \u201cfaggot\u201d), etc. and their plural forms, to explore the semantic and pragmatic differences of the expletives mentioned above in their different contextual environments."]}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "predicted_answer": "", "predicted_evidence": ["Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay BIBREF8 with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures BIBREF9. A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11.", "", "The bulk of work on detecting abusive posts online addressed particular types of such language like textual attacks and hate speech BIBREF7, aggression BIBREF3, and others. OGTD considers a more general definition of offensiveness inspired by the first layer of the hierarchical annotation model described in BIBREF5. BIBREF5 model distinguishes targeted from general profanity, and considers the target of offensive posts as indicators of potential hate speech posts (insults targeted at groups) and cyberbulling posts (insults targeted at individuals).", "Following the methodology described in BIBREF5 and others, including a recent comparable Danish dataset BIBREF20, we collected tweets using keywords such as sensitive or obscene language. Queries for tweets containing common curse words and expressions usually found in offensive messages in Greek as keywords (such as the well-known word for \u201casshole\u201d, \u201c\u03bc\u03b1\u03bb\u03ac\u03ba\u03b1\u03c2\u201d (malakas) or \u201cgo to hell\u201d, \u201c\u03c3\u03c4\u03bf \u03b4\u03b9\u03ac\u03bf\u03bb\u03bf\u201d (sto diaolo), etc.) returned a large number of tweets. Aiming to compile a dataset including offensive tweets of diverse types (sexist, racist, etc.)", "In the age of social media, offensive content online has become prevalent in recent years. There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types BIBREF0. BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources."]}
{"question_id": "211c242c028b35bb9cbd5e303bb6c750f859fd34", "predicted_answer": "", "predicted_evidence": ["In English there are many datasets available for document- and sentence-level sentiment analysis across different domains and at different levels of annotation BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance.", "In order to provide a simple baseline, we frame the extraction of opinion holders, targets, and phrases as a sequence labeling task and map the NAF tags to BIO tags for the opinions in each review. These tags serve as the gold labels which will need to be predicted at test time. We also perform classification of the polarity of opinion expressions.", "In this paper we have presented the MultiBooked corpus \u2013 a corpus of hotel reviews annotated for aspect-level sentiment analysis available in Basque and Catalan. The aim of this annotation project is to allow researchers to enable research on supervised aspect-level sentiment analysis in Basque and Catalan, as well as provide useful data for cross- and multi-lingual sentiment analysis. We also provide inter-annotator agreement scores and benchmarks, as well as making the corpus available to the community.", "For the extraction of opinion holders, targets, and expressions we train a Conditional Random Field (CRF) on standard features for supervised sequence labeling (word-, subword-, and part-of-speech information of the current word and previous words). For the classification of the polarity of opinion expressions, we use a Bag-of-Words approach to extract features and then train a linear SVM classifier", "Inter-annotator agreement is reported in Table TABREF17 ."]}
{"question_id": "9b05d5f723a8a452522907778a084b52e27fd924", "predicted_answer": "", "predicted_evidence": ["opinion targets,", "The annotation of each corpus was performed in three phases: first, each annotator annotated a small number of reviews (20-50), after which they compared annotations and discussed any differences. Second, the annotators annotated half of the remaining reviews and met again to discuss any new differences. Finally, they annotated the remaining reviews. For cases of conflict after the final iteration, a third annotator decided between the two.", "We used the KafAnnotator Tool BIBREF7 to annotate each review. This tool allows the user to select a span of tokens and to annotate them as any of the four labels mentioned in Section SECREF3 .", "Finally, in order to improve overlap in span selection, we instructed annotators to choose the smallest span possible that retains the necessary information. Even after several iterations, however, there were still discrepancies with difficult examples, such as the one shown in Example UID20 , where the opinion target could be either `attention', `the attention', or `the attention that the staff gave'.", "opinion holders,"]}
{"question_id": "21175d8853fd906266f884bced85c598c35b1cbc", "predicted_answer": "", "predicted_evidence": ["Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus BIBREF13 has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have concentrated on aspect-level sentiment analysis BIBREF14 , BIBREF15 , BIBREF16 .", "The movement towards multi-lingual datasets for sentiment analysis is important because many languages offer different challenges, such as complex morphology or highly productive word formation, which can not be overcome by focusing only on English data.", "During annotation, there were certain sentences which presented a great deal of problems for the annotators. Many of these are difficult because of 1) nested opinions, 2) implicit opinions reported only through the presence or absence of certain aspects, or 3) the difficulty to identify the span of an expression. Here, we give examples of each difficulty and detail how these were resolved during the annotation process.", "For evaluation, we perform a 10-fold cross-validation with 80 percent of the data reserved for training during each fold. For extraction and classification, we report the weighted INLINEFORM0 score. The results of the benchmark experiment (shown in Table TABREF23 ) show that these simple baselines achieve results which are somewhat lower but still comparable to similar tasks in English BIBREF5 . The drop is not surprising given that we use a relatively simple baseline system and due to the fact that Catalan and Basque have richer morphological systems than English, which were not exploited.", "Perfect agreement, therefore, is 1.0 and no agreement whatsoever is 0.0. Similar annotation projects BIBREF13 report INLINEFORM0 scores that range between 0.6 and 0.8 in general."]}
{"question_id": "87c00edc497274ae6a972c3097818de85b1b384f", "predicted_answer": "", "predicted_evidence": ["Here, Message, People, Action and Entity are types of meanings. startcat flag states that Message is the default start category for parsing and generation. simple_sent is a function accepting 3 parameters, of type People, Action, Entity. This function returns a meaning of Message category. Intuitively, each function in the abstract syntax represents a rule in a grammar. The combination of rules used to construct a meaning type can be seen as a syntax tree.", "Natural language generation (NLG) has been one of the key topics of research in natural language processing, which was highlighted by the huge body of work on NLG surveyed in BIBREF0, BIBREF1. With the advances of several devices capable of understanding spoken language and conducting conversation with human (e.g., Google Home, Amazon Echo) and the shrinking gap created by the digital devices, it is not difficult to foresee that the market and application areas of NLG systems will continue to grow, especially in applications whose users are non-experts. In such application, a user often asks for certain information and waits for the answer and a NLG module would return the answer in spoken language instead of text such as in question-answering systems or recommendation systems. The NLG system in these two applications uses templates to generate the answers in natural language for the users. A more advanced NLG system in this direction is described in BIBREF2, which works with ontologies annotated using the Attempto language and can generate a natural language description for workflows created by the systems built in the Phylotastic project.", "Natural language generation (NLG) has been one of the key topics of research in natural language processing, which was highlighted by the huge body of work on NLG surveyed in BIBREF0, BIBREF1. With the advances of several devices capable of understanding spoken language and conducting conversation with human (e.g., Google Home, Amazon Echo) and the shrinking gap created by the digital devices, it is not difficult to foresee that the market and application areas of NLG systems will continue to grow, especially in applications whose users are non-experts. In such application, a user often asks for certain information and waits for the answer and a NLG module would return the answer in spoken language instead of text such as in question-answering systems or recommendation systems. The NLG system in these two applications uses templates to generate the answers in natural language for the users.", "As described in BIBREF2, the author's system retrieves a set of atoms from an ASP program such as those in Listing where phylotastic FindScientificNamesFromWeb GET was shortened to service, propagates the atoms, and constructs a set of sentences having similar structure to the sentence \u201cThe input of phylotastic FindScientificNamesFromWeb GET is a web link. Its outputs are a set of species names and a set of scientific names\u201d. In this sentence, phylotastic FindScientificNamesFromWeb GET is the name of the service involved in the workflow of the Phylotastic project. All of the arguments of the atoms above are the names of classes and instances from Phylotastic ontology.", "The input of Program $\\Pi _3$ is the position ($pos$) of the word in the sentence. Program $\\Pi _3$ is called whenever there is a new complement component discovered. That way of recursive calls is to identify the maximal chunk of the words that support the main components of the sentence. The result of this module is a list of vocabularies for the next steps."]}
{"question_id": "de4e949c6917ff6933f5fa2a3062ba703aba014c", "predicted_answer": "", "predicted_evidence": ["We describe our method of generating natural language in two applications. The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2. Instead of requiring that the ontologies are annotated using Attempto, we use natural language sentences to annotate the ontologies. To test the feasibility of the approach, we also conduct another use case with the second ontology, that is entirely different from the ontologies used in the Phylotastic project. The ontology is about people and includes descriptions for certain class.", "The input of Program $\\Pi _3$ is the position ($pos$) of the word in the sentence. Program $\\Pi _3$ is called whenever there is a new complement component discovered. That way of recursive calls is to identify the maximal chunk of the words that support the main components of the sentence. The result of this module is a list of vocabularies for the next steps.", "With the small set of rules introducing in this paper to recognize sentence structure, there would be very limited 4-gram in the generated text appearing in original Wikipedia corpus. Therefore, we use BLEU-3 with equal weight distribution instead of BLEU-4 to assess the generated content. Table TABREF27 shows the summary of the number of assessable sentences from our system. Out of 62 sentences from 3 portals, the system cannot determine the structure 2 sentences in Mathematics due to their complexity. This low number of failure shows that our 5 proposed sentence structures effectively act as a lower bound on sentence recognition module.", "Given the seed components identified in Section SECREF15 and the above GF rules, a GF grammar for each sentence can be constructed. However, this grammar can only be used to generate fairly simple sentences. For example, for the sentence \u201cBill plays a popular board game with his close friends.\u201d, a GF grammar for structure #2 can be constructed, which can only generate the sentence \u201cBill plays game.\u201d because it does not contain any complement components identified in Section SECREF15. Therefore, we assgin a set of GF rules for the construction of each parameter in the GF rules in Table TABREF19. The set of GF rules has to follow two conventions. The first one is after applying the set of rules to some components of the sentence, the type of the production is one of the type in Table TABREF19, e.g. $NP$, $VP$, $Cl$, $V2$, .... The second convention is that the GF encoder will select the rules as the order from top to bottom in Table TABREF20.", "We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\\Pi _2$ (Listing ). The first four rules of $\\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in \u201cCathy is gorgeous,\u201d the part after tobe is an adjective, but in \u201cCathy is a beautiful girl,\u201d the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\\Pi _2$."]}
{"question_id": "4cf05da602669a4c09c91ff5a1baae6e30adefdf", "predicted_answer": "", "predicted_evidence": ["For comparison, fig:swesize compares this to the effect of decreasing the number of parameters in the LSTM by successively halving the hidden state size. Here the behavior is similar, but unlike the Swedish model which got somewhat better when closely related languages were added, the increase in cross-entropy is monotone. It would be interesting to investigate how the number of model parameters needs to be scaled up in order to accommodate the additional languages, but unfortunately the computational resources for such an experiment increases with the number of languages and would not be practical to carry out with our current equipment.", "Multilingual language models is not a new idea BIBREF3 , the novelty of our work lies primarily in the use of language vectors and the empirical evaluation using nearly a thousand languages.", "We base our experiments on a large collection of Bible translations crawled from the web, coming from various sources and periods of times. Any other multilingual data collection would work as well, but with the selected corpus we have the advantage that we cover the same genre and roughly the same coverage for each language involved. It is also easy to divide the data into training and test sets by using Bible verse numbers, which allows us to control for semantic similarity between languages in a way that would have been difficult in a corpus that is not multi-parallel. Altogether we have 1,303 translations in 990 languages that we can use for our purposes. These were chosen so that the model alphabet size is below 1000 symbols, which was satisfied by choosing only translations in Latin, Cyrillic or Greek script.", "We have shown that language vectors, dense vector representations of natural languages, can be learned efficiently from raw text and possess several interesting properties. First, they capture language similarity to the extent that language family trees can be reconstructed by clustering the vectors. Second, they allow us to interpolate between languages in a sensible way, and even allow adopting the model using a very small set of text, simply by optimizing the language vector.", "In our experiments we use 1024-dimensional LSTMs, 128-dimensional character embeddings, and 64-dimensional language embeddings. Layer normalization BIBREF5 is used, but no dropout or other regularization since the amount of data is very large (about 3 billion characters) and training examples are seen at most twice. For smaller models early stopping is used. We use Adam BIBREF6 for optimization. Training takes between an hour and a few days on a K40 GPU, depending on the data size."]}
{"question_id": "7380e62edcb11f728f6d617ee332dc8b5752b185", "predicted_answer": "", "predicted_evidence": ["If we have a sample of an unknown language or language variant, it is possible to estimate its language vector by backpropagating through the language model with all parameters except the language vector fixed. We found that a very small set of sentences is enough to give a considerable improvement in cross-entropy on held-out sentences. In this experiment, we used 32 sentences from the King James Version of the Bible. Using the resulting language vector, test set cross-entropy improved from 1.39 (using the Modern English language vector as initial value) to 1.35. This is comparable to the result obtained in sec:interpolation, except that here we do not restrict the search space to points on a straight line between two language vectors.", "More interesting results can be obtained if we interpolate between two language variants and compute cross-entropy of a text that represents an intermediate form. fig:eng-enm shows the cross-entropy of the King James Version of the Bible (published 1611), when interpolating between Modern English (1500\u2013) and Middle English (1050\u20131500). The optimal point turns out to be close to the midway point between them.", "We have shown that language vectors, dense vector representations of natural languages, can be learned efficiently from raw text and possess several interesting properties. First, they capture language similarity to the extent that language family trees can be reconstructed by clustering the vectors. Second, they allow us to interpolate between languages in a sensible way, and even allow adopting the model using a very small set of text, simply by optimizing the language vector.", "In additional experiments we found the overall structure of these clusterings to be relatively stable across models, but for very similar languages (such as Danish and the two varieties of Norwegian) the hierarchy might differ, and the some holds for languages or groups that are significantly different from the major groups. An example from fig:germanic is English, which is traditionally classified as a West Germanic language with strong influences from North Germanic as well as Romance languages. In the figure English is (weakly) grouped with the West Germanic languages, but in other experiments it is instead weakly grouped with North Germanic.", "In this section, we present several experiments with the model described. For exploring the language vector space, we use hierarchical agglomerative clustering for visualization. For measuring performance, we use cross-entropy on held out-data. For this, we use a set of the 128 most commonly translated Bible verses, to ensure that the held-out set is as large and overlapping as possible among languages."]}
{"question_id": "f37b01e0c366507308fca44c20d3f69621b94a6e", "predicted_answer": "", "predicted_evidence": ["Concurrent with this work, Johnson2016zeroshot conducted a study using neural machine translation (NMT), where a sub-word decoder is told which language to generate by means of a special language identifier token in the source sentence. This is close to our model, although beyond a simple interpolation experiment (as in our sec:generating) they did not further explore the language vectors, which would have been challenging to do given the small number of languages used in their study.", "For comparison, fig:swesize compares this to the effect of decreasing the number of parameters in the LSTM by successively halving the hidden state size. Here the behavior is similar, but unlike the Swedish model which got somewhat better when closely related languages were added, the increase in cross-entropy is monotone. It would be interesting to investigate how the number of model parameters needs to be scaled up in order to accommodate the additional languages, but unfortunately the computational resources for such an experiment increases with the number of languages and would not be practical to carry out with our current equipment.", "Since our language model is conditioned on a language vector, we can gain some intuitive understanding of the language space by generating text from different points in it. These points could be either one of the vectors learned during training, or some arbitrary other point. tab:interpolation shows text samples from different points along the line between Modern English [eng] and Middle English [enm]. Consistent with the results of Johnson2016zeroshot, it appears that the interesting region lies rather close to 0.5. Compare also to our fig:eng-deu, which shows that up until about a third of the way between English and German, the language model is nearly perfectly tuned to English.", "In our experiments we use 1024-dimensional LSTMs, 128-dimensional character embeddings, and 64-dimensional language embeddings. Layer normalization BIBREF5 is used, but no dropout or other regularization since the amount of data is very large (about 3 billion characters) and training examples are seen at most twice. For smaller models early stopping is used. We use Adam BIBREF6 for optimization. Training takes between an hour and a few days on a K40 GPU, depending on the data size.", "Neural language models BIBREF0 , BIBREF1 , BIBREF2 have become an essential component in several areas of natural language processing (NLP), such as machine translation, speech recognition and image captioning. They have also become a common benchmarking application in machine learning research on recurrent neural networks (RNN), because producing an accurate probabilistic model of human language is a very challenging task which requires all levels of linguistic analysis, from pragmatics to phonology, to be taken into account."]}
{"question_id": "95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "predicted_answer": "", "predicted_evidence": ["This way, we avoid the problem of \u201cspecializing\u201d MagiCoder for any given language. We plan to test MagiCoder on the English MedDRA and, moreover, we aim to test our procedure on different dictionaries (e.g., ICD-9 classification, WHO-ART, SNOMED CT). We are collecting several sources of manually annotated corpora, as potential testing platforms. Moreover, we plan to address the management of orthographical errors possibly contained in narrative ADR descriptions. We did not take into account this issue in the current version of MagiCoder. A solution could include an ad hoc (medical term-oriented) spell checker in VigiFarmaco, to point out to the user that she/he is doing some error in writing the current word in the free description field. This should drastically reduce users' orthographical errors without heavy side effects in MagiCoder development and performances.", "In written informal language, synonyms are frequently used. A natural evolution of our NLP software may be the addition of an Italian thesaurus dictionary. This would appear a trivial extension: one could try to match MedDRA both with original words and their synonyms, and try to maximize the set of retrieved terms. We performed a preliminary test, and we observed a drastic deterioration of MagiCoder performances (both in terms of correctness and completeness): on average, common PT percentages decreases of 24%. The main reason is related to the nature of Italian language: synonymical groups include words related by figurative meaning. For example, among the synonyms of the word \u201cfaccia\u201d (in English, \u201cface\u201d), one finds \u201cviso\u201d (in English \u201cvisage\u201d), which is semantically related, but also \u201cespressione\u201d (in English, \u201cexpression\u201d), which is not relevant in the considered medical context. Moreover, the use of synonyms of words in ADR text leads to an uncontrolled growth of the voted terms, that barely can be later dropped in the final terms release.", "For this purpose, we exploited VigiSegn, a data warehouse and OLAP system that has been developed for the Italian Pharmacovigilance National Center BIBREF3 . This system is based on the open source business intelligence suite Pentaho. VigiSegn offers a large number of encoded ADRs. The encoding has been manually performed and validated by experts working at pharmacovigilance centres. Encoding results have then been sent to the national regulatory authority, AIFA.", "Manual and automatic encodings of each report are finally compared through an SQL query. In order to have two uniform data sets, we compared only those reports where MagiCoder recognized at most six terms, i.e., the maximum number of terms that human experts are allowed to select through the VigiFarmaco user interface. Moreover, we map each LLT term recognized by both the human experts and MagiCoder to its corresponding preferred term. Results are discussed below in Section UID57 .", "The way the algorithm has been developed suggests that MagiCoder can be a valid tool also for narrative descriptions written in English. Indeed, the algorithm retrieves a set of words, which covers an LLT INLINEFORM0 , from a free-text description, only slightly considering the order between words or the structure of the sentence. This way, we avoid the problem of \u201cspecializing\u201d MagiCoder for any given language. We plan to test MagiCoder on the English MedDRA and, moreover, we aim to test our procedure on different dictionaries (e.g., ICD-9 classification, WHO-ART, SNOMED CT). We are collecting several sources of manually annotated corpora, as potential testing platforms. Moreover, we plan to address the management of orthographical errors possibly contained in narrative ADR descriptions. We did not take into account this issue in the current version of MagiCoder. A solution could include an ad hoc (medical term-oriented) spell checker in VigiFarmaco, to point out to the user that she/he is doing some error in writing the current word in the free description field."]}
{"question_id": "ab37ae82e38f64d3fa95782f2c791488f26cd43f", "predicted_answer": "", "predicted_evidence": ["Existing methodologies for NLP are discussed and an experimental comparison between NLP-based machine learning algorithms over data sets from different sources is proposed. Moreover, the authors address the issue of data imbalance for ADR description task. In BIBREF16 the authors propose to use association mining and Proportional Reporting Ratio (PRR, a well-know pharmacovigilance statistical index) to mine the associations between drugs and adverse reactions from the user contributed content in social media. In order to extract adverse reactions from on-line text (from health care communities), the authors apply the Consumer Health Vocabulary to generate ADR lexicon. ADR lexicon is a computerized collection of health expressions derived from actual consumer utterances, linked to professional concepts and reviewed and validated by professionals and consumers. Narrative text is preprocessed following standard NLP techniques (such as stop word removal, see Section SECREF12 ). An experiment using ten drugs and five adverse drug reactions is proposed.", "The Italian system of pharmacovigilance requires that in each local healthcare structure (about 320 in Italy) there is a qualified person responsible for pharmacovigilance. Her/his assignment is to collect reports of suspected ADRs and to send them to the National Network of Pharmacovigilance (RNF, in Italian) within seven days since they have been received. Once reports have been notified and sent to RNF they are analysed by both local pharmacovigilance centres and by the Drug Italian Agency (AIFA). Subsequently, they are sent to Eudravigilance BIBREF1 and to VigiBase BIBREF2 (the European and the worldwide pharmacovigilance network RNF is part of, respectively). In general, spontaneous ADR reports are filled out by health care professionals (e.g., medical specialists, general practitioners, nurses), but also by citizens. In last years, the number of ADR reports in Italy has grown rapidly, going from approximately ten thousand in 2006 to around sixty thousand in 2014 BIBREF3 , as shown in Figure FIGREF3 .", "Function INLINEFORM0 , returns the first INLINEFORM1 elements of an ordered set INLINEFORM2 . If INLINEFORM3 , the function returns the complete list of ordered terms and INLINEFORM4 nil values.", "VigiFarmaco; the output, i.e., a list of MedDRA terms, is stored in a table of TestDB.", "Recently, to increase the efficiency in collecting and managing ADR reports, a web application, called VigiFarmaco, has been designed and implemented for the Italian pharmacovigilance. Through VigiFarmaco, a spontaneous report can be filled out online by both healthcare professionals and citizens (through different user-friendly forms), as anonymous or registered users. The user is guided in compiling the report, since it has to be filled step-by-step (each phase corresponds to a different report section, i.e., \u201cPatient\u201d, \u201cAdverse Drug Reaction\u201d, \u201cDrug Treatments\u201d, and \u201cReporter\u201d, respectively). At each step, data are validated and only when all of them have been correctly inserted the report can be successfully submitted."]}
{"question_id": "6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "predicted_answer": "", "predicted_evidence": ["First, the description of an ADR by means of one of the seventy thousand MedDRA terms is a complex task. In most cases, the reporter who points out the adverse reaction is not an expert in MedDRA terminology. This holds in particular for citizens, but it is still valid for several professionals. Thus, describing ADRs by means of natural language sentences is simpler. Second, the choice of the suitable term(s) from a given list or from an autocompletion field can influence the reporter and limit her/his expressiveness. As a consequence, the quality of the description would be also in this case undermined. Therefore, VigiFarmaco offers a free-text field for specifying the ADR with all the possible details, without any restriction about the content or strict limits to the length of the written text. Consequently, MedDRA encoding has then to be manually implemented by qualified people responsible for pharmacovigilance, before the transmission to RNF.", "Automatic detection of adverse drug reactions from text has recently received an increasing interest in pharmacovigilance research. Narrative descriptions of ADRs come from heterogeneous sources: spontaneous reporting, Electronic Health Records, Clinical Reports, and social media. In BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 some NLP approaches have been proposed for the extraction of ADRs from text. In BIBREF13 , the authors collect narrative discharge summaries from the Clinical Information System at New York Presbyterian Hospital. MedLEE, an NLP system, is applied to this collection, for identifing medication events and entities, which could be potential adverse drug events. Co-occurrence statistics with adjusted volume tests were used to detect associations between the two types of entities, to calculate the strengths of the associations, and to determine their cutoff thresholds.", "Weights calculation: recognized terms are weighted depending on information about syntactical matching.", "Narrative descriptions of ADRs come from heterogeneous sources: spontaneous reporting, Electronic Health Records, Clinical Reports, and social media. In BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 some NLP approaches have been proposed for the extraction of ADRs from text. In BIBREF13 , the authors collect narrative discharge summaries from the Clinical Information System at New York Presbyterian Hospital. MedLEE, an NLP system, is applied to this collection, for identifing medication events and entities, which could be potential adverse drug events. Co-occurrence statistics with adjusted volume tests were used to detect associations between the two types of entities, to calculate the strengths of the associations, and to determine their cutoff thresholds. In BIBREF14 , the authors report on the adaptation of a machine learning-based system for the identification and extraction of ADRs in case reports. The role of NLP approaches in optimised machine learning algorithms is also explored in BIBREF15 , where the authors address the problem of automatic detection of ADR assertive text segments from several sources, focusing on data posted by users on social media (Twitter and DailyStrenght, a health care oriented social media).", "Sorting of voted terms and winning terms release: the set of voted term is pruned, terms are sorted and finally a solution (a set of winning terms) is released."]}
{"question_id": "71413505d7d6579e2a453a1f09f4efd20197ab4b", "predicted_answer": "", "predicted_evidence": ["Function INLINEFORM0 specifies whether a word INLINEFORM1 has been already covered (i.e., a term voted by INLINEFORM2 has been selected) in the (partial) solution during the term release: INLINEFORM3 holds 1 if INLINEFORM4 has been covered (with or without stemming) and it holds 0 otherwise. We assume that before starting the final phase of building the solution (i.e., the returned set of LLTs), INLINEFORM5 for any word INLINEFORM6 belonging to the description.", "The subsequent multi-value sorting based on computed weights is a sorting algorithm which complexity can be approximated to INLINEFORM0 , based on the comparison of objects of four elements (i.e., the weights of the four criteria). Since the number of the criteria-related weights involved in the multi-sorting is constant, it can be neglected. Thus, the complexity of multi-value sorting can be considered to be INLINEFORM1 .", "Online reports have grown up to become the 30% of the total number of Italian reports. As expected, it has been possible to observe that the average time between the dispatch of online reports and the insertion into RNF is sensibly shorter with respect to the insertion from printed reports. Notwithstanding, there is an operation which still requires the manual intervention of responsibles for pharmacovigilance also for online report revisions: the encoding in MedDRA terminology of the free text, through which the reporter describes one or more adverse drug reactions. MedDRA (Medical Dictionary for Regulatory Activities) is a medical terminology introduced with the purpose to standardize and facilitate the sharing of information about medicinal products in particular with respect to regulatory activities BIBREF5 . The description of a suspected ADR through narrative text could seem redundant/useless. Indeed, one could reasonably imagine sound solutions based either on an autocompletion form or on a menu with MedDRA terms.", "We call INLINEFORM0 the set of voted terms after the selection by the ordered-phrases criterium. We proceed then by ordering INLINEFORM1 : we use a multiple-value sorting on elements in INLINEFORM2 , for each INLINEFORM3 . The obtained subdictionary is dubbed as INLINEFORM4 and it has possibly most suitable solutions on top.", "MedDRA (Medical Dictionary for Regulatory Activities) is a medical terminology introduced with the purpose to standardize and facilitate the sharing of information about medicinal products in particular with respect to regulatory activities BIBREF5 . The description of a suspected ADR through narrative text could seem redundant/useless. Indeed, one could reasonably imagine sound solutions based either on an autocompletion form or on a menu with MedDRA terms. In these solutions, the description of ADRs would be directly encoded by the reporter and no expert work for MedDRA terminology extraction would be required. However, such solutions are not completely suited for the pharmacovigilance domain and the narrative description of ADRs remains a desirable feature, for at least two reasons. First, the description of an ADR by means of one of the seventy thousand MedDRA terms is a complex task. In most cases, the reporter who points out the adverse reaction is not an expert in MedDRA terminology."]}
{"question_id": "3e6b6820e7843209495b4f9a72177573afaa4bc3", "predicted_answer": "", "predicted_evidence": ["Finally, this project is an interdisciplinary endeavour, combining clinical psychology, input from individuals with lived experience of BD, and computational linguistics. While this comes with the challenges of cross-disciplinary research, it has the potential to apply and develop state-of-the-art NLP methods in a way that is psychologically and ethically sound as well as informed and approved by affected people to increase our knowledge of severe mental illnesses such as BD.", "BIBREF79 formulate guidelines for ethical social media health research that pertain especially to data collection and sharing. In line with these, we will only share anonymised and paraphrased excerpts from the texts, as it is often possible to recover a user name via a web search for the verbatim text of a post. However, we will make the original texts available as datasets to subsequent research under a data usage agreement. Since the (automatic) annotation of demographic variables in parts of our dataset constitutes especially sensitive information on minority status in conjunction with mental health, we will only share these annotations with researchers that demonstrate a genuine need for them, i.e. to verify our results or to investigate certain research questions.", "Lastly, most research, even when conducted with the best intentions, suffers from the dual-use problem BIBREF74 , in that it can be misused or have consequences that affect people's life negatively. For this reason, we refrain from publishing mental health classification methods, which could be used, for example, by health insurance companies for the risk assessment of applicants based on their social media profiles.", "As a central component we consider the involvement of individuals with lived experience in our project, an aspect which is missing in the discussion of ethical social media health research so far. The proposal has been presented to an advisory board of individuals with a BD diagnosis and was received positively. The advisory board will be consulted at several stages of the project to inform the research design, analysis, and publication of results. We believe that board members can help to address several of the raised ethical problems, e.g., shaping the research questions to avoid feeding into existing biases or overexposing certain groups and highlighting potentially harmful interpretations and uses of our results.", "Another important question is in which situations of encountering content indicative of a risk of self-harm or harm to others it would be appropriate or even required by duty of care for the research team to pass on information to authorities. Surprisingly, we could only find two mentions of this issue in social media research BIBREF81 , BIBREF82 . Acknowledging that suicidal ideation fluctuates BIBREF83 , we accord with the ethical review board's requirement in BIBREF81 to only analyse content posted at least three months ago. If the research team, which includes clinical psychologists, still perceives users at risk we will make use of the reporting facilities of Twitter and Reddit."]}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "predicted_answer": "", "predicted_evidence": ["Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 .", "In sum, our research questions are as follows: (1) How is personal recovery discussed online by individuals meeting criteria for BD? (2) What new insights do we get about personal recovery and factors that facilitate or hinder it? We will investigate these questions in two parts, looking at English-language data by westerners and at multilingual data by individuals of diverse ethnicities.", "The location of Twitter users can be automatically inferred from their tweets BIBREF49 or the (albeit noisy) location field in their user profiles BIBREF50 . Only one attempt to classify the location of Reddit users has been published so far BIBREF51 showing meagre results, indicating that the development of robust location classification approaches on this platform would constitute a valuable contribution. Some companies collect mental health-related online data and make them available to researchers subject to approval of their internal review boards, e.g., OurDataHelps by Qntfy or the peer-support forum provider 7 Cups. Unlike `raw' social media data, these datasets have richer user-provided metadata and explicit consent for research usage. On the other hand, less data is available, the process to obtain access might be tedious within the short timeline of a PhD project and it might be impossible to share the used portions of the data with other researchers. Therefore, we will follow up the possibilities of obtaining access to these datasets, but in parallel also collect our own datasets to avoid dependence on external data providers.", "Qualitative evidence mainly comes from (semi-)structured interviews and focus groups and has been criticised for small numbers of participants BIBREF10 , lacking complementary quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused cognitive-behavioural therapy BIBREF31 . Critically, previous research has taken place only in structured settings. What is more, the recovery concept emerged from research primarily conducted in English-speaking countries, mainly involving researchers and participants of Western ethnicity. This might have led to a lack of non-Western notions of wellbeing in the concept, such as those found in indigenous peoples BIBREF32 , limiting its the applicability to a general population. Indeed, the variation in BD prevalence rates from 0.1% in India to 4.4% in the US is striking. It has been shown that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 , BIBREF35 .", "The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society."]}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "predicted_answer": "", "predicted_evidence": ["In sum, our research questions are as follows: (1) How is personal recovery discussed online by individuals meeting criteria for BD? (2) What new insights do we get about personal recovery and factors that facilitate or hinder it? We will investigate these questions in two parts, looking at English-language data by westerners and at multilingual data by individuals of diverse ethnicities.", "The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 .", "A specific challenge will be to narrow down the data to parts relevant for personal recovery, since there is no control over the discussed topics compared to structured interviews. To investigate how individuals discuss personal recovery online and what (potentially unrecorded) aspects they associate with it, without a priori narrowing down the search-space to specific known keywords seems like a chicken-and-egg problem. We propose to address this challenge by an iterative approach similar to the one taken in a corpus linguistic study of cancer metaphors BIBREF63 . Drawing on results from previous qualitative research BIBREF24 , BIBREF23 , we will compile an initial dictionary of recovery-related terms. Next, we will examine a small portion of the dataset manually, which will be partly randomly sampled and partly selected to contain recovery-related terms. Based on this, we will be able to expand the dictionary and additionally automatically annotate semantic concepts of the identified relevant text passages using a semantic tagging approach such as the UCREL Semantic Analysis System (USAS) BIBREF64 .", "Hence, it seems timely to take into account the wealth of accounts of mental health difficulties and recovery stories from individuals of diverse ethnic and cultural backgrounds that are available in a multitude of languages on the internet. Corpus and computational linguistic methods are explicitly designed for processing large amounts of linguistic data BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , and as discussed above, recent advances have made it feasible to apply them to noisy user-generated texts from diverse domains, including mental health BIBREF42 , BIBREF43 . Computer-aided analysis of public social media data enables us to address several shortcomings in the scientific underpinning of personal recovery in BD by overcoming the small sample sizes of lab-collected data and including accounts from a more heterogeneous population.", "For language identification, Twitter employs an automatic tool BIBREF48 , which can be used to filter tweets according to 60 language codes, and there are free, fairly accurate tools such as the Google Compact Language Detector, which can be applied to Reddit and blog posts. The location of Twitter users can be automatically inferred from their tweets BIBREF49 or the (albeit noisy) location field in their user profiles BIBREF50 . Only one attempt to classify the location of Reddit users has been published so far BIBREF51 showing meagre results, indicating that the development of robust location classification approaches on this platform would constitute a valuable contribution. Some companies collect mental health-related online data and make them available to researchers subject to approval of their internal review boards, e.g., OurDataHelps by Qntfy or the peer-support forum provider 7 Cups. Unlike `raw' social media data, these datasets have richer user-provided metadata and explicit consent for research usage."]}
{"question_id": "4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "predicted_answer": "", "predicted_evidence": ["BIBREF72 discuss issues that can arise when constructing datasets from social media and conducting analyses or developing predictive models based on these data, which we review here in relation to our project: Demographic bias in sampling the data can lead to exclusion of minority groups, resulting in overgeneralisation of models based on these data. As discussed in the introduction, personal recovery research suffers from a bias towards English-speaking Western individuals of white ethnicity. By studying multilingual accounts of ethnically diverse populations we explicitly address the demographic bias of previous research. Topic overexposure is tricky to address, where certain groups are perceived as abnormal when research repeatedly finds that their language is different or more difficult to process. Unlike previous research BIBREF45 , BIBREF47 , BIBREF46 our goal is not to reveal particularities in the language of individuals affected by mental health problems. Instead, we will compare accounts of individuals with BD from different settings (structured interviews versus informal online discourse) and of different backgrounds.", "Furthermore, emotion and sentiment analysis constitute useful tools to investigate the emotions involved in talking about recovery and identify factors that facilitate or hinder it. There are many annotated datasets to train supervised classifiers BIBREF71 , BIBREF3 for these actively researched NLP tasks. Machine learning methods were found to usually outperform rule-based approaches based on look-ups in dictionaries such as LIWC. Again, most annotated resources are English, but state of the art approaches based on multilingual embeddings allow transferring models between languages BIBREF4 .", "As discussed in the introduction, personal recovery research suffers from a bias towards English-speaking Western individuals of white ethnicity. By studying multilingual accounts of ethnically diverse populations we explicitly address the demographic bias of previous research. Topic overexposure is tricky to address, where certain groups are perceived as abnormal when research repeatedly finds that their language is different or more difficult to process. Unlike previous research BIBREF45 , BIBREF47 , BIBREF46 our goal is not to reveal particularities in the language of individuals affected by mental health problems. Instead, we will compare accounts of individuals with BD from different settings (structured interviews versus informal online discourse) and of different backgrounds. While the latter bears the risk to overexpose certain minority groups, we will pay special attention to this in the dissemination of our results.", "For language identification, Twitter employs an automatic tool BIBREF48 , which can be used to filter tweets according to 60 language codes, and there are free, fairly accurate tools such as the Google Compact Language Detector, which can be applied to Reddit and blog posts. The location of Twitter users can be automatically inferred from their tweets BIBREF49 or the (albeit noisy) location field in their user profiles BIBREF50 . Only one attempt to classify the location of Reddit users has been published so far BIBREF51 showing meagre results, indicating that the development of robust location classification approaches on this platform would constitute a valuable contribution. Some companies collect mental health-related online data and make them available to researchers subject to approval of their internal review boards, e.g., OurDataHelps by Qntfy or the peer-support forum provider 7 Cups. Unlike `raw' social media data, these datasets have richer user-provided metadata and explicit consent for research usage.", "As a central component we consider the involvement of individuals with lived experience in our project, an aspect which is missing in the discussion of ethical social media health research so far. The proposal has been presented to an advisory board of individuals with a BD diagnosis and was received positively. The advisory board will be consulted at several stages of the project to inform the research design, analysis, and publication of results. We believe that board members can help to address several of the raised ethical problems, e.g., shaping the research questions to avoid feeding into existing biases or overexposing certain groups and highlighting potentially harmful interpretations and uses of our results."]}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "predicted_answer": "", "predicted_evidence": ["Finally, this project is an interdisciplinary endeavour, combining clinical psychology, input from individuals with lived experience of BD, and computational linguistics. While this comes with the challenges of cross-disciplinary research, it has the potential to apply and develop state-of-the-art NLP methods in a way that is psychologically and ethically sound as well as informed and approved by affected people to increase our knowledge of severe mental illnesses such as BD.", "The importance of the recovery concept in the design of mental health services has recently been prominently reinforced, suggesting \u2018recovery-oriented social enterprises as key component of the integrated service\u2019 BIBREF20 . We think that a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals as diverse as those it is supposed to serve. Therefore, we expect the proposed investigations of views on recovery by previously under-researched ethnic, language, and cultural groups to yield valuable insights on the appropriateness of the recovery approach for a wider population. The datasets collected in this project can serve as useful resources for future research. More generally, our social-media data-driven approach could be applied to investigate other areas of mental health if it proves successful in leading to relevant new insights.", "Since previous research mainly employed (semi-)structured interviews and we do not expect to necessarily find the same aspects emphasised in unstructured settings, even less so when looking at a more diverse and non-English speaking population, we will not derive hypotheses from existing recovery models for testing on the online data. Instead, we will start off with exploratory quantitative research using comparative analysis tools such as Wmatrix BIBREF62 to uncover important linguistic features, e.g., on keywords and key concepts that occur with unexpected frequency in our collected datasets relative to reference corpora. The underlying assumption is that keywords and key concepts are indicative of certain aspects of personal recovery, such as those specified in the CHIME model BIBREF24 , other previous research BIBREF22 , BIBREF23 , BIBREF60 , or novel ones. Comparing online sources with transcripts of structured interviews or subcorpora originating from different cultural backgrounds might uncover aspects that were not prominently represented in the accounts studied in prior research.", "As a central component we consider the involvement of individuals with lived experience in our project, an aspect which is missing in the discussion of ethical social media health research so far. The proposal has been presented to an advisory board of individuals with a BD diagnosis and was received positively. The advisory board will be consulted at several stages of the project to inform the research design, analysis, and publication of results. We believe that board members can help to address several of the raised ethical problems, e.g., shaping the research questions to avoid feeding into existing biases or overexposing certain groups and highlighting potentially harmful interpretations and uses of our results.", "Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 ."]}
{"question_id": "c2ce25878a17760c79031a426b6f38931cd854b2", "predicted_answer": "", "predicted_evidence": ["For CI, we select 6 Cipais, with the body length varying from 33 to 114 characters and with relatively sufficient training samples in CPCC, as our observation target. We generate 300 poems with the two models accordingly. Table 1 summarizes the correct rates of the two models under these 6 Cipais (a generated poem is considered to be correct in form if and only if its form fully matches the expected form). As can be seen, a tendency is the longer the body of CI, the worse the performance of the two models in form and, the more significant the gain in the form correct rate for the enhanced model (an extreme is in the case of Qinyuanchun where the correct rate is raised from 12.0% to 55.0%).", "According to CCPC1.0, Wuyan Jueju, Qiyan Jueju, Wuyan Lvshi, and Qiyan Lvshi constitute 67.96% of SHI, with 4.26%, 22.57%, 15.99%, and 25.14% respectively.", "UTF8gbsn\u4eba\u4e0d\u89c1\uff0c\u6c99\u5824\u91ce\u620d\uff0c\u4e71\u9e26\u557c\u82e6\u3002", "We present a unified format for formulating all types of training samples of SHI and CI by extending the format given in liao2019gpt. First, we change various punctuations between lines into the comma \u2018,\u2019, serving as a uniform separation label between two lines. Second, we utilize three separation labels, $[label_1]$ and $[label_2]$ to separate between form, title, and body of the poem respectively, and $[label_3]$ to separate two stanzas of CI if needed. Third, we enclose $[EOS]$ at the end of the body. Thus, the format for SHI is as follows:", "Chinese poetry is a rich treasure in Chinese traditional culture. For thousands of years, poetry is always considered as the crystallization of human wisdom and erudition by Chinese people and deeply influences the Chinese history from the mental and cultural perspective."]}
{"question_id": "1d263356692ed8cdee2a13f103a82d98f43d66eb", "predicted_answer": "", "predicted_evidence": ["CI is another primary type of Chinese poetry. In contrast to SHI, CI has nearly one thousand forms. Each form of CI (it is called Cipai scholarly) is defined by a fixed number of lines for the poem and, a fixed number of characters for a particular line which usually varies for different lines. The above settings for different Cipai are very distinct, for instance, the Cipai of Busuanzi contains 8 lines and 44 characters, as shown in Figure 2, whereas the Cipai of Manjianghong contains 22 lines and 94 characters. The high diversity regarding the forms of CI further significantly increases the difficulty of poem composition.", "Regarding this, we think the work of liao2019gpt could be improved in the following three respects. First, there is a large improving room for better fitting the form requirement of CI in the process of generation, especially for those with relatively long body length. Second, their formulation format for training samples can be supplemented, for example, the stanza structure of CI is missing. Third, using contemporary Chinese news corpus to pre-train the model may not be necessary, owing to distinctive differences in both meaning and form between contemporary Chinese and Chinese classical poetry language.", "UTF8gbsn\u5f85\u660e\u5e74\uff0c\u53c8\u5230\u6c49\u5bb6\u57ce\uff0c\u91cd\u56de\u987e\u3002", "1.1.1em", ""]}
{"question_id": "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "predicted_answer": "", "predicted_evidence": ["is an attention model that gives a probability distribution on source words INLINEFORM0 , which indicates how much the source word INLINEFORM1 is considered during the decoding step INLINEFORM2 to generate target side word INLINEFORM3 . The attention layer INLINEFORM4 can be as simple as a feed-forward network. INLINEFORM5 is a weighted sum of the encoding hidden state at each position of input sentence: DISPLAYFORM0", "Figure FIGREF23 shows a more detailed procedure. Decoding target stems is exactly the same as decoding target words in normal sequence to sequence model, which is predicted through a softmax layer based on the target output layer. All we need is to replace target words with target stems: DISPLAYFORM0", "BIBREF5 ( BIBREF5 ) proposed a hybrid architecture to deal with the OOV words in source side and any generated unknown tag in the target side. In their system, any OOV words on the source side are encoded at the character level, and if an unknown tag is predicted during decoding, another LSTM will be used to generate a sequence of target-side characters, which will be used as the replacement of the target side unknown word for the translation of a source OOV. However, their model may not work well when the target side is morphologically rich and the source side is not, because their hybrid network on the target side will only be used when an unknown tag is generated, which is always corresponding to a source unknown word. If most of the source side tokens are covered by the source vocabulary, the hybrid network may not have advantage on a morphologically rich target side language.", "Transformer BIBREF15 is a recently proposed model for sequence to sequence tasks. It discards the RNN structure for building the encoder and decoder blocks. Instead, only the attention mechanism is used to calculate the source and target hidden states.", "Subword based BIBREF3 and character-based ( BIBREF4 , BIBREF4 ; BIBREF5 , BIBREF5 ) NMT are the two directions of adjusting translation granularity, which can be helpful to our problem."]}
{"question_id": "c7f43c95db3d0c870407cd0e7becdd802463683b", "predicted_answer": "", "predicted_evidence": ["Scalability. We apply our noise distributions in NCE, from which negative sampling originates, to train word vectors. The implementation comes from wang2vec by BIBREF4, and we report the results of this task using CBOW. We include the unigram distribution Uni BIBREF18 and the sub-sampled unigram distribution Sub$^{1e-5}$ with a manually chosen threshold $10^{-5}$ for comparison. We draw three conclusions: (1) Uni$^{3/4}$ indeed works much better than Uni as claimed in BIBREF12; (2) Sub$^{1e-5}$ results in considerable improvements compared with Uni$^{3/4}$, especially on semantic questions; (3) Our Sub$^{L2}$ achieves the best performance consistently even with a larger vector size of 300.", "(1) Uni$^{3/4}$. The smoothed unigram distribution proposed by BIBREF12.", "We propose to employ a sub-sampled unigram distribution for better negative sampling, and design an approach to derive the required sub-sampling rate. Experimental results show that our noise distribution captures better linear relationships between words than the baselines. It adapts to different corpora and is scalable to NCE related work. The proposed semantics weighted model also achieves a success on the MSR sentence completion task. In summary, our work not only improves the quality of word vectors, but also sheds light on the understanding of Word2Vec.", "(2) wLSE-2. We use wLSE with a condition that the fitting line passes through the point $(\\log 1, \\log f_1)$. This method engages the most frequent word to further control the trend of the line. As a result, $\\hat{\\gamma }= f_1$ and", "We report the results of this task in the Synonym Selection column of Table TABREF42. For all the noise distributions, the results are not stable on TOEFL dataset since it is quite small. Still, Sub$^{L1}$ and Sub$^{L2}$ have comparable performance with Uni$^{3/4}$. In particular, Sub$^{L1}$ makes considerable improvements with Wiki10 corpus. As for LEX dataset, Sub$^{L1}$ and Sub$^{L2}$ outperform Uni$^{3/4}$ in both SG and CBOW models with BWLM corpus. With the other two corpora, Sub$^{L2}$ performs better than Sub$^{L1}$ and Uni$^{3/4}$ using CBOW model. But again, the SG model appears to be less boosted by Sub$^{L1}$ and Sub$^{L2}$ in terms of the corresponding results."]}
{"question_id": "4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "predicted_answer": "", "predicted_evidence": ["Scalability. We apply our noise distributions in NCE, from which negative sampling originates, to train word vectors. The implementation comes from wang2vec by BIBREF4, and we report the results of this task using CBOW. We include the unigram distribution Uni BIBREF18 and the sub-sampled unigram distribution Sub$^{1e-5}$ with a manually chosen threshold $10^{-5}$ for comparison. We draw three conclusions: (1) Uni$^{3/4}$ indeed works much better than Uni as claimed in BIBREF12; (2) Sub$^{1e-5}$ results in considerable improvements compared with Uni$^{3/4}$, especially on semantic questions; (3) Our Sub$^{L2}$ achieves the best performance consistently even with a larger vector size of 300. Note that even though Sub$^{1e-5}$ or Uni$^{3/4}$ performs better on syntactic questions with UMBC corpus, its results on semantic questions and the total dataset are much worse than Sub$^{L2}$.", "Optimality. Since our approach is built on assumptions and new concepts, we wonder whether the resulted $t_c$ is optimal. We select several values around $t_c$-2 and show the word analogy results in Fig. FIGREF48 (c). For CBOW, $t_c$-2 approaches the optimal point given the accuracy on semantic questions and the total dataset. For SG, the optimal point lies between $0.1\\,t_c$-2 and $t_c$-2, with negligible advantages relative to Sub$^{L2}$. Notice that the point $3.57\\,t_c$-2 corresponds to $10^{-5}$, showing much worse performance than Sub$^{L2}$. It indicates that trying a commonly used sub-sampling rate is inappropriate, and our approach is better.", "Since the conventional method ignores the syntactic structure of sentences, it should be biased to semantics. Thus, we modify the method with two steps: (1) applying sub-sampling on the words in the sentences (CM$^s$); and (2) using quantified semantics as weights to form a semantics weighted model (SWM) based on (1). Then we have", "We train the two models, SG and CBOW, using the original noise distribution and other two obtained by our approach, specifically,", "(3) UMBC. The UMBC WebBase corpus from the Stanford WebBase project\u2019s February 2007 Web crawl, with over 3 billion tokens."]}
{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "predicted_answer": "", "predicted_evidence": ["Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance. There is therefore a great need for annotated data from different sources, that is both sufficiently large to support the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that comes with editor-assigned keyphrases.", "Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts. Here, we fill this gap and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations.", "This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance.", "We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases.", "Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts."]}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "predicted_answer": "", "predicted_evidence": ["Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset.", "Variants of keyphrases recovered from the metadata occur in 8% of the documents and represent 810 sets of variants in the KPTimes test split. These variants often refer to the same concept (e.g. \u201cMarijuana; Pot; Weed\u201c), but can sometimes be simply semantically related (e.g. \u201cBridges; Tunnels\u201c). Thereafter, keyphrase variants will be used during model evaluation for reducing the number of mismatches associated with commonly used lexical overlap metrics.", "The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by BIBREF3 to include correlation constraints among keyphrases which we do not include here as it yields comparable results.", "This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance.", "Online news are particularly relevant to keyphrase generation since they are a natural fit for faceted navigation BIBREF6 or topic detection and tracking BIBREF7. Also, and not less importantly, they are available in large quantities and are sometimes accompanied by metadata containing human-assigned keyphrases initially intended for search engines. Here, we divert these annotations from their primary purpose, and use them as gold-standard labels to automatically build our dataset. More precisely, we collect data by crawling selected news websites and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from author-assigned keyphrases found in scholarly documents. Finally, we analyse the performance of state-of-the-art keyphrase generation models and investigate their transferability to the news domain and the impact of domain shift."]}
{"question_id": "07edc082eb86aecef3db5cad2534459c1310d6e8", "predicted_answer": "", "predicted_evidence": ["We explored the KPTimes dataset to better understand how it stands out from the existing ones. First, we looked at how editors tag news articles. Figure illustrates the difference between the annotation behaviour of readers, authors and editors through the number of times that each unique keyphrase is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have. For example, we observe that frequent keyphrases in KPTimes are close to topic descriptors (e.g. \u201cBaseball\u201c, \u201cPolitics and Government\u201c) while those appearing only once are very precise (e.g. \u201cMarley's Cafe\u201c, \u201cCatherine E. Connelly\u201c). Annotations in KPTimes are arguably more uniform and consistent, through the use of tag suggestions, which, as we will soon discuss in \u00a7SECREF12, makes it easier for supervised approaches to learn a good model.", "We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases.", "We further cleansed and filtered the dataset by removing duplicates, articles without content and those with too few (less than 2) or too many (more than 10) keyphrases. This process resulted in a set of 279 923 article-keyphrase pairs. We randomly divided this dataset into training (92.8%), development (3.6%) and test (3.6%) splits.", "Variants of keyphrases recovered from the metadata occur in 8% of the documents and represent 810 sets of variants in the KPTimes test split. These variants often refer to the same concept (e.g. \u201cMarijuana; Pot; Weed\u201c), but can sometimes be simply semantically related (e.g. \u201cBridges; Tunnels\u201c). Thereafter, keyphrase variants will be used during model evaluation for reducing the number of mismatches associated with commonly used lexical overlap metrics.", "We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm."]}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "predicted_answer": "", "predicted_evidence": ["The second baseline we consider, MultipartiteRank BIBREF17, represents the state-of-the-art in unsupervised graph-based keyphrase extraction. It relies on a multipartite graph representation to enforce topical diversity while ranking keyphrase candidates. Just as FirstPhrases, this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke BIBREF18.", "Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline \u2013that is, the first sentences from the document\u2013, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.", "Keyphrases are single or multi-word lexical units that best summarise a document BIBREF0. As such, they are of great importance for indexing, categorising and browsing digital libraries BIBREF1. Yet, very few documents have keyphrases assigned, thus raising the need for automatic keyphrase generation systems. This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated.", "As such, they are of great importance for indexing, categorising and browsing digital libraries BIBREF1. Yet, very few documents have keyphrases assigned, thus raising the need for automatic keyphrase generation systems. This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations.", "Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard. The score difference may be explained by the ratio of absent keyphrases that differs greatly between the reader-annotated datasets and JPTimes (see Table ), and thus question the use of these rather extractive datasets for evaluating keyphrase generation."]}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "predicted_answer": "", "predicted_evidence": ["We further cleansed and filtered the dataset by removing duplicates, articles without content and those with too few (less than 2) or too many (more than 10) keyphrases. This process resulted in a set of 279 923 article-keyphrase pairs. We randomly divided this dataset into training (92.8%), development (3.6%) and test (3.6%) splits.", "This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance.", "We train and evaluate several keyphrase generation models to understand the challenges of KPTimes and its usefulness for training models.", "To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites. We applied heuristics to identify the content (title, headline and body) of each article and regarded the keyphrases provided in the HTML metadata as the gold standard. A cherry-picked sample document is showcased in Figure , it allows to show present and absent keyphrases, as well as keyphrase variants (in this example News media and journalism).", "Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline \u2013that is, the first sentences from the document\u2013, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document."]}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_answer": "", "predicted_evidence": ["That is to say, the separate spaces that the sense embeddings and the (context) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data. Furthermore, this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful. Our first attempt to deal with such problem was to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the (informative) definition content words of senses, or taking the average of the GloVe embeddings of the (informative) context words in their training samples - did not give us a better result than our random initialization. Our preliminary experiments though in which we replaced all GloVe embeddings in the network with sense embeddings (using a method proposed by Chen et al. BIBREF11 ), showed considerable improvements in the results of some ambiguous words. That means both senses and context words (while they can be ambiguous by themselves) come from one vector space.", "Generally, there are three categories of WSD algorithms: supervised, knowledge-based, and unsupervised. Supervised algorithms consist of automatically inducing classification models or rules from labeled examples BIBREF4 . Knowledge-based WSD approaches are dependent on manually created lexical resources such as WordNet BIBREF5 and the Unified Medical Language System (UMLS) BIBREF6 . Unsupervised algorithms may employ topic modeling-based methods to disambiguate when the senses are known ahead of time BIBREF7 . For a thorough survey of WSD algorithms refer to Navigli BIBREF8 .", "Long Short-Term Memory (LSTM), introduced by Hochreiter and Schmidhuber (1997) BIBREF13 , is a gated recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. A Bidirectional LSTM is made up of two reversed unidirectional LSTMs BIBREF14 . For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense.", "Word Sense Disambiguation (WSD) is an important problem in Natural Language Processing (NLP), both in its own right and as a stepping stone to other advanced tasks in the NLP pipeline, applications such as machine translation BIBREF0 and question answering BIBREF1 . WSD specifically deals with identifying the correct sense of a word, among a set of given candidate senses for that word, when it is presented in a brief narrative (surrounding text) which is generally referred to as context. Consider the ambiguous word `cold'. In the sentence \u201cHe started to give me a cold shoulder after that experiment\u201d, the possible senses for cold can be cold temperature (S1), a cold sensation (S2), common cold (S3), or a negative emotional reaction (S4). Therefore, the ambiguous word cold is specified along with the sense set {S1, S2, S3, S4} and our goal is to identify the correct sense S4 (as the closest meaning) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context.", "Due to the replacement of their softmax layers with a sigmoid layer in our network, we need to impose a modification in the input of the model. For this purpose, not only the contextual features are going to make the input of the network, but also, the sense for which we are interested to find out whether that given context makes sense or not (no pun intended) would be provided to the network. Next, the context words would be transferred to a sequence of word embeddings while the sense would be represented as a sense embedding (the shaded embeddings in Fig. FIGREF4 ). For a set of candidate senses (i.e. INLINEFORM0 ) for an ambiguous term, after computing cosine similarities of each sense embedding with the word embeddings of the context words, we expect the sequence result of similarities between the true sense and the surrounding context communicate a pattern-like information that can be encoded through our BLSTM network; for the incorrect senses this premise does not hold. Several WSD studies already incorporated the idea of sense-context cosine similarities in their models BIBREF17 BIBREF18 ."]}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "predicted_answer": "", "predicted_evidence": ["Due to the replacement of their softmax layers with a sigmoid layer in our network, we need to impose a modification in the input of the model. For this purpose, not only the contextual features are going to make the input of the network, but also, the sense for which we are interested to find out whether that given context makes sense or not (no pun intended) would be provided to the network. Next, the context words would be transferred to a sequence of word embeddings while the sense would be represented as a sense embedding (the shaded embeddings in Fig. FIGREF4 ). For a set of candidate senses (i.e. INLINEFORM0 ) for an ambiguous term, after computing cosine similarities of each sense embedding with the word embeddings of the context words, we expect the sequence result of similarities between the true sense and the surrounding context communicate a pattern-like information that can be encoded through our BLSTM network; for the incorrect senses this premise does not hold.", "By applying softmax to the result of estimated classification values, INLINEFORM0 , we can show them as probabilities; this facilitates interpretation of the results.", "In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.", "Due to the replacement of their softmax layers with a sigmoid layer in our network, we need to impose a modification in the input of the model. For this purpose, not only the contextual features are going to make the input of the network, but also, the sense for which we are interested to find out whether that given context makes sense or not (no pun intended) would be provided to the network. Next, the context words would be transferred to a sequence of word embeddings while the sense would be represented as a sense embedding (the shaded embeddings in Fig. FIGREF4 ). For a set of candidate senses (i.e. INLINEFORM0 ) for an ambiguous term, after computing cosine similarities of each sense embedding with the word embeddings of the context words, we expect the sequence result of similarities between the true sense and the surrounding context communicate a pattern-like information that can be encoded through our BLSTM network; for the incorrect senses this premise does not hold. Several WSD studies already incorporated the idea of sense-context cosine similarities in their models BIBREF17 BIBREF18 .", "For one instance (or one document), the input of the network consists of a sense and a list of context words (left and right) which paired together form a list of context components. For the context D which encompasses the ambiguous term INLINEFORM0 , that takes the set of predefined candidate senses INLINEFORM1 , the input for the sense INLINEFORM2 for which we are interested in to find out whether the context is a proper match will be determined by Eq. ( EQREF6 ). Then, this input is copied (next) to INLINEFORM3 positions of the context to form the first pair of the context components. DISPLAYFORM0"]}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "predicted_answer": "", "predicted_evidence": ["Word Sense Disambiguation (WSD) is an important problem in Natural Language Processing (NLP), both in its own right and as a stepping stone to other advanced tasks in the NLP pipeline, applications such as machine translation BIBREF0 and question answering BIBREF1 . WSD specifically deals with identifying the correct sense of a word, among a set of given candidate senses for that word, when it is presented in a brief narrative (surrounding text) which is generally referred to as context. Consider the ambiguous word `cold'. In the sentence \u201cHe started to give me a cold shoulder after that experiment\u201d, the possible senses for cold can be cold temperature (S1), a cold sensation (S2), common cold (S3), or a negative emotional reaction (S4). Therefore, the ambiguous word cold is specified along with the sense set {S1, S2, S3, S4} and our goal is to identify the correct sense S4 (as the closest meaning) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context.", "Word Sense Disambiguation (WSD) is an important problem in Natural Language Processing (NLP), both in its own right and as a stepping stone to other advanced tasks in the NLP pipeline, applications such as machine translation BIBREF0 and question answering BIBREF1 . WSD specifically deals with identifying the correct sense of a word, among a set of given candidate senses for that word, when it is presented in a brief narrative (surrounding text) which is generally referred to as context. Consider the ambiguous word `cold'. In the sentence \u201cHe started to give me a cold shoulder after that experiment\u201d, the possible senses for cold can be cold temperature (S1), a cold sensation (S2), common cold (S3), or a negative emotional reaction (S4).", "In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.", "We outline the organization of the rest of the paper as follows. In Section 2, we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings. Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3. We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable. Finally, in Section 5, we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine.", "SensEval-3 data BIBREF3 on which the network is evaluated, consist of separate training and test samples. In order to find hyper-parameters of the network 5% of the training samples were used for the validation in advance. Once the hyper-parameters are selected, the whole network is trained on all training samples prior to testing. As to the loss function employed for the network, even though is it common to use (binary) cross entropy loss function when the last unit is a sigmoidal classification, we observed that mean square error led to better results for the final argmax classification (Eq. ( EQREF9 )) that we used. Regarding parameter optimization, RMSprop BIBREF19 is employed. Also, all weights including embeddings are updated during training."]}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "predicted_answer": "", "predicted_evidence": ["Multi-classifier BLSTM BIBREF15 consists of deep neural networks which make use of pre-trained word embeddings. While the lower layers of these networks are shared, upper layers of each network are responsible to individually classify the ambiguous that word the network is associated with. IMS+adapted CW BIBREF16 is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs. In contrast to Multi-classifier BLSTM, this model relies on features such as POS tags, collocations, and surrounding words to achieve their result. For these two models, softmax constitutes the output layers of all networks. htsa3 BIBREF22 was the winner of the SensEval-3 lexical sample. It is a Naive Bayes system applied mainly to raw words, lemmas, and POS tags with correction of the a-priori frequencies. IRST-Kernels BIBREF23 utilizes kernel methods for pattern abstraction, paradigmatic and syntagmatic information and unsupervised term proximity on British National Corpus (BNC), in SVM classifiers.", "In the past few years, there has been an increasing interest in training neural word embeddings from large unlabeled corpora using neural networks BIBREF9 BIBREF10 . Word embeddings are typically represented as a dense real-valued low dimensional matrix INLINEFORM0 (i.e. a lookup table) of size INLINEFORM1 , where INLINEFORM2 is the predefined embedding dimension and INLINEFORM3 is the vocabulary size. Each column of the matrix is an embedding vector associated with a word in the vocabulary and each row of the matrix represents a latent feature. These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model. GloVe BIBREF2 is one of the existing unsupervised learning algorithms for obtaining these vector representations of the words in which training is performed on aggregated global word-word co-occurrence statistics from a corpus.", "The first two algorithms represent the state-of-the-art models of supervised WSD when evaluated on SensEval-3. Multi-classifier BLSTM BIBREF15 consists of deep neural networks which make use of pre-trained word embeddings. While the lower layers of these networks are shared, upper layers of each network are responsible to individually classify the ambiguous that word the network is associated with. IMS+adapted CW BIBREF16 is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs. In contrast to Multi-classifier BLSTM, this model relies on features such as POS tags, collocations, and surrounding words to achieve their result. For these two models, softmax constitutes the output layers of all networks. htsa3 BIBREF22 was the winner of the SensEval-3 lexical sample. It is a Naive Bayes system applied mainly to raw words, lemmas, and POS tags with correction of the a-priori frequencies.", "We outline the organization of the rest of the paper as follows. In Section 2, we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings. Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3. We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable. Finally, in Section 5, we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine.", "Besides word embeddings, recently, computation of sense embeddings has gained the attention of numerous studies as well. For example, Chen et al. BIBREF11 adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval-2007 data BIBREF12 ."]}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "predicted_answer": "", "predicted_evidence": ["The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 .", "Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document. Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document. In the Mad cow disease example, the keyphrase extractor fails to identify export and scrapie as keyphrases, resulting in infrequency errors.", "TAGME is a powerful topic annotator. It identifies meaningful sequences of words in a short text and link them to a pertinent Wikipedia page, as shown in Figure . These links add a new topical dimension to the text that enable us to relate, classify or cluster short texts.", "The key contribution of this paper could be summarized as follows:", "The optimization problem is defined as: The goal of the optimization problem is to find the candidate keyphrase set INLINEFORM0 , such that the sum of the scores of the concepts annotated from the phrases in INLINEFORM1 is maximized."]}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "predicted_answer": "", "predicted_evidence": ["Traditional methods of unsupervised keyphrase extraction mostly focus on getting information of document from word frequency and document structure BIBREF0 , however, after years of attempting, the performance seems very hard to be improved any more. Based on this observation, it is reasonable to suspect that the document itself possibly cannot provide enough information for keyphrase extraction task.", "The key contribution of this paper could be summarized as follows:", "Figure shows part of an example document. In this figure, the gold keyphrases are marked with bold, and the keyphrases extracted by the TextRank system are marked with parentheses. We are going to illustrate the errors exist in most of present keyphrase extraction systems using this example. Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that frequently appears in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word BIBREF0 . It is not easy to reject a non-keyphrase containing a word with a high term frequency: many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate. To be more concrete, consider the news article in Figure . The word Cattle has a significant presence in the document.", "Existing methods of keyphrase extraction could be divided into two categories: supervised and unsupervised. While supervised approaches require human labeling, at the same time needs various kinds of training data to get better generalization performance, more and more researchers focus on unsupervised methods.", "To overcome the limitations of aforementioned approaches, we propose WikiRank, an unsupervised automatic keyphrase extraction approach that links semantic meaning to text"]}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "predicted_answer": "", "predicted_evidence": ["The optimization problem is defined as: The goal of the optimization problem is to find the candidate keyphrase set INLINEFORM0 , such that the sum of the scores of the concepts annotated from the phrases in INLINEFORM1 is maximized.", "Figure shows part of an example document. In this figure, the gold keyphrases are marked with bold, and the keyphrases extracted by the TextRank system are marked with parentheses. We are going to illustrate the errors exist in most of present keyphrase extraction systems using this example. Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that frequently appears in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word BIBREF0 . It is not easy to reject a non-keyphrase containing a word with a high term frequency: many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate. To be more concrete, consider the news article in Figure . The word Cattle has a significant presence in the document. Consequently, the system not only correctly predict British cattle as a keyphrase, but also erroneously predict cattle industry, cattle feed, and cattle brain as keyphrases, yielding overgeneration errors.", "We propose an algorithm to solve the optimization problem, as shown in Algorithm . In each iteration, we compute the score INLINEFORM0 for all candidate keyphrases INLINEFORM1 and include the INLINEFORM2 with highest score into INLINEFORM3 , in which INLINEFORM4 evaluates the score of concepts added to the new set INLINEFORM5 by adding INLINEFORM6 into INLINEFORM7 .", "Consider a subgraph of INLINEFORM0 , INLINEFORM1 , which captures all the concepts connected to INLINEFORM2 . In INLINEFORM3 , the set of vertices INLINEFORM4 is the union of the candidate keyphrase set INLINEFORM5 , and the set INLINEFORM6 of concepts that nodes in INLINEFORM7 connect to. The set of edges INLINEFORM8 of INLINEFORM9 is constructed with the edges connect nodes in INLINEFORM10 with nodes in INLINEFORM11 .", "We suggest that future work could incorporate more other semantic approaches to investigate keyphrase extraction task. Introducing the results of dependency parsing or semantic parsing (e.g., OntoUSP) in intermediate steps could be helpful."]}
{"question_id": "2711ae6dd532d136295c95253dbf202e37ecd3e7", "predicted_answer": "", "predicted_evidence": ["where $d_\\mathit {st} = \\cos (r_{s, l}^\\mathit {st},r_{t, l}^\\mathit {st})$, $d_\\mathit {tr} = \\cos (r_{s, l}^\\mathit {tr},r_{t, l}^\\mathit {tr})$, and $\\phi $ is a penalty function. In particular, we let", "In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model, i.e.", "While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4.", "First, we visualize the cosine similarities between decoder hidden states of a certain layer in both ART and NART models for sampled cases. Mathematically, for a set of hidden states $r_1, \\ldots , r_T$, the pairwise cosine similarity can be derived by $\\cos _{ij} = {\\left<r_i, r_j\\right>}/{(\\Vert r_i\\Vert \\cdot \\Vert r_j\\Vert )}.$ We then plot the heatmap of the resulting matrix $\\cos $. A typical example is shown in Figure FIGREF4, where the cosine similarities in the NART model are larger than those of the ART model, indicating that the hidden states across positions in the NART model are \u201csimilar\u201d.", "Mathematically, for a set of hidden states $r_1, \\ldots , r_T$, the pairwise cosine similarity can be derived by $\\cos _{ij} = {\\left<r_i, r_j\\right>}/{(\\Vert r_i\\Vert \\cdot \\Vert r_j\\Vert )}.$ We then plot the heatmap of the resulting matrix $\\cos $. A typical example is shown in Figure FIGREF4, where the cosine similarities in the NART model are larger than those of the ART model, indicating that the hidden states across positions in the NART model are \u201csimilar\u201d. Positions with highly-correlated hidden states tend to generate the same word and make the NART model output repetitive tokens, e.g., the yellow area on the top-left of Figure FIGREF4(b), while this does not happen in the ART model (Figure FIGREF4(a)). According to our statistics, 70% of the cosine similarities between hidden states in the ART model are less than 0.25, and 95% are less than 0.5."]}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "predicted_answer": "", "predicted_evidence": ["Second, we visualize the encoder-decoder attentions for sampled cases, shown in Figure FIGREF6. Good attentions between the source and target sentences are usually considered to lead to accurate translation while poor ones may cause wrong output tokens BIBREF0. In Figure FIGREF6(b), the attentions of the ART model almost covers all source tokens, while the attentions of the NART model do not cover \u201cfarm\u201d but with two \u201cmorning\u201d. This directly makes the translation result worse in the NART model. These phenomena inspire us to use the intermediate hidden information in the ART model to guide the learning process of the NART model.", "where $\\lambda $ and $\\mu $ are hyperparameters controlling the weight of different loss terms.", "In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model, i.e.", "In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models.", "Without the loss of generality, we discuss our method on a given paired sentence $(x,y)$. In real experiments, losses are averaged over all training data. For the teacher model, we use $a_{t,l,h}^\\mathit {tr}$ as the encoder-to-decoder attention distribution of $h$-th head in the $l$-th decoder layer at position $t$, and use $r_{t,l}^\\mathit {tr}$ as the output of the $l$-th decoder layer after feed forward network at position $t$. Correspondingly, $a_{t,l,h}^\\mathit {st}$ and $r_{t,l}^\\mathit {st}$ are used for the student model. We propose a hint-based training framework that contains two kinds of hints:"]}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "predicted_answer": "", "predicted_evidence": ["We observe that meaningful words in the source sentence are sometimes untranslated by the NART model, and the corresponding positions often suffer from ambiguous attention distributions. Therefore, we use the word alignment information from the ART model to help the training of the NART model.", "During training, $T_y$ does not need to be predicted as the target sentence is given. During testing, we have to predict the length of the target sentence for each source sentence. In many languages, the length of the target sentence can be roughly estimated from the length of the source sentence. We choose a simple method to avoid the computational overhead, which uses input length to determine target sentence length: $T_y = T_x + C$, where $C$ is a constant bias determined by the average length differences between the source and target training sentences. We can also predict the target length ranging from $[(T_x+C)-B, (T_x+C)+B]$, where $B$ is the halfwidth. By doing this, we can obtain multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used to further improve the performance.", "In order to speed up the inference process, a line of works begin to develop non-autoregressive translation models. These models break the autoregressive dependency by decomposing the joint probability with", "where $\\lambda $ and $\\mu $ are hyperparameters controlling the weight of different loss terms.", "We also visualize the hidden state cosine similarities and attention distributions for the NART model with hint-based training, as shown in Figure FIGREF4(c) and FIGREF6(c). With hints from hidden states, the hidden states similarities of the NART model decrease in general, and especially for the positions where the original NART model outputs incoherent phrases. The attention distribution of the NART model after hint-based training is more similar to the ART teacher model and less ambiguous comparing to the NART model without hints."]}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "predicted_answer": "", "predicted_evidence": ["We observe that meaningful words in the source sentence are sometimes untranslated by the NART model, and the corresponding positions often suffer from ambiguous attention distributions. Therefore, we use the word alignment information from the ART model to help the training of the NART model.", "Second, we visualize the encoder-decoder attentions for sampled cases, shown in Figure FIGREF6. Good attentions between the source and target sentences are usually considered to lead to accurate translation while poor ones may cause wrong output tokens BIBREF0. In Figure FIGREF6(b), the attentions of the ART model almost covers all source tokens, while the attentions of the NART model do not cover \u201cfarm\u201d but with two \u201cmorning\u201d. This directly makes the translation result worse in the NART model. These phenomena inspire us to use the intermediate hidden information in the ART model to guide the learning process of the NART model.", "First, we visualize the cosine similarities between decoder hidden states of a certain layer in both ART and NART models for sampled cases. Mathematically, for a set of hidden states $r_1, \\ldots , r_T$, the pairwise cosine similarity can be derived by $\\cos _{ij} = {\\left<r_i, r_j\\right>}/{(\\Vert r_i\\Vert \\cdot \\Vert r_j\\Vert )}.$ We then plot the heatmap of the resulting matrix $\\cos $. A typical example is shown in Figure FIGREF4, where the cosine similarities in the NART model are larger than those of the ART model, indicating that the hidden states across positions in the NART model are \u201csimilar\u201d.", "Our final training loss $\\mathcal {L}$ is a weighted sum of two parts stated above and the negative log-likelihood loss $\\mathcal {L}_\\mathit {nll}$ defined on bilingual sentence pair $(x, y)$, i.e.", "The lost of autoregressive dependency largely hurt the consistency of the output sentences, increase the difficulty in the learning process and thus lead to a low quality translation. Previous works mainly focus on adding different components into the NART model to improve the expressiveness of the network structure to overcome the loss of autoregressive dependency BIBREF5, BIBREF6, BIBREF7. However, the computational overhead of new components will hurt the inference speed, contradicting with the goal of the NART models: to parallelize and speed up neural machine translation models."]}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "predicted_answer": "", "predicted_evidence": ["In this section, we first describe the observations on the ART and NART models, and then discuss what kinds of information can be used as hints to help the training of the NART model. We follow the network structure in BIBREF8, use a copy of the source sentence as decoder input, remove the attention masks in decoder self-attention layers and add a positional attention layer as suggested in BIBREF5. We provide a visualization of ART and NART models we used in Figure FIGREF11 and a detailed description of the model structure in Appendix.", "We also visualize the hidden state cosine similarities and attention distributions for the NART model with hint-based training, as shown in Figure FIGREF4(c) and FIGREF6(c). With hints from hidden states, the hidden states similarities of the NART model decrease in general, and especially for the positions where the original NART model outputs incoherent phrases. The attention distribution of the NART model after hint-based training is more similar to the ART teacher model and less ambiguous comparing to the NART model without hints.", "The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.", "We pretrain Transformer BIBREF8 as the teacher model on each dataset, which achieves 33.26/27.30/31.29 in terms of BLEU BIBREF11 in IWSLT14 De-En, WMT14 En-De and De-En test sets. The student model shares the same number of layers in encoder/decoder, size of hidden states/embeddings and number of heads as the teacher models (Figure FIGREF11). Following BIBREF5, BIBREF12, we replace the target sentences by the decoded output of the teacher models.", "The lost of autoregressive dependency largely hurt the consistency of the output sentences, increase the difficulty in the learning process and thus lead to a low quality translation. Previous works mainly focus on adding different components into the NART model to improve the expressiveness of the network structure to overcome the loss of autoregressive dependency BIBREF5, BIBREF6, BIBREF7. However, the computational overhead of new components will hurt the inference speed, contradicting with the goal of the NART models: to parallelize and speed up neural machine translation models."]}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "predicted_answer": "", "predicted_evidence": ["In Table 6 we show the results obtained by the described representations employing the Multiclass Classifier. As can be appreciated, the proposed low dimensionality representation improves more than 35% the results obtained with the state-of-the-art representations. BOW obtains slightly better results than character 4-grams, and both of them improve significantly the ones obtained with tf-idf 2-grams. Instead of selecting the most frequent $n$ -grams, our approach takes advantage from the whole vocabulary and assigns higher weights to the most discriminative words for the different language varieties as shown in Equation 10 .", "Language variety identification aims at labelling texts in a native language (e.g. Spanish, Portuguese, English) with their specific variation (e.g. Argentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). Although at first sight language variety identification may seem a classical text classification problem, cultural idiosyncrasies may influence the way users construct their discourse, the kind of sentences they build, the expressions they use or their particular choice of words. Due to that, we can consider language variety identification as a double problem of text classification and author profiling, where information about how language is shared by people may help to discriminate among classes of authors depending on their language variety.", "We aim at analysing the error of LDR to better understand which varieties are the most difficult to discriminate. As can be seen in Table 7 , the Spanish variety is the easiest to discriminate. However, one of the highest confusions occurs from Argentinian to Spanish. Mexican and Spanish were considerably confused with Argentinian too. Finally, the highest confusion occurs from Peruvian to Chilean, although the lowest average confusion occurs with Peruvian. In general, Latin American varieties are closer to each other and it is more difficult to differentiate among them. Language evolves over time. It is logical that language varieties of nearby countries \u2014 as the Latin American ones \u2014 evolved in a more similar manner that the Spanish variety. It is also logical that even more language variety similarities are shared across neighbour countries, e.g. Chilean compared with Peruvian and Argentinian.", "We can find also several works focused on the task. In BIBREF2 the authors addressed the problem of identifying Arabic varieties in blogs and social fora. They used character $n$ -gram features to discriminate between six different varieties and obtained accuracies between 70%-80%. Similarly, BIBREF3 collected 1,000 news articles of two varieties of Portuguese. They applied different features such as word and character $n$ -grams and reported accuracies over 90%. With respect to the Spanish language, BIBREF4 focused on varieties from Argentina, Chile, Colombia, Mexico and Spain in Twitter. They used meta-learning and combined four types of features: i) character $n$ -gram frequency profiles, ii) character $n$ -gram language models, iii) Lempel-Ziv-Welch compression and iv) syllable-based language models. They obtained an interesting 60%-70% accuracy of classification.", "In order to analyse the robustness of the low dimensionality representation to different languages, we experimented with the development set of the DSLCC corpus from the Discriminating between Similar Languages task BIBREF1 . The corpus consists of 2,000 sentences per language or variety, with between 20 and 100 tokens per sentence, obtained from news headers. In Table 9 we show the results obtained with the proposed representation and the two distributed representations, Skip-gram and SenVec. It is important to notice that, in general, when a particular representation improves for one language is at cost of the other one. We can conclude that the three representations obtained comparative results and support the robustness of the low dimensionality representation."]}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "predicted_answer": "", "predicted_evidence": ["We are interested in discovering which kind of features capture higher differences among varieties. Our hypothesis is that language varieties differ mainly in lexicographic clues. We show an example in Table 1 .", "We have created the HispaBlogs dataset by collecting posts from Spanish blogs from five different countries: Argentina, Chile, Mexico, Peru and Spain. For each country, there are 450 and 200 blogs respectively for training and test, ensuring that each author appears only in one set. Each blog contains at least 10 posts. The total number of blogs is 2,250 and 1,000 respectively. Statistics of the number of words are shown in Table 3 .", "State-of-the-art representations are mainly based on $n$ -grams models, hence we tested character and word based ones, besides word with tf-idf weights. For each of them, we iterated $n$ from 1 to 10 and selected 1,000, 5,000 and 10,000 most frequent grams. The best results were obtained with the 10,000 most frequent BOW, character 4-grams and tf-idf 2-grams. Therefore, we will use them in the evaluation.", "To estimate $p(w_{t+j}|w_t)$ we used negative sampling BIBREF9 that is a simplified version of the Noise Contrastive Estimation (NCE) BIBREF10 , BIBREF11 which is only concerned with preserving vector quality in the context of Skip-gram learning. The basic idea is to use logistic regression to distinguish the target word $W_O$ from draws from a noise distribution $P_n(w)$ , having $k$ negative samples for each word. Formally, the negative sampling estimates $p(w_O|w_I)$ following Equation 24 :", "$$\\frac{1}{T} \\displaystyle \\sum _{t=1}^T \\displaystyle \\sum _{-c \\le j \\le c,j \\ne 0} \\log p(w_{t+j}|w_t)$$   (Eq. 23)"]}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "predicted_answer": "", "predicted_evidence": ["We define our loss as the cross-entropy of the correct start and end indices. In the case of multiple occurrences of the same answer, we only minimize the span of the lowest loss.", "Biomedical Word2Vec embeddings: We use the biomedical word embeddings provided by biomedicalword2vec. These are 200-dimensional Word2Vec embeddings BIBREF5 which were trained on $\\approx 10$ million PubMed abstracts.", "where $\\sigma $ denotes the sigmoid function. By computing the start probability via the sigmoid rather than softmax function (as used in FastQA), we enable the model to output multiple spans as likely answer spans. This generalizes the factoid QA network to list questions.", "The questions are categorized into different question types: factoid, list, summary and yes/no. Our work concentrates on answering factoid and list questions. For factoid questions, the system's responses are interpreted as a ranked list of answer candidates. They are evaluated using mean-reciprocal rank (MRR). For list questions, the system's responses are interpreted as a set of answers to the list question. Precision and recall are computed by comparing the given answers to the gold-standard answers. F1 score, i.e., the harmonic mean of precision and recall, is used as the official evaluation measure .", "To the embedding vectors, we concatenate a one-hot encoding of the question type (list or factoid). Note that these features are identical for all tokens."]}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "predicted_answer": "", "predicted_evidence": ["During the inference phase, we retrieve the top 20 answers span via beam search with beam size 20. From this sorted list of answer strings, we remove all duplicate strings. For factoid questions, we output the top five answer strings as our ranked list of answer candidates. For list questions, we use a probability cutoff threshold $t$ , such that $\\lbrace (i, j)|p_{span}^{i, j} \\ge t\\rbrace $ is the set of answers. We set $t$ to be the threshold for which the list F1 score on the development set is optimized.", "Our system is a neural network which takes as input a question and a context (i.e., the snippets) and outputs start and end pointers to tokens in the context. At its core, we use FastQA BIBREF2 , a state-of-the-art neural QA system. In the following, we describe our changes to the architecture and how the network is trained.", "In the input layer, the context and question tokens are mapped to high-dimensional word vectors. Our word vectors consists of three components, which are concatenated to form a single vector:", "On factoid questions, our system has been very successful, winning three out of five batches. On list questions, however, the relative performance varies significantly. We expect our system to perform better on factoid questions than list questions, because our pre-training dataset (SQuAD) does not contain any list questions.", "$$p_{start}^i = \\sigma (y_{start}^i)$$   (Eq. 8)"]}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "predicted_answer": "", "predicted_evidence": ["Experimental results of hard similarity and transitive sentence similarity are shown in Table TABREF23. We find that:", "The backward LSTM component follows the same recurrent state transition process as the forward LSTM component. Starting from an initial state $\\overleftarrow{\\mathbf {h}}^{n+1}$, which is a model parameter, it reads the input $\\mathbf {x}_n,\\mathbf {x}_{n-1},\\dots ,\\mathbf {x}_0$, changing its value to $\\overleftarrow{\\mathbf {h}}^n,\\overleftarrow{\\mathbf {h}}^{n-1},\\dots ,\\overleftarrow{\\mathbf {h}}^0$, respectively. The BiLSTM model uses the concatenated value of $\\overrightarrow{\\mathbf {h}}^t$ and $\\overleftarrow{\\mathbf {h}}^t$ as the hidden vector for $w_t$:", "Experimental results are shown in Figure FIGREF33. We find that knowledge-driven event embedding is a competitive baseline method, which incorporates world knowledge to improve the performances of event embeddings on the stock prediction. Sentiment is often discussed in predicting stock market, as positive or negative news can affect people's trading decision, which in turn influences the movement of stock market. In this study, we empirically show that event emotions are effective for improving the performance of stock prediction (+2.4%).", "One problem with tensors is curse of dimensionality, which limits the wide application of tensors in many areas. It is therefore essential to approximate tensors of higher order in a compressed scheme, for example, a low-rank tensor decomposition. To decrease the number of parameters in standard neural tensor network, we make low-rank approximation that represents each matrix by two low-rank matrices plus diagonal, as illustrated in Figure FIGREF7. Formally, the parameter of the $i$-th slice is $T_{appr}^{[i]}=T^{[i_1]}\\times T^{[i_2]}+diag(t^{[i]})$, where $T^{[i_1]}\\in \\mathbb {R}^{d\\times n}$, $T^{[i_2]}\\in \\mathbb {R}^{n\\times d}$, $t^{[i]}\\in \\mathbb {R}^d$, $n$ is a hyper-parameter, which is used for adjusting the degree of tensor decomposition.", "Given a training event corpus with annotated intents and emotions, our model jointly minimizes a linear combination of the loss functions on events, intents and sentiment:"]}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "predicted_answer": "", "predicted_evidence": ["One important reason for the problem is the lack of the external commonsense knowledge about the mental state of event participants when learning the objective event representations. In Figure FIGREF2 (a), two event participants \u201cPersonY\u201d and \u201cPersonZ\u201d may carry out a terrorist attack, and hence, they have the same intent: \u201cto bloodshed\u201d, which can help representation learning model maps two events into the neighbor vector space. In Figure FIGREF2 (b), a change to a single argument leads to a large semantic shift in the event representations, as the change of an argument can result in different emotions of event participants. Who \u201cbroke the record\u201d is likely to be happy, while, who \u201cbroke a vase\u201d may be sad. Hence, intent and sentiment can be used to learn more fine-grained semantic features for event embeddings.", "Neural Tensor Network This line of work use tensors to learn the interactions between the predicate and its subject/object BIBREF4, BIBREF5. According to the different usage of tensors, we have three baseline methods: Role Factor Tensor BIBREF5 which represents the predicate as a tensor, Predicate Tensor BIBREF5 which uses two tensors learning the interactions between the predicate and its subject, and the predicate and its object, respectively, NTN BIBREF4, which we used as the baseline event embedding model in this paper, and KGEB BIBREF18, which incorporates knowledge graph information in NTN.", "where $\\alpha , \\beta , \\gamma \\in [0,1]$ are model parameters to weight the three loss functions.", "Events are a kind of important objective information of the world. Structuralizing and representing such information as machine-readable knowledge are crucial to artificial intelligence BIBREF0, BIBREF1. The main idea is to learn distributed representations for structured events (i.e. event embeddings) from text, and use them as the basis to induce textual features for downstream applications, such as script event prediction and stock market prediction.", "It is therefore essential to approximate tensors of higher order in a compressed scheme, for example, a low-rank tensor decomposition. To decrease the number of parameters in standard neural tensor network, we make low-rank approximation that represents each matrix by two low-rank matrices plus diagonal, as illustrated in Figure FIGREF7. Formally, the parameter of the $i$-th slice is $T_{appr}^{[i]}=T^{[i_1]}\\times T^{[i_2]}+diag(t^{[i]})$, where $T^{[i_1]}\\in \\mathbb {R}^{d\\times n}$, $T^{[i_2]}\\in \\mathbb {R}^{n\\times d}$, $t^{[i]}\\in \\mathbb {R}^d$, $n$ is a hyper-parameter, which is used for adjusting the degree of tensor decomposition. The output of neural tensor layer is formalized as follows."]}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "predicted_answer": "", "predicted_evidence": ["We also use ATOMIC BIBREF7 as the event sentiment labeled dataset. In this dataset, the sentiment of the event is labeled as words. For example, the sentiment of \u201cPersonX broke vase\u201d is labeled as \u201c(sad, be regretful, feel sorry, afraid)\u201d. We use SenticNet BIBREF14 to normalize these emotion words ($W=\\lbrace w_1, w_2, \\dots , w_n\\rbrace $) as the positive (labeled as 1) or the negative (labeled as -1) sentiment. The sentiment polarity of the event $P_e$ is dependent on the polarity of the labeled emotion words $P_W$: $P_e=1$, if $\\sum _i P_{w_i}>0$, or $P_e=-1$, if $\\sum _i P_{w_i}<0$. We use the softmax binary classifier to learn sentiment enhanced event embeddings.", "(3) Our commonsense knowledge enhanced event representation learning approach outperformed all baseline methods across all datasets (achieving 78% and 200% improvements on hard similarity small and big dataset, respectively, compared to previous SOTA method), which indicates that commonsense knowledge is useful for distinguishing distinct events.", "The labeled dataset contains 230 event pairs (115 pairs each of similar and dissimilar types). Three different annotators were asked to give the similarity/dissimilarity rankings, of which only those the annotators agreed upon completely were kept. For each event representation learning method, we obtain the cosine similarity score of the pairs, and report the fraction of cases where the similar pair receives a higher cosine value than the dissimilar pair (we use Accuracy $\\in [0,1]$ denoting it). To evaluate the robustness of our approach, we extend this dataset to 1,000 event pairs (similar and dissimilar events each account for 50%), and we will release this dataset to the public.", "Previous work investigated compositional models for event embeddings. BIBREF2 granroth2016happens concatenate predicate and argument embeddings and feed them to a neural network to generate an event embedding. Event embeddings are further concatenated and fed through another neural network to predict the coherence between the events. Modi modi2016event encodes a set of events in a similar way and use that to incrementally predict the next event \u2013 first the argument, then the predicate and then next argument. BIBREF25 pichotta2016learning treat event prediction as a sequence to sequence problem and use RNN based models conditioned on event sequences in order to predict the next event. These three works all model narrative chains, that is, event sequences in which a single entity (the protagonist) participates in every event. BIBREF26 hu2017happens also apply an RNN approach, applying a new hierarchical LSTM model in order to predict events by generating descriptive word sequences.", "Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings BIBREF8."]}
{"question_id": "d38745a3910c380e6df97c7056a5dd9643fd365b", "predicted_answer": "", "predicted_evidence": ["We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models.", "We use the method of Mikolov et al. word2vec1 to answer these questions. We first $\\ell _2$ -normalize all of our word vectors. Then, to answer a question of the form \u201cA is to B as C is to X\u201d, we find the word $w$ which satisfies", "$$\\log \\sigma (w \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-w \\cdot \\tilde{c}_i)]$$   (Eq. 7)", "To do this, we use the Google analogy dataset BIBREF3 . This consists of 19544 questions of the form \u201cA is to B as C is to X\u201d. We split this collection into semantic and syntactic sections, based on whether the analogies between the words are driven by morphological changes or deeper semantic shifts. Example semantic questions are on capital-country relationships (\u201cParis is to France as Berlin is to X) and currency-country relationships. Example syntactic questions are adjective-adverb relationships (\u201camazing is to amazingly as apparent is to X\u201d) and opposites formed by prefixing a negation particle (\u201cacceptable is to unacceptable as aware is to X\u201d). This results in 5537 semantic analogies and 10411 syntactic analogies.", "One approach to smooth word distributions is to operate on the smallest meaningful semantic unit, the morpheme BIBREF6 , BIBREF7 . However, previous work on the morpheme level has all used external morphological analyzers. These require a separate pre-processing step, and cannot be adapted to suit the problem at hand."]}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "predicted_answer": "", "predicted_evidence": ["As the results show, our model performs the best out of all the methods at analysing morphologically rich words with multiple morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes.", "Evaluating morphological segmentation is a long-debated issue BIBREF28 . Traditional hard morphological analyzers are normally evaluated on border $F_1$ \u2013 that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above non-morpheme boundaries.", "$$\\log \\sigma (w \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-w \\cdot \\tilde{c}_i)]$$   (Eq. 7)", "Our model `reads' the word and outputs a sequence of word segments. We weight each segment, and then combine the segments to obtain the final word representation. These representations are trained to predict context words, as this has been shown to give word representations which capture word semantics well BIBREF11 . As the root morpheme has the most context-predictive power, we expect our model to assign high weight to this segment, thereby learning to separate root+affix structures.", "We use the WordSim353 dataset BIBREF29 , the test split of the MEN dataset BIBREF30 , and the Rare Word (RW) dataset BIBREF31 . The word pairs in the WordSim353 and MEN datasets are typically simple, commonly occurring words denoting basic concepts, whereas the RW dataset contains many morphologically derived words which have low corpus frequencies. This is reflected by how many of the test pairs in each dataset contain out of vocabulary (OOV) items: 3/353 and 6/1000 of the word pairs in WordSim353 and MEN, compared with 1083/2034 for the RW dataset."]}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "predicted_answer": "", "predicted_evidence": ["Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better.", "As word semantics is compositional, one might ask whether it is possible to learn morpheme representations, and compose them to obtain good word representations. Lazaridou et al. lazaridou demonstrated precisely this: one can derive good representations of morphemes distributionally, and apply tools from compositional distributional semantics to obtain good word representations. Luong et al. luong also trained a morphological composition model based on recursive neural networks. Botha and Blunsom Botha2014 built a language model incorporating morphemes, and demonstrated improvements in language modelling and in machine translation. All of these approaches incorporated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX BIBREF12 or an external morphological analyzer such as Morfessor BIBREF13 .", "$$w = \\operatornamewithlimits{argmax}_{w \\in V - \\lbrace a, b, c\\rbrace } \\cos (w, b - a + c)$$   (Eq. 28)", "We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models.", "One approach to smooth word distributions is to operate on the smallest meaningful semantic unit, the morpheme BIBREF6 , BIBREF7 . However, previous work on the morpheme level has all used external morphological analyzers. These require a separate pre-processing step, and cannot be adapted to suit the problem at hand."]}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "predicted_answer": "", "predicted_evidence": ["We also note that word nearest neighbours seem to be more semantically coherent when rarely-observed words are filtered out of the vocabulary, and more based on orthographic overlap when the entire vocabulary is included. This suggests that for rarely-observed words, the model is basing its predictions on orthographic analysis, whereas for more commonly observed words it can `memorize' the mapping between the orthography and word semantics.", "Finally, we trained Morfessor 2.0 BIBREF13 on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs is less than 0.1% of the total loss. We then used our trained Morfessor model to predict morpheme boundaries, and randomly permuted the morpheme boundaries and ranked them ahead of randomly permuted non-morpheme boundaries to calculate MAP.", "$$w = \\operatornamewithlimits{argmax}_{w \\in V - \\lbrace a, b, c\\rbrace } \\cos (w, b - a + c)$$   (Eq. 28)", "One exciting feature of character-level models is their ability to represent open-vocabulary words. After training, they can predict a vector for any word, not just words that they have seen before. Our model has an advantage in that it can split unknown words into known and unknown components. Hence, it can potentially generalise better over seen morphemes and words and apply existing knowledge to new cases.", "We use the method of Mikolov et al. word2vec1 to answer these questions. We first $\\ell _2$ -normalize all of our word vectors. Then, to answer a question of the form \u201cA is to B as C is to X\u201d, we find the word $w$ which satisfies"]}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "predicted_answer": "", "predicted_evidence": ["Endowing machines with the ability to understand natural language is a long-standing goal in NLP and holds the promise of revolutionizing the way in which people interact with machines and retrieve information. Richardson et al. richardson2013mctest proposed the task of machine comprehension, along with MCTest, a question answering dataset for evaluation. The ability of the machine to understand text is evaluated by posing a series of questions, where the answer to each question can be found only in the associated text. Solutions typically focus on some semantic interpretation of the text, possibly with some form of probabilistic or logic inference, to answer the question. Despite intensive recent work BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , the problem is far from solved.", "Questions in MCTest have two categories: \u201cone\u201d and \u201cmultiple\u201d. The label means one or multiple sentences from the document are required to answer this question. To correctly answer the first question in the example, the two blue sentences are required; for the second question instead, only the red sentence can help. The following observations hold for the whole MCTest. (i) Most of the sentences in the document are irrelavent for a given question. It hints that we need to pay attention to just some key regions. (ii) The answer candidates can be flexible text in length and abstraction level, and probably do not appear in the document. For example, candidate B for the second question is \u201coutside\u201d, which is one word and does not exist in the document, while the answer candidates for the first question are longer texts with some auxiliary words like \u201cBecause\u201d in the text. This requires our system to handle flexible texts via extraction as well as abstraction.", "Figure FIGREF1 shows one example in MCTest. Each example consists of one document, four associated questions; each question is followed by four answer candidates in which only one is correct. Questions in MCTest have two categories: \u201cone\u201d and \u201cmultiple\u201d. The label means one or multiple sentences from the document are required to answer this question. To correctly answer the first question in the example, the two blue sentences are required; for the second question instead, only the red sentence can help. The following observations hold for the whole MCTest. (i) Most of the sentences in the document are irrelavent for a given question. It hints that we need to pay attention to just some key regions. (ii) The answer candidates can be flexible text in length and abstraction level, and probably do not appear in the document. For example, candidate B for the second question is \u201coutside\u201d, which is one word and does not exist in the document, while the answer candidates for the first question are longer texts with some auxiliary words like \u201cBecause\u201d in the text.", "Multitask learning: Question typing is commonly used and proved to be very helpful in QA tasks BIBREF3 . Inspired, we stack a logistic regression layer over question representation INLINEFORM0 , with the purpose that this subtask can favor the parameter tuning of the whole system, and finally the question is better recognized and is able to find the answer more accurately.", "For the query and each sentence of D, we do element-wise 1-max-pooling (\u201cmax-pooling\u201d for short) BIBREF16 over phrase representations to form their representations at this level."]}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "predicted_answer": "", "predicted_evidence": ["Concretely, we propose HABCNN, a hierarchical attention-based convolutional neural network, to address this task in two roadmaps. In the first one, we project the document in two different ways, one based on question-attention, one based on answer-attention and then compare the two projected document representations to determine whether the answer matches the question. In the second one, every question-answer pair is reformatted into a statement, then the whole task is treated through textual entailment.", "Machine comprehension is an open-domain question-answering problem which contains factoid questions, but the answers can be derived by extraction or induction of key clues. Figure FIGREF1 shows one example in MCTest. Each example consists of one document, four associated questions; each question is followed by four answer candidates in which only one is correct. Questions in MCTest have two categories: \u201cone\u201d and \u201cmultiple\u201d. The label means one or multiple sentences from the document are required to answer this question. To correctly answer the first question in the example, the two blue sentences are required; for the second question instead, only the red sentence can help. The following observations hold for the whole MCTest. (i) Most of the sentences in the document are irrelavent for a given question. It hints that we need to pay attention to just some key regions. (ii) The answer candidates can be flexible text in length and abstraction level, and probably do not appear in the document.", "For Variant-I and Variant-II (second block of Table TABREF16 ), we can see that both modifications do harm to the original HABCNN-TE performance. The first variant, i.e, replacing the sentence-CNN in Figure FIGREF3 as GRU module is not helpful for this task. We suspect that this lies in the fundamental function of CNN and GRU. The CNN models a sentence without caring about the global word order information, and max-pooling is supposed to extract the features of key phrases in the sentence no matter where the phrases are located. This property should be useful for answer detection, as answers are usually formed by discovering some key phrases, not all words in a sentence should be considered. However, a GRU models a sentence by reading the words sequentially, the importance of phrases is less determined by the question requirement.", "The first variant, i.e, replacing the sentence-CNN in Figure FIGREF3 as GRU module is not helpful for this task. We suspect that this lies in the fundamental function of CNN and GRU. The CNN models a sentence without caring about the global word order information, and max-pooling is supposed to extract the features of key phrases in the sentence no matter where the phrases are located. This property should be useful for answer detection, as answers are usually formed by discovering some key phrases, not all words in a sentence should be considered. However, a GRU models a sentence by reading the words sequentially, the importance of phrases is less determined by the question requirement. The second variant, using a more complicated attention scheme to model biased D representations than simple cosine similarity based attention used in our model, is less effective to detect truly informative sentences or snippet. We doubt such kind of attention scheme when used in sentence sequences of large size.", "Addition-proj. First compute sentence representations for Q, A and all D sentences as the same way as Addition, then match the two sentences in D which have highest similarity with Q and A respectively."]}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_answer": "", "predicted_evidence": ["The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely.", "The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an AM whose architecture is the same as the first-stage in the two-stage system. This AM is used to generate temporal locations of each phoneme through CTC loss, so that we can train an another AM by using cross-entropy loss as the second-stage to predict the corresponding phonetic labels of the input frames, in which we only use ResNet14 without an RNN because we have the precise locations of each phoneme from the first stage. The third stage is similar, we use the intermediate features from the second stage to train an RNN network for LID task, also the loss in this stage is cross-entropy loss.", "where INLINEFORM0 is the utterance and INLINEFORM1 is a CTC path. Then the network can be trained to optimize the CTC function INLINEFORM2 by the given sequence labeling. For the LID task, we use the multi-class cross-entropy loss for classification: DISPLAYFORM0", "We convert the raw audio to 40-dimensional log Mel-filterbank coefficients with a frame-length of 25 ms, mean-normalized over the whole utterance. Then we stack all the log-mel filterbank features and feed it into the neural network, which is implemented in PyTorch. No voice activity detection (VAD) or other data augmentation approaches are applied. During the training process, we use Adam as the optimization method and set different learning rates and weight decay in different stages. We do not set dropout while training AM but set the dropout value=0.5 while training the LID network (the last stage).", "On the other hand, in many utterance analyzing tasks such as acoustic speech recognition (ASR), speaker verification (SV) and our LID, only a simple task or a specific aim is focused on. However, an utterance always has multi-dimensional information such as content, emotion, speaker and language and there are some certain correlations between them. Although the LID task is text-independent, which means the content of each utterance is totally different, different languages may have its own pronunciations or tones. Thus acoustic and language are two components in the LID task, BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 use the bottleneck features from an ASR system and feed to another neural network for recognition. Nevertheless, these ASR DNNs constituted by fully connected layers adds significant computational complexity and also require labels of physical states of a tied-state triphone."]}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "predicted_answer": "", "predicted_evidence": ["As Table TABREF24 shows, training networks in the first stage (with CTC loss) needs more time for convergence than training networks in the second or third stage (with cross-entropy loss). We can observe that the two-stage system spends less time while having a slightly higher accuracy compared to the three-stage system.", "Recently, the use of deep neural network (DNN) has been explored in LID tasks. The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors.", "The aim of language identification (LID) is to determine the language of an utterance and can be defined as a variable-length sequence classification task on the utterance-level. The task introduced in this paper is more challenging than general LID tasks cause we use a dialect database which contains 10 dialects in China. The dialects' regions are close to each other and they all belong to Chinese, so they have the same characters and similar pronunciations.", "First of all, we compare the two-stage system and the three-stage system trained with phonetic sequence annotation and dialect category label with the baseline trained only with dialect category label. The two multi-stage system have the same ResNet14 architecture and use 2-layer BLSTM as the RNN part with 256 nodes. From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task.", "The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely."]}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF12 shows the architecture of the two-stage system. The input is the sound feature of each utterance. We firstly train the AM with the ResNet14 followed by an RNN architecture, then the intermediate results computed by res-blocks are feed to the second stage as the input. The framework does not need to compress the feature sequence so it keeps all the information from the ResNet14 part. The network of second stage is 2-layer BLSTM. The final pooling strategy is average pooling on time-dimension so we can get the utterance-level category results from frame-level, and the output is the prediction of dialect category. We use CTC loss to train the AM so the network outputs can align with the phoneme sequences automatically and use cross-entropy loss to discriminate between dialects.", "Compared with multi-task training BIBREF27 , BIBREF28 in SV tasks, it should be emphasized that these stages should be trained step by step instead of multi-task learning with shared layers, that is to say we backpropagate the whole network while training AM, and only backpropagate the RNN part in the second stage, or the network will be degenerated and lost the information of acoustic knowledge.", "The remainder of the paper is organized as follows. Section 2 introduces some related works about ASR and section 3 introduces the ResNet14 structure and gives processing of the two multi-stage systems. We present details of the database and initialization methods of the network in Section 4. The results of experiments and analysis are shown in Section 5. Lastly we give the conclusion and some future work in Section 6.", "The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely.", "The phonetic sequence annotation of the corresponding text to each speech is also provided in the training set. There are 27 initials and 39 finals with 148 tones in the whole database."]}
{"question_id": "94d794df4a3109522c2ea09dad5d40e55d35df51", "predicted_answer": "", "predicted_evidence": ["Tab. TABREF9 summarizes our low-resource experiments. Our substitution-only system already outperforms the prior work of BIBREF3 . Allowing for deletions and insertions improves the ERRANT score on BEA-2019 Dev by 2.57 points. We report further gains on both test sets by ensembling two language models and increasing the beam size.", "with INLINEFORM0 for deletions, INLINEFORM1 for substitutions, INLINEFORM2 for insertions, and INLINEFORM3 for converting words to BPE tokens. Path scores in the FST in Eq. EQREF14 are the accumulated penalties INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . The INLINEFORM7 -parameters are tuned on the dev set using a variant of Powell search BIBREF16 . We apply standard FST operations like output projection, INLINEFORM8 -removal, determinization, minimization, and weight pushing BIBREF17 , BIBREF18 to help downstream decoding. Following BIBREF3 we then use the resulting transducer to constrain a neural LM beam decoder.", "Tab. TABREF33 contains our experiments with the Big configuration. In addition to W&I+LOCNESS over-sampling, back-translation with 5M sentences, and fine-tuning with checkpoint averaging, we report further gains by adding the language models from our low-resource system (Sec. SECREF15 ) and ensembling. Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set BIBREF27 . However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets. As we argued in Sec. SECREF18 , our results throughout this work suggest strongly that the optimal system parameters for these test sets are very different from each other, and that our final system settings are not optimal for CoNLL-2014.", "For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 .", "In a more condensed form, we can describe the final transducer as: DISPLAYFORM0"]}
{"question_id": "044c66c6b7ff7378682f24887b05e1af79dcd04f", "predicted_answer": "", "predicted_evidence": ["In a more condensed form, we can describe the final transducer as: DISPLAYFORM0", "As explained previously, we over-sample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fine-tune the NMT models on W&I+LOCNESS after convergence on the full training set. We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass. Fig. FIGREF30 plots the INLINEFORM0 score on the BEA-2019 dev set for two different setups. For the red curve, we average all checkpoints BIBREF26 (including the last unadapted checkpoint) up to a certain training iteration. Checkpoints are dumped every 500 steps. The green curve does not use any checkpoint averaging.", "with INLINEFORM0 for deletions, INLINEFORM1 for substitutions, INLINEFORM2 for insertions, and INLINEFORM3 for converting words to BPE tokens. Path scores in the FST in Eq. EQREF14 are the accumulated penalties INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . The INLINEFORM7 -parameters are tuned on the dev set using a variant of Powell search BIBREF16 . We apply standard FST operations like output projection, INLINEFORM8 -removal, determinization, minimization, and weight pushing BIBREF17 , BIBREF18 to help downstream decoding. Following BIBREF3 we then use the resulting transducer to constrain a neural LM beam decoder.", "In contrast to our low-resource submission, our restricted system entirely relies on neural models and does not use any external NLP tools, spell checkers, or hand-crafted confusion sets. For simplicity, we also chose to use standard implementations BIBREF19 of standard Transformer BIBREF6 models with standard hyper-parameters. This makes our final system easy to deploy as it is a simple ensemble of standard neural models with minimal preprocessing (subword segmentation). Our contributions on this track focus on NMT training techniques such as over-sampling, back-translation, and fine-tuning. We show that over-sampling effectively reduces domain mismatch. We found back-translation BIBREF5 to be a very effective technique to utilize unannotated training data. However, while over-sampling is commonly used in machine translation to balance the number of real and back-translated training sentences, we report that using over-sampling this way for GEC hurts performance. Finally, we propose a combination of checkpoint averaging BIBREF26 and continued training to adapt our NMT models to the target domain.", "In a first step, the source sentence is converted to an FST INLINEFORM0 (Fig. FIGREF3 ). This initial FST is augmented by composition (denoted with the INLINEFORM1 -operator) with various other FSTs to cover different error types. Composition is a widely used standard operation on FSTs and supported efficiently by FST toolkits such as OpenFST BIBREF8 . We construct the hypothesis space as follows:"]}
{"question_id": "903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "predicted_answer": "", "predicted_evidence": ["The automatic correction of errors in text [In a such situaction INLINEFORM0 In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task BIBREF0 , the CoNLL-2014 shared task BIBREF1 , and finally the BEA 2019 shared task BIBREF2 . This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop.", "Back-translation BIBREF5 has become the most widely used technique to use monolingual data in neural machine translation. Back-translation extends the existing parallel training set by additional training samples with real English target sentences but synthetic source sentences. Different methods have been proposed to synthesize the source sentence such as using dummy tokens BIBREF5 , copying the target sentence BIBREF29 , or sampling from or decoding with a reverse sequence-to-sequence model BIBREF5 , BIBREF30 , BIBREF4 . The most popular approach is to generate the synthetic source sentences with a reverse model that is trained to transform target to source sentences using beam search. In GEC, this means that the reverse model learns to introduce errors into a correct English sentence. Back-translation has been applied successfully to GEC by BIBREF4 . We confirm the effectiveness of back-translation in GEC and discuss some of the differences between applying this technique to grammatical error correction and machine translation.", "Tab. TABREF33 contains our experiments with the Big configuration. In addition to W&I+LOCNESS over-sampling, back-translation with 5M sentences, and fine-tuning with checkpoint averaging, we report further gains by adding the language models from our low-resource system (Sec. SECREF15 ) and ensembling. Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set BIBREF27 . However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets. As we argued in Sec.", "with INLINEFORM0 for deletions, INLINEFORM1 for substitutions, INLINEFORM2 for insertions, and INLINEFORM3 for converting words to BPE tokens. Path scores in the FST in Eq. EQREF14 are the accumulated penalties INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . The INLINEFORM7 -parameters are tuned on the dev set using a variant of Powell search BIBREF16 . We apply standard FST operations like output projection, INLINEFORM8 -removal, determinization, minimization, and weight pushing BIBREF17 , BIBREF18 to help downstream decoding. Following BIBREF3 we then use the resulting transducer to constrain a neural LM beam decoder.", "Previous works often suggested to remove unchanged sentences (i.e. source and target sentences are equal) from the training corpora BIBREF3 , BIBREF27 , BIBREF28 . We note that removing these identity mappings can be seen as measure to control the balance between precision and recall. As shown in Tab. TABREF26 , removing identities encourages the model to make more corrections and thus leads to higher recall but lower precision. It depends on the test set whether this results in an improvement in INLINEFORM0 score. For the subsequent experiments we found that removing identities in the parallel training corpora but not in the back-translated synthetic data works well in practice."]}
{"question_id": "ab95ca983240ad5289c123a2774f8e0db424f4a1", "predicted_answer": "", "predicted_evidence": ["We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training.", "For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 . Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by BIBREF7 .", "In a first step, the source sentence is converted to an FST INLINEFORM0 (Fig. FIGREF3 ). This initial FST is augmented by composition (denoted with the INLINEFORM1 -operator) with various other FSTs to cover different error types. Composition is a widely used standard operation on FSTs and supported efficiently by FST toolkits such as OpenFST BIBREF8 . We construct the hypothesis space as follows:", "Tab. TABREF33 contains our experiments with the Big configuration. In addition to W&I+LOCNESS over-sampling, back-translation with 5M sentences, and fine-tuning with checkpoint averaging, we report further gains by adding the language models from our low-resource system (Sec. SECREF15 ) and ensembling. Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set BIBREF27 . However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets. As we argued in Sec. SECREF18 , our results throughout this work suggest strongly that the optimal system parameters for these test sets are very different from each other, and that our final system settings are not optimal for CoNLL-2014.", "For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 ."]}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "predicted_answer": "", "predicted_evidence": ["Procedure. Following BIBREF2, we train and evaluate our system on the subset of WNC where the editor changed or deleted a single word in the source text. This yielded 53,803 training pairs (about a quarter of the WNC), from which we sampled 700 development and 1,000 test pairs. For fair comparison, we gave our baselines additional access to the 385,639 neutral examples when possible. We pretrained the tagging module for 4 epochs. We pretrained the editing module on the neutral portion of our WNC for 4 epochs. The joint system was trained on the same data as the tagger for 25,000 steps (about 7 epochs). We perform interference using beam search and a beam width of 4. All computations were performed on a single NVIDIA TITAN X GPU; training the full system took approximately 10 hours.", "Large-scale human evaluation suggests that while not without flaws, our algorithms can identify and reduce bias in encyclopedias, news, books, and political speeches, and do so better than state-of-the-art style transfer and machine translation systems. This work represents an important first step towards automatically managing bias in the real world. We release data and code to the public.", "", "We propose the task of neutralizing text, in which the algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed.", ""]}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "predicted_answer": "", "predicted_evidence": ["The growing presence of bias has marred the credibility of our news, educational systems, and social media platforms. Automatically reducing bias is thus an important new challenge for the Natural Language Processing and Artificial Intelligence community. By learning models to automatically detect and correct subjective bias in text, this work is a first step in this important direction. Nonetheless our scope was limited to single-word edits, which only constitute a quarter of the edits in our data, and are probably among the simplest instances of bias. We therefore encourage future work to tackle broader instances of multi-word, multi-lingual, and cross-sentence bias. Another important direction is integrating aspects of fact-checking BIBREF55, since a more sophisticated system would be able to know when a presupposition is in fact true and hence not subjective. Finally, our new join embedding mechanism can be applied to other modular neural network architectures.", "Debiasing. Many scholars have worked on removing demographic prejudice from meaning representations BIBREF48, BIBREF49, BIBREF5, BIBREF50, BIBREF51. Such studies begin with identifying a direction or subspace that capture the bias and then removing such bias component to make these representations fair across attributes like gender and age BIBREF3, BIBREF48. For instance, BIBREF50 introduced a regularization term for the language model to penalize the projection of the word embeddings onto that gender subspace, while BIBREF51 used adversarial training to remove directions of bias from hidden states.", "Identifying subjectivity in a sentence (explicitly or implicitly) is prerequisite to neutralizing it. We accordingly evaluate our model's (and 3,000 crowdworker's) ability to detect subjectivity using the procedure of BIBREF2 and the same 50k training examples as Section SECREF4 (Table TABREF51). For each sentence, we select the word with the highest predicted probability and test whether that word was in fact changed by the editor. The proportion of correctly selected words is the system's \u201caccuracy\u201d. Results are given in Table TABREF51.", "Examples of each error type are given in Table TABREF52 (two pages away). As the examples show, our models have have a tendency to simply remove words instead of finding a good replacement.", "Neural Language Generation. Several studies propose stepwise procedures for text generation, including sampling from a corpus BIBREF52 and identifying language ripe for modification BIBREF53. Most similar to us is BIBREF26 who localize a text's style to a fraction of its words. Our modular detection module performs a similar localization in a soft manner, and our steps are joined by a smooth conduit (the join embedding) instead of discrete logic. There is also work related to our concurrent model. The closest is BIBREF54, where a decoder was attached to BERT for question answering, or BIBREF23, where machine translation systems are initialized to LSTM and Transformer-based language models of the source text."]}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "predicted_answer": "", "predicted_evidence": ["Sentences from the campaign speeches of a prominent politician (United States President Donald Trump). We filtered out dialog-specific artifacts (interjections, phatics, etc) by removing all sentences with less than 4 tokens before sampling a test set.", "We thank the Japan-United States Educational Commission (Fulbright Japan) for their generous support. We thank Chris Potts, Hirokazu Kiyomaru, Abigail See, Kevin Clark, the Stanford NLP Group, and our anonymous reviewers for their thoughtful comments and suggestions. We gratefully acknowledge support of the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF15-1-0462 and the NSF via grant IIS-1514268. Diyi Yang is supported by a grant from Google.", "", "We propose two algorithms for this task, each with its own benefits. A modular algorithm enables human control and interpretability. A concurrent algorithm is simple to train and operate.", ""]}
{"question_id": "1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "predicted_answer": "", "predicted_evidence": ["Naturally each sign has different frame length after segmentation because each subject does a sign at different speed. It is possible that the same subject may do the same sign at different speeds at different times which makes the recognition challenging. Further, neighboring frames contain redundant information; and all joints will not have equal amount of motion or pattern in case of skeletal data.", "In addition to having the cross subject accuracy described in section SECREF29, we also want to know the impact of adding a test subject's data to the training process. It is obvious that adding test subject's data to the training must increase the accuracy of the network for the subject. However, we want to know how much or what fraction of data is necessary for significant improvement in performance. This is important for assessing the practial usability of a recognition system. In other words, we want to know how quickly or with what amount of data, the current system can be adapted for a subject completely unknown to the system. To do that, we first pick a test subject and train a model for the test subject with data from all other subjects in our dataset. Then we retrain the model with some fraction of data from the test subject. We keep increasing the fraction of data being used from the test subject in the retraining process up to $50\\%$.", "In addition to having the cross subject accuracy described in section SECREF29, we also want to know the impact of adding a test subject's data to the training process. It is obvious that adding test subject's data to the training must increase the accuracy of the network for the subject. However, we want to know how much or what fraction of data is necessary for significant improvement in performance. This is important for assessing the practial usability of a recognition system. In other words, we want to know how quickly or with what amount of data, the current system can be adapted for a subject completely unknown to the system. To do that, we first pick a test subject and train a model for the test subject with data from all other subjects in our dataset. Then we retrain the model with some fraction of data from the test subject.", "Figure FIGREF30 shows three confusion matrices for a subset of twelve sign classes for a subject. The top matrix is for AI-LSTM network, middle one is for Max CNN-LSTM and bottom one is for Spatial AI-LSTM. As seen in Figure FIGREF10 the sign pairs Alarm/Doorbell are similar in skeletal motion but have different hand shapes. Since Max CNN-LSTM includes hand shapes, it can successfully recognize it while other two models struggles. Same is true for some other signs like Email, Event, List, Order and Weather . Some other signs are better recognized by Spatial AI-LSTM network. It should be mentioned here that accuracy listed in Table TABREF28 shows average accuracy across all test subjects, while Figure FIGREF30 presents confusion matrix for a single test subject. For this particular subject overall test accuracy is 58%, 70% and 69% for AI-LSTM, Max CNN-LSTM and Spatial AI-LSTM network respectively.", "All of our experiments on ASL recognition were done with RGB video data and/or skeletal data. Skeletal data is a multivariate, multidimensional time series input where each body part acts as a variable and each of them have 3D coordinate data at each time step. The skeletal data provides motion trajectory of different body parts such as wrist, elbow and shoulder (total 25 such body parts) over whole video frames. This process is called skeletal tracking. Skeletal data provides high level motion of different body parts. These are useful for capturing discriminant features associated with different types of gestures. However, for better modeling of sign language, hand shape is crucial, as different signs may have similar motion but different hand shapes and orientation. Figure FIGREF10 presents one such example where the sign pair Alarm and Doorbell have exact same motion pattern according to skeletal data but have different hand shapes. We observe similar situation for sign pairs such as Kitchen/Room, Time/Movie, Quote/Camera, Lock/Stop and many more."]}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "predicted_answer": "", "predicted_evidence": ["We hypothesize that, some signs that have mostly similar skeletal motion pattern could be distinguishable using hand shape information. We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced.", "Standard CNN fails to capture the temporal information associated with data, which is important in video or any type of sequential data representation. To solve this problem, 3D convolution was introduced in BIBREF2. The key difference is that kernels are 3D and sub sampling (pooling) layers work across three dimensions.", "To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python).", "In addition to having the cross subject accuracy described in section SECREF29, we also want to know the impact of adding a test subject's data to the training process. It is obvious that adding test subject's data to the training must increase the accuracy of the network for the subject. However, we want to know how much or what fraction of data is necessary for significant improvement in performance. This is important for assessing the practial usability of a recognition system. In other words, we want to know how quickly or with what amount of data, the current system can be adapted for a subject completely unknown to the system. To do that, we first pick a test subject and train a model for the test subject with data from all other subjects in our dataset. Then we retrain the model with some fraction of data from the test subject.", "AI-LSTM, described in last section, works by modeling temporal dynamics of body joints' data over time. However, there can be spatial interactions with joints at a specific time step. It fails to capture any such interaction among joints in a given time. To incorporate spatial relationship among joints, we propose a simple novel data augmentation technique for skeletal data. We do this by origin transfer. For each frame in a gesture sample, we use each wrist joints as origin and transform all other joints' data by subtracting that origin from them. In this way spatial information is added to the input. We refer this model with spatial data augmentation as Spatial AI-LSTM. This augmentation technique is depicted in Figure FIGREF21. A sample data of form $R^{T \\times 6 \\times 3}$ results in a representation of $R^{T \\times 5 \\times 3}$ after subtracting left wrist joint (origin transfer). After this augmentation process, each sample is a $R^{20 \\times 16 \\times 3}$ matrix."]}
{"question_id": "f85520bbc594918968d7d9f33d11639055458344", "predicted_answer": "", "predicted_evidence": ["Equation DISPLAY_FORM18 shows 3D convolution function. In this case from each filter we get a 3D feature map and $F_{i,j,k}$ denotes value at $(i, j,k)$ location after convolution operation. The dot product is between two three-dimensional matrices (also called tensors).", "To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python).", "We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back\u2013propagation, both networks are trained on their own score.", "Most sign language recognition systems use RGB video data as input. These approaches model sequential dependencies using Hidden Markov Models (HMM). Zafrullah et al. BIBREF7 used colored gloves (worn on hands) during data collection and developed an HMM based framework for ASL phrase verification. They also used hand crafted features from Kinect skeletal data and accelerometers worn on hand BIBREF8. Huang et al. BIBREF1 demonstrated the effectiveness of using Convolutional neural network (CNN) with RGB video data for sign language recognition. Three dimensional CNN have been used to extract spatio-temporal features from video BIBREF2. Similar architecture was implemented for Italian gestures BIBREF9. Sun et al. BIBREF3 hypothesized that not all RGB frames in a video are equally important and assigned a binary latent variable to each frame in training videos for indicating the importance of a frame within a latent support vector machine model.", "All of our experiments on ASL recognition were done with RGB video data and/or skeletal data. Skeletal data is a multivariate, multidimensional time series input where each body part acts as a variable and each of them have 3D coordinate data at each time step. The skeletal data provides motion trajectory of different body parts such as wrist, elbow and shoulder (total 25 such body parts) over whole video frames. This process is called skeletal tracking. Skeletal data provides high level motion of different body parts. These are useful for capturing discriminant features associated with different types of gestures. However, for better modeling of sign language, hand shape is crucial, as different signs may have similar motion but different hand shapes and orientation. Figure FIGREF10 presents one such example where the sign pair Alarm and Doorbell have exact same motion pattern according to skeletal data but have different hand shapes."]}
{"question_id": "e4f2d59030b17867449cf5456118ab722296bebd", "predicted_answer": "", "predicted_evidence": ["Since we do not have additional data with the same annotations, we use the same UD dataset to train our tagger. To prevent overfitting, we only use the first 75% of training data for training. After training the taggers, we predict the case for the training, development, and test sets and use them for dependency parsing.", "Table 2 presents test results for every model on every language, establishing three results. First, they support previous findings that character-level models outperform word-based models\u2014indeed, the char-lstm model outperforms the word model on LAS for all languages except Hindi and Urdu for which the results are identical. Second, they establish strong baselines for the character-level models: the char-lstm generally obtains the best parsing accuracy, closely followed by char-cnn. Third, they demonstrate that character-level models rarely match the accuracy of an oracle model with access to explicit morphology. This reinforces a finding of BIBREF9 : character-level models are effective tools, but they do not learn everything about morphology, and they seem to be closer to oracle accuracy in agglutinative rather than in fusional languages.", "We consider several ways to compute the word representation $\\textbf {e}({w_i})$ in Eq. 2 :", "We treat the morphemes of a morphological annotation as a sequence and compose them using a bi-LSTM. We only use universal inflectional features defined in the UD annotation guidelines. For example, the morphological annotation of \u201cchases\u201d is $\\langle $ chase, person=3rd, num-SG, tense=Pres $\\rangle $ .", "Character trigrams are composed using a bi-LSTM, an approach that we previously found to be effective across typologies BIBREF9 ."]}
{"question_id": "e664b58ea034a638e7142f8a393a88aadd1e215e", "predicted_answer": "", "predicted_evidence": ["$$\\textbf {m}_j = [\\textbf {f}_{j1}, \\cdots , \\textbf {f}_{jk}]^\\top \\textbf {k}$$   (Eq. 43)", "To test this, we look at accuracy for word types that are empirically ambiguous\u2014those that have more than one morphological analysis in the training data. Note that by this definition, some ambiguous words will be seen as unambiguous, since they were seen with only one analysis. To make the comparison as fair as possible, we consider only words that were observed in the training data. Figure 1 compares the improvement of the oracle on ambiguous and seen unambiguous words, and as expected we find that handling of ambiguous words improves with the oracle in almost all languages. The only exception is Turkish, which has the least training data.", "We call $\\textbf {x}_i$ the embedding of $w_i$ since it depends on context-independent word and POS representations. We obtain a context-sensitive encoding $\\textbf {h}_i$ with a bidirectional LSTM (bi-LSTM), which concatenates the hidden states of a forward and backward LSTM at position $i$ . Using $\\textbf {h}_i^f$ and $\\textbf {h}_i^b$ respectively to denote these hidden states, we have:", "on table lies letter", "Every word type has its own learned vector representation."]}
{"question_id": "c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "predicted_answer": "", "predicted_evidence": ["where $f$ is again a function to compute label score:", "Next, we look at the learned attention vectors to understand which morphological features are important, focusing on the core arguments: nominal subjects, objects, and indirect objects. Since our model knows the case of each dependent, this enables us to understand what features it seeks in potential heads for each case. For simplicity, we only report results for words where both head and label predictions are correct.", "Table 9 and 10 present morphological tagging results for German and Russian. We found that German and Russian have similar pattern to Czech (Table 5 ), where morphological case seems to be preserved in the encoder because they are useful for dependency parsing. In these three fusional languages, contextual information helps character-level model to predict the correct case. However, its performance still behind the oracle.", "Table 3 shows how the character model improves over the word model for both non-OOV and OOV words. On the agglutinative languages Finnish and Turkish, where the OOV rates are 23% and 24% respectively, we see the highest LAS improvements, and we see especially large improvements in accuracy of OOV words. However, the effects are more mixed in other languages, even with relatively high OOV rates. In particular, languages with rich morphology like Czech, Russian, and (unvocalised) Arabic see more improvement than languages with moderately rich morphology and high OOV rates like Portuguese or Spanish. This pattern suggests that parameter sharing between pairs of observed training words can also improve parsing performance. For example, if \u201cdog\u201d and \u201cdogs\u201d are observed in the training data, they will share activations in their context and on their common prefix.", "For each word $w_i$ , we compute a distribution over all other word positions $j \\in \\lbrace 0,...,|w|\\rbrace /i$ denoting the probability that $w_j$ is the headword of $w_i$ ."]}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "predicted_answer": "", "predicted_evidence": ["We combine the morphological representation with the word's encoding via a sigmoid gating mechanism.", "Table 3 shows how the character model improves over the word model for both non-OOV and OOV words. On the agglutinative languages Finnish and Turkish, where the OOV rates are 23% and 24% respectively, we see the highest LAS improvements, and we see especially large improvements in accuracy of OOV words. However, the effects are more mixed in other languages, even with relatively high OOV rates. In particular, languages with rich morphology like Czech, Russian, and (unvocalised) Arabic see more improvement than languages with moderately rich morphology and high OOV rates like Portuguese or Spanish. This pattern suggests that parameter sharing between pairs of observed training words can also improve parsing performance. For example, if \u201cdog\u201d and \u201cdogs\u201d are observed in the training data, they will share activations in their context and on their common prefix.", "Do character-level models learn morphology? We view this as an empirical claim requiring empirical evidence. The claim has been tested implicitly by comparing character-level models to word lookup models BIBREF7 , BIBREF8 . In this paper, we test it explicitly, asking how character-level models compare with an oracle model with access to morphological annotations. This extends experiments showing that character-aware language models in Czech and Russian benefit substantially from oracle morphology BIBREF9 , but here we focus on dependency parsing (\u00a7 \"Dependency parsing model\" )\u2014a task that benefits substantially from morphological knowledge\u2014and we experiment with twelve languages using a variety of techniques to probe our models.", "Na stole le\u017eit pis\u02camo", "We use a neural graph-based dependency parser combining elements of two recent models BIBREF10 , BIBREF11 . Let $w = w_1, \\dots , w_{|w|}$ be an input sentence of length $|w|$ and let $w_0$ denote an artificial Root token. We represent the $i$ th input token $w_i$ by concatenating its word representation (\u00a7 \"Computing word representations\" ), $\\textbf {e}(w_i)$ and part-of-speech (POS) representation, $\\textbf {p}_i$ . Using a semicolon $(;)$ to denote vector concatenation, we have:"]}
{"question_id": "2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "predicted_answer": "", "predicted_evidence": ["The methods are evaluated on testing data randomly selected from the dataset with the ratio of 20%. The input data is not manipulated to manually balance the classes for any of the above methods. Therefore, the training and testing data retain the same distribution as the collected results (Section SECREF4). The methods are evaluated using F-1 score, Precision-Recall (PR) AUC, and Receiver-Operating-Characteristic (ROC) AUC.", "Identify Hate Keywords: One of the most common strategies is to identify the inappropriate terms in the post and then urge the user to stop using that work. For example, \u201cThe C word and language attacking gender is unacceptable. Please refrain from future use.\u201d This strategy is often used when the hatred in the post is mainly conveyed by specific hate keywords.", "Positive Tone Followed by Transitions: This is a strategy where the response consists of two parts combined with a transitional word, such as \u201cbut\u201d and \u201ceven though\u201d. The first part starts with affirmative terms, such as \u201cI understand\u201d, \u201cYou have the right to\u201d, and \u201cYou are free to express\u201d, showing kindness and understanding, while the second part is to alert the users that their post is inappropriate. For example, \u201cI understand your frustration, but the term you have used is offensive towards the disabled community. Please be more aware of your words.\u201d. Intuitively, compared with the response that directly warns, this strategy is likely more acceptable for the users and be more likely to clam down a quarrel full of hate speech.", "", "Q1: Which posts or comments in this conversation are hate speech?"]}
{"question_id": "a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "predicted_answer": "", "predicted_evidence": ["", "Q2: If there exists hate speech in the conversation, how would you respond to intervene? Write down a response that can probably hold it back (word limit: 140 characters).", "Q1: Which posts or comments in this conversation are hate speech?", "Positive Tone Followed by Transitions: This is a strategy where the response consists of two parts combined with a transitional word, such as \u201cbut\u201d and \u201ceven though\u201d. The first part starts with affirmative terms, such as \u201cI understand\u201d, \u201cYou have the right to\u201d, and \u201cYou are free to express\u201d, showing kindness and understanding, while the second part is to alert the users that their post is inappropriate. For example, \u201cI understand your frustration, but the term you have used is offensive towards the disabled community. Please be more aware of your words.\u201d. Intuitively, compared with the response that directly warns, this strategy is likely more acceptable for the users and be more likely to clam down a quarrel full of hate speech.", "Reddit: To retrieve high-quality conversational data that would likely include hate speech, we referenced the list of the whiniest most low-key toxic subreddits. Skipping the three subreddits that have been removed, we collect data from ten subreddits: r/DankMemes, r/Imgoingtohellforthis, r/KotakuInAction, r/MensRights, r/MetaCanada, r/MGTOW, r/PussyPass, r/PussyPassDenied, r/The_Donald, and r/TumblrInAction. For each of these subreddits, we retrieve the top 200 hottest submissions using Reddit's API. To further focus on conversations with hate speech in each submission, we use hate keywords BIBREF6 to identify potentially hateful comments and then reconstructed the conversational context of each comment. This context consists of all comments preceding and following a potentially hateful comment."]}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "predicted_answer": "", "predicted_evidence": ["In our experiments, we only utilized the text of the posts, but more information is available and can be utilized, such as the user information and the title of a Reddit submission.", "However, human evaluation (Table TABREF30) shows that the RL model creates responses that are potentially better at mitigating hate speech and are more diverse, which is consistent with BIBREF21. There is a larger performance difference with the Gab dataset, while the effectiveness and the diversity of the responses generated by the Seq2Seq model and the RL model are quite similar on the Reddit dataset. One possible reason is that the size of the training data from Reddit (around 8k) is only 30% the size of the training data from Gab. The inconsistency between the human evaluation results and the automatic ones indicates the automatic evaluation metrics listed in Table TABREF29 can hardly reflect the quality of the generated responses. As mentioned in Section SECREF4, annotators tend to have strategies for intervention. Therefore, generating the common parts of the most popular strategies for all the testing input can lead to high scores of these automatic evaluation metrics. For example, generating \u201cPlease do not use derogatory language.\u201d for all the testing Gab data can achieve 4.2 on BLEU, 20.4 on ROUGE, and 18.2 on METEOR.", "In order to validate and compare the quality of the generated results from each model, we also conducted human evaluations as previous research has shown that automatic evaluation metrics often do not correlate with human preference BIBREF32. We randomly sampled 450 conversations from the testing dataset. We then generated responses using each of the above models trained with the filtered conversation setting. In each assignment, a Mechanical Turk worker is presented 10 conversations, along with corresponding responses generated by the three models. For each conversation, the worker is asked to evaluate the effectiveness of the generated intervention by selecting a response that can best mitigate hate speech. 9 of the 10 questions are filled with the sampled testing data and the generated results, while the other is artificially constructed to monitor response quality. After selecting the 10 best mitigation measures, the worker is asked to select which of the three methods has the best diversity of responses over all the 10 conversations. Ties are permitted for answers. Assignments failed on the quality check are rejected.", "Gab: We collect data from all the Gab posts in October 2018. Similar to Reddit, we use hate keywords BIBREF6 to identify potentially hateful posts, rebuild the conversation context and clean duplicate conversations.", "Reinforcement Learning (RL): We also implement the Reinforcement Learning method described in Section SECREF5. The backbone of this model is the Seq2Seq model, which follows the same Seq2Seq network structure described above. This network is used to parameterize the probability of a response given the conversation. Besides this backbone Seq2Seq model, another Seq2Seq model is used to generate the backward probability. This network is trained in a similar way as the backbone Seq2Seq model, but with a response as input and the corresponding conversation as the target. In our implementation, the function of the first part of the reward ($\\log p(r|c)$) is conveyed by the MLE loss. A curriculum learning strategy is adopted for the reward of $\\log p_{back}(c|r)$ as in BIBREF28. Same as in BIBREF21 and BIBREF28, a baseline strategy is employed to estimate the average reward."]}
{"question_id": "a4cf0cf372f62b2dbc7f31c600c6c66246263328", "predicted_answer": "", "predicted_evidence": ["BIBREF17 added noises to extend the original sentences and trained a denoising auto-encoder to recover the original, constructing an end-to-end training network without any examples of compressed sentences in sequence to sequence framework. In doing so, the model has to exclude and reorder the noisy sentence input, and hence learns to output more semantic important, shorter but grammatically correct sentences. There are two types of noise used in the model: Additive Sampling Noise and Shuffle Noise.", "Specifically, for the multi-head attention-fusion layer, a compressed sentence-specific context representation $H_x^c$ is computed by the multi-head attention on the original sentence representation $H_x$ and the compressed sentence representation $H_c$:", "Finally, the context vector $o_{i}$ is used to compute translation probabilities of the next target word $\\textit {y}_i$ by a linear, potentially multi-layered function:", "Sentence compression usually relies on large-scale raw data together with their human-labeled data, which can be viewed as supervision, to train a sentence compression model BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12. For example, BIBREF11 proposed an attentive encoder-decoder recurrent neural network (RNN) to model abstractive text summarization. BIBREF13 furture proposed MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder sentence compression framework which reported state-of-the-art performance on both the Gigaword Corpus and DUC Corpus.", "According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:"]}
{"question_id": "f7b91b99279833f9f489635eb8f77c6d13136098", "predicted_answer": "", "predicted_evidence": ["In our model, representations of the original sentence and its compressed version were learned by a shared encoder. To explore the effect of the encoder parameters, we also designed a BBFNMT with two independent encoders to learn representations of the original sentence and its compressed version, respectively. Table TABREF41 shows results on the newstest2014 test set for the WMT14 EN-DE translation task.", "To let the translation have more focus over the source sentence information, efforts have been initiated on exploiting sentence segmentation, sentence simplification, and sentence compression for machine translation. BIBREF33 presented a approach to integrating the sentence skeleton information into a phrase-based statistic machine translation system. BIBREF34 proposed an approach to modeling syntactically-motivated skeletal structure of source sentence for statistic machine translation. BIBREF35 describe an early approach to skeleton-based translation, which decomposes input sentences into syntactically meaningful chunks. The central part of the sentence is identified and remains unaltered while other parts of the sentence are simplified. This process produces a set of partial, potentially overlapping translations which are recombined to form the final translation. BIBREF36 describe a \u201cdivide and translate\u201d approach to dealing with complex input sentences. They parse the input sentences, replace subclauses with placeholders and later substitute them with separately translated clauses. Their method requires training translation models on clause-level aligned parallel data with placeholders in order for the translation model to deal with the placeholders correctly.", "To let the translation have more focus over the source sentence information, efforts have been initiated on exploiting sentence segmentation, sentence simplification, and sentence compression for machine translation. BIBREF33 presented a approach to integrating the sentence skeleton information into a phrase-based statistic machine translation system. BIBREF34 proposed an approach to modeling syntactically-motivated skeletal structure of source sentence for statistic machine translation. BIBREF35 describe an early approach to skeleton-based translation, which decomposes input sentences into syntactically meaningful chunks. The central part of the sentence is identified and remains unaltered while other parts of the sentence are simplified. This process produces a set of partial, potentially overlapping translations which are recombined to form the final translation. BIBREF36 describe a \u201cdivide and translate\u201d approach to dealing with complex input sentences. They parse the input sentences, replace subclauses with placeholders and later substitute them with separately translated clauses.", "The BBFNMT (w/ independent params) slightly outperformed the proposed shared encoder model by a BLEU score of 0.15, but its parameters increased by approximately 30%. In contrast, the parameters in our model are comparable to the baseline Transformer (base). Considering the parameter scale, we took a shared encoder to learn source representation, which makes it easy to verify the effectiveness of the additional translation knowledge, such as our backbone knowledge.", "where $d_{model}$ represents the dimensions of the model. Similarly, the translated target words are used to generate the decoder hidden state $\\textbf {s}_i$ at the current time-step $i$. Generally, the self-attention function is further refined as multi-head self-attention to jointly consider information from different representation subspaces at different positions:"]}
{"question_id": "99e514acc0109b7efa4e3860ce1e8c455f5bb790", "predicted_answer": "", "predicted_evidence": ["5) The proposed BBFNMT (base) slightly outperformed the Transformer (big) which contains much more parameters than BBFNMT (base). This indicates that our improvement is not likely to be due to the increased number of parameters.", "Baseline systems include AllText and F8W BIBREF22, BIBREF25. F8W is simply the first 8 words of the input, and AllText uses the whole text as the compression output. The $F_1$ score of ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) was used to evaluate this task BIBREF26. We use beam search with a beam size of 5, the length length normalization of 0.5, and the coverage penalty of 0.2.", "where the projections are parameter matrices $\\textbf {W}_{h}^{Q}$$\\in $$\\mathbb {R}^{d_{model}\\times d_k}$, $\\textbf {W}_{h}^{K}$$\\in $$\\mathbb {R}^{d_{model}\\times d_k}$, $\\textbf {W}_{h}^{V}$$\\in $$ \\mathbb {R}^{d_{model}\\times d_v}$, and $\\textbf {W}^{O}$$\\in $$\\mathbb {R}^{hd_{v}\\times d_{model}}$. For example, there are $H$=8 heads, $d_{model}$ is 512, and $d_k$=$d_v$=512/8=64. A position-wise feed-forward network (FFN) layer is applied over the output of multi-head self-attention, and then is added with the matrix $\\textbf {V}$ to generate the final source representation $H_{x}$=$\\lbrace H^{x}_1, \\cdots , H^{x}_J\\rbrace $:", "Different with the length marker or length countdown input, to induce our model to output the compression sequence with desired length, we use beam search during generation to find the sequence $S^{^{\\prime }}$ that maximizes a score function $s(S^{^{\\prime }}, S)$ given a trained ESC model. The length normalization is introduced to account for the fact that we have to compare hypotheses of different length. Without some form of length-normalization regular $ln$, beam search will favor shorter sequences over longer ones on average since a negative log-probability is added at each step, yielding lower (more negative) scores for longer sentences. Moreover, a coverage penalty $cp$ is also added to favor the sequence that cover the source sentence meaning as much as possible according to the attention weights BIBREF20.", "In state-of-the-art Transformer-based encoder, self-attention mechanisms are good at capturing the general information in a sentence BIBREF2, BIBREF3, BIBREF4. However, it is difficult to distinguish which kind of information lying deeply under the language is really salient for learning source representation. Intuitively, when a person reads a source sentence, he/she often selectively focuses on the basic sentence meaning, and re-reads the entire sentence to understand its meaning completely. Take the English sentence in Table TABREF2 as an example. We manually annotate its basic meaning as a shorter sequence of words than in the original sentence, called backbone information. Obviously, these words with the basic meaning contain more important information for human understanding than the remaining words in the sentence. We argue that such backbone information is also helpful for learning source representation, and is not explicitly considered by the existing NMT system to enrich the source sentence representation."]}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "predicted_answer": "", "predicted_evidence": ["The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable.", "We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.", "In this paper, we propose GazSelfAttn, a novel gazetteer embedding approach that uses self-attention and match span encoding to build enhanced gazetteer representation. GazSelfAttn embeddings are concatenated with the input to a LSTM BIBREF10 or CNN BIBREF11 sequence layer and are trained end-to-end with the model. In addition, we show how to extract general gazetteers from the Wikidata, a structured knowledge-base which is part of the Wikipedia project.", "To prevent the neural NER model from overfitting on the gazetteers, we use gazetteers dropout BIBREF25. We randomly set to zero gazetteer embeddings $\\mathbf {g}_i$, so the gazetteer vectors that are input to the LSTM become zero. We tune the gazetteer dropout hyperparameter on the validation set.", "Note that gazetteer matching is unsupervised, i.e., we do not have a ground truth of correctly matched sentences for $M$. Furthermore, it is a hard task because of the many variations in writing and ambiguity of entities."]}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "predicted_answer": "", "predicted_evidence": ["Named-entity recognition (NER) is the task of tagging relevant entities such as person, location and organization in unstructured text. Modern NER has been dominated by neural models BIBREF0, BIBREF1 combined with contextual embeddings from language models (LMs) BIBREF2, BIBREF3, BIBREF4. The LMs are pre-trained on large amounts of unlabeled text which allows the NER model to use the syntactic and semantic information captured by the LM embeddings. On the popular benchmark datasets CoNLL-03 BIBREF5 and Ontonotes 5 BIBREF6, neural models with LMs achieved state-of-the-art results without gazetteers features, unlike earlier approaches that heavily relied on them BIBREF7. Gazetteers are lists that contain entities such as cities, countries, and person names. The gazetteers are matched against unstructured text to provide additional features to the model.", "Configuration. We use the Bi-LSTM-CNN-CRF model architecture with ELMo language model embeddings from BIBREF2, which consist of 50 dim pre-trained Glove word embeddings BIBREF20, 128 dim Char-CNN BIBREF21, BIBREF1 embeddings with filter size of 3 and randomly initialized 16 dim char embeddings, 1024 pre-trained ELMo pre-trained embeddings, two layer 200 dim Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) spans BIBREF22.", "In this paper, we propose GazSelfAttn, a novel gazetteer embedding approach that uses self-attention and match span encoding to build enhanced gazetteer representation. GazSelfAttn embeddings are concatenated with the input to a LSTM BIBREF10 or CNN BIBREF11 sequence layer and are trained end-to-end with the model. In addition, we show how to extract general gazetteers from the Wikidata, a structured knowledge-base which is part of the Wikipedia project.", "Our contributions are the following:", "Equations DISPLAY_FORM11- shows the gazetteer embedding $\\mathbf {g}_i$ computation for a token $t_i$. To compute $\\mathbf {g}_i$, given a set of $m$ gazetteer types $\\lbrace g^m_i\\rbrace $ and spans $\\lbrace s^m_i\\rbrace $, we execute the following procedure:"]}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "predicted_answer": "", "predicted_evidence": ["In addition, we tried moving the gazetteer embeddings to the CRF layer and using the LSTM token embeddings as attention keys but the F$_1$ degraded significantly. We experimented with adding auto-encoding loss similar to BIBREF16 and multi-head self-attention. However, we did not observe F$_1$ score improvements and sometimes small degradations.", "Equation DISPLAY_FORM11. We embed the sets $\\lbrace g^m_i\\rbrace $ and $\\lbrace s^m_i\\rbrace $ using the embedding matrices $\\mathbf {G}$ and $\\mathbf {S}$. Then, we do an element-wise addition, denoted $\\oplus $, of the corresponding types and spans embeddings to get a matrix $\\mathbf {E}_i$.", "Equation . Finally, we perform max pooling across the embeddings $\\mathbf {H}_i$ to obtain the final gazetteer embedding $\\mathbf {g}_i$.", "BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model.", "For future work we would like to evaluate GazSelfAttn on non-English language datasets and improve the multi-token gazetteer matching with fuzzy string matching. Also, we would like to explore transfer learning of gazetteer embeddings from high-resource to low-resource setting."]}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "predicted_answer": "", "predicted_evidence": ["Also, because the training of phonetic-and-semantic embedding is challenging, in the initial effort we slightly simplify the task by assuming all training utterances have been properly segmented into spoken words. Because there exist many approaches for segmenting utterances automatically BIBREF25 , and automatic segmentation plus phonetic embedding of spoken words has been successfully trained and reported before BIBREF25 , such an assumption is reasonable here.", "First look at the top part of Table TABREF18 for top-1 nearest accuracies for 1000 pairs of audio and text embeddings. Since column (a) (TXT-ph) considered precise phonetic structures but not semantics at all, the relatively high accuracies in column (a) for all three versions of audio embedding (i)(ii)(iii) implied the three versions of audio embedding were all rich of phonetic information. But when the semantics were embedded in (ii)(iii) (AUD-(ph-+se), AUD-(ph+se)), the phonetic structures were inevitably disturbed (0.519, 0.598 vs 0.637).", "In addition, we propose an approach for parallelizing the audio and text embeddings to be used for evaluating the phonetic and semantic information carried by the audio embeddings. These are described in Subsections SECREF2 , SECREF11 and SECREF14 respectively.", "In this paper we propose a framework to embed spoken words into vector representations carrying both the phonetic structure and semantics of the word. This is intrinsically challenging because the phonetic structure and the semantics of spoken words inevitably disturbs each other. But this phonetic-and-semantic embedding nature is desired and attractive, for example in the application task of spoken document retrieval. A parallelizing transformation between the audio and text embeddings is also proposed to evaluate whether such a goal is achieved.", "In this paper we further propose an approach of parallelizing a set of audio embeddings (for spoken words) with a set of text embeddings (for text words) which will be useful in evaluating the phonetic and semantic information carried by these embeddings."]}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "predicted_answer": "", "predicted_evidence": ["Also, the Stage 2 training in rows (ii)(iii) (AUD-(ph-+se), AUD-(ph+se)) gave higher accuracies than row (i) (AUD-ph) (0.339, 0.332 vs 0.124 in column (b)), which implied the Stage 2 training was successful. However, column (c) (TXT-(se,ph)) is for the text embedding considering both the semantic and phonetic information, so the two versions of phonetic-and-semantic audio embedding for rows (ii)(iii) had very close distributions (0.750, 0.800 in column (c)), or carried good extent of both semantics and phonetic structure. The above are made clearer by the numbers in bold which are the highest for each row, and the numbers in red which are the highest for each column.", "We used the 960 hours of \u201cclean\" and \u201cother\" parts of LibriSpeech dataset as the target archive for retrieval, which consisted of 1478 audio books with 5466 chapters. Each chapter included 1 to 204 utterances or 5 to 6529 spoken words. In our experiments, the queries were the keywords in the book titles, and the spoken documents were the chapters. We chose 100 queries out of 100 randomly selected book titles, and our goal was to retrieve query-relevant documents. For each query INLINEFORM0 , we defined two sets of query-relevant documents: The first set INLINEFORM1 consisted of chapters which included the query INLINEFORM2 . The second set INLINEFORM3 consisted of chapters whose content didn't contain INLINEFORM4 , but these chapters belonged to books whose titles contain INLINEFORM5 (so we assume these chapters are semantically related to INLINEFORM6 ).", "On the other hand, we also obtained three different types of text embedding (TXT) on the same set of top 1000, 3000 and 5000 words. Type (a) Phonetic Text embedding (TXT-ph) considered precise phonetic structure but not context or semantics at all. This was achieved by a well-trained sequence-to-sequence autoencoder encoding the precise phoneme sequence of a word into a latent embedding. Type (b) Semantic Text embedding considered only context or semantics but not phonetic structure at all, and was obtained by a standard skip-gram model using one-hot representations as the input (TXT-(se,1h)). Type (c) Semantic and Phonetic Text embedding (TXT-(se,ph)) considered context or semantics as well as the precise phonetic structure, obtained by a standard skip-gram model but using the Type (a) Phonetic Text embedding (TXT-ph) as the input. So these three types of text embeddings provided the reference embeddings obtained from text and/or phoneme sequences, not disturbed by audio signals at all.", "For example, in the first row for the query \u201cnations\", the chapter \u201cPrometheus the Friend of Man\" of the book titled \u201cMyths and Legends of All Nations\" is in INLINEFORM11 . The word \u201cnations\" is not in the content of this chapter. However, because the word \u201cking\" semantically related to \u201cnations\" is in the content, this chapter was ranked the 13th among all chapters whose content didn't contain the word \u201cnations\". This clearly verified why the semantics in the phonetic-and-semantic embeddings can remarkably improve the performance of spoken content retrieval.", "Assume we have the audio embeddings for a set of spoken words INLINEFORM0 INLINEFORM1 , where INLINEFORM2 is the embedding obtained for a spoken word INLINEFORM3 and INLINEFORM4 is the total number of distinct spoken words in the audio corpus. On the other hand, assume we have the text embeddings INLINEFORM5 INLINEFORM6 , where INLINEFORM7 is the embedding of the INLINEFORM8 -th text word for the INLINEFORM9 distinct text words. Although the distributions of INLINEFORM10 and INLINEFORM11 in their respective spaces are not parallel, that is, a specific dimension in the space for INLINEFORM12 does not necessarily correspond to a specific dimension in the space for INLINEFORM13 , there should exist some consistent relationship between the two distributions. For example, the relationships among the words {France, Paris, Germany} learned from context should be consistent in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces."]}
{"question_id": "fef5b65263c81299acc350a101dabaf5a8cb9c6e", "predicted_answer": "", "predicted_evidence": ["As shown in Figure FIGREF12 , similar to the Word2Vec skip-gram model BIBREF0 , we use two encoders: semantic encoder INLINEFORM0 and context encoder INLINEFORM1 to embed the semantics over phonetic embeddings INLINEFORM2 obtained in Stage 1. On the one hand, given a spoken word INLINEFORM3 , we feed its phonetic vector INLINEFORM4 obtained from Stage 1 into INLINEFORM5 as in the middle of Figure FIGREF12 , producing the semantic embedding (in yellow) of the spoken word INLINEFORM6 . On the other hand, given the context window size INLINEFORM7 , which is a hyperparameter, if a spoken word INLINEFORM8 is in the context window of INLINEFORM9 , then its phonetic vector INLINEFORM10 is a context vector of INLINEFORM11 .", "We compared the retrieval results with two versions of audio embedding: AUD-(ph+se) and AUD-ph. The results are listed in Table TABREF21 for two definitions of groundtruth for the query-relevant documents: the union of INLINEFORM0 and INLINEFORM1 and INLINEFORM2 alone. As can be found from this table, AUD-(ph+se) offered better retrieval performance than AUD-ph in both rows. Note that those chapters in INLINEFORM3 in the second row of the table did not include the query INLINEFORM4 , so couldn't be well retrieved using phonetic embedding alone. That is why the phonetic-and-semantic embedding proposed here can help.", "Mini-Batch Cycle Iterative Closest Point (MBC-ICP) BIBREF44 previously proposed as described below is used here. Given two sets of embeddings as mentioned above, INLINEFORM0 and INLINEFORM1 , they are first projected to their respective top INLINEFORM2 principal components by PCA. Let the projected sets of vectors of INLINEFORM3 and INLINEFORM4 be INLINEFORM5 and INLINEFORM6 respectively. If INLINEFORM7 can be mapped to the space of INLINEFORM8 by an affine transformation, the distributions of INLINEFORM9 and INLINEFORM10 would be similar after PCA BIBREF44 .", "For each query INLINEFORM0 and each document INLINEFORM1 , the relevance score of INLINEFORM2 with respect to INLINEFORM3 , INLINEFORM4 , is defined as follows: DISPLAYFORM0", "The optimization procedure of Stage 1 consists of four parts: (1) training INLINEFORM0 , INLINEFORM1 and INLINEFORM2 by minimizing INLINEFORM3 , (2) training INLINEFORM4 by minimizing INLINEFORM5 , (3) training INLINEFORM6 by minimizing INLINEFORM7 , and (4) training INLINEFORM8 by maximizing INLINEFORM9 . Parts (1)(2)(3) are jointly trained together, while iteratively trained with part (4) BIBREF45 ."]}
{"question_id": "f40e23adc8245562c8677f0f86fa5175179b5422", "predicted_answer": "", "predicted_evidence": ["Given a pair of phonetic vectors INLINEFORM0 , the training criteria for INLINEFORM1 and INLINEFORM2 is to maximize the similarity between INLINEFORM3 and INLINEFORM4 if INLINEFORM5 and INLINEFORM6 are contextual, while minimizing the similarity otherwise. The basic idea is parallel to that of text Word2Vec. Two different spoken words having similar context should have similar semantics. Thus if two different phonetic embeddings corresponding to two different spoken words have very similar context, they should be close to each other after projected by the semantic encoder INLINEFORM7 . The semantic and context encoders INLINEFORM8 and INLINEFORM9 learn to minimize the semantic loss INLINEFORM10 as follows: DISPLAYFORM0", "In this table, column (a) is the query INLINEFORM1 , column (b) is the title of a book INLINEFORM2 which had chapters in INLINEFORM3 , column (c) is a certain chapter INLINEFORM4 in INLINEFORM5 , column (d) is the rank of INLINEFORM6 out of all chapters whose content didn't contain INLINEFORM7 , and column (e) is a part of the content in INLINEFORM8 where the word in red is the word in INLINEFORM9 with the highest similarity to INLINEFORM10 . For example, in the first row for the query \u201cnations\", the chapter \u201cPrometheus the Friend of Man\" of the book titled \u201cMyths and Legends of All Nations\" is in INLINEFORM11 . The word \u201cnations\" is not in the content of this chapter. However, because the word \u201cking\" semantically related to \u201cnations\" is in the content, this chapter was ranked the 13th among all chapters whose content didn't contain the word \u201cnations\".", "Also, the Stage 2 training in rows (ii)(iii) (AUD-(ph-+se), AUD-(ph+se)) gave higher accuracies than row (i) (AUD-ph) (0.339, 0.332 vs 0.124 in column (b)), which implied the Stage 2 training was successful. However, column (c) (TXT-(se,ph)) is for the text embedding considering both the semantic and phonetic information, so the two versions of phonetic-and-semantic audio embedding for rows (ii)(iii) had very close distributions (0.750, 0.800 in column (c)), or carried good extent of both semantics and phonetic structure. The above are made clearer by the numbers in bold which are the highest for each row, and the numbers in red which are the highest for each column.", "In Stage 2, the two encoders INLINEFORM0 and INLINEFORM1 were both 2-hidden-layer fully-connected feedforward networks with size 256. The size of embedding vectors was set to be 128. The context window size was 5, and the negative sampling number was 5.", "We used the 960 hours of \u201cclean\" and \u201cother\" parts of LibriSpeech dataset as the target archive for retrieval, which consisted of 1478 audio books with 5466 chapters. Each chapter included 1 to 204 utterances or 5 to 6529 spoken words. In our experiments, the queries were the keywords in the book titles, and the spoken documents were the chapters. We chose 100 queries out of 100 randomly selected book titles, and our goal was to retrieve query-relevant documents. For each query INLINEFORM0 , we defined two sets of query-relevant documents: The first set INLINEFORM1 consisted of chapters which included the query INLINEFORM2 . The second set INLINEFORM3 consisted of chapters whose content didn't contain INLINEFORM4 , but these chapters belonged to books whose titles contain INLINEFORM5 (so we assume these chapters are semantically related to INLINEFORM6 ). Obviously INLINEFORM7 and INLINEFORM8 were mutually exclusive, and INLINEFORM9 were the target for semantic retrieval, but couldn't be retrieved based on the phonetic structures only."]}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "predicted_answer": "", "predicted_evidence": ["In the context of COLIEE 2016, our approach is to build a pipeline framework which addresses two important tasks: IR and QA. In Figure 1 , in training phase, a legal text corpus was built based on all articles. Each training query-article pair for LIR task and LQA task was represented as a feature vector. Those feature vectors were utilized to train a learning-to-rank (L2R) model (Ranking SVM) for IR and a classifier (CNN) for QA. The red arrows mean that those steps were prepared in advance. In the testing phase, given a query $q$ , the system extracts its features and computes the relevance score corresponding to each article by using the L2R model. Higher score yielded by SVM-Rank means the article is more relevant. As shown in Figure 1 , the article ranked first with the highest score, i.e.", "where $X = \\lbrace x_1,x_2,..,x_n\\rbrace $ and $Y=\\lbrace y_1,y_2,...,y_n\\rbrace $ are two TF-IDF vectors of a query $q$ and an article $A$ respectively.", "$$ J(q,A) = J(X,Y) = \\frac{\\sum _{i}^{} min(x_i,y_i)}{\\sum _{i}^{} max(x_i,y_i)}$$   (Eq. 16)", "Results from Table 5 also indicate that expanding the reference of an article negatively affects the performance of our model, reducing the F1-score by more than 0.02. This is because if we only expand the content of an article with the content of referential one, it is more likely to be noisy and distorted, leading to lower performance. Therefore, we conclude that a simple expansion of articles via their references does not always positively contribute to the performance of the model.", "In CNN models, we found that these models are sensitive to the initial value of parameters. Different values lead to large difference in results ( $\\pm $ 5%). Therefore, each model was run $n$ times (n=10) and we chose the best-optimized parameters against the validation set. Table 7 shows that CNN with additional features performs better. Also, CNN with LSI produces a better result as opposed to CNN with TF-IDF. We suspect that this is because TF-IDF vector is large but quite sparse (most values are zero), therefore it increases the number of parameters in CNN and consequently makes the model to be overfitted easily."]}
{"question_id": "fac273ecb3e72f2dc94cdbc797582d7225a8e070", "predicted_answer": "", "predicted_evidence": ["This work investigates Ranking SVM model and CNN for building a legal question answering system for Japan Civil Code. Experimental results show that feature selection affects significantly to the performance of SVM-Rank, in which a set of features consisting of (LSI, Manhattan, Jaccard) gives promising results for information retrieval task. For question answering task, the CNN model is sensitive to initial values of parameters and exerts higher accuracy when adding auxiliary features.", "In our current work, we have not yet fully explored the characteristics of legal texts in order to utilize these features for building legal QA system. Properties such as references between articles or structured relations in legal sentences should be investigated more deeply. In addition, there should be more evaluation of SVM-Rank and other L2R methods to observe how they perform on this legal data using the same feature set. These are left as our future work.", "Table 9 shows results with different scenarios. The result of No voting approach is influenced by IR task's performance, so the accuracy is not as high as using voting. The relevant score disparity between the first and second relevant article is large, which causes a worse result of Voting with ratio compared to Voting without ratio.", "In this stage, we illustrate our framework on COLIEE 2016 data. The framework was trained on XML files, from H18 to H23 and tested on XML file H24. Given a legal question, the framework first retrieves top five relevant articles and then transfers the question and relevant articles to CNN classifier. The running of framework was evaluated with 3 scenarios:", "Apparently, three paragraphs and the query share several words namely mandatary, remuneration, etc. In this case, however, the correct answer is only located in paragraph 1, which is ranked first in the single-paragraph model in contrast to two remaining paragraphs with lower ranks, 5th and 29th as shown in Table 12 ."]}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "predicted_answer": "", "predicted_evidence": ["Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers.", "For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.", "We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model\u2014i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation.", "For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.", "Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result. In related work on multilingual pretraining for low-resource ASR, Adams et al. BIBREF10 showed that pre-training on more languages helps, but it is not clear whether the improvement is due to including more languages, or just more data."]}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "predicted_answer": "", "predicted_evidence": ["We use code and hyperparameter settings from BIBREF4: the Adam optimizer BIBREF24 with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score. When training AST models, we regularize using dropout BIBREF25 with a ratio of $0.3$ over the embedding and LSTM layers BIBREF26; weight decay with a rate of $0.0001$; and, after the first 20 epochs, 30% of the time we replace the predicted output word by a random word from the target vocabulary. At test time we use beam decoding with a beam size of 5 and length normalization BIBREF27 with a weight of 0.6.", "Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions BIBREF7. However, building high-quality end-to-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.", "Finally, we hope to gain some understanding into why pretraining on ASR helps with AST, and specifically how the neural network representations change during pretraining and fine-tuning. We follow BIBREF34 and BIBREF9, who built diagnostic classifiers BIBREF35 to examine the representation of phonetic information in end-to-end ASR and AST systems, respectively. Unlike BIBREF34, BIBREF9, who used non-linear classifiers, we use a linear classifier to predict phone labels from the internal representations of the trained ASR or AST model.", "To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model\u2014i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining.", "Since we focus on investigating factors that might affect the AST improvements over the baseline when pretraining, we have chosen ASR datasets for pretraining that contrast in the number of hours and/or in the language similarity with Spanish. Statistics for each dataset are in the left half of Table TABREF7, with further details below."]}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "predicted_answer": "", "predicted_evidence": ["Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) BIBREF21, with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time BIBREF22 as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism BIBREF23 to predict the word at the current time step.", "This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining. However, there are big differences even amongst the languages with similar amounts of pretraining data. Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure FIGREF11. Therefore, although pretraining data size clearly influences AST performance, this appears to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness.", "Results for the two classification data sets (Figure FIGREF18) show very similar patterns. In both the ASR and the AST models, the pretraining data seems to make little difference to phonetic encoding at the early layers, and classification accuracy peaks at the second CNN layer. However, the RNN layers show a clear trend where phone classification accuracy drops off more slowly for models with better ASR/AST performance (i.e., zh $>$ fr $>$ pt). That is, the later RNN layers more transparently encode language-universal phonetic information.", "A number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario BIBREF8, BIBREF3, BIBREF5 or to pre-train parts of the model before fine-tuning on the end-to-end AST task BIBREF3. Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option. In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. BIBREF4 showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian BIBREF9 got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.", "Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result. In related work on multilingual pretraining for low-resource ASR, Adams et al. BIBREF10 showed that pre-training on more languages helps, but it is not clear whether the improvement is due to including more languages, or just more data."]}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "predicted_answer": "", "predicted_evidence": ["For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.", "For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set.", "This paper explored what factors help pretraining for low-resource AST. We performed careful comparisons to tease apart the effects of language relatedness and data size, ultimately finding that rather than either of these, the WER of the pre-trained ASR model is likely the best direct predictor of AST performance. Given equivalent amounts of data, we did not find multilingual pretraining to help more than monolingual pretraining, but we did find an added benefit from using speed perturbation to augment the AST data. Finally, analysis of the pretrained models suggests that those models with better WER are transparently encoding more language-universal phonetic information in the later RNN layers, and this appears to help with AST.", "Table TABREF16 (top) shows how data augmentation affects the results of the baseline 20h AST system, as well as three of the best-performing pretrained models from Table TABREF7. For these experiments only, we changed the learning rates of the augmented-data systems so that all models took about the same amount of time to train (see Figure FIGREF17). Despite a more aggressive learning schedule, the performance of the augmented-data systems surpasses that of the baseline and pretrained models, even those trained on the largest ASR sets (150-hr Chinese and 300-hr English).", "AST results for our pre-trained models are given in Table TABREF7. Pretraining improves AST performance in every case, with improvements ranging from 0.2 (pt-gp) to 4.3 (zh-ai-large). These results make it clear that language relatedness does not play a strong role in predicting AST improvements, since on the similar-sized GlobalPhone datasets, the two languages most related to Spanish (French and Portuguese) yield the highest and lowest improvements, respectively. Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these\u20144.3 BLEU points. This is nearly as much as the 6 point improvement reported by BIBREF4 when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words."]}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "predicted_answer": "", "predicted_evidence": ["To establish a baseline, in the first experiment, we trained a simple attention based seq-to-seq model. All the seq-to-seq networks in our experiments were trained using the Adam optimizer. We evaluate all models on both clean and far-field test sets.", "In this paper, Wasserstein GAN (WGAN) BIBREF16 is used. Following the notations of WGAN, we parametrize the seq-to-seq and discriminator models with $\\theta $ and $w$ respectively. The overall architecture depicted in Figure 1 remains the same, but the encoder distance in ( 2 ) is now replaced with the dual of Earth-Mover (EM) distance", "We evaluated the enhancer framework on the Wall Street Journal (WSJ) corpus with simulated far-field effects. The dev93 and eval92 sets were used for hyperparameter selection and evaluation respectively. The reverberant speech is generated with room impulse response (RIR) augmentation as in BIBREF18 , where each audio is convolved with a randomly chosen RIR signal. The clean and far-field audio durations are kept the same with valid convolution so that the encoder distance enhancer can be applied. We collected 1088 impulse responses, using a linear array of 8 microphones, 120 and 192 of which were held out for development and evaluation. The speaker was placed in a variety of configurations, ranging from 1 to 3 meters distance and 60 to 120 degrees inclination with respect to the array, for 20 different rooms. Mel spectrograms of 20 ms samples with 10 ms stride and 40 bins were used as input features to all of our baseline and enhancer models.", "$$\\max _{w\\in \\mathcal {W}}\n\\left\\lbrace \n\\mathbb {E}_{x}\n\\left[f_w(g_\\theta (x))\\right] -\n\\mathbb {E}_{\\widetilde{x},\\varepsilon }\n\\left[f_w(g_\\theta (\\widetilde{x} + \\varepsilon )\\right]\n\\right\\rbrace .$$   (Eq. 5)", "We use Algorithm \"EXPERIMENTAL SETUP\" to train the WGAN enhancer. The clipping parameter was 0.05 and $\\varepsilon $ was random normal with 0.001 standard deviation. We found that having a schedule for $n_\\text{critic}$ was crucial. Namely, we do not update the encoder parameters with WGAN gradients for the first 3000 steps. Then, we use the normal $n_\\text{critic}=5$ . We hypothesize that the initial encoder embedding is of poor quality and encouraging invariance at this stage through the critic gradients significantly hinders seq-to-seq training."]}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "predicted_answer": "", "predicted_evidence": ["\"\\(PP \\(TO\"", "Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets.", "Hypertension, asthma, and rhinosinusitis guidelines and gold standard datasets were applied to evaluate our model. Since two of these annotated corpora are new, our model is establishing a baseline. The asthma corpus was investigated previously by BIBREF4 .", "We propose a supervised machine learning model classifying sentences as to whether they express a condition or not. After we determine a sentence contain a condition, we use natural language processing and information extraction tools to extract conditions and resulting activities.", "Clinical decision-support system (CDSS) is any computer system intended to provide decision support for healthcare professionals, and using clinical data or knowledge BIBREF0 . The classic problem of diagnosis is only one of the clinical decision problems. Deciding which questions to ask, tests to order, procedures to perform, treatment to indicate, or which alternative medical care to try, are other examples of clinical decisions. CDSSs generally fall into two categories BIBREF0"]}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "predicted_answer": "", "predicted_evidence": ["We define the task as classification task. Given an input statement, classify it to one of the three categories: NC (no condition) if the statement doesn\u2019t have a condition; CA if the statement is a condition-action sentence; and CC (condition-consequence) if the statement has a condition which has a non-action consequence. For a CDSS, to determine both \"what is true\" about a patient and \"what to do\" with a patient, CC and CA statements can be merged to one category.", "Research on CIGs started about 20 years ago and became more popular in the late-1990s and early 2000s. Different approaches have been developed to represent and execute clinical guidelines over patient-specific clinical data. They include document-centric models, decision trees and probabilistic models, and \"Task-Network Models\"(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing networks of clinical actions and decisions that unfold over time. Serban et. al BIBREF6 developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Kaiser et. al BIBREF7 developed a method to identify activities to be performed during a treatment which are described in a guideline document. They used relations of the UMLS Semantic Network BIBREF8 to identify these activities in a guideline document. Wenzina and Kaiser BIBREF4 developed a rule-based method to automatically identifying conditional activities in guideline documents.They achieved a recall of 75% and a precision of 88% on chapter 4 of asthma guidelines which was mentioned before.", "\"\\((SBAR|PP) \\(IN\"", "Medical guidelines contain many condition-action statements. Condition-action statements provide information about expected process flow. If a guideline-based CDSS could extract and formalize these statements, it could help practitioners in the decision-making process. For example, it could help automatically asses the relationship between therapies, guidelines and outcomes, and in particular could help the impact of changing guidelines.", "We will briefly discuss the modeling and annotation of condition-action for medical usage in this section. Our corpus and method of identifying conditions in clinical guidelines is explained in section 3."]}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "predicted_answer": "", "predicted_evidence": ["For each sentence, we extracted POS tags, sequences of 3 POS tags, and combination of all POS tags of candidate conditions as features. For example, \"PP IN NP NP NNS PP IN NP NN PPINNP INNPNP NPNPNNS NPNNSPP NNSPPIN PPINNP INNPNN PPINNPNPNNSPPINNPNN PP IN NP NN PPINNP INNPNN PPINNPNN PP IN NP JJ NN NNS PPINNP INNPJJ NPJJNN JJNNNNS PPINNPJJNNNNS\" represents \"In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?\" in our model. Note that the glued together part of speech tags are not a formatting error but features automatically derived by our model (from consecutive part of speech tags).", "\"\\(PP \\(TO\"", "\"\\((SBAR|PP) \\(IN\"", "We extracted part of speech (POS) tags as our features for our model. Each candidate sentence has at least one candidate condition part. We extract these parts by regular expressions. Each part of sentence which starts with below patterns is a candidate condition part:", "Research on CIGs started about 20 years ago and became more popular in the late-1990s and early 2000s. Different approaches have been developed to represent and execute clinical guidelines over patient-specific clinical data. They include document-centric models, decision trees and probabilistic models, and \"Task-Network Models\"(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing networks of clinical actions and decisions that unfold over time. Serban et. al BIBREF6 developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Kaiser et. al BIBREF7 developed a method to identify activities to be performed during a treatment which are described in a guideline document. They used relations of the UMLS Semantic Network BIBREF8 to identify these activities in a guideline document."]}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "predicted_answer": "", "predicted_evidence": ["With the help of a domain expert, we annotated three sets of guidelines to create gold standards to measure the performance of our condition-action extracting models. The sets of guidelines are: hypertension BIBREF1 , chapter4 of asthma BIBREF2 , and rhinosinusitis BIBREF3 . Chapter 4 of asthma guidelines was selected for comparison with prior work of Wenzina and Kaiser BIBREF4 . We have annotated the guidelines for the conditions, consequences, modifiers of conditions, and type of consequences. These annotate sets of guidelines are available for experiments https://www.dropbox.com/.", "Most of the questions physicians need to consult about with CDSSs are from the latter category. Medical guidelines (also known as clinical guidelines, clinical protocols or clinical practice guidelines) are most useful at the point of care and answering to \"what to do\" questions.", "The annotation of the guidelines text (the next step), focused on determining whether there were condition statements in the candidate sentences or not. The instruction to the annotators were to try to paraphrase candidate sentences as sentences with \"if condition, then consequence\". If the transformed/paraphrased sentence conveyed the same meaning as the original, we considered to be a condition-consequence sentence. Then we we could annotate condition and consequence parts. For example, we paraphrased \"Beta-blockers, including eye drops, are contraindicated in patients with asthma\" to \"If patients have asthma, then beta-blockers, including eye drops, are contraindicated\". The paraphrased sentence conveys same meaning. So it became a condition-consequence sentence in our dataset.", "\"\\(PP \\(TO\"", "For example, \"(ROOT (S (PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension))))) (, ,) (VP (VBZ does) (S (VP (VBG initiating) (S (NP (NP (JJ antihypertensive) (JJ pharmacologic) (NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))))) (. ?)))\" is the constituent parsed tree of \"In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?\". \"(PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension)))))\" and \"(PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))\" are two candidate condition parts in this example."]}
{"question_id": "97708d93bccc832ea671dc31a76dad6a121fcd60", "predicted_answer": "", "predicted_evidence": ["To assess the reliability of ratings, we calculated the intra-class correlation coefficient (ICC), which measures inter-observer reliability on ordinal data for more than two raters BIBREF30 . The overall ICC across all three datasets is 0.45 ( $p<0.001$ ), which corresponds to a moderate agreement. In general, we find consistent differences in inter-annotator agreement per system and dataset, with lower agreements for lols than for rnnlg and TGen. Agreement is highest for the SFHotel dataset, followed by SFRest and Bagel (details provided in supplementary material).", "$\\bullet $ Report results on two different domains and three different datasets, which allows us to draw more general conclusions.", "As shown in previous sections, word-based metrics moderately agree with humans on bad quality output, but cannot distinguish output of good or medium quality. Table 5 provides examples from our three systems. Again, we observe different behaviour between WOMs and sim scores. In Example 1, lols generates a grammatically correct English sentence, which represents the meaning of the MR well, and, as a result, this utterance received high human ratings (median = 6) for informativeness, naturalness and quality. However, WOMs rate this utterance low, i.e. scores of bleu1-4, nist, lepor, cider, rouge and meteor normalised into the 1-6 range all stay below 1.5. This is because the system-generated utterance has low overlap with the human/corpus references.", "We find that utterances with low human ratings of informativeness and naturalness correlate significantly better ( $p<0.05$ ) with automatic metrics than those with average and good human ratings. For example, as shown in Figure 3 , the correlation between WBMs and human ratings for utterances with low informativeness scores ranges between $0.3 \\le \\rho \\le 0.5$ (moderate correlation), while the highest correlation for utterances of average and high informativeness barely reaches $\\rho \\le 0.2$ (very weak correlation). The same pattern can be observed for correlations with quality and naturalness ratings.", "We find that utterances with low human ratings of informativeness and naturalness correlate significantly better ( $p<0.05$ ) with automatic metrics than those with average and good human ratings. For example, as shown in Figure 3 , the correlation between WBMs and human ratings for utterances with low informativeness scores ranges between $0.3 \\le \\rho \\le 0.5$ (moderate correlation), while the highest correlation for utterances of average and high informativeness barely reaches $\\rho \\le 0.2$ (very weak correlation). The same pattern can be observed for correlations with quality and naturalness ratings. This discrepancy in correlation results between low and other user ratings, together with the fact that the majority of system outputs are rated \u201cgood\" for informativeness (79%), naturalness (64%) and quality (58%), whereas low ratings do not exceed 7% in total, could explain why the overall correlations are low (Section \"Relation of Human and Automatic Metrics\" ) despite the observed trends in relationship between average system-level performance scores (Section \"System Evaluation\" )."]}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "predicted_answer": "", "predicted_evidence": ["Table 6 summarises results published by previous studies in related fields which investigate the relation between human scores and automatic metrics. These studies mainly considered WBMs, while we are the first study to consider GBMs. Some studies ask users to provide separate ratings for surface realisation (e.g. asking about `clarity' or `fluency'), whereas other studies focus only on sentence planning (e.g. `accuracy', `adequacy', or `correctness'). In general, correlations reported by previous work range from weak to strong. The results confirm that metrics can be reliable indicators at system-level BIBREF4 , while they perform less reliably at sentence-level BIBREF2 . Also, the results show that the metrics capture realization better than sentence planning. There is a general trend showing that best-performing metrics tend to be the more complex ones, combining word-overlap, semantic similarity and term frequency weighting. Note, however, that the majority of previous works do not report whether any of the metric correlations are significantly different from each other.", "Finally, note that the datasets considered in this study are fairly small (between 404 and 2.3k human references per domain). To remedy this, systems train on de-lexicalised versions BIBREF10 , which bears the danger of ungrammatical lexicalisation BIBREF13 and a possible overlap between testing and training set BIBREF15 . There are ongoing efforts to release larger and more diverse data sets, e.g. BIBREF16 , BIBREF45 .", "Quality of Data: Our corpora contain crowdsourced human references that have grammatical errors, e.g. \u201cFifth Floor does not allow childs\u201d (SFRest reference). Corpus-based methods may pick up these errors, and word-based metrics will rate these system utterances as correct, whereas we can expect human judges to be sensitive to ungrammatical utterances. Note that the parsing score (while being a crude approximation of grammaticality) achieves one of our highest correlation results against human ratings, with $|\\rho |=.31$ . Grammatical errors raise questions about the quality of the training data, especially when being crowdsourced. For example, BIBREF3 find that human experts assign low rankings to their original corpus text. Again, weighting BIBREF37 or reference-less approaches BIBREF33 might remedy this issue.", "[htp] max width=1", "Word-based metrics make two strong assumptions: They treat human-generated references as a gold standard, which is correct and complete. We argue that these assumptions are invalid for corpus-based NLG, especially when using crowdsourced datasets. Grammar-based metrics, on the other hand, do not rely on human-generated references and are not influenced by their quality. However, these metrics can be easily manipulated with grammatically correct and easily readable output that is unrelated to the input. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient."]}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "predicted_answer": "", "predicted_evidence": ["Table TABREF9 shows the Rouge scores measured in our experiments.", "We implement the selection of top-ranking features for both the original and modified models slightly differently to BIBREF7 : all words in the vocabulary are ranked by their value in the centroid vector. On a development dataset, a parameter is tuned that defines the proportion of the ranked vocabulary that is represented in the centroid vector and the rest is set to zero. This variant resulted in more stable behavior for different amounts of input documents.", "Extractive multi-document summarization (MDS) aims to summarize a collection of documents by selecting a small number of sentences that represent the original content appropriately. Typical objectives for assembling a summary include information coverage and non-redundancy. A wide variety of methods have been introduced to approach MDS.", "Both the global optimization and the sentence preselection have a positive impact on the performance.", "For testing, we use the DUC2004 Task 2 dataset from the Document Understanding Conference (DUC). The dataset consists of 50 document clusters containing 10 documents each. For tuning hyperparameters, we use the CNN/Daily Mail dataset BIBREF8 which provides summary bulletpoints for individual news articles. In order to adapt the dataset for MDS, 50 CNN articles were randomly selected as documents to initialize 50 clusters. For each of these seed articles, 9 articles with the highest word-overlap in the first 3 sentences were added to that cluster. This resulted in 50 documents clusters, each containing 10 topically related articles. The reference summaries for each cluster were created by interleaving the sentences of the article summaries until a length contraint (100 words) was reached."]}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "predicted_answer": "", "predicted_evidence": ["This baseline can easily be adapted to work at the summary-level instead the sentence level. This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection. A simple greedy algorithm is used to find the best summary under a length constraint.", "Table TABREF9 shows the Rouge scores measured in our experiments.", "The sentences are ranked separately in each document by their cosine similarity to the centroid vector, in decreasing order. The INLINEFORM0 best sentences of each document are selected as candidates.", "In order to keep the method efficient, we outline different methods to select a small number of candidate sentences from each document in the input collection before constructing the summary.", "BIBREF7 implement this original model with the following modifications:"]}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "predicted_answer": "", "predicted_evidence": ["Extractive multi-document summarization (MDS) aims to summarize a collection of documents by selecting a small number of sentences that represent the original content appropriately. Typical objectives for assembling a summary include information coverage and non-redundancy. A wide variety of methods have been introduced to approach MDS.", "The modified methods can also be used as strong baselines for future experiments in multi-document summarization.", "The modified sentence selection method is less efficient than the orginal method since at each iteration the score of a possible summary has to be computed for all remaining candidate sentences. It may not be noticeable for a small number of input sentences. However, it would have an impact if the amount of input documents was larger, e.g. for the summarization of top-100 search results in document retrieval.", "In this paper we show that simple modifications to the centroid-based method can bring its performance to the same level as state-of-the-art methods on the DUC2004 dataset. The resulting summarization methods are unsupervised, efficient and do not require complicated feature engineering or training.", "This model, which includes the anti-redundancy filter and the selection of top-ranking features, is treated as the \"original\" centroid-based model in this paper."]}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "predicted_answer": "", "predicted_evidence": ["The presented methods for restricting the input to a maximum of INLINEFORM0 sentences per document lead to additional improvements while reducing computation effort, if global optimization is being used. These methods could be useful for other summarization models that rely on pairwise similarity computations between all input sentences, or other properties which would slow down summarization of large numbers of input sentences.", "The modified methods can also be used as strong baselines for future experiments in multi-document summarization.", "Therefore, we explore different methods for reducing the number of input sentences before applying the greedy sentence selection algorithm to make the model more suited for larger inputs. It is also important to examine how this affects Rouge scores.", "The sentences are ranked separately in each document by their cosine similarity to the centroid vector, in decreasing order. The INLINEFORM0 best sentences of each document are selected as candidates.", "In this paper we show that simple modifications to the centroid-based method can bring its performance to the same level as state-of-the-art methods on the DUC2004 dataset. The resulting summarization methods are unsupervised, efficient and do not require complicated feature engineering or training."]}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "predicted_answer": "", "predicted_evidence": ["Tag questions - We built a list of tag questions (e.g.,, \u201cdidn't you?\u201d, \u201caren't we?\u201d) from a grammar site and use them as binary indicators.", "We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.", "Reddit: BIBREF10 (2018) introduced an extensive collection of sarcastic and non-sarcastic posts collected from different subreddits. In Reddit, authors mark their sarcastic intent of their posts by adding \u201c/s\u201d at the end of a post/comment. We collected 50K instances from the corpus for our experiments (denoted as INLINEFORM0 ), where the sarcastic and non-sarcastic replies are at least two sentences (i.e., we discard posts that are too short). For brevity, we denote ironic utterances as INLINEFORM1 and non-ironic utterances as INLINEFORM2 . Both INLINEFORM3 and INLINEFORM4 datasets are balanced between the INLINEFORM5 and INLINEFORM6 classes. We uuse 80% of the datasets for training, 10% for development, and the remaining 10% for testing.", "Rhetorical Questions - Rhetorical Questions (for brevity INLINEFORM0 ) have the structure of a question but are not typical information seeking questions. We follow the hypothesis introduced by BIBREF15 (2017) that questions that are in the middle of a comment are more likely to be RQ since since questions followed by text cannot be typical information seeking questions. Presence of INLINEFORM1 is used as a binary feature.", "Tropes are figurative use of expressions."]}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "predicted_answer": "", "predicted_evidence": ["Emoticon - Emoticons are frequently used to emphasize the ironic intent of the user. In the example \u201cI love the weather ;) #irony\u201d, the emoticon \u201c;)\u201d (wink) alerts the reader to a possible ironic interpretation of weather (i.e., bad weather). We collected a comprehensive list of emoticons (over one-hundred) from Wikipedia and also used standard regular expressions to identify emoticons in our datasets. Beside using the emoticons directly as binary features, we use their sentiment as features as well (e.g., \u201cwink\u201d is regarded as positive sentiment in MPQA).", "Hashtag - Particularly in INLINEFORM0 , hashtags often represent the sentiment of the author. For example, in the ironic tweet \u201cnice to wake up to cute text. #suck\u201d, the hashtag \u201c#suck\u201d depicts the negative sentiment. We use binary sentiment feature (positive or negative) to identify the sentiment of the hashtag, while comparing against the MPQA sentiment lexicon. Often multiple words are combined in a hashtag without spacing (e.g., \u201cfun\u201d and \u201cnight\u201d in #funnight). We use an off-the-shelf tool to split words in such hashtags and then checked the sentiment of the words.", "In this paper, we examine the role of irony markers in social media for irony recognition. Although punctuations, capitalization, and hyperboles are previously used as features in irony detection BIBREF6 , BIBREF7 , here we thoroughly analyze a set of theoretically-grounded types of irony markers, such as tropes (e.g., metaphors), morpho-syntactic indicators (e.g., tag questions), and typographic markers (e.g., emoji) and their use in ironic utterances. Consider the following two irony examples from INLINEFORM0 and INLINEFORM1 given in Table TABREF2 .", "Finally, we collected another set of irony posts from BIBREF10 , but this time we collected posts from specific topical subreddits. We collected irony posts about politics (e.g., subreddits: politics, hillary, the_donald), sports (e.g., subreddits: nba, football, soccer), religion (e.g., subreddits: religion) and technology (e.g., subreddits: technology). Table TABREF27 presents the mean and SD for each genre. We observe that users use tropes such as hyperbole and INLINEFORM0 , morpho-syntactic markers such as exclamation and interjections and multiple-punctuations more in politics and religion than in technology and sports. This is expected since subreddits regarding politics and religion are often more controversial than technology and sports and the users might want to stress that they are ironic or sarcastic using the markers.", "Rhetorical Questions - Rhetorical Questions (for brevity INLINEFORM0 ) have the structure of a question but are not typical information seeking questions. We follow the hypothesis introduced by BIBREF15 (2017) that questions that are in the middle of a comment are more likely to be RQ since since questions followed by text cannot be typical information seeking questions. Presence of INLINEFORM1 is used as a binary feature."]}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "predicted_answer": "", "predicted_evidence": ["Emoticon - Emoticons are frequently used to emphasize the ironic intent of the user. In the example \u201cI love the weather ;) #irony\u201d, the emoticon \u201c;)\u201d (wink) alerts the reader to a possible ironic interpretation of weather (i.e., bad weather). We collected a comprehensive list of emoticons (over one-hundred) from Wikipedia and also used standard regular expressions to identify emoticons in our datasets. Beside using the emoticons directly as binary features, we use their sentiment as features as well (e.g., \u201cwink\u201d is regarded as positive sentiment in MPQA).", "Interjections - Interjections seem to undermine a literal evaluation and occur frequently in ironic utterances (e.g., \u201c`yeah\", `wow\u201d, \u201cyay\u201d,\u201couch\u201d etc.). Similar to tag questions we assembled interjections (a total of 250) from different grammar sites.", "Twitter: We use a set of 350K tweets for our experiments. The ironic/sarcastic tweets are collected using hashtags, such as #irony, #sarcasm, and #sarcastic whereas the non-sarcastic tweets do not contain these hashtags, but they might include sentiment hashtags, such as #happy, #love, #sad, #hate (similar to BIBREF8 , BIBREF9 ). As pre-processing, we removed the retweets, spam, duplicates, and tweets written in languages other than English. Also, we deleted all tweets where the hashtags of interest were not located at the very end (i.e., we eliminated \u201c#sarcasm is something that I love\u201d). We lowercased the tweets, except the words where all the characters are uppercased.", "Rhetorical Questions - Rhetorical Questions (for brevity INLINEFORM0 ) have the structure of a question but are not typical information seeking questions. We follow the hypothesis introduced by BIBREF15 (2017) that questions that are in the middle of a comment are more likely to be RQ since since questions followed by text cannot be typical information seeking questions. Presence of INLINEFORM1 is used as a binary feature.", "Both utterances are labeled as ironic by their authors (using hashtags in INLINEFORM0 and the /s marker in INLINEFORM1 ). In the INLINEFORM2 example, the author uses several irony markers such as Rhetorical question (e.g., \u201care you telling\u201d ...) and metaphor (e.g., \u201cgolden age\u201d). In the INLINEFORM3 example, we notice the use of capitalization (\u201cAWESOME\u201d) and emoticons (\u201c:P\u201d (tongue out)) that the author uses to alert the readers that it is an ironic tweet."]}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "predicted_answer": "", "predicted_evidence": ["This is primarily due to the removal of hyperboles that frequently appear in ironic utterances in INLINEFORM2 . Removing typographic markers (e.g., emojis, emoticons, etc.) have the maximum negative effect on the Precision for the irony INLINEFORM3 category, since particular emojis and emoticons appear regularly in ironic utterances (Table TABREF25 ). For INLINEFORM4 , Table TABREF24 shows that removal of typographic markers such as emoticons does not affect the F1 scores, whereas the removal of morpho-syntactic markers, e.g., tag questions, interjections have a negative effect on the F1. Table TABREF25 and Table TABREF26 represent the INLINEFORM5 most discriminative features for both categories based on the feature weights learned during the SVM training for INLINEFORM6 and INLINEFORM7 , respectively. Table TABREF25 shows that for INLINEFORM8 , typographic features such as emojis and emoticons have the highest feature weights for both categories.", "Both utterances are labeled as ironic by their authors (using hashtags in INLINEFORM0 and the /s marker in INLINEFORM1 ). In the INLINEFORM2 example, the author uses several irony markers such as Rhetorical question (e.g., \u201care you telling\u201d ...) and metaphor (e.g., \u201cgolden age\u201d). In the INLINEFORM3 example, we notice the use of capitalization (\u201cAWESOME\u201d) and emoticons (\u201c:P\u201d (tongue out)) that the author uses to alert the readers that it is an ironic tweet.", "We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.", "Metaphors - Metaphors often facilitate ironic representation and are used as markers. We have drawn metaphors from different sources (e.g., 884 and 8,600 adjective/noun metaphors from BIBREF11 and BIBREF12 , respectively, and used them as binary features. We also evaluate the metaphor detector BIBREF13 over INLINEFORM0 and INLINEFORM1 datasets. We considered metaphor candidates that have precision INLINEFORM2 0.75 (see BIBREF13 (2017)).", "With the advent of social media, irony and sarcasm detection has become an active area of research in Natural Language Processing (NLP) BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Most computational studies have focused on building state-of-the-art models to detect whether an utterance or comment is ironic/sarcastic or not, sometimes without theoretical grounding. In linguistics and discourse studies, BIBREF4 (2000) and later BIBREF5 (2010) have studied two theoretical aspects of irony in the text: irony factors' and irony markers. Irony factors are characteristics of ironic utterances that cannot be removed without destroying the irony. In contrast, irony markers are a meta-communicative clue that \u201calert the reader to the fact that a sentence is ironical\u201d BIBREF4 . They can be removed and the utterance is still ironic."]}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "predicted_answer": "", "predicted_evidence": ["We also report performance of state-of-the-art systems in each of these languages, all of which make explicit use of syntactic features, marcheggiani2017lstm excepted. While this results in better performance on many languages, our model has the advantage of not relying on a syntactic parser, and is hence more applicable to languages with lower resources. However, the results suggest that syntactic information is critical for strong performance on German, which has the fewest predicates and thus the least semantic annotation for a semantics-only model to learn from. Nevertheless, our baseline is on par with the best published scores for Chinese, and it shows strong performance on most languages.", "This third variant takes inspiration from the \u201cfrustratingly easy\u201d architecture of daumeiii2007easy for domain adaptation. In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language. Each of these language-specific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to the third layer. This adds a greater degree of language-specific processing while still sharing representations across languages. It also uses the language identification vector and multilingual word vectors in the input.", "For argument labeling, every token in the sentence is assigned one of the argument labels, or INLINEFORM0 if the model predicts it is not an argument to the indicated predicate.", "We use the hidden representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He2017-deepsrl, who did not handle predicate senses, and of marcheggiani2017lstm, who used a separate model. These two predictions are made independently, with separate softmaxes over different last-layer parameters; we then combine the losses for each task when training. For predicate sense disambiguation, since the predicate has been identified, we choose from a small set of valid predicate senses as the tag for that token. This set of possible senses is selected based on the training data: we map from lemmatized tokens to predicates and from predicates to the set of all senses of that predicate.", "The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks."]}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "predicted_answer": "", "predicted_evidence": ["This third variant takes inspiration from the \u201cfrustratingly easy\u201d architecture of daumeiii2007easy for domain adaptation. In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language. Each of these language-specific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to the third layer. This adds a greater degree of language-specific processing while still sharing representations across languages. It also uses the language identification vector and multilingual word vectors in the input.", "In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.", "Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined. Hence, we use separate label representations for each language's labels. Similarly, while (for example) eng:look and spa:mira may be semantically connected, the senses look.01 and mira.01 may not correspond. Hence, predicate sense representations are also language-specific.", "For argument labeling, every token in the sentence is assigned one of the argument labels, or INLINEFORM0 if the model predicts it is not an argument to the indicated predicate.", "We use the hidden representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He2017-deepsrl, who did not handle predicate senses, and of marcheggiani2017lstm, who used a separate model. These two predictions are made independently, with separate softmaxes over different last-layer parameters; we then combine the losses for each task when training. For predicate sense disambiguation, since the predicate has been identified, we choose from a small set of valid predicate senses as the tag for that token. This set of possible senses is selected based on the training data: we map from lemmatized tokens to predicates and from predicates to the set of all senses of that predicate."]}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "predicted_answer": "", "predicted_evidence": ["We present our results in Table TABREF11 . We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure FIGREF10 ).", "Our basic model adapts the span-based dependency SRL model of He2017-deepsrl. This adaptation treats the dependent arguments as argument spans of length 1. Additionally, BIO consistency constraints are removed from the original model\u2014 each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by marcheggiani2017lstm.", "The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 .", "The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-of-speech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these. Each training instance consists of the annotations for a single predicate. These representations are then passed through a deep, multi-layer bidirectional LSTM BIBREF4 , BIBREF5 with highway connections BIBREF6 .", "The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels."]}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "predicted_answer": "", "predicted_evidence": ["We also report performance of state-of-the-art systems in each of these languages, all of which make explicit use of syntactic features, marcheggiani2017lstm excepted. While this results in better performance on many languages, our model has the advantage of not relying on a syntactic parser, and is hence more applicable to languages with lower resources. However, the results suggest that syntactic information is critical for strong performance on German, which has the fewest predicates and thus the least semantic annotation for a semantics-only model to learn from. Nevertheless, our baseline is on par with the best published scores for Chinese, and it shows strong performance on most languages.", "The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 .", "Our basic model adapts the span-based dependency SRL model of He2017-deepsrl. This adaptation treats the dependent arguments as argument spans of length 1. Additionally, BIO consistency constraints are removed from the original model\u2014 each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by marcheggiani2017lstm.", "In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.", "We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table TABREF3 ."]}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "predicted_answer": "", "predicted_evidence": ["Concretely, we compared our session segmentation with fixed-length context, used in BIBREF11 . That is to say, the competing method always regards two previous utterances as context. We hired three workers to annotate the results with three integer scores (0\u20132 points, indicating bad, borderline, and good replies, respectively.) We sampled 30 queries from the test set of 100 sessions. For each query, we retrieved 10 candidates and computed p@1 and nDCG scores BIBREF33 (averaged over three annotators). Provided with previous utterances as context, each worker had up to 1000 sentences to read during annotation.", "Human-computer dialogue systems can be roughly divided into several categories. Template- and rule-based systems are mainly designed for certain domains BIBREF4 , BIBREF5 , BIBREF13 . Although manually engineered templates can also be applied in the open domain like BIBREF14 , but their generated sentences are subject to 7 predefined forms, and hence are highly restricted. Retrieval methods search for a candidate reply from a large conversation corpus given a user-issued utterance as a query BIBREF7 . Generative methods can synthesize new replies by statistical machine translation BIBREF15 , BIBREF16 or neural networks BIBREF8 .", "In open-domain conversations, context information (one or a few previous utterances) is particularly important to language understanding BIBREF1 , BIBREF9 , BIBREF10 , BIBREF11 . As dialogue sentences are usually casual and short, a single utterance (e.g., \u201cThank you.\u201d in Figure FIGREF2 ) does not convey much meaning, but its previous utterance (\u201c...writing an essay\u201d) provides useful background information of the conversation. Using such context will certainly benefit the conversation system.", "In our study, we prefer the simple yet effective TextTiling approach for open-domain dialogue session segmentation, but enhance it with modern advances of word embeddings, which are robust in capturing semantics of words. We propose a tailored algorithm for word embedding learning by combining a query and context as a \u201cvirtual document\u201d; we also propose several heuristics for similarity measuring.", "TextTiling w/ tf INLINEFORM0 idf features. We implemented TextTiling ourselves according to BIBREF12 ."]}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "predicted_answer": "", "predicted_evidence": ["We also notice that heuristic-avg is worse than other similarity measures. As this method is mathematically equivalent to the average of word-by-word similarity, it may have an undesirable blurring effect.", "MMD. We applied the MinMax-Dotplotting (MMD) approach proposed by Ye et al. BIBREF24 . We ran the executable program provided by the authors.", "In this paper, we propose the concept of virtual sentences to learn word embeddings for conversation data. We concatenate a query INLINEFORM0 and its reply INLINEFORM1 as a virtual sentence INLINEFORM2 . We also use all words (other than the current one) in the virtual sentence as context (Figure 2). Formally, the context INLINEFORM3 of the word INLINEFORM4 is given by DISPLAYFORM0", "We thank anonymous reviewers for useful comments and Jingbo Zhu for sharing the MMD executable program. This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61272343 and 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032), and the National Basic Research Program (973 Program No. 2014CB340405).", "For each word INLINEFORM0 in INLINEFORM1 , our intuition is to find the most related word in INLINEFORM2 , given by the INLINEFORM3 part; their relatedness is also defined by the cosine measure. Then the sentence-level similarity is obtained by the average similarity score of words in INLINEFORM4 . This method is denoted as heuristic-max."]}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_answer": "", "predicted_evidence": ["We apply a TextTiling-like algorithm for session segmentation. The original TextTiling is proposed by Hearst BIBREF12 . The main idea is to measure the similarity of each adjacent sentence pair; then \u201cvalleys\u201d of similarities are detected for segmentation.", "In open-domain conversations, context information (one or a few previous utterances) is particularly important to language understanding BIBREF1 , BIBREF9 , BIBREF10 , BIBREF11 . As dialogue sentences are usually casual and short, a single utterance (e.g., \u201cThank you.\u201d in Figure FIGREF2 ) does not convey much meaning, but its previous utterance (\u201c...writing an essay\u201d) provides useful background information of the conversation. Using such context will certainly benefit the conversation system.", "We used hierarchical softmax to approximate the probability.", "Human-computer dialog/conversation is one of the most challenging problems in artificial intelligence. Given a user-issued utterance (called a query in this paper), the computer needs to provide a reply to the query. In early years, researchers have developed various domain-oriented dialogue systems, which are typically based on rules or templates BIBREF4 , BIBREF5 , BIBREF6 . Recently, open-domain conversation systems have attracted more and more attention in both academia and industry (e.g., XiaoBing from Microsoft and DuMi from Baidu). Due to high diversity, we can hardly design rules or templates in the open domain. Researchers have proposed information retrieval methods BIBREF7 and modern generative neural networks BIBREF8 , BIBREF9 to either search for a reply from a large conversation corpus or generate a new sentence as the reply.", "We further conducted in-depth analysis of different strategies of training word-embeddings and matching heuristics in Table TABREF21 . For word embeddings, we trained them on the 3M-sentence dataset with three strategies: (1) virtual-sentence context proposed in our paper; (2) within-sentence context, where all words (except the current one) within a sentence (either a query or reply) are regarded as the context; (3) window-based context, which is the original form of BIBREF25 : the context is the words in a window (previous 2 words and future 2 words in the sentence). We observe that our virtual-sentence strategy consistently outperforms the other two in all three matching heuristics. The results suggest that combining a query and a reply does provide more information in learning dialogue-specific word embeddings."]}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "predicted_answer": "", "predicted_evidence": ["$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$   (Eq. 8)", "[t] seed question set $S$ candidate questions $E$ $E \\leftarrow S$ $Q \\leftarrow S$ $I \\leftarrow 0$ len $(Q) > 0$ and $I < I_{max}$ $I = I + 1$ $q_{cur}$ $\\leftarrow $ $E$0 .Pop() $E$1 in WebExp $E$2 not $E$3 .contains $E$4 $E$5 .Append( $E$6 ) $E$7 .Push( $E$8 ) Question expansion method", "We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical.", "We propose a system for generating questions from KB that significantly reduces the human effort by leveraging the massive web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g. performsActivity $\\Rightarrow $ how to) and placeholders for the subject (#X#) and the object (#Y#). A seed question set is then generated by applying the templates on the KB. The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate.", "Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work BIBREF9 , BIBREF10 relies on massive human-labeled data. Treating question generation as a machine translation problem, serban-EtAl:2016:P16-1 train a neural machine translation (NMT) system with 10,000 $\\langle $ triple, question $\\rangle $ pairs. At test time, input triples are \u201ctranslated\u201d into questions with the NMT system. On the other hand, the question part of the 10,000 pairs are human generated, which requires a large amount of human effort. In addition, the grammaticality and naturalness of generated questions can not be guaranteed (as seen in Table 1 )."]}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "predicted_answer": "", "predicted_evidence": ["We propose a system for generating questions from KB that significantly reduces the human effort by leveraging the massive web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g. performsActivity $\\Rightarrow $ how to) and placeholders for the subject (#X#) and the object (#Y#). A seed question set is then generated by applying the templates on the KB. The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate.", "Shown in Figure 1 , our system contains the sub-modules of question template construction, seed question generation, question expansion and selection. Given an input KB, a small set of question templates is first constructed such that each template is associated with a predicate, then a seed question set is generated by applying the template set on the input KB, before finally more questions are generated from related questions that are iteratively retrieved from a search engine with already-obtained questions as search queries (section \"Experiments\" ). Taking our in-house KB of power tool domain as an example, template \u201chow to use #X#\u201d is first constructed for predicate \u201cperformsActivity\u201d. In addition, seed question \u201chow to use jigsaw\u201d is generated by applying the template on triple \u201c $\\langle $ jigsaw, performsActivity, CurveCut $\\rangle $ \u201d, before finally questions (Figure 2 ) are retrieved from Google with the seed question.", "Question generation is important as questions are useful for student assessment or coaching purposes in educational or professional contexts, and a large-scale corpus of question and answer pairs is also critical to many NLP tasks including question answering, dialogue interaction and intelligent tutoring systems. There has been much literature so far BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 studying question generation from text. Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase BIBREF7 and DBPedia BIBREF8 , are freely available, and entities and their relations are already present in KBs but not for texts.", "We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d.", "The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with serban-EtAl:2016:P16-1 on 500 random selected triples from Freebase BIBREF7 . Evaluated by 3 human graders, questions generated by our system are significantly better then serban-EtAl:2016:P16-1 on grammaticality and naturalness."]}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "predicted_answer": "", "predicted_evidence": ["where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:", "We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d.", "We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs.", "We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical.", "We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset."]}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "predicted_answer": "", "predicted_evidence": ["Shown in Algorithm \"Experiments\" , the expanded question set $E$ is initialized as the seed question set (Line 1). In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to $E$ if $E$ does not contain them (Lines 6-10). As there may be a large number of questions generated in the loop, we limit the maximum number of iterations with $I_{max}$ (Line 4).", "Shown in Table 1 , we compare our questions with serban-EtAl:2016:P16-1 where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from serban-EtAl:2016:P16-1 are either ungrammatical (such as \u201cwho was someone who was involved in the leukemia ?\u201d and \u201cwhats the title of a book of the subject of the bible ?\u201d), unnatural (\u201cwhat 's one of the mountain where can you found in argentina in netflix ?\u201d) or confusing (\u201cwho was someone who was involved in the leukemia ?\u201d).", "We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset.", "$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$   (Eq. 7)", "where $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds."]}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "predicted_answer": "", "predicted_evidence": ["We first compute perplexities for each individual question and then group the question instances according to the interviewee's gender class. Throughout we use the Mann-Whitney $U$ statistical significance test, unless otherwise noted.", "Figure 1 shows that a gender bias with respect to whether game-related language is used exists for both typical and atypical questions. However, additional analysis reveals that the difference in mean perplexity values between genders is highly statistically significantly larger for atypical questions, suggesting that gender bias is more salient among the more unusual queries.", "There has been an increasing level of attention to and discussion of gender bias in sports, ranging from differences in pay and prize money to different levels of focus on off-court topics in interviews by journalists. With respect to the latter, Cover the Athlete, an initiative that urges the media to focus on sport performance, suggests that female athletes tend to get more \u201csexist commentary\" and \u201cinappropriate interview questions\" than males do; the organization put out an attention-getting video in 2015 purportedly showing male athletes' awkward reactions to receiving questions like those asked of female athletes. However, it is not universally acknowledged that female athletes attract more attention for off-court activities. For instance, a manual analysis by BIBREF0 [ BIBREF0 ] of online articles revealed significantly more descriptors associated with the physical appearance and personal lives of male basketball players in comparison to female ones.", "Below are some sample questions of low-perplexity and high-perplexity values:", "clay, challenger(s), tie, sets, practiced, tiebreaker, maybe, see, impression, serve, history, volley, chance, height, support, shots, server(s), greatest, way, tiebreaks, tiebreakers, era, lucky, luck;"]}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "predicted_answer": "", "predicted_evidence": ["where $h$ is a hidden state, $z$ is a message from the other agent, $u$ is a distribution over actions, and $x$ is an observation of the world. A single hidden layer with 256 units and a $\\tanh $ nonlinearity is used for the MLP. The GRU hidden state is also of size 256, and the message vector is of size 64.", "Proposition 1", "The translation criterion in the previous section makes no reference to listener actions at all. The shapes example in sec:philosophy shows that some model performance might be lost under translation. It is thus reasonable to ask whether this translation model of sec:models can make any guarantees about the effect of translation on behavior. In this section we explore the relationship between belief-preserving translations and the behaviors they produce, by examining the effect of belief accuracy and strategy mismatch on the reward obtained by cooperating agents.", "We use the version of the XKCD dataset prepared by McMahan15Colors. Here the input feature vector is simply the LAB representation of each color, and the message inventory taken to be all unigrams that appear at least five times.", "Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network. DCPs have been shown to solve a variety of coordination problems, including reference games BIBREF0 , logic puzzles BIBREF1 , and simple control BIBREF2 . Appealingly, the agents' communication protocol can be learned via direct backpropagation through the communication channel, avoiding many of the challenging inference problems associated with learning in classical decentralized decision processes BIBREF3 ."]}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "predicted_answer": "", "predicted_evidence": ["Here we review some related public datasets and show the gap that Fluent Speech Commands fills.", "This previous work has all been conducted on datasets that are closed-source or too small to test hypotheses about the amount of data required to generalize well. The lack of a good open-source dataset for end-to-end SLU experiments makes it difficult for most people to perform high-quality, reproducible research on this topic. We therefore created a new SLU dataset, the \u201cFluent Speech Commands\u201d dataset, which Fluent.ai releases along with this paper.", "Each audio is labeled with three slots: action, object, and location. A slot takes on one of multiple values: for instance, the \u201clocation\u201d slot can take on the values \u201cnone\u201d, \u201ckitchen\u201d, \u201cbedroom\u201d, or \u201cwashroom\u201d. We refer to the combination of slot values as the intent of the utterance. The dataset has 31 unique intents in total. We do not distinguish between domain, intent, and slot prediction, as is sometimes done in SLU BIBREF25 .", "The Grabo, Domotica, and Patcor datasets are three related datasets of spoken commands for robot control and card games developed by KU Leuven and used in BIBREF8 . These datasets are free, but have only a small number of speakers and phrases.", "ATIS is an SLU dataset consisting of utterances related to travel planning. This dataset can only be obtained expensively from the Linguistic Data Consortium."]}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "predicted_answer": "", "predicted_evidence": ["Negotiations, either between individuals or entities, are ubiquitous in everyday human interactions ranging from sales to legal proceedings. Being a good negotiator is a complex skill, requiring the ability to understand the partner's motives, ability to reason and to communicate effectively, making it a challenging task for an automated system. While research in building automatically negotiating agents has primarily focused on agent-agent negotiations BIBREF0, BIBREF1, there is a recent interest in agent-human negotiations BIBREF2 as well. Such agents may act as mediators or can be helpful for pedagogical purposes BIBREF3.", "We focus on buyer-seller negotiations BIBREF4 where two individuals negotiate the price of a given product. Leveraging the recent advancements BIBREF7, BIBREF8 in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner (Figure FIGREF3). Early prediction of outcomes is essential for effective planning of an automatically negotiating agent. Although there have been attempts to gain insights into negotiations BIBREF9, BIBREF10, to the best of our knowledge, we are the first to study early natural language cues through a data-driven neural system (Section SECREF3). Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well.", "", "Defining the problem: Say we are provided with a product scenario $S$, a tuple: (Category, Title, Listing Price, Target Price). Define the interactions between a buyer and seller using a sequence of $n$ events $E_n:<e_{1}, e_{2}, ..., e_{n}>$, where $e_{i}$ occurs before $e_{j}$ iff $i<j$. Event $e_{i}$ is also a tuple: (Initiator, Type, Data). Initiator is either the Buyer or Seller, Type can be one of (message, offer, accept, reject or quit) and Data consists of either the corresponding natural language dialogue, offer price or can be empty. Nearly $80\\%$ of events in CB dataset are of type `message', each consisting a textual message as Data. An offer is usually made and accepted at the end of each negotiation. Since the offers directly contain the agreed price (which we want to predict), we only consider `message' events in our models.", "We compare two variants for BERT-based models. First, for the BERT method, we keep only the first [CLS] token in the input and then train the model with fine-tuning using a single feed-forward network on top of the [CLS] representation. Secondly, we call our complete approach as BERT+GRU, where we use a recurrent network with BERT fine-tuning, as depicted in Figure FIGREF3."]}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "predicted_answer": "", "predicted_evidence": ["The prices discussed during the negotiation still play a crucial role in making the predictions. In fact, in only $65\\%$ of the negotiations, the first price is quoted within the first $0.4$ fraction of the events. This is visible in higher performance as more events are seen after this point. This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Price-only model for these categories over others. The models also show evidence of capturing buyer interest. By constructing artificial negotiations, we observe that the model predictions at $f$=$0.2$ increase when the buyer shows more interest in the product, indicating more willingness to pay. With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middle-ground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning BIBREF6, BIBREF4.", "We presented a framework to attempt early predictions of the agreed product prices in buyer-seller negotiations. We construct sentence templates to encode the product scenario, exchanged messages and associated auxiliary information into the same hidden space. By combining a recurrent network and the pre-trained BERT encoder, our model leverages natural language cues in the exchanged messages to predict the negotiation outcomes early on in the conversation. With this capability, such a framework can be used in a feedback mechanism to guide the planning of a negotiating agent.", "", "Prices-only, which does not incorporate any knowledge from natural language, fails to beat the average baseline even with $60\\%$ of the negotiation history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on. Incorporating natural language in both the scenario and event messages paves the way to leverage such cues and make better predictions early on in the conversation, as depicted in the plots. Both BERT and BERT-GRU consistently perform well on the complete test set. There is no clear winner, although using a recurrent network proves to be more helpful in the early stages of the negotiation. Note that BERT method still employs multiple [SEP] tokens along with alternating segment embeddings (Section SECREF3). Without this usage, the fine-tuning pipeline proves to be inadequate.", "We present our results in Figure FIGREF6. We also show Accuracy$\\pm 10$ for different product categories in the Appendix. First, Target Price (TP) and (TP+LP)/2 prove to be strong baselines, with the latter achieving $61.07\\%$ Accuracy$\\pm 10$. This performance is also attested by relatively strong numbers on the other metrics as well. Prices-only, which does not incorporate any knowledge from natural language, fails to beat the average baseline even with $60\\%$ of the negotiation history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on."]}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "predicted_answer": "", "predicted_evidence": ["__START__ mr. speaker , i yield 2 minutes to the gentlewoman from california ( mrs. capps ) pointed out , after the knowledge was available and was continued to pursue the use of this compound as an additive to the fuels of our automobiles . those communities now are stuck with the costs of either cleaning up that drinking water supply , finding an alternative source and dealing with it , and they must do so . to suggest now that we are going to be giving to seniors , to keep them in nursing homes with alzheimer's and with parkinson's disease , just keep cutting it . give more tax breaks to the richest one-tenth of 1 percent . they call it the death tax . i think that is a flaw in the bill . that leads to the second point . the bill specifically mentions weight gain and obesity . well , i think most of us have a sense of what obesity is .", "i am so proud of the fact that every iraqi school child on the opening day of school had received a book bag with the seal of the u.s. , pencils , pads , all kinds of things , free of charge . i had just come back from iraq , and they had been there on the first day of this new congress , the republican majority is publicly demonstrating what has been evident for some time , and that is its arrogance , its pettiness , its shortsighted focus on their political life rather than to decide how we are each of us fit to govern . here is the thing . we have this rules package before us . they did some flash last night so that the press is saying , oh , they blinked . they did blink on a couple of different scores , but the fundamental challenge to the ethical standard of the house being enforced is still in this rules package are destructive , and they are unethical . mr. speaker , i reserve the balance of my time .", "i am so proud of the fact that every iraqi school child on the opening day of school had received a book bag with the seal of the u.s. , pencils , pads , all kinds of things , free of charge . i had just come back from iraq , and they had been there on the first day of this new congress , the republican majority is publicly demonstrating what has been evident for some time , and that is its arrogance , its pettiness , its shortsighted focus on their political life rather than to decide how we are each of us fit to govern . here is the thing . we have this rules package before us . they did some flash last night so that the press is saying , oh , they blinked . they did blink on a couple of different scores , but the fundamental challenge to the ethical standard of the house being enforced is still in this rules package are destructive , and they are unethical . mr.", "this bill also cuts job search assistance through the employment service by 11 percent and cut state unemployment insurance and employment service offices are cut $ 245 million eliminating help for 1.9 million people . this bill is no better for those attending college full-time . despite the fact that college costs have increased by $ 3 , 095 , 34 percent , since 2001 . consumers are expected to pay 52 percent more for natural gas , 30 percent more for home heating oil , you are expected to pay three times as much as you did 4 years ago , the first year president bush took office . winter is around the corner , and so are skyrocketing increases in home heating costs . families who heat with natural gas could see their fuel costs increase more than 70 percent in some parts of the country . this honorable response to the tragedy of september 11 puts to shame what has been proposed today in the wake of hurricane katrina , that the workers in the afflicted area who are trying to put that area back together are not even going to be allowed to get a decent prevailing wage that they would otherwise be guaranteed under davis-bacon .", "for example , i mean probably all of us have had a mom or a grandmom or an uncle to whom we say , hey , i noticed your legs are swelling again . fluid retention . fluid retention . now , that can be from a variety of causes . that is not from increased caloric intake . that could have been , for example , from a food additive , maybe a cause that was not known to the public of some kind of additive in something that they had eaten or drank . it may have been something that interfered with one of their medications and led to fluid retention . i am just making up hypotheticals here . or , the hypothetical , perhaps you have something that is actually a heart poison from some food additive that has no calories in it , zero calories in it , but over a period of time does bad things to the ability of under this bill , which i believe is absolutely essential for our health system . at a time when our country has been severely impacted by natural disasters , it is extremely urgent that congress maintain csbg funding at its current level so that the delivery of much needed services to low-income people is not disrupted ."]}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "predicted_answer": "", "predicted_evidence": ["yet the bill cuts $ 437 million out of training and employment services . that is the lowest level of adult training grants in a decade . this bill also cuts the community college initiative , the president's initiative for community colleges , an effort to train workers for high-skill , high-paying jobs . it cuts that effort by INLINEFORM0 125 million from funds provided last year , denying the help that the president was talking about giving to 100 , 000 americans of a continued education to help them get a new job . this bill also cuts job search assistance through the employment service by 11 percent and cut state unemployment insurance and employment service offices are cut $ 245 million eliminating help for 1.9 million people . this bill is no better for those attending college full-time . despite the fact that college costs have increased by $ 3 , 095 , 34 percent , since 2001 . consumers are expected to pay 52 percent more for natural gas , 30 percent more for home heating oil , you are expected to pay three times as much as you did 4 years ago , the first year president bush took office .", "i am just making up hypotheticals here . or , the hypothetical , perhaps you have something that is actually a heart poison from some food additive that has no calories in it , zero calories in it , but over a period of time does bad things to the ability of under this bill , which i believe is absolutely essential for our health system . at a time when our country has been severely impacted by natural disasters , it is extremely urgent that congress maintain csbg funding at its current level so that the delivery of much needed services to low-income people is not disrupted . we have a responsibility to protect our environment \u2013 as well as the diverse forms of life that share it . the bipartisan substitute will help us achieve the goal . i urge my colleagues on both sides of the aisle to protect the benefits that our constituents earned and deserve and to prevent the increase in the number of frivolous filings . __END__", "those communities now are stuck with the costs of either cleaning up that drinking water supply , finding an alternative source and dealing with it , and they must do so . to suggest now that we are going to be giving to seniors , to keep them in nursing homes with alzheimer's and with parkinson's disease , just keep cutting it . give more tax breaks to the richest one-tenth of 1 percent . they call it the death tax . i think that is a flaw in the bill . that leads to the second point . the bill specifically mentions weight gain and obesity . well , i think most of us have a sense of what obesity is . weight gain is a whole different issue , and weight gain may occur not from obesity , not from getting fat , not from putting on too many calories ; weight gain can occur for a variety of medical reasons related to a variety of different causes . for example , i mean probably all of us have had a mom or a grandmom or an uncle to whom we say , hey , i noticed your legs are swelling again .", "__START__ mr. speaker , i yield 2 minutes to the gentlewoman from california ( mrs. capps ) pointed out , after the knowledge was available and was continued to pursue the use of this compound as an additive to the fuels of our automobiles . those communities now are stuck with the costs of either cleaning up that drinking water supply , finding an alternative source and dealing with it , and they must do so . to suggest now that we are going to be giving to seniors , to keep them in nursing homes with alzheimer's and with parkinson's disease , just keep cutting it . give more tax breaks to the richest one-tenth of 1 percent . they call it the death tax . i think that is a flaw in the bill . that leads to the second point . the bill specifically mentions weight gain and obesity . well , i think most of us have a sense of what obesity is .", "fluid retention . now , that can be from a variety of causes . that is not from increased caloric intake . that could have been , for example , from a food additive , maybe a cause that was not known to the public of some kind of additive in something that they had eaten or drank . it may have been something that interfered with one of their medications and led to fluid retention . i am just making up hypotheticals here . or , the hypothetical , perhaps you have something that is actually a heart poison from some food additive that has no calories in it , zero calories in it , but over a period of time does bad things to the ability of under this bill , which i believe is absolutely essential for our health system . at a time when our country has been severely impacted by natural disasters , it is extremely urgent that congress maintain csbg funding at its current level so that the delivery of much needed services to low-income people is not disrupted . we have a responsibility to protect our environment \u2013 as well as the diverse forms of life that share it ."]}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "predicted_answer": "", "predicted_evidence": ["it is time for congress , as part of the national marine sanctuary program , but there have been no hearings on this bill or any other bill to protect our oceans . let us reject this unnecessary task force and get down to some real work . mr. speaker , i reserve the balance of my time to the gentleman from maryland ( mr. cardin ) , who is the ranking member , was part and parcel of that , as well as the gentleman from virginia ( chairman tom davis ) is trying to do to improve the integrity of driver's licenses , but i find it interesting that the state of utah , while the gentleman from utah ( mr. bishop ) is arguing that they are not getting enough money for education , the state of utah legislature passed measures saying they do not want any kind of investigation of themselves . the republicans control the white house , they control the senate , and they control the house of representatives . mr.", "and what about creating a circus atmosphere that drains resources from this congress do you not understand . shamefully , the house will not have an opportunity to vote on the hastings-menendez independent katrina commission legislation , because republicans have blocked us from offering it . just as they always do , republicans block what they can not defeat . despite what republicans will suggest , today's debate is not about politics . it is about the need for truth to assure the american people that we will not allow their retirement checks to be slashed to pay for private accounts . it is time for congress , as part of the national marine sanctuary program , but there have been no hearings on this bill or any other bill to protect our oceans . let us reject this unnecessary task force and get down to some real work . mr. speaker , i reserve the balance of my time to the gentleman from maryland ( mr. cardin ) , who is the ranking member , was part and parcel of that , as well as the gentleman from virginia ( chairman tom davis ) is trying to do to improve the integrity of driver's licenses , but i find it interesting that the state of utah , while the gentleman from utah ( mr.", "it cuts that effort by INLINEFORM0 125 million from funds provided last year , denying the help that the president was talking about giving to 100 , 000 americans of a continued education to help them get a new job . this bill also cuts job search assistance through the employment service by 11 percent and cut state unemployment insurance and employment service offices are cut $ 245 million eliminating help for 1.9 million people . this bill is no better for those attending college full-time . despite the fact that college costs have increased by $ 3 , 095 , 34 percent , since 2001 . consumers are expected to pay 52 percent more for natural gas , 30 percent more for home heating oil , you are expected to pay three times as much as you did 4 years ago , the first year president bush took office . winter is around the corner , and so are skyrocketing increases in home heating costs . families who heat with natural gas could see their fuel costs increase more than 70 percent in some parts of the country .", "and one of the most significant broken promises is in the area of making higher educational opportunities more available to minority and low-income students . i am so proud of the fact that every iraqi school child on the opening day of school had received a book bag with the seal of the u.s. , pencils , pads , all kinds of things , free of charge . i had just come back from iraq , and they had been there on the first day of this new congress , the republican majority is publicly demonstrating what has been evident for some time , and that is its arrogance , its pettiness , its shortsighted focus on their political life rather than to decide how we are each of us fit to govern . here is the thing . we have this rules package before us . they did some flash last night so that the press is saying , oh , they blinked . they did blink on a couple of different scores , but the fundamental challenge to the ethical standard of the house being enforced is still in this rules package are destructive , and they are unethical .", "let us reject this unnecessary task force and get down to some real work . mr. speaker , i reserve the balance of my time to the gentleman from maryland ( mr. cardin ) , who is the ranking member , was part and parcel of that , as well as the gentleman from virginia ( chairman tom davis ) is trying to do to improve the integrity of driver's licenses , but i find it interesting that the state of utah , while the gentleman from utah ( mr. bishop ) is arguing that they are not getting enough money for education , the state of utah legislature passed measures saying they do not want any kind of investigation of themselves . the republicans control the white house , they control the senate , and they control the house of representatives . mr. speaker , is it possible for us to let this young woman take her leave in peace . __END__"]}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "predicted_answer": "", "predicted_evidence": ["what it also does is minimizes the opportunity of those who can secure their local lawyer to get them into a state court and burdens them with the responsibility of finding some high-priced counsel that they can not afford to buy food . seven million more people , an increase of 12 percent , and what does this combination of reconciliation in order to give tax cuts to people making more than $ 500 , 000 . footnote right there . what about the committees of jurisdiction already in existence in congress . and what about creating a circus atmosphere that drains resources from this congress do you not understand . shamefully , the house will not have an opportunity to vote on the hastings-menendez independent katrina commission legislation , because republicans have blocked us from offering it . just as they always do , republicans block what they can not defeat . despite what republicans will suggest , today's debate is not about politics . it is about the need for truth to assure the american people that we will not allow their retirement checks to be slashed to pay for private accounts .", "In this report we have presented a novel approach of training a system on speech transcripts in order to generate new speeches. We have shown that n-grams and J&K POS tag filter are very effective as language and topic model for this task. We have shown how to combine these models to a system that produces good results. Furthermore, we have presented different methods to evaluate the quality of generated texts. In an experimental evaluation our system performed very well. In particular, the grammatical correctness and the sentence transitions of most speeches were very good. However, there are no comparable systems which would allow a direct comparison.", "For the speech generation task we have also pursued a sentence-based approach in the beginning of this project. The idea of the sentence-based approach is to take whole sentences from the training data and concatenate them in a meaningful way. We start by picking a speech of the desired class at random and take the first sentence of it. This will be the start sentence of our speech. Then we pick 20 speeches at random from the same class. We compare our first sentence with each sentence in those 20 speeches by calculating a similarity measure. The next sentence is than determined by the successor of the sentence with the highest similarity. In case no sentence shows sufficient similarity (similarity score below threshold) we just take the successor of our last sentence. In the next step we pick again 20 speeches at random and compare each sentence with the last one in order to find the most similar sentence. This will be repeated until we come across the speech-termination token or the generated speech reaches a certain length.", "it may have been something that interfered with one of their medications and led to fluid retention . i am just making up hypotheticals here . or , the hypothetical , perhaps you have something that is actually a heart poison from some food additive that has no calories in it , zero calories in it , but over a period of time does bad things to the ability of under this bill , which i believe is absolutely essential for our health system . at a time when our country has been severely impacted by natural disasters , it is extremely urgent that congress maintain csbg funding at its current level so that the delivery of much needed services to low-income people is not disrupted . we have a responsibility to protect our environment \u2013 as well as the diverse forms of life that share it . the bipartisan substitute will help us achieve the goal . i urge my colleagues on both sides of the aisle to protect the benefits that our constituents earned and deserve and to prevent the increase in the number of frivolous filings .", ", pencils , pads , all kinds of things , free of charge . i had just come back from iraq , and they had been there on the first day of this new congress , the republican majority is publicly demonstrating what has been evident for some time , and that is its arrogance , its pettiness , its shortsighted focus on their political life rather than to decide how we are each of us fit to govern . here is the thing . we have this rules package before us . they did some flash last night so that the press is saying , oh , they blinked . they did blink on a couple of different scores , but the fundamental challenge to the ethical standard of the house being enforced is still in this rules package are destructive , and they are unethical . mr. speaker , i reserve the balance of my time . mr. chairman , this bill frightens me . it scares me ."]}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "predicted_answer": "", "predicted_evidence": ["Multi-distillation BIBREF18 combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: BIBREF19 adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher's knowledge with initialization and additional losses leads to substantial gains.", "Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: $L_{ce} = \\sum _i t_i * \\log (s_i)$ where $t_i$ (resp. $s_i$) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following BIBREF8 we used a softmax-temperature: $p_i = \\frac{\\exp (z_i / T)}{\\sum _j \\exp (z_j / T)}$ where $T$ controls the smoothness of the output distribution and $z_i$ is the model score for the class $i$. The same temperature $T$ is applied to the student and the teacher at training time, while at inference, $T$ is set to 1 to recover a standard softmax.", "On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available.", "Task-specific distillation Most of the prior works focus on building task-specific distillation setups. BIBREF15 transfer fine-tune classification model BERT to an LSTM-based classifier. BIBREF16 distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. BIBREF17 use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher's knowledge to pre-train with additional distillation signal.", "The trend toward bigger models raises several concerns. First is the environmental cost of exponentially scaling these models' computational requirements as mentioned in BIBREF3, BIBREF4. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption."]}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "predicted_answer": "", "predicted_evidence": ["We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 86.9 F1 and 79.1 EM, i.e. within 2 points of the full model.", "General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark BIBREF10, a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (BIBREF11) encoder followed by two BiLSTMs.", "Size and inference speed", "On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available.", "In this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models. We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices."]}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "predicted_answer": "", "predicted_evidence": ["Recently, deep learning has been used for encoding acoustic information into vectors BIBREF8 , BIBREF9 , BIBREF10 . Existing works have shown that it is possible to transform audio word segments into fixed dimensional vectors. The transformation successfully produces vector space where word audio segments with similar phonetic structures are closely located. In BIBREF10 , the authors train a Siamese convolutional neural network with side information to obtain embeddings that separate same-word pairs and different-word pairs. Human annotated data is required under this supervised learning scenario. Besides supervised approaches BIBREF11 , BIBREF10 , BIBREF12 , BIBREF13 , unsupervised approaches are also proposed to reduce the annotation effort BIBREF14 . As for the unsupervised learning for the audio embedding, LSTM-based sequence-to-sequence autoencoder demonstrates a promising result BIBREF14 .", "In order to further investigate the performance of INLINEFORM0 , we visualize the vector representation of two sets of word pairs differing by only one phoneme from French and German as below:", "The goal for Audio Word2Vec model is to identify the phonetic patterns in acoustic feature sequences such as MFCCs. Given a sequence INLINEFORM0 where INLINEFORM1 is the acoustic feature at time INLINEFORM2 , and INLINEFORM3 is the length, Audio Word2Vec transforms the features into fixed-length vector INLINEFORM4 with dimension INLINEFORM5 based on the phonetic structure.", "Existing works have shown that it is possible to transform audio word segments into fixed dimensional vectors. The transformation successfully produces vector space where word audio segments with similar phonetic structures are closely located. In BIBREF10 , the authors train a Siamese convolutional neural network with side information to obtain embeddings that separate same-word pairs and different-word pairs. Human annotated data is required under this supervised learning scenario. Besides supervised approaches BIBREF11 , BIBREF10 , BIBREF12 , BIBREF13 , unsupervised approaches are also proposed to reduce the annotation effort BIBREF14 . As for the unsupervised learning for the audio embedding, LSTM-based sequence-to-sequence autoencoder demonstrates a promising result BIBREF14 . The model is trained to minimize the reconstruction error of the input audio sequence and then provides the embedding, namely Audio Word2Vec, from its bottleneck layer. This is done without any annotation effort.", "INLINEFORM1 consists of an Encoder RNN (the left part of Figure FIGREF3 ) and a RNN Decoder (the right part). Given an audio segment represented as an acoustic feature sequence INLINEFORM2 of any length INLINEFORM3 , the RNN Encoder reads each acoustic feature INLINEFORM4 sequentially and the hidden state INLINEFORM5 is updated accordingly. After the last acoustic feature INLINEFORM6 has been read and processed, the hidden state INLINEFORM7 of the Encoder RNN is viewed as the learned representation INLINEFORM8 of the input sequence (the purple block in Figure FIGREF3 ). The Decoder RNN takes INLINEFORM9 as the initial state of the RNN cell, and generates a output INLINEFORM10 . Instead of taking INLINEFORM11 as the input of the next time step, a zero vector is fed in as input to generate INLINEFORM12 , and so on. This structure is called the historyless decoder."]}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF3 depicts the structure of Sequence-to-sequence Autoencoder ( INLINEFORM0 ), which integrates the RNN Encoder-Decoder framework with Autoencoder for unsupervised learning of audio segment representations. INLINEFORM1 consists of an Encoder RNN (the left part of Figure FIGREF3 ) and a RNN Decoder (the right part). Given an audio segment represented as an acoustic feature sequence INLINEFORM2 of any length INLINEFORM3 , the RNN Encoder reads each acoustic feature INLINEFORM4 sequentially and the hidden state INLINEFORM5 is updated accordingly. After the last acoustic feature INLINEFORM6 has been read and processed, the hidden state INLINEFORM7 of the Encoder RNN is viewed as the learned representation INLINEFORM8 of the input sequence (the purple block in Figure FIGREF3 ). The Decoder RNN takes INLINEFORM9 as the initial state of the RNN cell, and generates a output INLINEFORM10 . Instead of taking INLINEFORM11 as the input of the next time step, a zero vector is fed in as input to generate INLINEFORM12 , and so on.", "INLINEFORM1 consists of an Encoder RNN (the left part of Figure FIGREF3 ) and a RNN Decoder (the right part). Given an audio segment represented as an acoustic feature sequence INLINEFORM2 of any length INLINEFORM3 , the RNN Encoder reads each acoustic feature INLINEFORM4 sequentially and the hidden state INLINEFORM5 is updated accordingly. After the last acoustic feature INLINEFORM6 has been read and processed, the hidden state INLINEFORM7 of the Encoder RNN is viewed as the learned representation INLINEFORM8 of the input sequence (the purple block in Figure FIGREF3 ). The Decoder RNN takes INLINEFORM9 as the initial state of the RNN cell, and generates a output INLINEFORM10 . Instead of taking INLINEFORM11 as the input of the next time step, a zero vector is fed in as input to generate INLINEFORM12 , and so on. This structure is called the historyless decoder. Based on the principles of Autoencoder BIBREF33 , BIBREF34 , the target of the output sequence INLINEFORM13 is the input sequence INLINEFORM14 .", "The goal for Audio Word2Vec model is to identify the phonetic patterns in acoustic feature sequences such as MFCCs. Given a sequence INLINEFORM0 where INLINEFORM1 is the acoustic feature at time INLINEFORM2 , and INLINEFORM3 is the length, Audio Word2Vec transforms the features into fixed-length vector INLINEFORM4 with dimension INLINEFORM5 based on the phonetic structure.", "The Decoder RNN takes INLINEFORM9 as the initial state of the RNN cell, and generates a output INLINEFORM10 . Instead of taking INLINEFORM11 as the input of the next time step, a zero vector is fed in as input to generate INLINEFORM12 , and so on. This structure is called the historyless decoder. Based on the principles of Autoencoder BIBREF33 , BIBREF34 , the target of the output sequence INLINEFORM13 is the input sequence INLINEFORM14 . In other words, the RNN Encoder and Decoder are jointly trained by minimizing the reconstruction error, measured by the general mean squared error INLINEFORM15 . Because the input sequence is taken as the learning target, the training process does not need any labeled data. The fixed-length vector representation INLINEFORM16 will be a meaningful representation for the input audio segment INLINEFORM17 because the whole input sequence INLINEFORM18 can be reconstructed from INLINEFORM19 by the RNN Decoder.", "For audio, all languages are uttered by human beings with a similar vocal tract structure, and therefore share some common acoustic patterns. This fact implies that knowledge obtained from one spoken language can be transferred onto other languages. This paper verifies that sequence-to-sequence autoencoder is not only able to transform audio word segments into fixed-length vectors, the model is also transferable to the languages it has never heard before. We also demonstrate its promising applications with a query-by-example spoken term detection (STD) experiment. In the query-by-example STD experiment, even without tunning with partial low-resource language segments, the autoencoder can still produce high-quality vectors."]}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "predicted_answer": "", "predicted_evidence": ["In this section, we first examine how changing the hidden layer size of the RNN Encoder/Decoder, the dimension of Audio Word2Vec, affects the MAP performance of query-by-example STD (Section 6.1). After obtaining the best hidden layer size, we analyze the transferability of the Audio Word2Vec by comparing the cosine similarity of the learned representations to phoneme sequence edit distance (Section 6.2) . Visualization of multiple word pairs in different target languages is also provided (Section 6.3). Last but not least, we performed the query-by-example STD on target languages (Section 6.4). These experiments together verify that INLINEFORM0 is capable of extracting common phonetic structure in human language and thus is transferable to various languages.", "(0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages.", "In the proposed approach, we first train an INLINEFORM0 using the high-resource source language, as shown in the upper part of Fig. FIGREF4 , and then the encoder is used to transform the audio segment of a low-resource target language. It is also possible to fine-tune the parameters of INLINEFORM1 with the target language. In the following experiments, we found that in some cases the STD performance of the encoder without fine-tuning with the low-resource target language can be as good as the one with fine-tuning.", "Before evaluating the language transfer result, we first experimented on the primary INLINEFORM0 model in the source language (English). The results are shown in Fig. FIGREF12 . Here we compare the representations of INLINEFORM1 and INLINEFORM2 . Furthermore, we examined the influence of the dimension of Audio Word2Vector in terms of MAP. We also compared the MAP results on large testing database (250K segments) and small database (20K).", "The Decoder RNN takes INLINEFORM9 as the initial state of the RNN cell, and generates a output INLINEFORM10 . Instead of taking INLINEFORM11 as the input of the next time step, a zero vector is fed in as input to generate INLINEFORM12 , and so on. This structure is called the historyless decoder. Based on the principles of Autoencoder BIBREF33 , BIBREF34 , the target of the output sequence INLINEFORM13 is the input sequence INLINEFORM14 . In other words, the RNN Encoder and Decoder are jointly trained by minimizing the reconstruction error, measured by the general mean squared error INLINEFORM15 . Because the input sequence is taken as the learning target, the training process does not need any labeled data. The fixed-length vector representation INLINEFORM16 will be a meaningful representation for the input audio segment INLINEFORM17 because the whole input sequence INLINEFORM18 can be reconstructed from INLINEFORM19 by the RNN Decoder."]}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "predicted_answer": "", "predicted_evidence": ["BIBREF13 is related to our research because they also used a bootstrapping method to discover offensive language from a large-scale Twitter corpus. However, their bootstrapping model is driven by mining hateful Twitter users, instead of content analysis of tweets as in our approach. Furthermore, they recognize hateful Twitter users by detecting explicit hateful indicators (i.e., keywords) in their tweets while our bootstrapping system aim to detect both explicit and implicit expressions of online hate speech.", "The first section of Table 2 shows the performance of the two supervised models when applied to 62 million tweets collected around election time. We can see that the logistic regression model suffers from an extremely low precision, which is less than 10%. While this classifier aggressively labeled a large number of tweets as hateful, only 121,512 tweets are estimated to be truly hateful. In contrast, the supervised LSTM classifier has a high precision of around 79%, however, this classifier is too conservative and only labeled a small set of tweets as hateful.", "Our work focuses on the need to capture both explicit and implicit hate speech from an unbiased corpus. To address these issues, we proposed a weakly supervised two-path bootstrapping model to identify hateful language in randomly sampled tweets. Starting from 20 seed rules, we found 210 thousand hateful tweets from 62 million tweets collected during the election. Our analysis shows a strong correlation between temporal distributions of hateful tweets and the election time, as well as the partisan impetus behind large amounts of inflammatory language. In the future, we will look into linguistic phenomena that often occur in hate speech, such as sarcasm and humor, to further improve hate speech detection performance.", "We randomly sampled 10 million tweets from 67 million tweets collected from Oct. 1st to Oct. 24th using Twitter API. These 10 million tweets were used as the unlabeled tweet set in bootstrapping learning. Then we continued to collect 62 million tweets spanning from Oct.25th to Nov.15th, essentially two weeks before the US election day and one week after the election. The 62 million tweets will be used to evaluate the performance of the bootstrapped slur term learner and LSTM classifier. The timestamps of all these tweets are converted into EST. By using Twitter API, the collected tweets were randomly sampled to prevent a bias in the data set.", "First, we train a traditional feature-based classification model using logistic regression (LR). We apply the same set of features as mentioned in BIBREF0 . The features include character-level bigrams, trigrams, and four-grams."]}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "predicted_answer": "", "predicted_evidence": ["We use Pytorch 1.2.0 as the backend framework, Scikit-learn 0.20.3 for SVM and dataset splits, and gensim 3.8.1 for Doc2Vec model.", "During the pre-processing stage where the documents are split into chunks, we utilise a cluster of Azure Virtual Machines with 32 GB RAM and 16 cores, which are optimised for CPU usage. A similar cluster is used during the hyperparameter optimisation, however, with 112 GB RAM. Reading from the remote disk (Azure Blob Storage) is rather time-consuming, since the corpus comprises lengthy documents. Thus, to accelerate the training, we chose nodes with abundant memory in order to load everything in memory just once (required roughly one hour for that process).", "The majority of researchers evaluate their document classifying models on the following four datasets: Reuters-21578 BIBREF8, ArXiv Academic Paper Dataset - AAPD BIBREF9, IMDB reviews BIBREF10, and Yelp 2014 reviews BIBREF11. However, these commonly used datasets do not contain large documents, which conflicts with one of the main objectives of this study. Note that our definition of `document' in this specific context is a document that has at least 5000 words.", "For the optimisation of the BiLSTM with attention model, we use Adam optimiser with a learning rate of 0.001, batch size of 1,000 and distinct values for each one of the other hyperparameters. Analogously, the SVM classifier consists of the Radial Basis Function (RBF) as the kernel function and a different value of gamma and the penalty parameter for each different model. The intention of the distinct values used for each model is to optimise each model separately so as to enable them to reach their best performance.", "We evaluate the proposed model on a document classification dataset; 70% of the data is used for the training and the remaining 30% is equally divided and used for tuning and testing our model."]}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "predicted_answer": "", "predicted_evidence": ["One potential extension of this work would be to apply powerful yet computationally expensive pre-processing techniques to the various documents. Techniques such as Named Entity Recognition (NER) could enable the training of the whole corpus in Doc2Vec by removing the undesired noise. Furthermore, the projections of the document embeddings at the end of our pipeline are shown to have clearly defined boundaries and thus they can be valuable for different NLP tasks, such as estimating document similarities. In the legal industry, this can contribute to identifying usages of legal templates and clauses.", "Almost all documents of type \u201c10-K\u201d begin with lines that contain identical headings. In order to enable the machine to truly comprehend why a document of type \u201c10-K\u201d should be categorised to that filing type, we remove the first six lines where the identical text is located. The model is then able to focus on finding common features that exist in documents of the same filing type, rather than focusing on just capturing the few sentences that are the same in almost all of the documents of type \u201c10-K\u201d. A similar procedure is followed with the documents of type \u201c10-Q\u201d.", "A more thorough investigation of the test accuracy scores indicate that documents of type \u201cEX-99.1\" are the ones that get misclassified the most, whereas the remaining four types of documents are in general classified correctly at a considerably higher rate. As confusion matrix plot in Figure FIGREF18 highlights, there are cases that documents of type \u201cEX-10.1\" are misclassified as \u201cEX-99.1\", however, the reverse occurs more frequently. Further exploration of documents of type \u201cEX-99.1\" reveals that these documents often contain homogeneous agreements or clauses with the ones embodied in documents of type \u201cEX-10.1\".", "For the optimisation of the BiLSTM with attention model, we use Adam optimiser with a learning rate of 0.001, batch size of 1,000 and distinct values for each one of the other hyperparameters. Analogously, the SVM classifier consists of the Radial Basis Function (RBF) as the kernel function and a different value of gamma and the penalty parameter for each different model. The intention of the distinct values used for each model is to optimise each model separately so as to enable them to reach their best performance.", "where $W\\in R^{c\\times d}$ is the weight matrix, $c$ and $d$ are the number of classes and the number of dimensions of the hidden states respectively and $b\\in R^d$ is the bias term. Hence, the final vector $s_{i}$ is a c-dimension vector comprising the probability of that document belonging to each class."]}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "predicted_answer": "", "predicted_evidence": ["We apply convolution between INLINEFORM0 and several filters (convolution kernels). For each filter INLINEFORM1 , we apply a nonlinear activation function INLINEFORM2 and a max-pooling on the output to obtain a feature vector. Let INLINEFORM3 be the stride, INLINEFORM4 be the window, INLINEFORM5 be the hidden weight of a filter, respectively. The feature vector of INLINEFORM6 obtained by INLINEFORM7 is given by: DISPLAYFORM0", "A Long Short-term Memory (LSTM) BIBREF9 Unit is a kind of unit for RNN that keeps information from long range context. We use a bi-directional RNN of LSTM to encode the document feature from the sequence of the word features.", "In the earliest works, people limited the size of the vocabulary, which is not able to exploit the potential generalization ability on the rare words BIBREF15 . It has made people explore alternative methods for the softmax function to efficiently train all the words, e.g., hierarchical softmax BIBREF16 , noise-contrastive estimation BIBREF17 and negative sampling BIBREF18 . However, the temporal complexity of the softmax function is not the only thing suffering the high-dimension vocabulary. Scalable word vocabulary leads to a large embedding layer, hence huge neural network with millions of parameters, which costs quite a few gigabytes to store. BIBREF19 proposed a convolutional neural network (CNN) that takes characters as the input for text classification and outperforms the previous models for large datasets. They showed the character-level CNNs are effective for text classification without the need for words. BIBREF1 introduced a recurrent neural network (RNN) language model that takes character embeddings encoded by convolutional layers as the input.", "However, the temporal complexity of the softmax function is not the only thing suffering the high-dimension vocabulary. Scalable word vocabulary leads to a large embedding layer, hence huge neural network with millions of parameters, which costs quite a few gigabytes to store. BIBREF19 proposed a convolutional neural network (CNN) that takes characters as the input for text classification and outperforms the previous models for large datasets. They showed the character-level CNNs are effective for text classification without the need for words. BIBREF1 introduced a recurrent neural network (RNN) language model that takes character embeddings encoded by convolutional layers as the input. Their model has much fewer parameters than the models using word embeddings, and reached the performance of the state-of-the-art on English, and outperformed baselines on morphologically rich languages. However, for Chinese and Japanese, the character vocabulary is also large, and the character embeddings are blind to the semantic information of the radicals.", "We observed no significant improvement. Probably for two-class sentiment classification, a full-connected layer with ReLU is not necessary between the CNN encoder and the bi-directional RNN encoder, hence the highway network learned to pass the inputs directly to the outputs all the time."]}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "predicted_answer": "", "predicted_evidence": ["To mitigate this issue, we perform a two-step beam search where the direct model pre-prunes the vocabulary BIBREF9. For beam size $k_1$, and for each beam, we collect $k_2$ possible next word extensions according to the direct model. Next, we score the resulting $k_1 \\times k_2$ partial candidates with the channel model and then prune this set to size $k_1$. Other approaches to pre-pruning may be equally beneficial but we adopt this approach for simplicity. A downside of online decoding with the channel model approach is the high computational overhead: we need to invoke the channel model $k_1 \\times k_2$ times compared to just $k_1$ times for the direct model.", "Next, we evaluate online decoding with a noisy channel setup compared to just a direct model () as well as an ensemble of two direct models (). Table TABREF16 shows that adding a language model to () gives a good improvement BIBREF18 over a single direct model but ensembling two direct models is slightly more effective (). The noisy channel approach () improves by 1.9 BLEU over on news2017 and by 0.9 BLEU over the ensemble. Without per word scores, accuracy drops because the direct model and the channel model are not balanced and their weight shifts throughout decoding. Our simple approach outperforms strong online ensembles which illustrates the advantage over incremental architectures BIBREF9 that do not match vanilla seq2seq models by themselves.", "For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017. For reranking, we train models with a 40K joint byte pair encoding vocabulary (BPE; BIBREF11). To be able to use the language model during online decoding, we use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (Zh-En), we pre-process WMT'17 data following BIBREF12, we develop on dev2017 and test on news2017. For IWSLT'14 De-En we follow the setup of BIBREF13 and measure case-sensitive tokenized BLEU. For WMT De-En, En-De and Zh-En we measure detokenized BLEU BIBREF14.", "For En-De, De-En, Zh-En we use big Transformers and for IWSLT De-En a base Transformer BIBREF3 as implemented in fairseq BIBREF17. For online decoding experiments, we do not share encoder and decoder embeddings since the source and target vocabularies were learned separately. We report average accuracy of three random initializations of a each configuration. We generally use $k_1=5$ and $k_2=10$. We tune $\\lambda _1$, and a length penalty on the validation set.", "Table TABREF20 shows that the noisy channel model outperforms the baseline () by up to 4.0 BLEU for very large beams, the ensemble by up to 2.9 BLEU () and the best right-to-left configuration by 1.4 BLEU (). The channel approach improves more than other methods with larger n-best lists by adding 2.4 BLEU from $k_1=5$ to $k_1=100$. Other methods improve a lot less with larger beams, e.g., has the next largest improvement of 1.4 BLEU when increasing the beam size but this is still significantly lower than for the noisy channel approach. Adding a language model benefits all settings (, , ) but the channel approach benefits most ( vs ). The direct model with a language model () performs better than for online decoding, likely because the constrained re-ranking setup mitigates explaining away effects (cf. Table TABREF16)."]}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "predicted_answer": "", "predicted_evidence": ["Previous work on neural noisy channel modeling relied on a complex latent variable model that incrementally processes source and target prefixes BIBREF9. This trades efficiency for accuracy because their model performs significantly less well than a vanilla sequence to sequence model. For languages with similar word order, it can be sufficient to predict the first target token based on a short source prefix, but for languages where word order differs significantly, we may need to take the entire source sentence into account to make a decision.", "Interestingly, both or give only modest improvements compared to . Although previous work demonstrated that reranking with can improve over , we show that the channel model is important to properly leverage the language model without suffering from explaining away effects BIBREF23, BIBREF24. Test results on all language directions confirm that performs best (Table TABREF21).", "The noisy channel approach applies Bayes' rule to model $p(y|x) = p(x|y) p(y)/ p(x)$, that is, the channel model $p(x|y)$ operating from the target to the source and a language model $p(y)$. We do not model $p(x)$ since it is constant for all $y$. We compute the channel model probabilities as follows:", "Using the channel model in online decoding enables searching a much larger space compared to n-best list re-ranking. However, online decoding is also challenging because the channel model needs to score the entire source sequence given a partial target which can be hard. To measure this, we simulate different target prefix lengths in an n-best list re-ranking setup. The n-best list is generated by the direct model and we re-rank it for different target prefixes of the candidate hypothesis. As in SECREF14, we measure BLEU of the selected full candidate hypothesis. Figure FIGREF19 shows that the channel model enjoys much larger benefits from more target context than re-ranking with just the direct model and an LM () or re-ranking with a direct ensemble (). This experiment shows the importance of large context sizes for the channel approach to work well. It indicates that the channel approach may not be able to effectively exploit the large search space in online decoding due to the limited conditioning context provided by partial target prefixes.", "The noisy channel approach is an alternative which is used in statistical machine translation BIBREF6, BIBREF7. It entails a channel model probability $p(x|y)$ that operates in the reverse direction as well as a language model probability $p(y)$. The language model can be estimated on unpaired data and can take a separate form to the channel model. Noisy channel modeling mitigates explaining away effects that result in the source being ignored for highly likely output prefixes BIBREF8."]}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "predicted_answer": "", "predicted_evidence": ["We plot the averaged performances on varying amounts of training data for each target domain in Figure 3 . Note that the improvements are even higher for the experiments with smaller training data. In particular, ZAT shows an improvement of $14.67$ in absolute F1-score over CRF when training with 500 instances. ZAT achieves an F1-score of 76.04% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 75%. Thus the ZAT model achieves better F1-score with only one-fourth the training data.", "We first describe our approach called Zero-Shot Adaptive Transfer model (ZAT) in detail. We then describe the dataset we used for our experiments. Using this data, we conduct experiments comparing our ZAT model with a set of state-of-the-art models: Bag-of-Expert (BoE) models and their non-expert counterparts BIBREF4 , and the Concept Tagger model BIBREF5 , showing that our model can lead to significant F1-score improvements. This is followed by an in-depth analysis of the results. We then provide a survey of related work and concluding remarks.", "Following BIBREF4 , We use two expert domains containing reusable slots: timex and location. The timex domain consists of utterances containing the slots $date$ , $time$ and $duration$ . The location domain consists of utterances containing $location$ , $location\\_type$ and $place\\_name$ slots. Both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse. Data for these domains was sampled from the input utterances from our commercial digital assistant. Each reusable domain contains about a million utterances. There is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains. The data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains.", "For domain adaptation with zero-shot models, we first construct a joint training dataset by combining the training datasets of size 2000 from all domains except for a target domain. We then train a base model on the joint dataset. We sample input examples during training and evaluation for each slot to include both positive examples (which have the slot) and negative examples (which do not have the slot) with a ratio of 1 to 3. After the base model is trained, domain adaptation is simply done by further training the base model on varying amounts of the training data of the target domain. Note that the size of the joint dataset for each target domain is 18,000, which is dramatically smaller than millions of examples used for training expert models in the BoE approach. Furthermore, there are a lot of utterances in the joint dataset where no slots from the target domain is present.", "Table 2 shows the F1-scores obtained by the different methods for each of the 10 domains. LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively."]}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "predicted_answer": "", "predicted_evidence": ["CNN performed the best in the deep learning techniques and BERT was the choice by the researchers in the advanced deep learning techniques. The code-mix languages are the new non official language which we can see on the web. There isn\u2019t much work done on code-mix language data. Also, a lot of work is done in SA of Hindi language as compared to the other Indian languages like Gujarati, Marathi, Telugu. There is a lot of work carried out in the sentence level of sentiment analysis. There is a need for more SA work to be carried out at document level or aspect. Also, there are very few papers which have multi domain dataset. In majority of the papers, analysis is carried out on the movie reviews and the political domain data. There is a need for research on the other domains like festivals, development, education, sociology etc. Also, there is negligible research done on the data collected from Instagram and LinkedIn.", "In this process the SA is carried out on the document or paragraph as a whole. Whenever a document is about a single subject it is best to carry out document level SA. Examples of document level SA datasets are speeches of the word leaders, movie review, mobile review etc.", "The future work will involve the investigation on using the advance deep learning model such as Bert in mix code language classification. We have collected over 20000 reviews (combination of Marathi and English). We would be comparing the state of the art methods discussed in the current paper during our investigation and discussed the insightful.", "SentiWordNet(SWN) is a opinion based lexicon derived from the WordNets. WordNets are the lexical database which consist of words with short definition and example. SWN consist of dictionary words and the numeric positive and negative sentiment score of each word. WordNets and SWNs are researchers common choice when carrying out SA on document level. Pundlik et al. BIBREF5 were working on multi-domain Hindi language dataset. The architecture implemented in the paper BIBREF5 contained two steps. Domain classification which was the first step was performed using ontology based approach. Sentiment classification being the second step was performed using HNSW and Language Model (LM) Classifier. There was a comparative study done on the results by the HNSW and HNSW + LM Classifiers. The combination of HNSW and LM Classifier gave better classification results as compared to HNSW BIBREF5.", "The process of SA is carried out in 6 major steps which are data extraction, annotation, pre-processing, feature extraction, modelling, evaluation. Figure FIGREF3 shows the steps in the SA task and the explanation of each step is as follows."]}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "predicted_answer": "", "predicted_evidence": ["Social media and designated online cooking platforms have made it possible for large populations to share food culture (diet, recipes) by providing a vast amount of food-related data. Despite the interest in food culture, global eating behavior still contributes heavily to diet-related diseases and deaths, according to the Lancet BIBREF0. Nutrition assessment is a demanding, time-consuming and expensive task. Moreover, the conventional approaches for nutrition assessment are cumbersome and prone to errors. A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning.", "Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g. sugar, oil) or the structure of the meal (mixed food, soups, etc.).", "Social media and designated online cooking platforms have made it possible for large populations to share food culture (diet, recipes) by providing a vast amount of food-related data. Despite the interest in food culture, global eating behavior still contributes heavily to diet-related diseases and deaths, according to the Lancet BIBREF0. Nutrition assessment is a demanding, time-consuming and expensive task. Moreover, the conventional approaches for nutrition assessment are cumbersome and prone to errors. A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7.", "Nutrition assessment is a demanding, time-consuming and expensive task. Moreover, the conventional approaches for nutrition assessment are cumbersome and prone to errors. A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g.", "A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g. sugar, oil) or the structure of the meal (mixed food, soups, etc."]}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "predicted_answer": "", "predicted_evidence": ["Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g. sugar, oil) or the structure of the meal (mixed food, soups, etc.).", "A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g. sugar, oil) or the structure of the meal (mixed food, soups, etc.", "The performance of approaches based on machine learning relies heavily on the quantity and quality of the available data. To this end, a number of efforts have been made to compile informative datasets to be used for machine learning approaches. Most of the early released food databases were assembled only by image data for a special kind of meal. In particular, the first publicly available database was the Pittsburgh Fast-Food Image Dataset (PFID) BIBREF9, which contains only fast food images taken under laboratory conditions. After the recent breakthrough in deep learning models, a number of larger databases were introduced. Bossard et al. BIBREF10 introduced the Food-101 dataset, which is composed of 101 food categories represented by 101'000 food images.", "The proposed method is trained and evaluated on Recipe1M BIBREF15, the largest publicly available multi-modal food database. Recipe1M provides over 1 million recipes (ingredients and instructions), accompanied by one or more images per recipe, leading to 13 million images. The large corpus is supplemented with semantic information (1048 meal classes) for injecting an additional source of information in potential models. In the table in Figure FIGREF1, the structure of recipes belonging to different semantic classes is displayed. Using a slightly adjusted pre-processing than that in BIBREF15 (elimination of noisy instruction sentences), the training set, validation set and test set contain 254,238 and 54,565 and 54,885 matching pairs, respectively. In BIBREF15, the authors chose the overall amount of instructions per recipe as one criterion for a valid matching pair. But we simply removed instruction sentences that contain only punctuation and gained some extra data for training and validation.", "Most of the early released food databases were assembled only by image data for a special kind of meal. In particular, the first publicly available database was the Pittsburgh Fast-Food Image Dataset (PFID) BIBREF9, which contains only fast food images taken under laboratory conditions. After the recent breakthrough in deep learning models, a number of larger databases were introduced. Bossard et al. BIBREF10 introduced the Food-101 dataset, which is composed of 101 food categories represented by 101'000 food images. This was followed by several image-based databases, such as the UEC-100 BIBREF11 and its augmented version, the UEC-256 BIBREF12 dataset, with 9060 food images referring to 100 Japanese food types and 31651 food images referring to 256 Japanese food types, respectively. Xu et al."]}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "predicted_answer": "", "predicted_evidence": ["We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. The classification is completely based on the BERT paper, i.e., we represent the first and second paragraph as a single packed sequence, with the first paragraph using the A embedding and the second paragraph using the B embedding. In the case of feature extraction, the network weights freeze and CLS token are fed to the classifier. In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task. First, just the last sentence of the first paragraph and the beginning sentence of the second paragraph has been used for classification. We wanted to know whether two sentences are enough for ordering classification or not. After that, we increased the number of tokens and accuracy respectively increases. We found this method very promising and the accuracy significantly increases with respect to previous methods (Table 3 ).", "First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. The classification is completely based on the BERT paper, i.e., we represent the first and second paragraph as a single packed sequence, with the first paragraph using the A embedding and the second paragraph using the B embedding. In the case of feature extraction, the network weights freeze and CLS token are fed to the classifier. In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task. First, just the last sentence of the first paragraph and the beginning sentence of the second paragraph has been used for classification. We wanted to know whether two sentences are enough for ordering classification or not. After that, we increased the number of tokens and accuracy respectively increases. We found this method very promising and the accuracy significantly increases with respect to previous methods (Table 3 ). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories.", "In this paper, we are introducing a new dataset which we call ParagraphOrdering, and test the ability of the mentioned models on this newly introduced dataset. We have got inspiration from \"Learning and Using the Arrow of Time\" paper BIBREF6 for defining our task. They sought to understand the arrow of time in the videos; Given ordered frames from the video, whether the video is playing backward or forward. They hypothesized that the deep learning algorithm should have the good grasp of the physics principle (e.g. water flows downward) to be able to predict the frame orders in time.", "Recurrent neural networks (RNN) and architectures based on RNNs like LSTM BIBREF0 has been used to process sequential data more than a decade. Recently, alternative architectures such as convolutional networks BIBREF1 , BIBREF2 and transformer model BIBREF3 have been used extensively and achieved the state of the art result in diverse natural language processing (NLP) tasks. Specifically, pre-trained models such as the OpenAI transformer BIBREF4 and BERT BIBREF5 which are based on transformer architecture, have significantly improved accuracy on different benchmarks.", "Different approaches have been used to solve this task. The best result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around $84\\%$ accuracy on test set which outperforms other models significantly."]}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "predicted_answer": "", "predicted_evidence": ["Generation phase. Feeding a word ($\\widehat{w_{0}}$) into the trained LSTM network model, will output the word most likely to occur after $\\widehat{w_{0}}$ as $\\widehat{w_{1}}$ depending on $P(\\widehat{w_{1}}~|~\\widehat{w_{0}})$. If we want to generate a text body of $n$ words, we feed $\\widehat{w_{1}}$ to the RNN model and get the next word by evaluating $P(\\widehat{w_{2}} ~|~\\widehat{w_{0}},\\widehat{w_{1}})$. This is done repeatedly to generate a text sequence with n words: $\\widehat{w_{0}}$, $\\widehat{w_{1}}$, $\\widehat{w_{2}}$, ..., $\\widehat{w_{n}}$.", "Tables TABREF12 and TABREF13 describe some statistical details about the legitimate and malicious datasets used in this system. We define length ($L$) as the number of words in the body of an email. We define Vocabulary ($V$) as the number of unique words in an email.", "Evaluation dataset. We compared our system's output against a small set of automatically generated emails provided by the authors of BIBREF6. The provided set consists of 12 emails automatically generated using the Dada Engine and manually generated grammar rules. The set consists of 6 emails masquerading as Hillary Clinton emails and 6 emails masquerading as emails from Sarah Palin.", "3392 Phishing emails from Jose Nazario's Phishing corpus (Source 2)", "We review two types of errors observed in the evaluation of our RNN text generation models developed in this study. First, the text generated by multiple RNN models suffer from repetitive tags and words. The example of the email body below demonstrates an incoherent and absurd piece of text generated by the RNN trained on legitimate emails and 50% of phishing emails with a temperature of 0.5."]}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "predicted_answer": "", "predicted_evidence": ["This kind of repetitive text generation was observed a number of times. However, we have not yet investigated the reasons for these repetitions. This could be an inherent problem of the LSTM model, or it could be because of the relatively small training dataset we have used. A third issue could be the temperature setting. More experiments are needed to determine the actual causes.", "Dear registered secur= online, number: hearing from This trade guarded please account go to pay it. To modify your Account then fill in necessary from your notification preferences, please PayPal account provided with the integrity of information on the Alerts tab.", "PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to bring the", "Example II at Temperature = 0.7:", "Example I at Temperature = 0.5:"]}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "predicted_answer": "", "predicted_evidence": ["The trained model is used to generate the email body based on the nature of the input. We varied the sampling technique of generating the new characters for the text generation.", "Malicious dataset. The malicious dataset was difficult to acquire. We used two malicious sources of data mentioned below:", "Primarily, for the reasons stated above, we have used multiple email datasets, belonging to both legitimate and malicious classes, for training the system model and also in the quantitative evaluation and comparison steps. For our training model, we use a larger ratio of malicious emails compared to legitimate data (approximate ratio of benign to malicious is 1:4).", "Example (B):", "Sir we account access will do so may not the emails about the $<$NET$>$ This $<$NET$>$ is included at 3 days while when to $<$NET$>$ because link below to update your account until the deadline we will received this information that we will know that your $<$NET$>$ account information needs"]}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "predicted_answer": "", "predicted_evidence": ["ergin2012 look into Turkish obituary data from H\u00fcrriyet, a major Turkish daily newspaper, from 1970 to 2009, with the goal of finding expressions of religiosity and constructions of death in relation to gender and temporal variations together with markers of status. Their results show that the obituaries considered are relying on \u201can emotional tone of loss\u201d and that the spiritual preferences are linked to the status and appartenance to a specific social class.", "The Personal Information class serves the purpose to classify most of the introductory clauses in obituaries. We have chosen to refer to a sentence as Personal Information when it includes the name of the deceased, the date of death, the cause of death, or the place of death. For example John Doe, 64, of Newport, found eternal rest on Nov. 22, 2018.", "vital develop a methodology to link mortality data from internet sources with administrative data from electronic health records. To do so they implement and evaluate the performance of different linkage methods. The electronic health records are from patients in Rennes, France and the extracted obituaries are all available online obituaries from French funeral home websites. They evaluate three different linkage methods and obtain almost perfect precisions with all methods. They conclude that using obituaries published online could address the problem of long delays in the sharing of mortality data whereas online obituaries could be considered as reliable data source for real-time suveillance of mortality in patients with cancer.", "We collected obituaries from three websites: The Daily Item, where obituaries from the USA are published, Remembering.CA, which covers obituaries from Canada, and The London Free Press, which covers obituaries from London (see Table TABREF5). The obituaries on The Daily Item and The London Free Press are dedicated websites where people could publish their obituaries. Remembering.CA is an aggregator and shows obituaries published from different sources. The total set consists of 20058 obituaries.", "With a focus on computational linguistics, obituarymining1 analyze text data from obituary websites, with the intention to use it to prevent identity theft. The goal was to evaluate how \u201coften and how accurately name and address fragments extracted from these notices developed into complete name and address information corresponding to the deceased individual\u201d. They use a knowledge base with name and address information, extracte the name and address fragments from the text and match them against the knowledge base to create a set of name and address candidates. This result set is then compared to an authoritative source in order to determine which of the candidate records actually correspond to the name and address of an individual reported as deceased."]}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "predicted_answer": "", "predicted_evidence": ["Here, $q(f,e)$ is a lexical score defined as:", "Building a machine translation (MT) system requires lots of bilingual data. Neural MT models BIBREF0 , which become the current standard, are even more difficult to train without huge bilingual supervision BIBREF1 . However, bilingual resources are still limited to some of the selected language pairs\u2014mostly from or to English.", "To simulate such situations, we drop some words randomly from a clean target sentence BIBREF11 :", "From a clean target sentence, we corrupt its word order by random permutations. We limit the maximum distance between an original position and its new position like unmt-facebook:", "The word translation using nearest neighbor search does not consider context around the current word. In many cases, the correct translation is not the nearest target word but other close words with morphological variations or synonyms, depending on the context."]}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "predicted_answer": "", "predicted_evidence": ["To examine the effect of each noise type in denoising autoencoder, we tuned each parameter of the noise and combined them incrementally (Table 2 ). Firstly, for permutations, a significant improvement is achieved from $d_\\text{per} = 3$ , since a local reordering usually involves a sequence of 3 to 4 words. With $d_\\text{per} > 5$ , it shuffles too many consecutive words together, yielding no further improvement. This noise cannot handle long-range reordering, which is usually a swap of words that are far from each other, keeping the words in the middle as they are.", "Add $\\delta _i$ to index $i$ and sort the incremented indices $i + \\delta _i$ in an increasing order.", "Similarly, word-by-word translation cannot handle the contrary case: when a source word should be translated into more than one target words, or a target word should be generated from no source words for fluency. For example, a German word \u201cim\u201d must be \u201cin the\u201d in English, but word translation generates only one of the two English words. Another example is shown in Figure 2 .", "$\nq(f,e) = \\frac{d(f,e) + 1}{2}\n$", "Rearrange the words to be in the new positions, to which their original indices have moved by Step 2."]}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_answer": "", "predicted_evidence": ["Upon this hidden layer, we further stack a Sigmoid layer to predict the probabilities of corresponding discourse arguments:", "Features used in SVM are taken from the state-of-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair BIBREF16 . In order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cutoff of 5 to remove rare features, following Lin et al. lin2009recognizing.", "$$ \n\\mathcal {L}(\\theta , \\phi ) \\simeq -\\text{KL}&(q_\\phi (\\mathbf {z}|x^{(t)}, y^{(t)})||q_\\phi ^{\\prime }(\\mathbf {z}|x^{(t)})) \\\\\n\\qquad + \\frac{1}{L}&\\sum _{l=1}^L \\log p_\\theta (x^{(t)},y^{(t)}|\\tilde{z}^{(t,l)}) \\\\\n\\text{where}~\\tilde{z}^{(t,l)} = \\mu ^{(t)}& + \\sigma ^{(t)} \\odot \\epsilon ^{(l)}~\\text{and}~\\epsilon ^{(l)}\\sim \\mathcal {N}(0,\\mathbf {I}) $$   (Eq. 24)", "Conventional approaches to implicit DRR often treat the relation recognition as a classification problem, where discourse arguments and relations are regarded as the inputs and outputs respectively. Generally, these methods first generate a representation for a discourse, denoted as $\\mathbf {x}$ (e.g., manual features in SVM-based recognition BIBREF6 , BIBREF7 or sentence embeddings in neural networks-based recognition BIBREF8 , BIBREF9 ), and then directly model the conditional probability of the corresponding discourse relation $\\mathbf {y}$ given $\\mathbf {x}$ , i.e. $p(\\mathbf {y}|\\mathbf {x})$ . In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation $\\mathbf {x}$ . Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data.", "here, $x_{1}^{\\prime } \\in \\mathbb {R}^{d_{x_{1}}}$ and $x_{2}^{\\prime } \\in \\mathbb {R}^{d_{x_{2}}}$ are the real-valued representations of the reconstructed $x_1$ and $x_2$ respectively. We assume that $p_\\theta (\\mathbf {x}|\\mathbf {z})$ is a multivariate Bernoulli distribution because of the bag-of-word representation. Therefore the logarithm of $p(x|z)$ is calculated as the sum of probabilities of words in discourse arguments as follows:"]}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "predicted_answer": "", "predicted_evidence": ["LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.", "Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \u201cAccidents and Natural Disasters\u201d, the aspects are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d.", "The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0", "The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0", "Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit."]}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "predicted_answer": "", "predicted_evidence": ["The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0", "where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.", "where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.", "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.", "Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \u201cSony Virtual Reality PS4\u201d, and \u201c`Bitcoin Mt. Gox Offlile\u201d' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \u201cSony Virtual Reality PS4\u201d, many readers talked about the product of \u201cOculus\u201d, hence the word \u201coculus\u201d is assigned a high salience by our model."]}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "predicted_answer": "", "predicted_evidence": ["Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0", "Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).", "Based on the news and comments of the topic \u201cSony Virtual Reality PS4\u201d, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \u201cOculus\u201d, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \u201cMr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\u201d.", "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.", "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0"]}
{"question_id": "86d1c990c1639490c239c3dbf5492ecc44ab6652", "predicted_answer": "", "predicted_evidence": ["The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.", "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy.", "With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report \u201cThe most important announcements from Google's big developers' conference\u201d. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.", "Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.", "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS."]}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .", "Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.", "Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0", "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.", "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments."]}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "predicted_answer": "", "predicted_evidence": ["Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0", "VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0", "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0", "Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.", "Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words."]}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "predicted_answer": "", "predicted_evidence": ["Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.", "With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report \u201cThe most important announcements from Google's big developers' conference\u201d. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.", "Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \u201cSony Virtual Reality PS4\u201d, and \u201c`Bitcoin Mt. Gox Offlile\u201d' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \u201cSony Virtual Reality PS4\u201d, many readers talked about the product of \u201cOculus\u201d, hence the word \u201coculus\u201d is assigned a high salience by our model.", "VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0", "As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments."]}
{"question_id": "b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "predicted_answer": "", "predicted_evidence": ["After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.", "Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \u201cSony Virtual Reality PS4\u201d, and \u201c`Bitcoin Mt. Gox Offlile\u201d' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \u201cSony Virtual Reality PS4\u201d, many readers talked about the product of \u201cOculus\u201d, hence the word \u201coculus\u201d is assigned a high salience by our model.", "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:", "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0", "The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents."]}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "predicted_answer": "", "predicted_evidence": ["[8] Freiwald, W. A. & Tsao, D. Y. (2010). Functional compartmentalization and viewpoint generalization within the macaque face-processing system. Science, 330(6005), 845-851.", "[30] Barbas, H. (2015). General cortical and special prefrontal connections: principles from structure to function. Annual review of neuroscience, 38, 269-289.", "Language is the most remarkable characteristics distinguishing mankind from animals. Theoretically, all kinds of information such as object properties, tasks and goals, commands and even emotions can be described and conveyed by language [21]. We trained with LGI eight different syntaxes (in other word, eight different tasks), and LGI demonstrates its understanding by correctly interacting with the vision system. After learning \u2018this is 9\u2019, it is much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units, which could be shared with the following sentence learning.", "[2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.", "[1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652."]}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "predicted_answer": "", "predicted_evidence": ["[9] Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.", "[4] Baddeley, A., Gathercole, S. & Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158.", "In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text.", "Language guided imagination is the nature of human thinking and intelligence. Normally, the real-time tasks or goals are conveyed by language, such as \u2018to build a Lego car\u2019. To achieve this goal, first, an agent (human being or machine) needs to know what\u2019s car, and then imagine a vague car instance, based on which the agent can plan to later collect wheel, window and chassis blocks for construction. Imagining the vague car is the foundation for decomposing future tasks. We trained the LGI network with a human-like cumulative learning process, from learning the meaning of words, to understanding complicated syntaxes, and finally organizing the thinking process with language. We trained the LGI to associate object name with corresponding instances by \u2018this is \u2026\u2019 syntax; and trained the LGI to produce a digit instance, when there comes the sentence \u2018give me a [number]\u2019. In contrast, traditional language models could only serve as a word dependency predictor rather than really understand the sentence.", "Imagination is another key component of human thinking. For the game Go [22, 23], the network using a reinforcement learning strategy has to be trained with billions of games in order to acquire a feeling (Q value estimated for each potential action) to move the chess. As human beings, after knowing the rule conveyed by language, we can quickly start a game with proper moves using a try-in-imagination strategy without requiring even a single practice. With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily."]}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "predicted_answer": "", "predicted_evidence": ["[19] Petanjek, Z., Juda\u0161, M., Kostovi\u0107, I. & Uylings, H. B. (2007). Lifespan alterations of basal dendritic trees of pyramidal neurons in the human prefrontal cortex: a layer-specific pattern. Cerebral cortex, 18(4), 915-929.", "[6] Simonyan, K. & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.", "Imagination is another key component of human thinking. For the game Go [22, 23], the network using a reinforcement learning strategy has to be trained with billions of games in order to acquire a feeling (Q value estimated for each potential action) to move the chess. As human beings, after knowing the rule conveyed by language, we can quickly start a game with proper moves using a try-in-imagination strategy without requiring even a single practice. With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily.", "[12] Yasuda, R., Sabatini, B. L. & Svoboda, K. (2003). Plasticity of calcium channels in dendritic spines. Nature neuroscience, 6(9), 948.", "[1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652."]}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "predicted_answer": "", "predicted_evidence": ["Language guided imagination is the nature of human thinking and intelligence. Normally, the real-time tasks or goals are conveyed by language, such as \u2018to build a Lego car\u2019. To achieve this goal, first, an agent (human being or machine) needs to know what\u2019s car, and then imagine a vague car instance, based on which the agent can plan to later collect wheel, window and chassis blocks for construction. Imagining the vague car is the foundation for decomposing future tasks. We trained the LGI network with a human-like cumulative learning process, from learning the meaning of words, to understanding complicated syntaxes, and finally organizing the thinking process with language. We trained the LGI to associate object name with corresponding instances by \u2018this is \u2026\u2019 syntax; and trained the LGI to produce a digit instance, when there comes the sentence \u2018give me a [number]\u2019. In contrast, traditional language models could only serve as a word dependency predictor rather than really understand the sentence.", "[27] Jasmin, K., Lima, C. F. & Scott, S. K. (2019). Understanding rostral\u2013caudal auditory cortex contributions to auditory perception. Nature Reviews Neuroscience, in press.", "Based on the same network, LGI continued to learn syntax \u2018this is \u2026\u2019. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.", "(5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly. On the other hand, the full path of clear imagination may explain the dream mechanism. Figure 7.B shows the short cut imagination process, where LGI also regarded the rotated \u20189\u2019 as digit 6, which suggests the AIT activation does not encode the digit identity, but the untangled features of input image or imagined image. Those high level cortices beyond visual cortex could be the place for identity representation.", "[10] Anonymous A. (2019). The development, recognition, and learning mechanisms of animal-like neural network. Advances in Neural Information Processing Systems, in submission"]}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "predicted_answer": "", "predicted_evidence": ["Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger.", "Modern autoencoder techniques could synthesize an unseen view for the desired viewpoint. Using car as an example [17], during training, the autoencoder learns the 3D characteristics of a car with a pair of images from two views of the same car together with the viewpoint of the output view. During testing, the autoencoder could predict the desired image from a single image of the car given the expected viewpoint. However, this architecture is task-specific, namely that the network can only make predictions on cars' unseen views. To include multiple tasks, we added an additional PFC layer that can receive task commands conveyed via language stream and object representation via the visual encoder pathway, and output the modulated images according to task commands and the desired text prediction associated with the images. In addition, by transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image.", "[29] Husain, M. & Roiser, J. (2018). Neuroscience of apathy and anhedonia: a transdiagnostic approach. Nature Reviews Neuroscience, 19, 470-484.", "The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from binary vector, whose detailed discussion can be referred to the Supplementary section A) to classify the predicted symbol instead of softmax operation which has no neuroscience foundation.", "[6] Simonyan, K. & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556."]}
{"question_id": "bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "predicted_answer": "", "predicted_evidence": ["[10] Anonymous A. (2019). The development, recognition, and learning mechanisms of animal-like neural network. Advances in Neural Information Processing Systems, in submission", "For human brain development, the visual and auditory systems mature in much earlier stages than the PFC [19]. To mimic this process, our PFC subsystem was trained separately after vision and language components had completed their functionalities. We have trained the network to accumulatively learn eight syntaxes, and the related results are shown in the following section. Finally, we demonstrate how the network forms a thinking loop with text language and imagined pictures.", "[24] Saxton, D., Grefenstette, E., Hill, F. & Kohli, P. (2019). Analysing Mathematical Reasoning Abilities of Neural Models. arXiv preprint arXiv:1904.01557.", "In addition, the error backpropagation technique is generally used to modify network weights to learn representation and achieve training objectives [11]. However, in neuroscience, it is the activity-dependent molecular events (e.g. the inflow of calcium ion and the switching of glutamate N-methyl-D-aspartate receptor etc.) that modify synaptic connections [12, 13]. Indeed, the real neural feedback connection provides the top-down imagery information [14], which is usually ignored by AI network constructions due to the concept of error backpropagation. What\u2019s more, our concurrent paper [10] demonstrates that the invariance property of visual recognition under the rotation, scaling, and translation of an object is supported by coordinated population coding rather than the max-pooling mechanism [15]. The softmax classification is usually used to compute the probability of each category (or word) in the repository (or vocabulary) before prediction.", "[19] Petanjek, Z., Juda\u0161, M., Kostovi\u0107, I. & Uylings, H. B. (2007). Lifespan alterations of basal dendritic trees of pyramidal neurons in the human prefrontal cortex: a layer-specific pattern. Cerebral cortex, 18(4), 915-929."]}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "predicted_answer": "", "predicted_evidence": ["As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese are made against English words (see Figure FIGREF27 for examples\u2014you notice many English words such as \u201cGB-18030\u201d and \u201cGemfile\u201d in non-English text). You also notice a number of grammatical edits in Chinese (e.g., confusion between the possessive particle de and the adjectival particle de) and Japanese (e.g., omissions of case particles such as wo, no, and ni). This demonstrates that the dataset can serve as a rich source of not only spelling but also naturally-occurring grammatical errors.", "We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted \u201ctypo-ness\u201d score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes.", "Due to this lack of large-scale datasets, many research studies BIBREF4, BIBREF2, BIBREF5 resort to automatic generation of artificial errors (also called pseudo-errors). Although such methods are efficient and have seen some success, they do not guarantee that generated errors reflect the range and the distribution of true errors made by humans BIBREF6.", "Extract eligible repositories and typo commits from GitHub based on the meta data of the repository and the commit message", "Spelling correction BIBREF0, BIBREF1, BIBREF2 and grammatical error correction (GEC) BIBREF3 are two fundamental tasks that have important implications for downstream NLP tasks and for education in general. In recent years, the use of statistical machine translation (SMT) and neural sequence-to-sequence (seq2seq) models has been becoming increasingly popular for solving these tasks. Such modern NLP models are usually data hungry and require a large amount of parallel training data consisting of sentences before and after the correction. However, only relatively small datasets are available for these tasks, compared to other NLP tasks such as machine translation. This is especially the case for spelling correction, for which only a small number of datasets consisting of individual misspelled words are available, including the Birkbeck spelling error corpus and a list of typos collected from Twitter."]}
{"question_id": "d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "predicted_answer": "", "predicted_evidence": ["Has 50 or more starts,", "We remove the first type of edits by using language detection, and detect (not remove) the second type of edits by building a supervised classifier. The following subsections detail the process. See Figure FIGREF15 (right) for an overview of the typo filtering process.", "Not all the edits collected in the process described so far are related to typos in natural language text. First, edits may also be made to parts of a repository that are written in programming language versus human language. Second, not every edit in a commit described \u201ctypo\u201d is necessarily a typo edit, because a developer may make a single commit comprised of multiple edits, some of which may not be typo-related.", "As one way to complement this lack of resources, Wikipedia has been utilized as a rich source of textual edits, including typos BIBREF7, BIBREF8, BIBREF9. However, the edits harvested from Wikipedia are often very noisy and diverse in their types, containing edits from typos to adding and modifying information. To make the matters worse, Wikipedia suffers from vandalism, where articles are edited in a malicious manner, which requires extensive detection and filtering.", "Repository ... in git terms, a repository is a database of files whose versions are controlled under git. A single repository may contain multiple files and directories just like a computer file system."]}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "predicted_answer": "", "predicted_evidence": ["As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese are made against English words (see Figure FIGREF27 for examples\u2014you notice many English words such as \u201cGB-18030\u201d and \u201cGemfile\u201d in non-English text). You also notice a number of grammatical edits in Chinese (e.g., confusion between the possessive particle de and the adjectival particle de) and Japanese (e.g., omissions of case particles such as wo, no, and ni). This demonstrates that the dataset can serve as a rich source of not only spelling but also naturally-occurring grammatical errors.", "See Figure FIGREF33 for an overview of the distributions of these computed statistics per category for English. We observed similar trends for other two languages (Chinese and Japanese), except for a slightly larger number of spell edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target.", "Edit ... in this paper, an edit is a pair of lines to which changes are made in a commit (note the special usage here). The line before the change is called the source and the line after is the target. In other words, an edit is a pair of the source and the target. Note that a single edit may contain changes to multiple parts of the source (for example, multiple words that are not contiguous).", "Repository ... in git terms, a repository is a database of files whose versions are controlled under git. A single repository may contain multiple files and directories just like a computer file system.", "Has a permissive license."]}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "predicted_answer": "", "predicted_evidence": ["We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted \u201ctypo-ness\u201d score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes.", "We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings to date. We made the dataset publicly available (https://github.com/mhagiwara/github-typo-corpus) along with the automatically assigned typo labels as well as the source code to extract typos. We also provide the detailed analyses of the dataset, where we demonstrate that the F measure of existing spell checkers merely reaches $\\sim 0.5$, arguing that the GitHub Typo Corpus provides a new, rich source of naturally-occurring misspellings and grammatical errors that complement existing datasets.", "We then applied Aspell and Enchant, two commonly used spell checking libraries, and measured their performance against each one of the edit types. The results show that the performance of the spell checkers is fairly low ($F0.5 \\approx 0.5$) even for its main target category (SPELL), which suggests that the GitHub Typo Corpus contains many challenging typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and grammatical error correctors.", "The authors would like to thank Tomoya Mizumoto at RIKEN AIP/Future Corporation and Kentaro Inui at RIKEN AIP/Tohoku University for their useful comments and discussion on this project.", "where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of the input text to some degree, and by taking the ratio between the source and the target, the feature can represent the degree to which the fluency is improved before and after the edit."]}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "predicted_answer": "", "predicted_evidence": ["In order to create a high-quality, large-scale dataset of misspelling and grammatical errors (collectively called typos in this paper), we leverage the data from GitHub, the largest platform for hosting and sharing repositories maintained by git, a popular version control system commonly used for software development. Changes made to git repositories (called commits, see Section 3 for the definition) are usually tagged with commit messages, making detection of typos a trivial task. Also, GitHub suffers less from vandalism, since commits in many repositories are code reviewed, a process where every change is manually reviewed by other team members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues.", "Extract eligible repositories and typo commits from GitHub based on the meta data of the repository and the commit message", "Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset.", "Repository ... in git terms, a repository is a database of files whose versions are controlled under git. A single repository may contain multiple files and directories just like a computer file system.", "Ratio of the target perplexity over the source calculated by a language model"]}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "predicted_answer": "", "predicted_evidence": ["Not all the edits collected in the process described so far are related to typos in natural language text. First, edits may also be made to parts of a repository that are written in programming language versus human language. Second, not every edit in a commit described \u201ctypo\u201d is necessarily a typo edit, because a developer may make a single commit comprised of multiple edits, some of which may not be typo-related.", "Has a size between 1MB and 1GB, and", "As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese are made against English words (see Figure FIGREF27 for examples\u2014you notice many English words such as \u201cGB-18030\u201d and \u201cGemfile\u201d in non-English text). You also notice a number of grammatical edits in Chinese (e.g., confusion between the possessive particle de and the adjectival particle de) and Japanese (e.g., omissions of case particles such as wo, no, and ni). This demonstrates that the dataset can serve as a rich source of not only spelling but also naturally-occurring grammatical errors.", "In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories:", "This section describes the process for collecting a large amount of typos from GitHub, which consists two steps: 1) collecting target repositories that meet some criteria and 2) collecting commits and edits from them. See Figure FIGREF15 for the overview of the typo-collecting process."]}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "predicted_answer": "", "predicted_evidence": ["We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings.", "Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound.", "Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.", "As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings.", "Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$."]}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "predicted_answer": "", "predicted_evidence": ["Since the introduction of pre-trained word embeddings such as word2vec BIBREF0 and GloVe BIBREF1, a lot of efforts have been devoted to developing universal sentence embeddings. Initial attempts at learning sentence representation using unsupervised approaches did not yield satisfactory performance. Recent work BIBREF2 has shown that models trained in supervised fashion on datasets like Stanford Natural Language Inference (SNLI) corpus BIBREF3 can consistently outperform unsupervised methods like SkipThought vectors BIBREF4. More recently, Universal Sentence Encoder BIBREF5 equipped with the Transformer BIBREF6 as the encoder, co-trained on a large amount of unsupervised training data and SNLI corpus, has demonstrated surprisingly good performance with minimal amounts of supervised training data for a transfer task.", "Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance.", "Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.", "Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time.", "Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods."]}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "predicted_answer": "", "predicted_evidence": ["A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage.", "Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$.", "Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time.", "In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined.", "Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected."]}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "predicted_answer": "", "predicted_evidence": ["When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer.", "Passages that consist of multiple sentences are coherent units of natural languages that convey information at a pragmatic or discourse level. While there are many models for generating and evaluating sentence embeddings, there hasn't been a lot of work on passage level embedding generation and evaluation.", "When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline.", "Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected.", "Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time."]}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "predicted_answer": "", "predicted_evidence": ["Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected.", "Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time.", "We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings.", "A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage.", "In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined."]}
{"question_id": "f7f2968feb28c2907266c892f051ae9f7d6286e6", "predicted_answer": "", "predicted_evidence": ["A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage.", "However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT.", "We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets.", "Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods.", "We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage. We fine-tune all BERT parameters except the word embedding layers."]}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "predicted_answer": "", "predicted_evidence": ["In this paper, we present a machine learning model trained on course books currently in use in L2 Swedish classrooms. Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1).", "Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1). Our document-level model achieves a state-of-the-art performance (F-score of 0.8), however, there is room for improvement in sentence-level predictions. We plan to make our results available through the online intelligent computer-assisted language learning platform L\u00e4rka, both as corpus-based exercises for teachers and learners of L2 Swedish and as web-services for researchers and developers.", "Linguistic information provided by Natural Language Processing (NLP) tools has good potential for turning the continuously growing amount of digital text into interactive and personalized language learning material. Our work aims at overcoming one of the fundamental obstacles in this domain of research, namely how to assess the linguistic complexity of texts and sentences from the perspective of second and foreign language (L2) learners.", "Readability for the Swedish language has a rather long tradition. One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarthetsindex, `Readability index') proposed in BIBREF16 . This measure combines the average number of words per sentence in the text with the percentage of long words, i.e. tokens consisting of more than six characters. Besides traditional formulas, supervised machine learning approaches have also been tested. Swedish document-level readability with a native speaker focus is described in BIBREF4 and BIBREF17 . For L2 Swedish, only a binary sentence-level model exists BIBREF8 , but comprehensive and highly accurate document- and sentence-level models for multiple proficiency levels have not been developed before.", "Lexical (Lex): Similar to BIBREF8 , we used information from the Kelly list BIBREF21 , a lexical resource providing a CEFR level and frequencies per lemma based on a corpus of web texts. Thus, this word list is entirely independent from our dataset. Instead of percentages, we used incidence scores (IncSc) per 1000 words to reduce the influence of sentence length on feature values. The IncSc of a category was computed as 1000 divided by the number of tokens in the text or sentence multiplied by the count of the category in the sentence. We calculated the IncSc of words belonging to each CEFR level (#6 - #11). In features #12 and #13 we considered difficult all tokens whose level was above the CEFR level of the text or sentence."]}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "predicted_answer": "", "predicted_evidence": ["The previously mentioned studies target mainly native language (L1) readers including people with low literacy levels or mild cognitive disabilities. Our focus, however, is on building a model for predicting the proficiency level of texts and sentences used in L2 teaching materials. This aspect has been explored for English BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , French BIBREF13 , Portuguese BIBREF14 and, without the use of NLP, for Dutch BIBREF15 .", "Readability for the Swedish language has a rather long tradition. One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarthetsindex, `Readability index') proposed in BIBREF16 . This measure combines the average number of words per sentence in the text with the percentage of long words, i.e. tokens consisting of more than six characters. Besides traditional formulas, supervised machine learning approaches have also been tested. Swedish document-level readability with a native speaker focus is described in BIBREF4 and BIBREF17 . For L2 Swedish, only a binary sentence-level model exists BIBREF8 , but comprehensive and highly accurate document- and sentence-level models for multiple proficiency levels have not been developed before.", "In the future, a more detailed investigation is needed to understand the performance drop between document and sentence level. Acquiring more sentence-level annotated data and exploring new features relying on lexical-semantic resources for Swedish would be interesting directions to pursue. Furthermore, we intend to test the utility of this approach in a real-world web application involving language learners and teachers.", "In this paper, we present a machine learning model trained on course books currently in use in L2 Swedish classrooms. Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1).", "not forming a coherent text, in the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in. In Table TABREF3 , columns 2-5 give an overview of the distribution of texts across levels and their mean length in sentences. The distribution of sentences per level is presented in the last two columns of Table TABREF3 . COCTAILL contained a somewhat more limited amount of B2 and C1 level sentences in the form of lists and language examples, possibly because learners handle larger linguistic units with more ease at higher proficiency levels."]}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "predicted_answer": "", "predicted_evidence": ["In this paper, we present a machine learning model trained on course books currently in use in L2 Swedish classrooms. Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1). Our document-level model achieves a state-of-the-art performance (F-score of 0.8), however, there is room for improvement in sentence-level predictions.", "In this paper, we present a machine learning model trained on course books currently in use in L2 Swedish classrooms. Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1).", "Morphological (Morph): We included the variation (the ratio of a category to the ratio of lexical tokens - i.e. nouns, verbs, adjectives and adverbs) and the IncSc of all lexical categories together with the IncSc of punctuations, particles, sub- and conjunctions (#34, #51). Some additional features, using insights from L2 teaching material BIBREF20 , captured fine-grained inflectional information such as the IncSc of neuter gender nouns and the ratio of different verb forms to all verbs (#52 - #56). Instead of simple type-token ratio (TTR) we used a bilogarithmic and a square root TTR as in BIBREF3 . Moreover, nominal ratio BIBREF4 , the ratio of pronouns to prepositions BIBREF13 , and two lexical density features were also included: the ratio of lexical words to all non-lexical categories (#48) and to all tokens (#49).", "Linguistic information provided by Natural Language Processing (NLP) tools has good potential for turning the continuously growing amount of digital text into interactive and personalized language learning material. Our work aims at overcoming one of the fundamental obstacles in this domain of research, namely how to assess the linguistic complexity of texts and sentences from the perspective of second and foreign language (L2) learners.", "Readability for the Swedish language has a rather long tradition. One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarthetsindex, `Readability index') proposed in BIBREF16 . This measure combines the average number of words per sentence in the text with the percentage of long words, i.e. tokens consisting of more than six characters. Besides traditional formulas, supervised machine learning approaches have also been tested. Swedish document-level readability with a native speaker focus is described in BIBREF4 and BIBREF17 . For L2 Swedish, only a binary sentence-level model exists BIBREF8 , but comprehensive and highly accurate document- and sentence-level models for multiple proficiency levels have not been developed before."]}
{"question_id": "a70656fc61bf526dd21db7d2ec697b29a5a9c24e", "predicted_answer": "", "predicted_evidence": ["To establish the external validity of our approach, we tested it on a subset of L\u00e4SBarT BIBREF4 , a corpus of Swedish easy-to-read (ETR) texts previously employed for Swedish L1 readability studies BIBREF4 , BIBREF17 . We used 18 fiction texts written for children between ages nine to twelve, half of which belonged to the ETR category and the rest were unsimplified. Our model generalized well to unseen data, it classified all ETR texts as B1 and all ordinary texts as C1 level, thus correctly identifying in all cases the relative difference in complexity between the documents of the two categories.", "Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1). Our document-level model achieves a state-of-the-art performance (F-score of 0.8), however, there is room for improvement in sentence-level predictions. We plan to make our results available through the online intelligent computer-assisted language learning platform L\u00e4rka, both as corpus-based exercises for teachers and learners of L2 Swedish and as web-services for researchers and developers.", "Accuracy scores using other learning algorithms were significantly lower (see Table TABREF10 ), therefore, we report only the results of the logistic regression classifier in the subsequent sections.", "Previously published results on sentence-level data include BIBREF6 , who report 66% accuracy for a binary classification task for English and BIBREF7 who obtained an accuracy between 78.9% and 83.7% for Italian binary class data using different kinds of datasets. Neither of these studies, however, had a non-native speaker focus. BIBREF8 report 71% accuracy for Swedish binary sentence-level classification from an L2 point of view. Both the adjacent accuracy of our sentence-level model (92%) and the accuracy score obtained with that model on SenRead (73%) improve on that score. It is also worth mentioning that the labels in the dataset from BIBREF8 were based on the assumption that all sentences in a text belong to the same difficulty level which, being an approximation (as also Figure FIGREF15 shows), introduced some noise in that data.", "Linguistic information provided by Natural Language Processing (NLP) tools has good potential for turning the continuously growing amount of digital text into interactive and personalized language learning material. Our work aims at overcoming one of the fundamental obstacles in this domain of research, namely how to assess the linguistic complexity of texts and sentences from the perspective of second and foreign language (L2) learners."]}
{"question_id": "f381b0ef693243d67657f6c34bbce015f6b1fd07", "predicted_answer": "", "predicted_evidence": ["Although more analysis would be needed to refine the sentence-level model, our current results indicate that a rich feature set that considers multiple linguistic dimensions may result in an improved performance. In the future, the dataset could be expanded with more gold-standard sentences, which may improve accuracy. Furthermore, an interesting direction to pursue would be to verify whether providing finer-grained readability judgments is a more challenging task also for human raters.", "Although the majority baseline in the case of sentences was 7% higher than the one for texts (Table TABREF9 ), the classification accuracy for sentences using all features was only 63.4%. This is a considerable drop (-18%) in performance compared to the document level (81.3% accuracy). It is possible that the features did not capture differences between the sentences because the amount of context is more limited on the fine-grained level. It is interesting to note that, although there was no substantial performance difference between Lex and All at a document level, the model with all the features performed 7% better at sentence level.", "Not only was accuracy very low with LIX, but this measure also classified 91.6% of the instances as B2 level. Length-based, semantic and syntactic features in isolation showed similar or only slightly better performance than the baselines, therefore we excluded them from Table TABREF9 . Lexical features, however, had a strong discriminatory power without an increase in bias towards the majority classes. Using this subset of features only, we achieved approximately the same performance (0.8 F) as with the complete set of features, All (0.81 F). This suggests that lexical information alone can successfully distinguish the CEFR level of course book texts at the document level. Using the complete feature set we obtained 81% accuracy and 97% adjacent accuracy (when misclassifications to adjacent classes are considered correct). The same scores with lexical features (Lex) only were 80.3% (accuracy) and 98% (adjacent accuracy).", "In this paper, we present a machine learning model trained on course books currently in use in L2 Swedish classrooms. Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1). Our document-level model achieves a state-of-the-art performance (F-score of 0.8), however, there is room for improvement in sentence-level predictions.", "In the future, a more detailed investigation is needed to understand the performance drop between document and sentence level. Acquiring more sentence-level annotated data and exploring new features relying on lexical-semantic resources for Swedish would be interesting directions to pursue. Furthermore, we intend to test the utility of this approach in a real-world web application involving language learners and teachers."]}
{"question_id": "c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "predicted_answer": "", "predicted_evidence": ["In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets. The model reads the tweets in a form of character one-hot encodings and uses them for training with their hashtags as labels. To train the model, we use our collected dataset which consists of $\\sim $3.7M tweets.", "LIWC: We use a set of linguistic categories from the LIWC linguistic dictionary BIBREF20. The used categories are: pronoun, anx, cogmech, insight, cause, discrep, tentat, certain, inhib, incl.", "Bias Cues: We rely on a set of lexicons to capture the bias in text. We model the presence of the words in one of the following cues categories: assertives verbs BIBREF14, bias BIBREF15, factive verbs BIBREF16, implicative verbs BIBREF17, hedges BIBREF18, report verbs BIBREF15. A previous work has used these bias cues to identify bias in suspicious news posts in Twitter BIBREF19.", "Emotions: Since the results of the previous works BIBREF2, BIBREF7 showed that IRA efforts engineered to seed discord among individuals in US, we use emotions features to detect their emotional attempts to manipulate the public opinions (e.g. fear spreading behavior). For that, we use the NRC emotions lexicon BIBREF9 that contains $\\sim $14K words labeled using the eight Plutchik's emotions.", "In order to evaluate our feature set, we use Random Selection, Majority Class, and bag-of-words baselines. In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets."]}
{"question_id": "e0b54906184a4ad87d127bed22194e62de38222b", "predicted_answer": "", "predicted_evidence": ["After the 2016 US elections, Twitter has detected a suspicious attempt by a large set of accounts to influence the results of the elections. Due to this event, an emerging research works about the Russian troll accounts started to appear BIBREF2, BIBREF3, BIBREF0, BIBREF4, BIBREF5.", "In this work, we identify online trolls in Twitter, namely IRA trolls, from a textual perspective. We study the effect of a set of text-based features and we propose a machine learning model to detect them. We aim to answer three research questions: RQ1. Does the thematic information improve the detection performance?, RQ2. Can we detect IRA trolls from only a textual perspective? and RQ3. How IRA campaign utilized the emotions to affect the public opinions?", "To model the identification process of the Russian trolls, we considered a large dataset of both regular users (legitimate accounts) and IRA troll accounts. Following we describe the dataset. In Table TABREF6 we summarizes its statistics.", "Given our dataset, we applied Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8 on the tweets after a prepossessing step where we maintained only nouns and proper nouns. In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes.", "False Positive Cases. The proposed features showed to be effective in the classification process. We are interested in understanding the causes of misclassifying some of IRA trolls. Therefore, we manually investigated the false positive tweets and we found that there are three main reasons: 1) Some trolls were tweeting in a questioning way by asking about general issues; we examined their tweets but we did not find a clear ideological orientation or a suspicious behaviour in their tweets. 2) Some accounts were sharing traditional social media posts (e.g. \"http://t.co/GGpZMvnEAj cat vs trashcan\"); the majority of the false positive IRA trolls are categorized under this reason. In addition, these posts were given a false theme name; the tweet in the previous example assigned to Attacking Hillary theme. 3) Lack of content. Some of the misclassified trolls mention only external links without a clear textual content. This kind of trolls needs a second step to investigate the content of the external links."]}
{"question_id": "1f8044487af39244d723582b8a68f94750eed2cc", "predicted_answer": "", "predicted_evidence": ["A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets. The model reads the tweets in a form of character one-hot encodings and uses them for training with their hashtags as labels. To train the model, we use our collected dataset which consists of $\\sim $3.7M tweets. To represent the tweets in this baseline, we use the decoded embedding produced by the model and we feed them to the Logistic Regression classifier.", "The analysis work on IRA trolls not limited only to the tweets content, but it also considered the profile description, screen name, application client, geo-location, timezone, and number of links used per each media domain BIBREF3. There is a probability that Twitter has missed some IRA accounts that maybe were less active than the others. Based on this hypothesis, the work in BIBREF0 built a machine learning model based on profile, language distribution, and stop-words usage features to detect IRA trolls in a newly sampled data from Twitter. Other works tried to model IRA campaign not only by focusing on the trolls accounts, but also by examining who interacted with the trolls by sharing their contents BIBREF6. Similarly, the work BIBREF5 proposed a model that made use of the political ideologies of users, bot likelihood, and activity-related account metadata to predict users who spread the trolls\u2019 contents.", "A previous work BIBREF0 showed that such suspicious accounts are not bots in a strict sense and they argue that they could be considered as \u201csoftware-assisted human workers\". According to BIBREF1, the online suspicious accounts can be categorized into 3 main types: Robots, Cyborgs, and Human Spammers. We consider IRA accounts as another new emerging type called trolls, which is similar to Cyborgs except that the former focuses on targeting communities instead of individuals.", "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).", "Given $V_i$ as the concatenation of the previous features vectors of a tweet$_i$, we represent each user's tweets by considering the average and standard deviation of her tweets' $V_{1,2,..N}$ in each theme $j$ independently and we concatenate them. Mathematically, a user $x$ final feature vector is defined as follows:"]}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "predicted_answer": "", "predicted_evidence": ["Given that the Emotions features boosted the F1$_{macro}$ with the highest value comparing to the other theme-based features, in Figure FIGREF34 we analyze IRA trolls from emotional perspective to answer RQ3. The analysis shows that the themes that were used to attack immigrants (Black People and Islam and War) have the fear emotion in their top two emotions. While on the other hand, a theme like Supporting Trump has a less amount of fear emotion, and the joy emotion among the top emotions.", "A previous work BIBREF0 showed that such suspicious accounts are not bots in a strict sense and they argue that they could be considered as \u201csoftware-assisted human workers\". According to BIBREF1, the online suspicious accounts can be categorized into 3 main types: Robots, Cyborgs, and Human Spammers. We consider IRA accounts as another new emerging type called trolls, which is similar to Cyborgs except that the former focuses on targeting communities instead of individuals.", "Linguistic Analysis. We measure statistically significant differences in the cues markers of Morality, LIWC, Bias and Subjectivity, Stance, and Bad and Sexual words across IRA trolls and regular users. These findings presented in Table TABREF38 allows for a deeper understanding of IRA trolls.", "In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes.", "After the 2016 US elections, Twitter has detected a suspicious attempt by a large set of accounts to influence the results of the elections. Due to this event, an emerging research works about the Russian troll accounts started to appear BIBREF2, BIBREF3, BIBREF0, BIBREF4, BIBREF5."]}
{"question_id": "1f011fa772ce802e74eda89f706cdb1aa2833686", "predicted_answer": "", "predicted_evidence": ["False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as \u201cPizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.", "In this work, we identify online trolls in Twitter, namely IRA trolls, from a textual perspective. We study the effect of a set of text-based features and we propose a machine learning model to detect them. We aim to answer three research questions: RQ1. Does the thematic information improve the detection performance?, RQ2. Can we detect IRA trolls from only a textual perspective? and RQ3. How IRA campaign utilized the emotions to affect the public opinions?", "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).", "A previous work BIBREF0 showed that such suspicious accounts are not bots in a strict sense and they argue that they could be considered as \u201csoftware-assisted human workers\". According to BIBREF1, the online suspicious accounts can be categorized into 3 main types: Robots, Cyborgs, and Human Spammers. We consider IRA accounts as another new emerging type called trolls, which is similar to Cyborgs except that the former focuses on targeting communities instead of individuals.", "In order to evaluate our feature set, we use Random Selection, Majority Class, and bag-of-words baselines. In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets."]}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "predicted_answer": "", "predicted_evidence": ["DISPLAYFORM0", "DISPLAYFORM0", "The final output of Encoder is a document embedding INLINEFORM0 , derived from LSTM's hidden states INLINEFORM1 of Reader. Given our goal of developing a general-purpose model for embedding documents, we would like INLINEFORM2 to be semantically rich to encode as much key information as possible. To this end, we impose an additional objective on Encoder: the final document embedding needs to be able to reproduce the key words in the document, as illustrated in Figure FIGREF1 .", "The log likelihood objective of the Neural Reader is then given by maximizing the probability of INLINEFORM0 being the set of key sentences, denoted as INLINEFORM1 :", "where INLINEFORM0 is implemented as a softmax function with output dimensionality being the size of the vocabulary."]}
{"question_id": "ab097db03652b8b38edddc074f23e2adf9278cba", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is implemented by a Convolutional Neural Network (CNN) with a max-pooling operation, in a way similar to BIBREF6 . Note that other modeling choices, such as an RNN, are possible as well. We used a CNN here because of its simplicity and high efficiency when running on GPUs. The sentence encoder generates an embedding INLINEFORM1 of 150 dimensions for each sentence.", "DISPLAYFORM0", "To compare embedding methods in academic paper clustering, we calculate F1, V-measure (a conditional entropy-based clustering measure BIBREF11 ), and ARI (Adjusted Rand index BIBREF12 ). As shown in Table TABREF18 , similarly to document retrieval, Paragraph Vector performed better than word2vec averagings in clustering documents, while our KeyVec consistently performed the best among all the compared methods.", "Since the Reader operates in embedding space, we first represent discrete words in each sentence by their word embeddings. The sentence encoder in Reader then derives sentence embeddings from the word representations to capture the semantics of each sentence. After that, a Recurrent Neural Network (RNN) is employed to derive document-level semantics by consolidating constituent sentence embeddings. Finally, we identify key sentences in every document by computing the probability of each sentence being salient.", "The Neural Reader learns to understand the topics of every given input document with paying attention to the salient sentences. It computes a dense representation for each sentence in the given document, and derives its probability of being a salient sentence. The identified set of salient sentences, together with the derived probabilities, will be used by the Neural Encoder to generate a document-level embedding."]}
{"question_id": "5d4190403eb800bb17eec71e979788e11cf74e67", "predicted_answer": "", "predicted_evidence": ["Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.", "In this paper, we introduce KeyVec, a neural network model that learns densely distributed representations for documents of variable-length. In order to capture semantics, the document representations are trained and optimized in a way to recover key information of the documents. In particular, given a document, the KeyVec model constructs a fixed-length vector to be able to predict both salient sentences and key words in the document. In this way, KeyVec conquers the problem of prior embedding models which treat every word and every sentence equally, failing to identify the key information that a document conveys. As a result, the vectorial representations generated by KeyVec can naturally capture the topics of the documents, and thus should yield good performance in downstream tasks.", "In recent years, the use of word representations, such as word2vec BIBREF0 , BIBREF1 and GloVe BIBREF2 , has become a key \u201csecret sauce\u201d for the success of many natural language processing (NLP), information retrieval (IR) and machine learning (ML) tasks. The empirical success of word embeddings raises an interesting research question: Beyond words, can we learn fixed-length distributed representations for pieces of texts? The texts can be of variable-length, ranging from paragraphs to documents. Such document representations play a vital role in a large number of downstream NLP/IR/ML applications, such as text clustering, sentiment analysis, and document retrieval, which treat each piece of text as an instance. Learning a good representation that captures the semantics of each document is thus essential for the success of such applications.", "where INLINEFORM0 is the set of non-key sentences. Intuitively, this likelihood function gives the probability of each sentence in the generated key sentence set INLINEFORM1 being a key sentence, and the rest of sentences being non-key ones.", "Given the embeddings of sentences INLINEFORM0 in a document INLINEFORM1 , Neural Reader computes the probability of each sentence INLINEFORM2 being a key sentence, denoted as INLINEFORM3 ."]}
{"question_id": "56d41e0fcc288c1e65806ae77097d685c83e22db", "predicted_answer": "", "predicted_evidence": ["In the document clustering task, we aim to cluster the academic papers by the venues in which they are published. There are a total of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation. Each academic paper is represented as a vector of 100 dimensions.", "The final output of Encoder is a document embedding INLINEFORM0 , derived from LSTM's hidden states INLINEFORM1 of Reader. Given our goal of developing a general-purpose model for embedding documents, we would like INLINEFORM2 to be semantically rich to encode as much key information as possible. To this end, we impose an additional objective on Encoder: the final document embedding needs to be able to reproduce the key words in the document, as illustrated in Figure FIGREF1 .", "where INLINEFORM0 is implemented as a softmax function with output dimensionality being the size of the vocabulary.", "In this paper, we introduce KeyVec, a neural network model that learns densely distributed representations for documents of variable-length. In order to capture semantics, the document representations are trained and optimized in a way to recover key information of the documents. In particular, given a document, the KeyVec model constructs a fixed-length vector to be able to predict both salient sentences and key words in the document. In this way, KeyVec conquers the problem of prior embedding models which treat every word and every sentence equally, failing to identify the key information that a document conveys. As a result, the vectorial representations generated by KeyVec can naturally capture the topics of the documents, and thus should yield good performance in downstream tasks.", "The goal of the document retrieval task is to decide if a document should be retrieved given a query. In the experiments, our document pool contained 669 academic papers published by IEEE, from which top- INLINEFORM0 relevant papers are retrieved. We created 70 search queries, each composed of the text in a Wikipedia page on a field of study (e.g., https://en.wikipedia.org/wiki/Deep_learning). We retrieved relevant papers based on cosine similarity between document embeddings of 100 dimensions for Wikipedia pages and academic papers. For each query, a good document-embedding model should lead to a list of academic papers in one of the 70 fields of study."]}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "predicted_answer": "", "predicted_evidence": ["In this paper, we introduce KeyVec, a neural network model that learns densely distributed representations for documents of variable-length. In order to capture semantics, the document representations are trained and optimized in a way to recover key information of the documents. In particular, given a document, the KeyVec model constructs a fixed-length vector to be able to predict both salient sentences and key words in the document. In this way, KeyVec conquers the problem of prior embedding models which treat every word and every sentence equally, failing to identify the key information that a document conveys. As a result, the vectorial representations generated by KeyVec can naturally capture the topics of the documents, and thus should yield good performance in downstream tasks.", "Since the Reader operates in embedding space, we first represent discrete words in each sentence by their word embeddings. The sentence encoder in Reader then derives sentence embeddings from the word representations to capture the semantics of each sentence. After that, a Recurrent Neural Network (RNN) is employed to derive document-level semantics by consolidating constituent sentence embeddings. Finally, we identify key sentences in every document by computing the probability of each sentence being salient.", "To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.", "In recent years, the use of word representations, such as word2vec BIBREF0 , BIBREF1 and GloVe BIBREF2 , has become a key \u201csecret sauce\u201d for the success of many natural language processing (NLP), information retrieval (IR) and machine learning (ML) tasks. The empirical success of word embeddings raises an interesting research question: Beyond words, can we learn fixed-length distributed representations for pieces of texts? The texts can be of variable-length, ranging from paragraphs to documents. Such document representations play a vital role in a large number of downstream NLP/IR/ML applications, such as text clustering, sentiment analysis, and document retrieval, which treat each piece of text as an instance. Learning a good representation that captures the semantics of each document is thus essential for the success of such applications.", "To compare embedding methods in academic paper clustering, we calculate F1, V-measure (a conditional entropy-based clustering measure BIBREF11 ), and ARI (Adjusted Rand index BIBREF12 ). As shown in Table TABREF18 , similarly to document retrieval, Paragraph Vector performed better than word2vec averagings in clustering documents, while our KeyVec consistently performed the best among all the compared methods."]}
{"question_id": "31cba86bc45970337ba035ecf36d8954a9a5206a", "predicted_answer": "", "predicted_evidence": ["The White Paper received considerable attention in Icelandic media and its results were discussed in the Icelandic Parliament. In 2014, the Parliament unanimously accepted a resolution where the Minister of Education, Science and Culture was given mandate to appoint an expert group which should come up with a long-term LT plan for Icelandic. The group delivered its report to the Minister in December 2014. The result was that a small LT Fund was established in 2015.", "S\u00cdM has created teams across the member organisations, each taking charge of a core project and/or defined subtasks. This way the best use of resources is ensured, since the team building is not restricted to one organisation per project. One project manager coordinates the work and handles communication and reporting to Almannar\u00f3mur and the expert panel.", "During the last years, a strong centre for speech technology has been established at RU, where development in speech recognition and synthesis has been ongoing since 2011. Acoustic data for speech recognition was collected and curated at RU BIBREF12, BIBREF13, BIBREF14 and a baseline speech recognition system for Icelandic was developed BIBREF15. Specialised speech recognisers have also been developed at RU for the National University Hospital and Althingi BIBREF16, BIBREF17, BIBREF18. A work on a baseline speech synthesis system for Icelandic has also been carried out at RU BIBREF19, BIBREF20.", "The AMI has recently compiled an English-Icelandic parallel corpus, ParIce, the first parallel corpus built for the purposes of MT research and development for Icelandic BIBREF38. The primary goal of the compilation of ParIce was to build a corpus large enough and of good enough quality for training useful MT systems. ParIce currently consists of 39 million Icelandic words in 3.5 million segment pairs. The largest parts of ParIce consists of film and TV subtitles from the Opus corpus BIBREF39, and texts from the European Medicines Agency document portal, included in the Tilde MODEL corpus BIBREF40.", "Parallel data. Icelandic's rich morphology and relatively free word order is likely to demand large amount of training data in order to develop MT systems that produce adequate and fluent translations. The ParIce corpus currently consists of only 3.5 million sentence pairs which is rather small in relation to parallel corpora in general. The goal of this phase is to create an aligned and filtered parallel corpus of translated documents from the European Economic Area (EEA) domain (e.g. regulations and directives). As of 2017, around 7,000 documents were available in Icelandic with corresponding documents in English. The aim is to pair all accessible documents in the course of the project."]}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "predicted_answer": "", "predicted_evidence": ["The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8.", "The White Paper received considerable attention in Icelandic media and its results were discussed in the Icelandic Parliament. In 2014, the Parliament unanimously accepted a resolution where the Minister of Education, Science and Culture was given mandate to appoint an expert group which should come up with a long-term LT plan for Icelandic. The group delivered its report to the Minister in December 2014. The result was that a small LT Fund was established in 2015.", "The general targets of the STEVIN programme were reached to a large extent. According to a report on the results of the programme BIBREF3, it resulted in a network with strong ties between academia and industry, beneficial for future utilisation of the STEVIN results. The evaluators of the programme qualified it as successful, but had recommendations for a future programme, if initiated. They suggested more interaction with other similar (inter)national R&D programmes, asserted that the complexity of IPR issues had been seriously underestimated and called for a better clarification of the role of open-source. The total cost of the STEVIN programme was over 10 million euros, of which well over 80% was spent on R&D projects.", "After the LT Programme ended in 2004, researchers from three institutions, UI, RU, and the \u00c1rni Magn\u00fasson Institute for Icelandic Studies (AMI), joined forces in a consortium called the Icelandic Centre for Language Technology (ICLT), in order to follow up on the tasks of the Programme. In the following years, these researchers developed a few more tools and resources with support from The Icelandic Research Fund, notably a rule-based tagger, a shallow parser, a lemmatiser, and a historical treebank BIBREF9.", "Iceland was an active participant in the META-NORD project, a subproject of META-NET, from 2011 to 2013. Within that project, a number of language resources for Icelandic were collected, enhanced, and made available, both through META-SHARE and through a local website, m\u00e1lf\u00f6ng.is (m\u00e1lf\u00f6ng being a neologism for `language resources'). Among the main deliveries of META-NET were the Language White Papers BIBREF10. The paper on Icelandic BIBREF11 highlighted the alarming status of Icelandic LT. Icelandic was among four languages that received the lowest score, \u201csupport is weak or non-existent\u201d in all four areas that were evaluated."]}
{"question_id": "b59f3a58939f7ac007d3263a459c56ebefc4b49a", "predicted_answer": "", "predicted_evidence": ["Tagged corpora. The IGC BIBREF21 contains 1.3 billion running words, tagged and lemmatised. It is much bigger than previous tagged corpora, most notably the Icelandic Frequency Dictionary (IFD; Pind et al., 1991), which was the first morphologically tagged corpus of Icelandic texts, containing half a million words tokens from various texts, and the Tagged Icelandic Corpus (M\u00cdM; Helgad\u00f3ttir et al,. 2012), a balanced corpus of texts from the first decade of the 21st century, containing around 25 million tokens. A gold standard tagged corpus was created from a subset of M\u00cdM BIBREF23. Some revisions of the morphosyntactic tagset used for tagging Icelandic texts will be done in the programme, and the gold standard updated accordingly.", "However, LT is highly language dependent and it takes considerable resources to develop LT for new languages. The recent LT development has focused on languages that have both a large number of speakers and huge amounts of digitized language resources, like English, German, Spanish, Japanese, etc. Other languages, that have few speakers and/or lack digitized language resources, run the risk of being left behind.", "The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8.", "We have described a five-year, national LT programme for Icelandic. The goal is to make Icelandic useable in communication and interactions in the digital world. Further, to establish graduate and post-graduate education in LT in Iceland to enable the building of strong knowledge centres in LT in the country.", "Back-translation. In order to augment the training data, back-translated texts will be used. Monolingual Icelandic texts will be selected and translated to English with one of the baseline system (see below). By doing so, more training data can be obtained for the en$\\rightarrow $is direction. An important part of using back-translated texts during training is filtering out translations that may otherwise lead to poor quality of the augmented part."]}
{"question_id": "b4b7333805cb6fdde44907747887a971422dc298", "predicted_answer": "", "predicted_evidence": ["The AMI has recently compiled an English-Icelandic parallel corpus, ParIce, the first parallel corpus built for the purposes of MT research and development for Icelandic BIBREF38. The primary goal of the compilation of ParIce was to build a corpus large enough and of good enough quality for training useful MT systems. ParIce currently consists of 39 million Icelandic words in 3.5 million segment pairs. The largest parts of ParIce consists of film and TV subtitles from the Opus corpus BIBREF39, and texts from the European Medicines Agency document portal, included in the Tilde MODEL corpus BIBREF40.", "The Icelandic Ministry of Education, Science and Culture signed an agreement with Almannar\u00f3mur in August 2018, giving Almannar\u00f3mur officially the function of organising the execution of the LT programme for Icelandic. Following a European Tender published in March 2019, Almannar\u00f3mur decided to make an agreement with a consortium of universities, institutions, associations, and private companies (nine in total) in Iceland (listed in Table TABREF6) to perform the research and development part of the programme. This Consortium for Icelandic LT (Samstarf um \u00edslenska m\u00e1lt\u00e6kni \u2013 S\u00cdM) is a joint effort of LT experts in Iceland from academia and industry. S\u00cdM is not a legal entity but builds the cooperation on a consortium agreement signed by all members.", "Pre- and postprocessing. Preprocessing in MT is the task of changing the training corpus/source text in some manner for the purpose of making the translation task easier or mark particular words/phrases that should not be translated. Postprocessing is then the task of restoring the generated target language to its normal form. An example of pre- and postprocessing in our project is the handling of named entities (NEs). NEs are found and matched within source and target sentence pairs in the training corpus, and replaced by placeholders with information about case and singular/plural number. NE-to-placeholder substitution is implemented in the input and placeholder-to-NE substitution in the output pipelines of the translation system.", "Lexicon acquisition tool. When constructing and maintaining lexical databases, such as DIM, the Icelandic wordnet or other related resources, it is vital to be able to systematically add neologies and words that are missing from the datasets, especially those commonly used in the language. Within the LT programme a flexible lexicon acquisition tool will be developed. It will be able to identify and collect unknown words and word forms, together with statistics, through structured lexical acquisition from the Icelandic Gigaword Corpus, which is constantly being updated, and other data sources in the same format.", "MT interface. An API and a web user interface for the three baseline systems, mentioned in item 3 above, will be developed to give interested parties access to the systems under development, and to establish a testing environment in which members of the public can submit their own text. Thus, results from the three systems can be compared directly, as well as to the translations produced by Google Translate. Moreover, in this part, a crowd-sourcing mechanism will be developed, i.e. a functionality to allow users to submit improved translations back to the system for inclusion in the training corpus."]}
{"question_id": "871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "predicted_answer": "", "predicted_evidence": ["Back-translation. In order to augment the training data, back-translated texts will be used. Monolingual Icelandic texts will be selected and translated to English with one of the baseline system (see below). By doing so, more training data can be obtained for the en$\\rightarrow $is direction. An important part of using back-translated texts during training is filtering out translations that may otherwise lead to poor quality of the augmented part.", "The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8.", "S\u00cdM has created teams across the member organisations, each taking charge of a core project and/or defined subtasks. This way the best use of resources is ensured, since the team building is not restricted to one organisation per project. One project manager coordinates the work and handles communication and reporting to Almannar\u00f3mur and the expert panel.", "The IceNLP and Greynir parsers will be evaluated and either one of them or both developed further. We will also adapt a UD-parser to Icelandic UD-grammar.", "Our MT project in the new LT programme consists of the following sub-parts:"]}
{"question_id": "3fafde90eebc1c00ba6c3fb4c5b984009393ce7f", "predicted_answer": "", "predicted_evidence": ["Modern society is an information society bombarded from all sides by an increasing number of different pieces of information. The 21st century has brought us the rapid development of media, especially in the internet ecosystem. This change has caused the transfer of many areas of our lives to virtual reality. New forms of communication have been established. Their development has created the need for analysis of related data. Nowadays, unstructured information is available in digital form, but how can we analyse and summarise billions of newly created texts that appear daily on the internet? Natural language analysis techniques, statistics and machine learning have emerged as tools to help us. In recent years, particular attention has focused on sentiment analysis. This area is defined as the study of opinions expressed by people as well as attitudes and emotions about a particular topic, product, event, or person. Sentiment analysis determines the polarisation of the text. It answers the question as to whether a particular text is a positive, negative, or neutral one.", "This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively.", "In this paper, we describe briefly the whole workflow and present a prototype implementation. Currently, existing solutions for sentiment annotation offer mostly analysis on the level of entire documents, and if you go deeper to the level of individual product features, they are only superficial and poorly prepared for the analysis of large volumes of data. This can especially be seen in scientific articles where the analysis is carried out on a few hundred reviews only. It is worth mentioning that this task is extremely problematic because of the huge diversity of languages and the difficulty of building a single solution that can cover all the languages used in the world. Natural language analysis often requires additional pre-processing steps, especially at the stage of preparing the data for analysis, and steps specific for each language. Large differences can be seen in the analysis of the Polish language (a highly inflected language) and English (a grammatically simpler one). We propose a solution that will cover several languages, however in this prototype implementation we focused on English texts only.", "In this paper, we present analysis and workflow inspired by the work of Joty, Carenini and Ng BIBREF0 . We experimented with several methods in order to validate aspect-based sentiment analysis approaches and in the next steps we want to customise our implementation for the Polish language.", "The overall characteristics and flow organisation can be seen in Figure FIGREF9 . Each of the mentioned steps of the proposed method is described in the following subsections."]}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "predicted_answer": "", "predicted_evidence": ["For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level.", "This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively.", "A sentiment analysis can be made at the level of (1) the whole document, (2) the individual sentences, or (what is currently seen as the most attractive approach) (3) at the level of individual fragments of text. Regarding document level analysis BIBREF8 , BIBREF9 - the task at this level is to classify whether a full opinion expresses a positive, negative or neutral attitude. For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions.", "Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion).", "Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets."]}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "predicted_answer": "", "predicted_evidence": ["The analysis is performed separately for each source document, and as the output we get Discourse Trees (DT) such as in Figure FIGREF3 . At this stage, existing discourse parsers will model the structure and the labels of a DT separately. They do not take into account the sequential dependencies between the DT constituents. Then existing discourse parsers will apply greedy and sub-optimal parsing algorithms and build a Discourse Tree. During this stage, and to cope with the mentioned limitation The inferred (posterior) probabilities can be used from CRF parsing models in a probabilistic CKY-like bottom-up parsing algorithm BIBREF14 which is non-greedy and optimal. Finally, discourse parsers do not discriminate between intra-sentential parsing (i.e., building the DTs for individual sentences) and multi-sentential parsing (i.e., building a DT for the whole document) BIBREF0 . Hence, this part of the analysis will extract for us distributed information about the relationship between different EDUs from parsed texts.", "In this paper, we describe briefly the whole workflow and present a prototype implementation. Currently, existing solutions for sentiment annotation offer mostly analysis on the level of entire documents, and if you go deeper to the level of individual product features, they are only superficial and poorly prepared for the analysis of large volumes of data. This can especially be seen in scientific articles where the analysis is carried out on a few hundred reviews only. It is worth mentioning that this task is extremely problematic because of the huge diversity of languages and the difficulty of building a single solution that can cover all the languages used in the world. Natural language analysis often requires additional pre-processing steps, especially at the stage of preparing the data for analysis, and steps specific for each language. Large differences can be seen in the analysis of the Polish language (a highly inflected language) and English (a grammatically simpler one). We propose a solution that will cover several languages, however in this prototype implementation we focused on English texts only.", "The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level.", "A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information. In the literature, there exists some initial work related to aspects. There exist initial solutions that use SVM-based algorithms BIBREF12 or conditional random field classifiers BIBREF13 with manually engineered features. There also exist some solutions based on deep neural networks, such as connecting sentiments with the corresponding aspects based on the constituency parse tree BIBREF11 .", "We have proposed a comprehensive flow of analysing aspects and assigning sentiment orientation to them. The advantages of such an analysis are that: it is a grammatically-based and coherent solution, it shows opinion distribution, it doesn't need any aspect ontology, it is not limited to the number of aspects and really important, it doesn't need training data (unsupervised method). The method proved it has a big potential in generating summary overviews for aspect and sentiment distribution across analysed documents. In our next steps, we want to improve the aspect extraction phase, probably using neural network approaches. Moreover, we want to expand the analysis of the Polish language."]}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_answer": "", "predicted_evidence": ["Discourse Trees of individual documents are processed (the order of EDU is not changed) to form association rules. Then, an Aspect-Rhetorical Relation Graph based on a set of these rules is created. Each node represents an aspect and each edge is one of the relations between the EDU\u2019s aspects. A graph will be created for all documents used in the experiment. The graph can be represented with weighted edges (association rules confidence, a number of such relations in the whole graph etc.), but there is a need to check and compare different types of graph representations. Then, it is possible to characterise the whole graph and each node (aspect) with graph metrics (PageRank BIBREF16 , degree, betweenness or other metrics). These metrics will be used for estimating the cut threshold \u2013 removing uninformative or redundant aspects. Hence, we will end up with only the most important aspects derived from analysed corpora. Then the graph will be transformed into an Aspect Hierarchical Tree.", "However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information.", "The goal of discourse analysis in our method is the segmentation of the text for the basic units of discourse structures EDU (Elementary Discourse Units) and connecting them to determine semantic relations. The analysis is performed separately for each source document, and as the output we get Discourse Trees (DT) such as in Figure FIGREF3 . At this stage, existing discourse parsers will model the structure and the labels of a DT separately. They do not take into account the sequential dependencies between the DT constituents. Then existing discourse parsers will apply greedy and sub-optimal parsing algorithms and build a Discourse Tree. During this stage, and to cope with the mentioned limitation The inferred (posterior) probabilities can be used from CRF parsing models in a probabilistic CKY-like bottom-up parsing algorithm BIBREF14 which is non-greedy and optimal.", "However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only.", "Modern society is an information society bombarded from all sides by an increasing number of different pieces of information. The 21st century has brought us the rapid development of media, especially in the internet ecosystem. This change has caused the transfer of many areas of our lives to virtual reality. New forms of communication have been established. Their development has created the need for analysis of related data. Nowadays, unstructured information is available in digital form, but how can we analyse and summarise billions of newly created texts that appear daily on the internet? Natural language analysis techniques, statistics and machine learning have emerged as tools to help us. In recent years, particular attention has focused on sentiment analysis. This area is defined as the study of opinions expressed by people as well as attitudes and emotions about a particular topic, product, event, or person. Sentiment analysis determines the polarisation of the text. It answers the question as to whether a particular text is a positive, negative, or neutral one."]}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "predicted_answer": "", "predicted_evidence": ["Focusing here on Wikipedia concepts, we adopt as an initial \u201cground truth\u201d the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called \u201cedit wars\u201d. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset.", "Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.", "Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.", "To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the \u201cgeneral opinion\u201d about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively \u2013 from the rich edit history thereof.", "The definition of which concepts are controversial is controversial by itself; an accurate definition of this elusive notion attracted the attention of researchers from various fields, see for example some recent attempts in BIBREF0, BIBREF1, BIBREF2."]}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "predicted_answer": "", "predicted_evidence": ["Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.", "Content analysis of controversial Wikipedia articles has been used to evaluate the level of controversy of other documents (e.g., web pages) by mapping them to related Wikipedia articles BIBREF5. BIBREF6 further build a language model, which enhances predictions made by existing classifiers, by inferring word probabilities from Wikipedia articles prominent in Wikipedia controversy features (mainly signals in edit history as discussed above) and from articles retrieved by manually selected query terms, believed to indicate controversy.", "In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned.", "To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the \u201cgeneral opinion\u201d about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively \u2013 from the rich edit history thereof.", "In a preliminary task, we looked for words which may designate sentences associated with controversial concepts. To this end, we ranked the words appearing in positive sentences according to their information gain for this task. The top of the list comprises the following: that, sexual, people, movement, religious, issues, rights."]}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "predicted_answer": "", "predicted_evidence": ["Focusing here on Wikipedia concepts, we adopt as an initial \u201cground truth\u201d the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called \u201cedit wars\u201d. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset.", "Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.", "Content analysis of controversial Wikipedia articles has been used to evaluate the level of controversy of other documents (e.g., web pages) by mapping them to related Wikipedia articles BIBREF5. BIBREF6 further build a language model, which enhances predictions made by existing classifiers, by inferring word probabilities from Wikipedia articles prominent in Wikipedia controversy features (mainly signals in edit history as discussed above) and from articles retrieved by manually selected query terms, believed to indicate controversy.", "Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.", "Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I."]}
{"question_id": "376e8ed6e039e07c892c77b7525778178d56acb7", "predicted_answer": "", "predicted_evidence": ["Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.", "BIBREF7 detect controversy in news items by inspecting terms with excessive frequency in contexts containing sentiment words, and BIBREF8 study controversy in user comments of news articles using lexicons. Finally, BIBREF9 suggest that controversy is not a universal but rather a community-related concept, and, therefore, should be studied in context.", "We consider three datasets, two of which are a contribution of this work.", "The definition of which concepts are controversial is controversial by itself; an accurate definition of this elusive notion attracted the attention of researchers from various fields, see for example some recent attempts in BIBREF0, BIBREF1, BIBREF2.", "Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this."]}
{"question_id": "4de6bcddd46726bf58326304b0490fdb9e7e86ec", "predicted_answer": "", "predicted_evidence": ["Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.", "The definition of which concepts are controversial is controversial by itself; an accurate definition of this elusive notion attracted the attention of researchers from various fields, see for example some recent attempts in BIBREF0, BIBREF1, BIBREF2.", "We consider three datasets, two of which are a contribution of this work.", "Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote.", "Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores."]}
{"question_id": "e831ce6c406bf5d1c493162732e1b320abb71b6f", "predicted_answer": "", "predicted_evidence": ["In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned.", "Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.", "Most people would agree, for example, that Global warming is a controversial concept, whereas Summer is not. However, the concept Pollution may be seen as neutral by some, yet controversial by others, who associate it with environmental debates. In other words, different people may have different opinions, potentially driven by different contexts salient in their mind. Yet, as reported in the sequel, an appreciable level of agreement can be reached, even without explicit context.", "Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data \u2013 the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.", "Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$."]}
{"question_id": "634a071b13eb7139e77872ecfdc135a2eb2f89da", "predicted_answer": "", "predicted_evidence": ["The definition of which concepts are controversial is controversial by itself; an accurate definition of this elusive notion attracted the attention of researchers from various fields, see for example some recent attempts in BIBREF0, BIBREF1, BIBREF2.", "Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote.", "Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.", "We first examined the estimators in $k$-fold cross-validation scheme on the datasets I and II with $k=10$: the set of positive (controversial) concepts was split into 10 equal size sets, and the corresponding sentences were split accordingly. Each set was matched with similarly sized sets of negative (non-controversial) concepts and corresponding sentences. For each fold, a model was generated from the training sentences and used to score the test concepts. Scores were converted into a binary classification, as described in SECREF3, and accuracy was computed accordingly. Finally, the accuracy over the $k$ folds was averaged.", "Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data \u2013 the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$."]}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "predicted_answer": "", "predicted_evidence": ["Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.", "To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the \u201cgeneral opinion\u201d about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively \u2013 from the rich edit history thereof.", "In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned.", "BIBREF7 detect controversy in news items by inspecting terms with excessive frequency in contexts containing sentiment words, and BIBREF8 study controversy in user comments of news articles using lexicons. Finally, BIBREF9 suggest that controversy is not a universal but rather a community-related concept, and, therefore, should be studied in context.", "Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages."]}
{"question_id": "267d70d9f3339c56831ea150d2213643fbc5129b", "predicted_answer": "", "predicted_evidence": ["Through the great research progress with deep neural networks (DNNs), the combination of a convolutional neural network and a recurrent neural network (CNN+RNN) is a successful model for both feature extraction and sequential processing BIBREF1 . Although there is no clear division, a CNN is often used for image processing, whereas an RNN is used for text processing. Moreover, these two domains are integrated. One successful application is image caption generation with CNN+LSTM (CNN+Long-Short Term Memory) BIBREF2 . This technique enables text to be automatically generated from an image input. However, we believe that image captions require human intuition and emotion. In the present paper, we help to guide an image caption has funny expression. In the following, we introduce related research on humorous image caption generation.", "In the experimental section, we compare the proposed method based on Funny Score and BoketeDB pre-trained parameters with a baseline provided by MS COCO Pre-trained CNN+LSTM. We also compare the results of the NJM with funny captions provided by humans. In an evaluation by humans, the results provided by the proposed method were ranked lower than those provided by humans (22.59% vs. 67.99%) but were ranked higher than the baseline (9.41%). Finally, we show the generated funny captions for several images.", "We conducted evaluations to confirm the effectiveness of the proposed method. We describe the experimental method in Section SECREF11 , and the experimental results are presented in Section SECREF12 .", "Comparison with MS COCO BIBREF5 . MS COCO contains a correspondence for each of 160,000 images to one of five types of captions. In comparison with MS COCO, BoketeDB has approximately half the number of the images and 124% the number of captions.", "Laughter is a special, higher-order function that only humans possess. In the analysis of laughter, as Wikipedia says, \u201cLaughter is thought to be a shift of composition (schema)\", and laughter frequently occurs when there is a change from a composition of receiver. However, the viewpoint of laughter differs greatly depending on the position of the receiver. Therefore, the quantitative measurement of laughter is very difficult. Image Ogiri on web services such as \"Bokete\" BIBREF0 have recently appeared, where users post funny captions for thematic images and the captions are evaluated in an SNS-like environment. Users compete to obtain the greatest number of \u201cstars\u201d. Although quantification of laughter is considered to be a very difficult task, the correspondence between evaluations and images on Bokete allows us to treat laughter quantitatively. Image captioning is an active topic in computer vision, and we believe that humorous image captioning can be realized. The main contributions of the present paper are as follows:"]}
{"question_id": "477da8d997ff87400c6aad19dcc74f8998bc89c3", "predicted_answer": "", "predicted_evidence": ["In this subsection, we present the experimental results along with a discussion. Table TABREF10 shows the experimental results of the questionnaire. A total of 16 personal questionnaires were completed. Table TABREF10 shows the percentages of captions of each rank for each method of caption generation considered herein. Captions generated by humans were ranked \u201cfunniest\u201d 67.99% of the time, followed by the NJM at 22.59%. The baseline captions, STAIR caption, were ranked \u201cfunniest\u201d 9.41% of the time. These results suggest that captions generated by the NJM are less funny than those generated by humans. However, the NJM is ranked much higher than STAIR caption.", "We are currently posting funny captions generated by the NJM to the Bokete Ogiri website in order to evaluate the proposed method. Here, we compare the proposed method with STAIR captions. As reported by Bokete users, the funny captions generated by STAIR caption averaged 1.71 stars, whereas the NJM averaged 3.23 stars. Thus, the NJM is funnier than the baseline STAIR caption according to Bokete users. We believe that this difference is the result of using (i) Funny Score to effectively train the generator regarding funny captions and (ii) the self-collected BoketeDB, which is a large-scale database for funny captions.", "We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.", "In the present paper, we proposed a method by which to generate captions that draw laughter. We built the BoketeDB, which contains pairs comprising a theme (image) and a corresponding funny caption (text) posted on the Bokete Ogiri website. We effectively trained a funny caption generator with the proposed Funny Score by weight evaluation. Although we adopted CNN+LSTM as a baseline, we have been exploring an effective scoring function and database construction. The experiments of the present study suggested that the NJM was much funnier than the baseline STAIR caption.", "The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch."]}
{"question_id": "4485e32052741972877375667901f61e602ec4de", "predicted_answer": "", "predicted_evidence": ["The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.", "We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.", "We compare the proposed method with two other methods of generating funny captions: 1) human generated captions, which are highly ranked on Bokete (indicated by \u201cHuman\" in Table TABREF10 ), and 2) Japanese image caption generation using CNN+LSTM pre-trained by STAIR caption BIBREF7 . Based on the captions provided by MS COCO, the STAIR caption is translated from English to Japanese (indicated by \u201cSTAIR caption\u201d in Table TABREF10 ). We use a questionnaire as the evaluation method. We selected a total of 30 themes from the Bokete Ogiri website that included \u201cpeople\u201d, \u201ctwo or more people\u201d, \u201canimals\u201d, \u201clandscape\u201d, \u201cinorganics\u201d, and \u201cillustrations\u201d. The questionnaire asks respondents to rank the captions provided by humans, the NJM, and STAIR caption in order of \u201cfunniness\u201d. The questionnaire does not reveal the origins of the captions.", "Laughter is a special, higher-order function that only humans possess. In the analysis of laughter, as Wikipedia says, \u201cLaughter is thought to be a shift of composition (schema)\", and laughter frequently occurs when there is a change from a composition of receiver. However, the viewpoint of laughter differs greatly depending on the position of the receiver. Therefore, the quantitative measurement of laughter is very difficult. Image Ogiri on web services such as \"Bokete\" BIBREF0 have recently appeared, where users post funny captions for thematic images and the captions are evaluated in an SNS-like environment. Users compete to obtain the greatest number of \u201cstars\u201d. Although quantification of laughter is considered to be a very difficult task, the correspondence between evaluations and images on Bokete allows us to treat laughter quantitatively. Image captioning is an active topic in computer vision, and we believe that humorous image captioning can be realized. The main contributions of the present paper are as follows:", "BoketeDB"]}
{"question_id": "df4895c6ae426006e75c511a304d56d37c42a1c7", "predicted_answer": "", "predicted_evidence": ["Finally, we present the visual results in Figure FIGREF14 , which includes examples of funny captions obtained using NJM. Although the original caption is in Japanese, we also translated the captions into English. Enjoy!", "Comparison with MS COCO BIBREF5 . MS COCO contains a correspondence for each of 160,000 images to one of five types of captions. In comparison with MS COCO, BoketeDB has approximately half the number of the images and 124% the number of captions.", "In the present paper, we proposed a method by which to generate captions that draw laughter. We built the BoketeDB, which contains pairs comprising a theme (image) and a corresponding funny caption (text) posted on the Bokete Ogiri website. We effectively trained a funny caption generator with the proposed Funny Score by weight evaluation. Although we adopted CNN+LSTM as a baseline, we have been exploring an effective scoring function and database construction. The experiments of the present study suggested that the NJM was much funnier than the baseline STAIR caption.", "In the experimental section, we compare the proposed method based on Funny Score and BoketeDB pre-trained parameters with a baseline provided by MS COCO Pre-trained CNN+LSTM. We also compare the results of the NJM with funny captions provided by humans. In an evaluation by humans, the results provided by the proposed method were ranked lower than those provided by humans (22.59% vs. 67.99%) but were ranked higher than the baseline (9.41%). Finally, we show the generated funny captions for several images.", "We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one."]}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "predicted_answer": "", "predicted_evidence": ["Calculating thus the values of INLINEFORM0 for all reference corrections, INLINEFORM1 for all feature values, INLINEFORM2 for all the INLINEFORM3 features in INLINEFORM4 , we are in a position to calculate the RHS of ( EQREF9 ). When this trained classifier is given an erroneous text, features are extracted from this text and the repair works by replacing the erroneous word by a correction that maximizes ( EQREF9 ), INLINEFORM5", "Speech-enabled natural-language question-answering interfaces to enterprise application systems, such as Incident-logging systems, Customer-support systems, Marketing-opportunities systems, Sales data systems etc., are designed to allow end-users to speak-out the problems/questions that they encounter and get automatic responses. The process of converting human spoken speech into text is performed by an Automatic Speech Recognition (ASR) engine. While functional examples of ASR with enterprise systems can be seen in day-to-day use, most of these work under constraints of a limited domain, and/or use of additional domain-specific cues to enhance the speech-to-text conversion process. Prior speech-and-natural language interfaces for such purposes have been rather restricted to either Interactive Voice Recognition (IVR) technology, or have focused on building a very specialized speech engine with domain specific terminology that recognizes key-words in that domain through an extensively customized language model, and trigger specific tasks in the enterprise application system.", "We downloaded this survey data and hand crafted a total of 293 textual questions BIBREF13 which could answer the survey data. A set of 6 people (L2 English) generated 50 queries each with the only constraint that these queries should be able to answer the survey data. In all a set of 300 queries were crafted of which duplicate queries were removed to leave 293 queries in all. Of these, we chose 250 queries randomly and distributed among 5 Indian speakers, who were asked to read aloud the queries into a custom-built audio data collecting application. So, in all we had access to 250 audio queries spoken by 5 different Indian speakers; each speaking 50 queries.", "For all our analysis, we used only those utterances that had an accuracy 70% but less that INLINEFORM0 , namely, 486 instances (see Table TABREF14 , Figure FIGREF13 ). An example showing the same utterance being recognized by four different ASR engines is shown in Figure FIGREF12 . Note that we used INLINEFORM1 corresponding to Ga, Ki and Ku in our analysis (accuracy INLINEFORM2 ) and not INLINEFORM3 corresponding to Ps which has an accuracy of INLINEFORM4 only. This is based on our observation that any ASR output that is lower that INLINEFORM5 accurate is so erroneous that it is not possible to adapt and steer it towards the expected output.", "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors."]}
{"question_id": "54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "predicted_answer": "", "predicted_evidence": ["In case of the other ASR engines, namely, Kaldi with US acoustic models (Ku), Kaldi with Indian Acoustic models (Ki) and PocketSphinx ASR (Ps) we first took the queries corresponding to the 250 utterances and built a statistical language model (SLM) and a lexicon using the scripts that are available with PocketSphinx BIBREF14 and Kaldi BIBREF15 . This language model and lexicon was used with the acoustic model that were readily available with Kaldi and Ps. In case of Ku we used the American English acoustic models, while in case of Ki we used the Indian English acoustic model. In case of Ps we used the Voxforge acoustic models BIBREF16 . Each utterance was passed through Kaldi ASR for two different acoustic models to get INLINEFORM0 corresponding to Ku and Ki. Similarly all the 250 audio utterance were passed through the Ps ASR to get the corresponding INLINEFORM1 for Ps.", "Note that the accuracy is computed as the number of deletions, insertions, substitutions that are required to convert the ASR output to the textual reference (namely, INLINEFORM0 ) and is a common metric used in speech literature BIBREF17 .", "Figure FIGREF11 and Table TABREF14 capture the performance of the different speech recognition engines. The performance of the ASR engines varied, with Ki performing the best with 127 of the 250 utterances being correctly recognized while Ps returned only 44 correctly recognized utterances (see Table TABREF14 , Column 4 named \"Correct\") of 250 utterances. The accuracy of the ASR varied widely. For instance, in case of Ps there were as many as 97 instances of the 206 erroneously recognized utterances which had an accuracy of less than 70%.", "In our experiment, we used a total of 570 misrecognition errors (for example, (dear, beer) and (have, has) derived from INLINEFORM0 or (than twenty, jewelry) derived from INLINEFORM1 ) in the 486 sentences. We performed 10-fold cross validation, each fold containing 513 INLINEFORM2 pairs for training and 57 pairs for testing, Note that we assume the erroneous words in the ASR output being marked by a human oracle, in the training as well as the testing set. Suppose the following example ( INLINEFORM3 ) occurs in the training set: INLINEFORM4 INLINEFORM5", "Clearly, in this case the INLINEFORM0 pair is (than twenty, jewelry)."]}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF11 and Table TABREF14 capture the performance of the different speech recognition engines. The performance of the ASR engines varied, with Ki performing the best with 127 of the 250 utterances being correctly recognized while Ps returned only 44 correctly recognized utterances (see Table TABREF14 , Column 4 named \"Correct\") of 250 utterances. The accuracy of the ASR varied widely. For instance, in case of Ps there were as many as 97 instances of the 206 erroneously recognized utterances which had an accuracy of less than 70%.", "", "The ASR output ( INLINEFORM0 ) are then given as input in the Evo-Devo and Machine Learning mechanism of adaptation.", "We assume the use of a gp-ASR in the rest of the paper. Though we use examples of natural language sentences in the form of queries or questions, it should be noted that the description is applicable to any conversational natural language sentence.", "In our experiment, we used a total of 570 misrecognition errors (for example, (dear, beer) and (have, has) derived from INLINEFORM0 or (than twenty, jewelry) derived from INLINEFORM1 ) in the 486 sentences. We performed 10-fold cross validation, each fold containing 513 INLINEFORM2 pairs for training and 57 pairs for testing, Note that we assume the erroneous words in the ASR output being marked by a human oracle, in the training as well as the testing set. Suppose the following example ( INLINEFORM3 ) occurs in the training set: INLINEFORM4 INLINEFORM5"]}
{"question_id": "99554d0c76fbaef90bce972700fa4c315f961c31", "predicted_answer": "", "predicted_evidence": ["Note that the accuracy is computed as the number of deletions, insertions, substitutions that are required to convert the ASR output to the textual reference (namely, INLINEFORM0 ) and is a common metric used in speech literature BIBREF17 .", "The ASR output ( INLINEFORM0 ) are then given as input in the Evo-Devo and Machine Learning mechanism of adaptation.", "This language model and lexicon was used with the acoustic model that were readily available with Kaldi and Ps. In case of Ku we used the American English acoustic models, while in case of Ki we used the Indian English acoustic model. In case of Ps we used the Voxforge acoustic models BIBREF16 . Each utterance was passed through Kaldi ASR for two different acoustic models to get INLINEFORM0 corresponding to Ku and Ki. Similarly all the 250 audio utterance were passed through the Ps ASR to get the corresponding INLINEFORM1 for Ps. A sample utterance and the output of the four engines is shown in Figure FIGREF12 .", "We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences. Fine-tuning the repair and fitness functions, namely Equation (), would probably yield much better performance accuracies. However, experimental results confirm that the proposed Evo-Devo mechanism is an approach that is able to adapt INLINEFORM0 to get closer to INLINEFORM1 . We present a snapshot of the experiments with Google ASR (Ga) and calculate accuracy with respect to the user spoken question as shown in Table TABREF16 .", "For all our analysis, we used only those utterances that had an accuracy 70% but less that INLINEFORM0 , namely, 486 instances (see Table TABREF14 , Figure FIGREF13 ). An example showing the same utterance being recognized by four different ASR engines is shown in Figure FIGREF12 . Note that we used INLINEFORM1 corresponding to Ga, Ki and Ku in our analysis (accuracy INLINEFORM2 ) and not INLINEFORM3 corresponding to Ps which has an accuracy of INLINEFORM4 only. This is based on our observation that any ASR output that is lower that INLINEFORM5 accurate is so erroneous that it is not possible to adapt and steer it towards the expected output."]}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "predicted_answer": "", "predicted_evidence": ["To determine whether these gains come from the diversity of training languages or just the larger amount of training data, we trained models on the 15 hour subset and the full 81 hours of the English wsj corpus, which corresponds to the amount of data of four GlobalPhone languages. More data does help to some degree, as Figure FIGREF21 shows. But, except for Mandarin, training on just two languages (46 hours) already works better.", "Supervised models trained on these high-resource languages are evaluated on the same set of zero-resource languages as in Section SECREF2 . Transcriptions of the latter are still never used during training.", "By mapping every discovered word token to the ground truth word with which it overlaps most, average cluster purity can be calculated as the total proportion of correctly mapped tokens in all clusters. More than one cluster may be mapped to the same ground truth word type. In a similar way, we can calculate unsupervised word error rate (WER), which uses the same cluster-to-word mapping but also takes insertions and deletions into account. Here we consider two ways to perform the cluster mapping: many-to-one, where more than one cluster can be assigned the same word label (as in purity), or one-to-one, where at most one cluster is mapped to a ground truth word type (accomplished in a greedy fashion). We also compute the gender and speaker purity of the clusters, where we want to see clusters that are as diverse as possible on these measures, i.e., low purity. To explicitly evaluate how accurate the model performs segmentation, we compare the proposed word boundary positions to those from forced alignments of the data (falling within a single true phoneme from the boundary).", "Table TABREF36 compares MFCCs, cae features (with and without vtln) and bnfs as input to the system of BIBREF4 . It shows that both vtln and bnfs help on all metrics, with improvements ranging from small to more substantial and bnfs clearly giving the most benefit. The effects of vtln are mostly confined to reducing both gender and speaker purity of the identified clusters (which is desirable) while maintaining the performance on other metrics. This means that the learned representations have become more invariant to variation in speaker and gender, which is exactly what vtln aims to do. However, this appears to be insufficient to also help other metrics, aligning with the experiments in BIBREF4 that indicate that improvements on the other metrics are hard to obtain.", "We use several metrics to compare the resulting segmented word tokens to ground truth forced alignments of the data. By mapping every discovered word token to the ground truth word with which it overlaps most, average cluster purity can be calculated as the total proportion of correctly mapped tokens in all clusters. More than one cluster may be mapped to the same ground truth word type. In a similar way, we can calculate unsupervised word error rate (WER), which uses the same cluster-to-word mapping but also takes insertions and deletions into account. Here we consider two ways to perform the cluster mapping: many-to-one, where more than one cluster can be assigned the same word label (as in purity), or one-to-one, where at most one cluster is mapped to a ground truth word type (accomplished in a greedy fashion). We also compute the gender and speaker purity of the clusters, where we want to see clusters that are as diverse as possible on these measures, i.e., low purity."]}
{"question_id": "9a52a33d0ae5491c07f125454aea9a41b9babb82", "predicted_answer": "", "predicted_evidence": ["However, in principle it could be possible to use the target language data to fine tune the bnfs in an unsupervised fashion, improving performance further. We explored this possibility by simply training a cae using bnfs as input rather than PLPs. That is, we trained the cae with the same word pairs as before, but replaced VTLN-adapted MFCCs with the 10-lingual bnfs as input features, without any other changes in the training procedure. Table TABREF23 (penultimate row) shows that the cae trained with utd pairs is able to slightly improve on the bnfs in some cases, but this is not consistent across all languages and for Croatian the cae features are much worse. On the other hand, when trained using gold standard pairs (final row), the resulting cae features are consistently better than the input bnfs. This indicates that bnfs can in principle be improved by target-language fine-tuning, but the top-down supervision needs to be of higher quality than the current UTD system provides.", "For multilingual training, we closely follow the existing Kaldi recipe for the Babel corpus. We train a tdnn BIBREF36 with block softmax BIBREF37 , i.e. all hidden layers are shared between languages, but there is a separate output layer for each language. For each training instance only the error at the corresponding language's output layer is used to update the weights. This architecture is illustrated in Figure FIGREF17 . The tdnn has six 625-dimensional hidden layers followed by a 39-dimensional bottleneck layer with ReLU activations and batch normalization. Each language then has its own 625-dimensional affine and a softmax layer. The inputs to the network are 40-dimensional MFCCs with all cepstral coefficients to which we append i-vectors for speaker adaptation. The network is trained with stochastic gradient descent for 2 epochs with an initial learning rate of INLINEFORM0 and a final learning rate of INLINEFORM1 .", "Table TABREF22 shows results on the Xitsonga and Buckeye English corpora. Here we compare ABX error rates computed with the zrsc 2015 BIBREF0 evaluation scripts with ap on the same-different task. To the best of our knowledge, this is the first time such a comparison has been made. The results on both tasks correlate well, especially when looking at the ABX cross-speaker error rate because the same-different evaluation as described in Section SECREF11 also focuses on cross-speaker pairs. As might be expected vtln only improves cross-speaker, but not within-speaker ABX error rates.", "To understand why the features that help with word and phone discrimination are a problem for the UTD system, we examined the similarity plots for several pairs of utterances. Figures FIGREF24 and FIGREF29 show that cae features and bnfs look quite different from PLPs. Dark areas indicate acoustic similarity and diagonal line segments therefore point to phonetically similar sequences. In Figure FIGREF24 both utterances contain the words estados unidos, but shorter and more faint lines can also be seen for rough matches like the last two syllables of servicio and visas. The ZRTools utd toolkit identifies these diagonal lines with fast computer vision techniques BIBREF22 and then runs a segmental-dtw algorithm only in the candidate regions for efficient discovery of matches.", "So far we have shown that multilingual bnfs work better than any of the features trained using only the target language data. However, in principle it could be possible to use the target language data to fine tune the bnfs in an unsupervised fashion, improving performance further. We explored this possibility by simply training a cae using bnfs as input rather than PLPs. That is, we trained the cae with the same word pairs as before, but replaced VTLN-adapted MFCCs with the 10-lingual bnfs as input features, without any other changes in the training procedure. Table TABREF23 (penultimate row) shows that the cae trained with utd pairs is able to slightly improve on the bnfs in some cases, but this is not consistent across all languages and for Croatian the cae features are much worse. On the other hand, when trained using gold standard pairs (final row), the resulting cae features are consistently better than the input bnfs."]}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "predicted_answer": "", "predicted_evidence": ["By mapping every discovered word token to the ground truth word with which it overlaps most, average cluster purity can be calculated as the total proportion of correctly mapped tokens in all clusters. More than one cluster may be mapped to the same ground truth word type. In a similar way, we can calculate unsupervised word error rate (WER), which uses the same cluster-to-word mapping but also takes insertions and deletions into account. Here we consider two ways to perform the cluster mapping: many-to-one, where more than one cluster can be assigned the same word label (as in purity), or one-to-one, where at most one cluster is mapped to a ground truth word type (accomplished in a greedy fashion). We also compute the gender and speaker purity of the clusters, where we want to see clusters that are as diverse as possible on these measures, i.e., low purity. To explicitly evaluate how accurate the model performs segmentation, we compare the proposed word boundary positions to those from forced alignments of the data (falling within a single true phoneme from the boundary).", "For initial monolingual training of asr systems for the high-resource languages, we follow the Kaldi recipes for the GlobalPhone and WSJ corpora and train a sgmm system for each language to get initial context-dependent state alignments; these states serve as targets for dnn training.", "For baseline features, we use Kaldi BIBREF21 to extract MFCCs+ INLINEFORM0 + INLINEFORM1 and PLPs+ INLINEFORM2 + INLINEFORM3 with a window size of 25 ms and a shift of 10 ms, and we apply per-speaker cmn. We also evaluated MFCCs and PLPs with vtln. The acoustic model used to extract the warp factors was a diagonal-covariance gmm with 1024 components. A single GMM was trained unsupervised on each language's training data.", "All experiments in this section are evaluated using the same-different task BIBREF26 , which tests whether a given speech representation can correctly classify two speech segments as having the same word type or not. For each word pair in a pre-defined set INLINEFORM0 the dtw cost between the acoustic feature vectors under a given representation is computed. Two segments are then considered a match if the cost is below a threshold. Precision and recall at a given threshold INLINEFORM1 are defined as INLINEFORM2", "In this work we instead consider a very simple feature-space adaptation method, vtln, which normalizes a speaker's speech by warping the frequency-axis of the spectra. vtln models are trained using maximum likelihood estimation under a given acoustic model\u2014here, an unsupervised gmm. Warp factors can then be extracted for both the training data and for unseen data."]}
{"question_id": "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86", "predicted_answer": "", "predicted_evidence": ["Recent years have seen increasing interest in \u201czero-resource\u201d speech technology: systems developed for a target language without using transcribed data or other hand-curated resources from that language. Such systems could potentially be applied to tasks such as endangered language documentation or query-by-example search for languages without a written form. One challenge for these systems, highlighted by the zrsc shared tasks of 2015 BIBREF0 and 2017 BIBREF1 , is to improve subword modeling, i.e., to extract or learn speech features from the target language audio. Good features should be more effective at discriminating between linguistic units, e.g. words or subwords, while abstracting away from factors such as speaker identity and channel noise.", "In all 6 languages, even bnfs from a monolingual tdnn already considerably outperform the cae trained with utd pairs. Adding another language usually leads to an increase in ap, with the bnfs trained on 8\u201310 high-resource languages performing the best, also always beating the gold cae. The biggest performance gain is obtained from adding a second training language\u2014further increases are mostly smaller. The order of languages has only a small effect, although for example adding other Slavic languages is generally associated with an increase in ap on Croatian. This suggests that it may be beneficial to train on languages related to the zero-resource language if possible, but further experiments need to be conducted to quantify this effect.", "We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.", "The ZRSCs were motivated largely by questions in artificial intelligence and human perceptual learning, and focused on approaches where no transcribed data from any language is used. Yet from an engineering perspective it also makes sense to explore how training data from higher-resource languages can be used to improve speech features in a zero-resource language.", "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization."]}
{"question_id": "19578949108ef72603afe538059ee55b4dee0751", "predicted_answer": "", "predicted_evidence": ["DISPLAYFORM0", "Multi-document summarization of news events offers the challenge of outputting a well-organized summary which covers an event comprehensively while simultaneously avoiding redundancy. The input documents may differ in focus and point of view for an event. We present an example of multiple input news documents and their summary in Figure TABREF2 . The three source documents discuss the same event and contain overlaps in content: the fact that Meng Wanzhou was arrested is stated explicitly in Source 1 and 3 and indirectly in Source 2. However, some sources contain information not mentioned in the others which should be included in the summary: Source 3 states that (Wanzhou) is being sought for extradition by the US while only Source 2 mentioned the attitude of the Chinese side.", "PG-BRNN The PG-BRNN model is a pointer-generator implementation from OpenNMT. As in the original paper BIBREF1 , we use a 1-layer bi-LSTM as encoder, with 128-dimensional word-embeddings and 256-dimensional hidden states for each direction. The decoder is a 512-dimensional single-layer LSTM. We include this for reference in addition to PG-Original, as our Hi-MAP code builds upon this implementation.", "Finally, compression ratio is defined as the word ratio between the articles and its summaries: DISPLAYFORM0", "The context vector INLINEFORM0 and the decoder hidden state INLINEFORM1 are then passed to two linear layers to produce the vocabulary distribution INLINEFORM2 . For each word, there is also a copy probability INLINEFORM3 . It is the sum of the attention weights over all the word occurrences:"]}
{"question_id": "44435fbd4087fa711835d267036b6a1f82336a22", "predicted_answer": "", "predicted_evidence": ["Following the setting from BIBREF11 , we report ROUGE BIBREF37 scores, which measure the overlap of unigrams (R-1), bigrams (R-2) and skip bigrams with a max distance of four words (R-SU). For the neural abstractive models, we truncate input articles to 500 tokens in the following way: for each example with INLINEFORM0 source input documents, we take the first 500 INLINEFORM1 tokens from each source document. As some source documents may be shorter, we iteratively determine the number of tokens to take from each document until the 500 token quota is reached. Having determined the number of tokens per source document to use, we concatenate the truncated source documents into a single mega-document. This effectively reduces MDS to SDS on longer documents, a commonly-used assumption for recent neural MDS papers BIBREF10 , BIBREF38 , BIBREF11 .", "liu18wikisum trains abstractive sequence-to-sequence models on a large corpus of Wikipedia text with citations and search engine results as input documents. However, no analogous dataset exists in the news domain. To bridge the gap, we introduce Multi-News, the first large-scale MDS news dataset, which contains 56,216 articles-summary pairs. We also propose a hierarchical model for neural abstractive multi-document summarization, which consists of a pointer-generator network BIBREF1 and an additional Maximal Marginal Relevance (MMR) BIBREF14 module that calculates sentence ranking scores based on relevancy and redundancy. We integrate sentence-level MMR scores into the pointer-generator model to adapt the attention weights on a word-level. Our model performs competitively on both our Multi-News dataset and the DUC 2004 dataset on ROUGE scores. We additionally perform human evaluation on several system outputs.", "Summarization is a central problem in Natural Language Processing with increasing applications as the desire to receive content in a concise and easily-understood format increases. Recent advances in neural methods for text summarization have largely been applied in the setting of single-document news summarization and headline generation BIBREF0 , BIBREF1 , BIBREF2 . These works take advantage of large datasets such as the Gigaword Corpus BIBREF3 , the CNN/Daily Mail (CNNDM) dataset BIBREF4 , the New York Times dataset BIBREF5 and the Newsroom corpus BIBREF6 , which contain on the order of hundreds of thousands to millions of article-summary pairs.", "Multi-document summarization of news events offers the challenge of outputting a well-organized summary which covers an event comprehensively while simultaneously avoiding redundancy. The input documents may differ in focus and point of view for an event. We present an example of multiple input news documents and their summary in Figure TABREF2 . The three source documents discuss the same event and contain overlaps in content: the fact that Meng Wanzhou was arrested is stated explicitly in Source 1 and 3 and indirectly in Source 2. However, some sources contain information not mentioned in the others which should be included in the summary: Source 3 states that (Wanzhou) is being sought for extradition by the US while only Source 2 mentioned the attitude of the Chinese side.", "Summarization is a central problem in Natural Language Processing with increasing applications as the desire to receive content in a concise and easily-understood format increases. Recent advances in neural methods for text summarization have largely been applied in the setting of single-document news summarization and headline generation BIBREF0 , BIBREF1 , BIBREF2 . These works take advantage of large datasets such as the Gigaword Corpus BIBREF3 , the CNN/Daily Mail (CNNDM) dataset BIBREF4 , the New York Times dataset BIBREF5 and the Newsroom corpus BIBREF6 , which contain on the order of hundreds of thousands to millions of article-summary pairs. However, multi-document summarization (MDS), which aims to output summaries from document clusters on the same topic, has largely been performed on datasets with less than 100 document clusters such as the DUC 2004 BIBREF7 and TAC 2011 BIBREF8 datasets, and has benefited less from advances in deep learning methods."]}
{"question_id": "86656aae3c27c6ea108f5600dd09ab7e421d876a", "predicted_answer": "", "predicted_evidence": ["In Table TABREF30 and Table TABREF31 we report ROUGE scores on DUC 2004 and Multi-News datasets respectively. We use DUC 2004, as results on this dataset are reported in lebanoff18mds, although this dataset is not the focus of this work. For results on DUC 2004, models were trained on the CNNDM dataset, as in lebanoff18mds. PG-BRNN and CopyTransformer models, which were pretrained by OpenNMT on CNNDM, were applied to DUC without additional training, analogous to PG-Original. We also experimented with training on Multi-News and testing on DUC data, but we did not see significant improvements. We attribute the generally low performance of pointer-generator, CopyTransformer and Hi-MAP to domain differences between DUC and CNNDM as well as DUC and Multi-News. These domain differences are evident in the statistics and extractive metrics discussed in Section 3.", "In addition to automatic evaluation, we performed human evaluation to compare the summaries produced. We used Best-Worst Scaling BIBREF39 , BIBREF40 , which has shown to be more reliable than rating scales BIBREF41 and has been used to evaluate summaries BIBREF42 , BIBREF32 . Annotators were presented with the same input that the systems saw at testing time; input documents were truncated, and we separated input documents by visible spaces in our annotator interface. We chose three native English speakers as annotators. They were presented with input documents, and summaries generated by two out of four systems, and were asked to determine which summary was better and which was worse in terms of informativeness (is the meaning in the input text preserved in the summary?), fluency (is the summary written in well-formed and grammatical English?) and non-redundancy (does the summary avoid repeating information?).", "Our model outperforms PG-MMR when trained and tested on the Multi-News dataset. We see much-improved model performances when trained and tested on in-domain Multi-News data. The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU. Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model). Our PG-MMR results correspond to PG-MMR w Cosine reported in lebanoff18mds. We trained their sentence regression model on Multi-News data and leave the investigation of transferring regression models from SDS to Multi-News for future work.", "DISPLAYFORM0", "For the second term of Equation SECREF21 , instead of choosing the maximum score from all candidates except for INLINEFORM0 , which is intended to find the candidate most similar to INLINEFORM1 , we choose to apply a self-attention model on INLINEFORM2 and all the other candidates INLINEFORM3 . We then choose the largest weight as the final score:"]}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "predicted_answer": "", "predicted_evidence": ["CopyTransformer Instead of using an LSTM, the CopyTransformer model used in Gehrmann:18 uses a 4-layer Transformer of 512 dimensions for encoder and decoder. One of the attention heads is chosen randomly as the copy distribution. This model and the PG-BRNN are run without the bottom-up masked attention for inference from Gehrmann:18 as we did not find a large improvement when reproducing the model on this data.", "As our focus was on deep methods for MDS, we only tested several non-neural baselines. However, other classical methods deserve more attention, for which we refer the reader to Hong14 and leave the implementation of these methods on Multi-News for future work.", "First We concatenate the first sentence of each article in a document cluster as the system summary. For our dataset, First- INLINEFORM0 means the first INLINEFORM1 sentences from each source article will be concatenated as the summary. Due to the difference in gold summary length, we only use First-1 for DUC, as others would exceed the average summary length.", "MMR In addition to incorporating MMR in our pointer generator network, we use this original method as an extractive summarization baseline. When testing on DUC data, we set these extractive methods to give an output of 100 tokens and 300 tokens for Multi-News data.", "where INLINEFORM0 is the decoder input. The final probability distribution is a weighted sum of the vocabulary distribution and copy probability:"]}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "predicted_answer": "", "predicted_evidence": ["Effect of BPE on $D$: Whether viewed as a merging of frequent subwords into a relatively less frequent compound, or splitting of rare words into relatively frequent subwords, it alters the class distribution by moving the probability mass of classes. Hence, by altering class distribution, it also alters $D$.", "We define precision $P$ for a class similar to the unigram precision in BLEU and extend its definition to the unigram recall $R$. For the sake of clarity, consider a test dataset $T$ of $N$ pairs of parallel sentences, $(x^{(i)}, y^{(i)})$ where $x$ and $y$ are source and reference sequences respectively. We use single reference $y^{(i)}$ translations for this analysis. For each $x^{(i)}$, let $h^{(i)}$ be the translation hypothesis from an MT model.", "We categorize the related work into the subsections as following:", "Since the parameters of ML classification models are estimated from training data, certain biases in the training data affect the final performance of model. Among those biases, class imbalance is a topic of our interest. Class imbalance is said to exist when one or more classes are not of approximately equal frequency in data. The effect of class imbalance has been extensively studied in several domains where classifiers are used (see Section SECREF32). With neural networks, the imbalanced learning is mostly targeted to computer vision tasks; NLP tasks are underexplored BIBREF4. Word types in natural language models follow a Zipfian distribution, i.e. in any natural language corpus, we observe that a few types are extremely frequent and the vast number of others lie on the long tail of infrequency. The Zipfian distribution thus causes two problems to the classifier based NLG systems:", "$\\rho _{F, R}$ is strong and negative. This is an indication that frequent classes have relatively higher recall than infrequent classes. If the rank increases, recall decreases in relation to it, leading to $\\rho _{F, R} < 0$."]}
{"question_id": "23fe8431058f2a7b7588745766fc715f271aad07", "predicted_answer": "", "predicted_evidence": ["$\\rho _{F, R}$ is strong and negative. This is an indication that frequent classes have relatively higher recall than infrequent classes. If the rank increases, recall decreases in relation to it, leading to $\\rho _{F, R} < 0$.", "Open-ended Vocabulary: Treating each word type in the vocabulary as a class of ML classifier does not cover the entire vocabulary, because the vocabulary is open-ended and classifiers model a finite set of classes only.", "NLP tasks such as sentiment analysis BIBREF0, BIBREF1, spam detection, etc., are modeled as classification tasks where instances are independently classified. Tasks such as part-of-speech tagging BIBREF2, and named entity recognition BIBREF3 are some examples for sequence tagging in which tokens are classified into tags within the context of sequences. Similarly, we can cast neural machine translation (NMT), an example of a natural language generation (NLG) task, as a form of classification task where tokens are classified within an autoregressor (see Section SECREF2) .", "$\\rho _{F, P}$ is strong and positive. This is an indication that frequent classes have relatively less precision than infrequent classes. If the rank increases (i.e frequency is decreases), precision increases in relation to it, leading to $\\rho _{F, P} > 0$.", "Imbalanced Classes: There are a few extremely frequent types and many infrequent types, causing an extreme imbalance. Such an imbalance, in other domains where classifiers are used, has been known to cause undesired biases and severe degradation in the performance BIBREF4."]}
{"question_id": "e5b2eb6a49c163872054333f8670dd3f9563046a", "predicted_answer": "", "predicted_evidence": ["Even though BPE encoding indirectly reduces the class imbalance compared to words and characters, it does not completely eliminate it. The class distributions after applying BPE contain sufficient imbalance for biasing the classes, and affecting the recall of rare classes. Hence more work is needed in directly addressing the Zipfian imbalance.", "This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116, and by research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, Air Force Laboratory, DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.", "Several variations of NMT models have been proposed and refined: sutskever2014seq2seq, cho2014learning introduced recurrent neural network (RNN) based encoder-decoder models for sequence-to-sequence translation learning. bahdanau2014nmtattn introduced the attention mechanism and luong2015effectiveAttn proposed several variations that became essential components of many future models. RNN modules, either LSTM BIBREF12 or GRU BIBREF13, were the popular choice for composing encoder and decoder of NMT. The encoder used bidirectional information, but the decoder was unidirectional, typically left-to-right, to facilitate autoregressive generation. gehring2017CNNMT showed used convolutional neural network (CNN) architecture that outperformed RNN models. vaswani2017attention proposed another alternative called Transformer whose main components are feed-forward and attention networks. There are only a few models that perform non-autoregressive NMT BIBREF14, BIBREF15.", "Experiments #3, #4, #5, #6 show that with BPE, decreasing the vocabulary indeed improves BLEU. Hence the larger BPE vocabulary such as $32k$ and $64k$ are not the best choice.", "For a comparison, word and character segmentation have no influence on $\\mu $. However, the trim size of word and character vocabulary has an effect on class imbalance $D$ and Out-of-Vocabulary (OOV) tokens and is presented in Figures FIGREF9 and FIGREF9, respectively. The summary of word, character, and BPE with respect to $D$ and $\\mu $ is presented in Table TABREF10."]}
{"question_id": "73760a45b23b2ec0cab181f82953fb296bb6cd19", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF26, as a visualization of Table TABREF15, shows a trend that the correlation (i.e. frequency bias) is lower with smaller vocabulary sizes. However, there still exists some correlation in $\\rho _{F, R}$ since the class imbalance, $D > 0$.", "sennrich-etal-2016-bpe introduced byte pair encoding (BPE) as a simplified way for solving OOV words without using back-off models. They noted that BPE improved the translation of not only the OOV words, but also some of rare in-vocabulary words. In their work, the vocabulary size was arbitrary, and large as $60k$ and $100k$.", "Specifically, a biased classifier overclassifies frequent classes, leading to over recall but poor precision of frequent words, and underclassifies rare classes, leading to poor recall of rare words. An improvement in balancing the class distribution, therefore, debiases in this regard, leading to improvement in the precision of frequent classes as well as recall of infrequent classes. BLEU focuses only on the precision of classes; except for adding a global brevity penalty, it is ignorant to the poor recall of infrequent classes. Therefore, the numbers reported in Table TABREF15 capture only a part of the improvement from balanced classes. In this section we perform a detailed analysis of the impact of class balancing by considering both precision and recall of classes. We accomplish this in two stages: First, we define a method to measure the bias of the model for classes based on their frequencies. Second, we track the bias in relation to vocabulary size and class imbalance on all our experiments.", "Regarding the problem of imbalanced classes, steedman-2008-last states that \u201cthe machine learning techniques that we rely on are actually very bad at inducing systems for which the crucial information is in rare events\u201d. However, to the best of our knowledge, this problem has not yet been directly addressed in the NLG setting.", "NLP tasks such as sentiment analysis BIBREF0, BIBREF1, spam detection, etc., are modeled as classification tasks where instances are independently classified. Tasks such as part-of-speech tagging BIBREF2, and named entity recognition BIBREF3 are some examples for sequence tagging in which tokens are classified into tags within the context of sequences. Similarly, we can cast neural machine translation (NMT), an example of a natural language generation (NLG) task, as a form of classification task where tokens are classified within an autoregressor (see Section SECREF2) ."]}
{"question_id": "ec990c16896793a819766bc3168c02556ef69971", "predicted_answer": "", "predicted_evidence": ["Regarding the problem of imbalanced classes, steedman-2008-last states that \u201cthe machine learning techniques that we rely on are actually very bad at inducing systems for which the crucial information is in rare events\u201d. However, to the best of our knowledge, this problem has not yet been directly addressed in the NLG setting.", "We perform NMT experiments using the base Transformer architecture BIBREF8. A common practice, as seen in vaswani2017attention's experimental setup, is to learn BPE vocabulary jointly for the source and target languages, which facilitates three-way weight sharing between the encoder's input, the decoder's input, and the decoder's output embeddings (classifier's class embeddings) BIBREF9. To facilitate fine-grained analysis of source and target vocabulary sizes and their effect on class imbalance, our models separately learn source and target vocabularies; weight sharing between the encoder's and decoder's embeddings is thus not possible. For the target language, however, we share weights between the decoder's input embeddings and the classifier's class embeddings.", "Imbalanced Classes: There are a few extremely frequent types and many infrequent types, causing an extreme imbalance. Such an imbalance, in other domains where classifiers are used, has been known to cause undesired biases and severe degradation in the performance BIBREF4.", "When a model is used in a domain mismatch scenario, i.e. where a test set's distribution does not match the training set's distribution, model performance generally degrades. It is not surprising that frequency-biased classifiers show particular degradation in domain mismatch scenarios, as types that were infrequent in the training distribution and were ignored by learning algorithm may appear with high frequency in the newer domain. koehn2017sixchallenges showed empirical evidence of poor generalization of NMT to out-of-domain datasets.", "$\\rho _{F, R}$ is strong and negative. This is an indication that frequent classes have relatively higher recall than infrequent classes. If the rank increases, recall decreases in relation to it, leading to $\\rho _{F, R} < 0$."]}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "predicted_answer": "", "predicted_evidence": ["Envisioning NMT models as a token classifier with an autoregressor helped in analysing the weaknesses of each component independently. The class imbalance was found to cause bias in the token classifier. We showed that BPE vocabulary size is not arbitrary, and it can be tuned to address the class imbalance and sequence lengths appropriately. Our analysis provided an explanation why BPE encoding is more effective compared to word and character models for sequence generation.", "Since the parameters of ML classification models are estimated from training data, certain biases in the training data affect the final performance of model. Among those biases, class imbalance is a topic of our interest. Class imbalance is said to exist when one or more classes are not of approximately equal frequency in data. The effect of class imbalance has been extensively studied in several domains where classifiers are used (see Section SECREF32). With neural networks, the imbalanced learning is mostly targeted to computer vision tasks; NLP tasks are underexplored BIBREF4. Word types in natural language models follow a Zipfian distribution, i.e. in any natural language corpus, we observe that a few types are extremely frequent and the vast number of others lie on the long tail of infrequency. The Zipfian distribution thus causes two problems to the classifier based NLG systems:", "Open-ended Vocabulary: Treating each word type in the vocabulary as a class of ML classifier does not cover the entire vocabulary, because the vocabulary is open-ended and classifiers model a finite set of classes only.", "morishita-etal-2018-improving viewed BPE more generally in the sense that both character and word vocabularies as two special cases of BPE vocabulary. Their analysis was different than ours in a way that they viewed BPE with varied vocabulary sizes as hierarchical features which were used in addition to a fixed BPE vocabulary size of $16k$ on the target language. DBLP:journals/corr/abs-1810-08641 offer an efficient way to search BPE vocabulary size for NMT. kudo-2018-subword used BPE segmentation as a regularization by introducing sampling based randomness to the BPE segmentation. For the best of our knowledge, no previous work exists that analyzed BPE's effect on class imbalance or answered `why certain BPE vocabularies are better than others?'.", "We define precision $P$ for a class similar to the unigram precision in BLEU and extend its definition to the unigram recall $R$. For the sake of clarity, consider a test dataset $T$ of $N$ pairs of parallel sentences, $(x^{(i)}, y^{(i)})$ where $x$ and $y$ are source and reference sequences respectively. We use single reference $y^{(i)}$ translations for this analysis. For each $x^{(i)}$, let $h^{(i)}$ be the translation hypothesis from an MT model."]}
{"question_id": "9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "predicted_answer": "", "predicted_evidence": ["Since the parameters of ML classification models are estimated from training data, certain biases in the training data affect the final performance of model. Among those biases, class imbalance is a topic of our interest. Class imbalance is said to exist when one or more classes are not of approximately equal frequency in data. The effect of class imbalance has been extensively studied in several domains where classifiers are used (see Section SECREF32). With neural networks, the imbalanced learning is mostly targeted to computer vision tasks; NLP tasks are underexplored BIBREF4. Word types in natural language models follow a Zipfian distribution, i.e. in any natural language corpus, we observe that a few types are extremely frequent and the vast number of others lie on the long tail of infrequency. The Zipfian distribution thus causes two problems to the classifier based NLG systems:", "NLP tasks such as sentiment analysis BIBREF0, BIBREF1, spam detection, etc., are modeled as classification tasks where instances are independently classified. Tasks such as part-of-speech tagging BIBREF2, and named entity recognition BIBREF3 are some examples for sequence tagging in which tokens are classified into tags within the context of sequences. Similarly, we can cast neural machine translation (NMT), an example of a natural language generation (NLG) task, as a form of classification task where tokens are classified within an autoregressor (see Section SECREF2) .", "The contributions of this paper are as follows: We offer a simplified view of NMT architectures by re-envisioning them as two high-level components: a classifier and an autoregressor (Section SECREF2). For the best performance of the classifier, we argue that the balanced class distribution is desired, and describe a method to measure class imbalance in a Zipfian distribution (Section SECREF6). For the best performance of the autoregressor, we argue that it is desired to have shorter sequences (Section SECREF7). In Section SECREF8, we describe how BPE vocabulary relates with the desired settings for both classifier and autoregressor. Our experimental setup is described in Section SECREF3, followed by the analysis of results in Section SECREF4 that offers an explanation with evidence for why some vocabulary sizes are better than others. Section SECREF5 uncovers the impact of class imbalance, particularly the discrimination on classes based on their frequency. Section SECREF6 provides an overview of the related work, followed by a conclusion in Section SECREF7.", "Subwords obtained through e.g. byte pair encoding (BPE) BIBREF5 addresses the open-ended vocabulary problem by using only a finite set of subwords. Due to the benefit and simplicity of BPE, it is rightfully part of the majority of current NMT models. However, the choice of vocabulary size used for BPE is a hyperparameter whose effect is not well understood. In practice, BPE vocabulary choice is either arbitrary or chosen from several trial-and-errors.", "Several variations of NMT models have been proposed and refined: sutskever2014seq2seq, cho2014learning introduced recurrent neural network (RNN) based encoder-decoder models for sequence-to-sequence translation learning. bahdanau2014nmtattn introduced the attention mechanism and luong2015effectiveAttn proposed several variations that became essential components of many future models. RNN modules, either LSTM BIBREF12 or GRU BIBREF13, were the popular choice for composing encoder and decoder of NMT. The encoder used bidirectional information, but the decoder was unidirectional, typically left-to-right, to facilitate autoregressive generation. gehring2017CNNMT showed used convolutional neural network (CNN) architecture that outperformed RNN models. vaswani2017attention proposed another alternative called Transformer whose main components are feed-forward and attention networks."]}
{"question_id": "2929e92f9b4939297b4d0f799d464d46e8d52063", "predicted_answer": "", "predicted_evidence": ["Another way to capture the interaction between past and future context per time step is to add a token-level self-attentive mechanism on top of the same BiLSTM formulation introduced in Section SECREF2. Given the hidden features $H$ of a whole sequence, the model projects each hidden state to different subspaces, depending on whether it is used as the query vector to consult other hidden states for each word token, the key vector to compute its dot-similarities with incoming queries, or the value vector to be weighted and actually convey information to the querying token. As different aspects of a task can call for different attention, multiple attention heads running in parallel are used BIBREF19.", "WNUT 2017 Emerging NER \u2013 a dataset providing maximally diverse, noisy, and drifting user-generated text BIBREF22. The training set consists of previously annotated tweets \u2013 social media text with non-standard spellings, abbreviations, and unreliable capitalization BIBREF23; the development set consists of newly sampled YouTube comments; the test set includes text newly drawn from Twitter, Reddit, and StackExchange. Besides drawing new samples from diverse topics across different sources, the shared task also filtered out text containing surface forms of entities seen in the training set. The resulting dataset requires models to generalize to emerging contexts and entities instead of relying on familiar surface cues.", "You and I (work-of-art)", "Throughout this paper, $X$ denotes the $n$-by-$d_x$ matrix of sequence features, where $n$ is the sentence length and $d_x$ is either 364 (with GloVe) or 464 (with Twitter).", "On top of an input feature sequence, BiLSTM is used to capture the future and the past for each time step. Following BIBREF1, 4 distinct LSTM cells \u2013 two in each direction \u2013 are stacked to capture higher level representations:"]}
{"question_id": "1dcfcfa46dbcffc2fc7be92dd57df9620258097b", "predicted_answer": "", "predicted_evidence": ["Now, the catch is that phrase 1 and phrase 3 have exactly the same past context for \"and\". Hence the same $\\overrightarrow{H}_2$ and the same $\\overrightarrow{s}_2$, i.e., $\\overrightarrow{s}^1_2=\\overrightarrow{s}^3_2$. Similarly, $\\overrightarrow{s}^2_2=\\overrightarrow{s}^4_2$, $\\overleftarrow{s}^1_2=\\overleftarrow{s}^4_2$, and $\\overleftarrow{s}^2_2=\\overleftarrow{s}^3_2$. Rewriting the constraints with these equalities gives", "Finally, suppose there are $d_p$ token classes, the probability of each of which is given by the composition of affine and softmax transformations:", "You and I (work-of-art)", "Key and I", "With the un-shifted covariance matrix of the projected $\\overrightarrow{H}\\ ||\\ \\overleftarrow{H}$, Att-BiLSTM-CNN correlates past and future context for each token in a dot-product, multiplicative manner."]}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "predicted_answer": "", "predicted_evidence": ["Moreover, disambiguating fine-grained entity types is also a challenging task. For example, entities of language and NORP often take the same surface forms. Figure FIGREF19 shows an example containing \"Dutch\" and \"English\". While \"English\" was much more frequently used as a language and was identified correctly, the \"Dutch\" mention was tricky for Baseline. The attention heat map (Figure FIGREF24) further tells the story that Att has relied on its attention head to make context-aware decisions. Overall, both cross-structures were much better at disambiguating these fine-grained types (4.1%/0.8%/3.3%/3.4%).", "All experiments for Baseline-, Cross-, and Att-BiLSTM-CNN used the same model parameters given in Section SECREF3. The training minimized per-token cross-entropy loss with the Nadam optimizer BIBREF24 with uniform learning rate 0.001, batch size 32, and 35% dropout. Each training lasted 400 epochs when using GloVe embedding (OntoNotes), and 1600 epochs when using Twitter embedding (WNUT). The development set of each dataset was used to select the best epoch to restore model weights for testing. Following previous work on NER, model performances were evaluated with strict mention F1 score. Training of each model on each dataset repeated 6 times to report the mean score and standard deviation.", "You and Peele", "Section SECREF3 formulates the three Baseline, Cross, and Att-BiLSTM-CNN models. The section gives a concrete proof that patterns forming an XOR cannot be modeled by Baseline-BiLSTM-CNN used in all previous work. Cross-BiLSTM-CNN and Att-BiLSTM-CNN are shown to have additive and multiplicative cross-structures respectively to deal with the problem. Section SECREF4 evaluates the approaches on two challenging NER datasets spanning a wide range of domains with complex, noisy, and emerging entities. The cross-structures bring consistent improvements over the prevalently used Baseline-BiLSTM-CNN without additional gazetteers, POS taggers, language-modeling, or multi-task supervision. The improved core module surpasses comparable previous models on OntoNotes 5.0 and WNUT 2017 by 1.4% and 4.6% respectively.", "Motivated by the limitation of the conventional Baseline-BiLSTM-CNN for sequence labeling, this paper proposes the use of Cross-BiLSTM-CNN by changing the deep structure in Section SECREF2 to"]}
{"question_id": "b537832bba2eb6d34702a9d71138e661c05a7c3a", "predicted_answer": "", "predicted_evidence": ["Enc-Dec: a standard encoder-decoder model without any episodic memory module.", "We investigate the effect of the number of retrieved examples for local adaptation to the performance of the model in Table TABREF42 . In both tasks, the model performs better as the number of neighbors increases. Recall that the goal of the local adaptation phase is to shape the output distribution of a test example to peak around relevant classes (or spans) based on retrieved examples from the memory. As a result, it is reasonable for the performance of the model to increase with more neighbors (up to a limit) given a key network that can reliably compute similarities between the test example and stored examples in memory and a good adaptation method.", "We use the following dataset orders (chosen randomly) for text classification:", "Comparing to the performance of the multitask model MTL\u2014which is as an upper bound on achievable performance\u2014we observe that there is still a gap between continual models and the multitask model. MbPA++ has the smallest performance gap. For text classification, MbPA++ outperforms single-dataset models in terms of averaged performance (70.6 vs. 60.7), demonstrating the success of positive transfer. For question answering, MbPA++ still lags behind single dataset models (62.0 vs. 66.0). Note that the collection of single-dataset models have many more parameters since there is a different set of model parameters per dataset. See Appendix SECREF8 for detailed results of multitask and single-dataset models.", "Replay: a model that uses stored examples for sparse experience replay without local adaptation. We perform experience replay by sampling 100 examples from the memory and perform a gradient update after every 10,000 training steps, which gives us a 1% replay rate."]}
{"question_id": "1002bd01372eba0f3078fb4a951505278ed45f2e", "predicted_answer": "", "predicted_evidence": ["In this paper, we investigate the role of episodic memory for learning a model of language in a lifelong setup. We propose to use such a component for sparse experience replay and local adaptation to allow the model to continually learn from examples drawn from different data distributions. In experience replay, we randomly select examples from memory to retrain on. Our model only performs experience replay very sparsely to consolidate newly acquired knowledge with existing knowledge in the memory into the model. We show that a 1% experience replay to learning new examples ratio is sufficient. Such a process bears some similarity to memory consolidation in human learning BIBREF16 . In local adaptation, we follow Memory-based Parameter Adaptation BIBREF7 and use examples retrieved from memory to update model parameters used to make a prediction of a particular test example.", "Note that since there is no dataset descriptor provided to our model, this decoder is used to predict all classes in all datasets, which we assume to be known in advance.", "Our main contributions in this paper are:", "In text classification, following the original BERT model, we take the representation of the first token INLINEFORM0 from BERT (i.e., the special beginning-of-document symbol) and add a linear transformation and a softmax layer to predict the class of INLINEFORM1 . INLINEFORM2", "We consider a continual (lifelong) learning setup where a model needs to learn from a stream of training examples INLINEFORM0 . We assume that all our training examples in the series come from multiple datasets of the same task (e.g., a text classification task, a question answering task), and each dataset comes one after the other. Since all examples come from the same task, the same model can be used to make predictions on all examples. A crucial difference between our continual learning setup and previous work is that we do not assume that each example comes with a dataset descriptor (e.g., a dataset identity). As a result, the model does not know which dataset an example comes from and when a dataset boundary has been crossed during training. The goal of learning is to find parameters INLINEFORM1 that minimize the negative log probability of training examples under our model: INLINEFORM2"]}
{"question_id": "3450723bf66956486de777f141bde5073e4a7694", "predicted_answer": "", "predicted_evidence": ["Table TABREF33 provides a summary of our main results. We report (macro-averaged) accuracy for classification and INLINEFORM0 score for question answering. We provide complete per-dataset (non-averaged) results in Appendix SECREF7 . Our results show that A-GEM outperforms the standard encoder-decoder model Enc-Dec, although it is worse than MbPA on both tasks. Local adaptation (MbPA) and sparse experience replay (Replay) help mitigate catastrophic forgetting compared to Enc-Dec, but a combination of them is needed to achieve the best performance (MbPA++).", "We use a pretrained INLINEFORM0 model BIBREF12 as our example encoder and key network. INLINEFORM1 has 12 Transformer layers, 12 self-attention heads, and 768 hidden dimensions (110M parameters in total). We use the default BERT vocabulary in our experiments.", "Our model is augmented with an episodic memory module that stores previously seen examples throughout its lifetime. The episodic memory module is used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer. We first describe the architecture of our episodic memory module, before discussing how it is used at training and inference (prediction) time in \u00a7 SECREF3 .", "MbPA BIBREF7 : an episodic memory model that uses stored examples for local adaptation without sparse experience replay. The original MbPA formulation has a trainable key network. Our MbPA baseline uses a fixed key network since MbPA with a trainable key network performs significantly worse.", "Figure FIGREF34 shows INLINEFORM0 score and accuracy of various models on the test set corresponding to the first dataset seen during training as the models are trained on more datasets. The figure illustrates how well each model retains its previously acquired knowledge as it learns new knowledge. We can see that MbPA++ is consistently better compared to other methods."]}
{"question_id": "36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "predicted_answer": "", "predicted_evidence": ["", "There are many datasets created for this task BIBREF15, BIBREF16. In this work, we have used the dataset and benchmarks provided by the PAN 2018 shared task on author profiling BIBREF15. As the dataset contains a constant number of 100 tweets per user, accuracy tests are performed both on user and tweet level (tweet-level predictions are made by removing the user-level attention). Tweet-level accuracy tests show interesting results during hyperparameter optimization. When the tweet-level predictions are averaged to produce user-level predictions, it is seen that the hyperparameters that gave the best results in terms of tweet-level accuracy, performs worse in user-level accuracy. The better user-level models, with different hyperparameters, that gave the highest user-level accuracy are observed to slightly overfit on tweet-level. It leads us to believe that the overfitting in the tweet-level predictions in best user-level models acts similar to an attention mechanism by over-emphasizing some distinctive tweets and ignoring the rest.", "where $W_\\alpha $ is a learnable weight matrix that is used to multiply each output of the RNN, $t_i$ is the feature vector of $i$th tweet, $b$ is a learnable bias vector, $w_i$ is a learnable attention weight, $A_i$ is the attention context vector, $v_i$ is the attention value for $i$th tweet, $o_i$ is attention output vector for the corresponding tweet, $K$ is the output vector for user. Matrix $W_\\alpha $ and vectors $w_i$ and $b$ are learned parameters.", "[1]In their paper, authors report a result of 82.21 in English but we couldn't verify their accuracy in our repetitions by using their software and the same dataset. [2]Since their software is not provided, we directly take the accuracy values from their paper.", "We also would like to kindly remind our readers that although the model is self-learning, there might still exist a gender bias in the evaluation of the model due to the data itself. Since the model learns to predict the gender directly from tweets of the twitter users, any bias the twitter users have might be reflected in the model predictions."]}
{"question_id": "28e50459da60ceda49fe1578c12f3f805b288bd0", "predicted_answer": "", "predicted_evidence": ["Attention layer outputs a single feature vector that corresponds to a user, which is then fed to a fully-connected layer to lower the dimension to the number of classes.", "There are many datasets created for this task BIBREF15, BIBREF16. In this work, we have used the dataset and benchmarks provided by the PAN 2018 shared task on author profiling BIBREF15. As the dataset contains a constant number of 100 tweets per user, accuracy tests are performed both on user and tweet level (tweet-level predictions are made by removing the user-level attention). Tweet-level accuracy tests show interesting results during hyperparameter optimization. When the tweet-level predictions are averaged to produce user-level predictions, it is seen that the hyperparameters that gave the best results in terms of tweet-level accuracy, performs worse in user-level accuracy. The better user-level models, with different hyperparameters, that gave the highest user-level accuracy are observed to slightly overfit on tweet-level. It leads us to believe that the overfitting in the tweet-level predictions in best user-level models acts similar to an attention mechanism by over-emphasizing some distinctive tweets and ignoring the rest.", "There are many datasets created for this task BIBREF15, BIBREF16. In this work, we have used the dataset and benchmarks provided by the PAN 2018 shared task on author profiling BIBREF15. As the dataset contains a constant number of 100 tweets per user, accuracy tests are performed both on user and tweet level (tweet-level predictions are made by removing the user-level attention). Tweet-level accuracy tests show interesting results during hyperparameter optimization. When the tweet-level predictions are averaged to produce user-level predictions, it is seen that the hyperparameters that gave the best results in terms of tweet-level accuracy, performs worse in user-level accuracy. The better user-level models, with different hyperparameters, that gave the highest user-level accuracy are observed to slightly overfit on tweet-level.", "CNN model (denoted CNNwA on results) is based on BIBREF10 where each character in the tweet is represented with a character embedding of size 25, which is trained along the neural network. All characters are lower-cased. Non-alphabetical characters such as punctuation are kept with a view to capturing some information on the profile of the user since they are heavily used in twitter as emoticons.", "A feature vector for each tweet is created by feeding tweets to RNN separately. In order to discriminate tweets with respect to their information carrying capacity on its author's gender, Bahdanau attention mechanism BIBREF20 is used to combine the tweets rather than concatenating them before feeding to the network or averaging their predictions later. Figure FIGREF4 shows the tweet-level attention layer in detail which is calculated by the following formulas:"]}
{"question_id": "e1f61500eb733f2b95692b6a9a53f8aaa6f1e1f6", "predicted_answer": "", "predicted_evidence": ["Recently, neural network-based models have been proposed to solve this problem. Rather than explicitly extracting features, the aim is to develop an architecture that implicitly learns. In author profiling, both style and content-based features were proved useful BIBREF8 and neural networks are able to capture both syntactic and semantic regularities. In general, syntactic information is drawn from the local context. On the other hand, semantic information is often captured with larger window sizes. Thus, CNNs are preferred to obtain style-based features while RNNs are the methods of choice for addressing content-based features BIBREF9. In literature, CNN BIBREF10 or RNN BIBREF11, BIBREF12, BIBREF13 is used on this task. BIBREF11 obtain state-of-the-art performance among neural methods by proposing a model architecture where they process text through RNN with GRU cells. Also, the presence of an attention layer is shown to boost the performance of neural methods BIBREF11, BIBREF10.", "A bidirectional RNN with GRU BIBREF19 cells are used in this model where the number of cells is a hyperparameter. Among the tested range (50-150 with intervals of 25), best accuracy on validation set is obtained by 150 cells in English and 100 cells in Spanish and Arabic. An attention mechanism is used on word-level in addition to tweet-level to capture the important parts of each tweet as shown in Figure FIGREF2.", "The expectation is to see that the best setup on tweet-level also gives the best performance in user-level, but the outcome is the opposite: Best setups on tweet-level always fall behind best user-level setups. Performance differences between various setups can be seen in Figure FIGREF12 where accuracies of the best three models in terms of tweet-level and best three models in terms of user-level are shown for all languages. It can be observed that the best tweet-level setups are almost $4\\%$ worse in terms of user-level accuracy. Deeper investigation shows that the best user-level models exhibit slight overfitting on tweet-level, in training. Although overfitting normally leads to poor generalization, in this case we believe that this overfitting acts similar to an attention mechanism by over-emphasizing some important tweets and ignoring uninformative ones in the process. Even though this leads to poor tweet-level accuracy, it improves the user-level accuracy of the models as it can be seen from the Figure FIGREF12.", "In this work, we propose a model that relies on RNN with attention mechanism (RNNwA). A bidirectional RNN with attention mechanism both on word level and tweet level is trained with word embeddings. The final representation of the user is fed to a fully connected layer for prediction. Since combining some hand-crafted features with a learned linear layer has shown to perform well in complex tasks like Semantic Role Labeling (SRL) BIBREF14, an improved version of the model (RNNwA + n-gram) is also tested with hand-crafted features. In the improved version, LSA-reduced n-gram features are concatenated with the neural representation of the user. Then the result is fed into a fully-connected layer to make prediction. Models are tested in three languages; English, Spanish, and Arabic, and the improved version achieves state-of-the-art accuracy on English, and competitive results on Spanish and Arabic corpus.", "On the other hand, the improved model (RNNwA + n-gram), where neural and hand-crafted features are concatenated, increases the accuracy of the proposed model by approximately $0,5$% on English and approximately 2% in Spanish and Arabic. This also supports our intuition that the performance of neural models can be improved by hand-crafted features, which is based on the study of BIBREF14. As can be seen in Table TABREF11, the improved model outperforms the state-of-the-art method of BIBREF3 in English and produces competitive results in Spanish and Arabic."]}
{"question_id": "da4d07645edaf7494a8cb5216150a00690da01f7", "predicted_answer": "", "predicted_evidence": ["For two FSTs $T_1$, $T_2$ over semiring $\\mathbb {K}$,", "Based on the algorithm described in BIBREF3, we allow the states $(q_1, q_2)$ such that $q_2 = (q_c, q_p), q_c \\in Q_c, q_p=0 $ to be pre-composed, where $q_c$ and $q_p$ denote states in $G_c$ and $G_p$, respectively. States in $G_c$ with a class label transition will be ignored during pre-composition.", "Handling out-of-vocabulary (OOV) words in speech recognition is very important especially for contact name recognition. We replace the normal class (contact) FST with a mono-phone FST by adding monophone words in the lexicon BIBREF2, BIBREF8, BIBREF9. By using s monophone FST, we avoid the necessity of adding new words into lexicon on-the-fly, which significantly simplifies the system. We use silence phone \"SIL\" to represent the word boundary. These monophone words will not be applied with silence phone in lexicon since they are not real words.", "Since the non-terminal states are composed on-the-fly, it means the states of recognition FST will also contain personalized information that cannot be used by other users or service threads.", "Our goal is to pre-compose the most frequently encountered states. However, if some frequent states are far from the start state, they may not be identified by naive BFS. In this case, it is very time and memory consuming to increase the depth of the BFS. Moreover, if we simply use a offline corpus of utterances to analyze the frequency of all states, some highly frequent states could be blocked by less frequent states. Thus, the easiest way is to do pre-composition using real utterances."]}
{"question_id": "c0cebef0e29b9d13c165b6f19f6ca8393348c671", "predicted_answer": "", "predicted_evidence": ["We experiment with both the naive BFS and the proposed data-driven pre-composition methods. For the data-driven approach, we randomly picked 500 utterances from the evaluation data set as warm up utterances. We use an empty contact FST to be replaced into the root LM to avoid personalized states during warm-up decoding. In order to evaluate the benefit of the proposed private cache to store the personalized language model, we group multiple utterances from a user into virtual dialog sessions of one, two, or five turns.", "Our results also show that increasing the steps of naive BFS will not help the RTF, since it may compose infrequently encountered states, resulting in unnecessary memory usage. Using the proposed data-driven warm-up performs better in both marginal memory efficiency and RTF than naive BFS. Both pre-composition methods can also be combined.", "We define $T_P$ as a partial composed FST structure for $T=T_1 \\circ T_2$, where $P \\subseteq Q$ is the set of pre-composed states. In real time decoding, the on-the-fly composition will be performed on top of the pre-initialized $T_P$, which is similar to previous work BIBREF3. In a production environment, multiple threads will share the same pre-composed FST $T_P$ structure, while each thread will own a private FST structure.", "For two FSTs $T_1$, $T_2$ over semiring $\\mathbb {K}$,", "The personalized class FST (contact FST) only contains monophone words. Determinization and minimization are applied to the contact FST with disambiguation symbols. The disambiguation symbols are removed after graph optimization. The decoding experiments are performed on a server with 110 GB memory and 24 processors."]}
{"question_id": "5695908a8c6beb0e3863a1458a1b93aab508fd34", "predicted_answer": "", "predicted_evidence": ["We experiment with both the naive BFS and the proposed data-driven pre-composition methods. For the data-driven approach, we randomly picked 500 utterances from the evaluation data set as warm up utterances. We use an empty contact FST to be replaced into the root LM to avoid personalized states during warm-up decoding. In order to evaluate the benefit of the proposed private cache to store the personalized language model, we group multiple utterances from a user into virtual dialog sessions of one, two, or five turns.", "is the class language model transducer obtained by replacing the class labels in generic root FST $G_c$ with class FSTs $G_p$ for different classes, where $\\mathcal {C}$ denotes the set of all supported classes.", "Experiments are performed on two data sets. The first contains 7,500 utterances from the calling domain from Facebook employees. This includes commands like \u201cPlease call Jun Liu now\". The second consists of approximately 10,000 utterances from other common domains, such as weather, time, and music. Note that we include the contact FST for both calling and non-calling utterances, as we do not assume knowledge of the user's intent a priori. Each user has a contact FST containing 500 contacts on average. We keep up to five pronunciations for each name, generated by a grapheme-to-phoneme model.", "We would like to thank Mike Seltzer, Christian Fuegen, Julian Chan, and Dan Povey for useful discussions about the work.", "Naive breath-first-search (BFS) is the most obvious way to perform pre-composition. We iterate over all states within a specific distance from the start state of decoding FST. It generalizes to a full set pre-composition when the search depth is large."]}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "predicted_answer": "", "predicted_evidence": ["More recent approaches, and currently the best performing ones, utilize recurrent neural networks (rnns) to transform content into dense low-dimensional semantic representations that are then used for classification BIBREF1 , BIBREF7 . All of these approaches rely solely on lexical and semantic features of the text they are applied to. Waseem and Hovy c53cecce142c48628b3883d13155261c adopted a more user-centric approach based on the idea that perpetrators of hate speech are usually segregated into small demographic groups; they went on to show that gender information of authors (i.e., users who have posted content) is a helpful indicator. However, Waseem and Hovy focused only on coarse demographic features of the users, disregarding information about their communication with others. But previous research suggests that users who subscribe to particular stereotypes that promote hate speech tend to form communities online. For example, Zook zook mapped the locations of racist tweets in response to President Obama's re-election to show that such tweets were not uniformly distributed across the United States but formed clusters instead.", "Author profiling has emerged as a powerful tool for NLP applications, leading to substantial performance improvements in several downstream tasks, such as text classification, sentiment analysis and author attribute identification BIBREF8 , BIBREF9 , BIBREF10 . The relevance of information gained from it is best explained by the idea of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined along various axes, e.g., location, age, language, etc. The strength of author profiling lies in that if we have information about members of a community $c$ defined by some similarity criterion, and we know that the person $p$ belongs to $c$ , we can infer information about $p$ . This concept has a straightforward application to our task: knowing that members of a particular community are prone to creating hateful content, and knowing that the author p is connected to this community, we can leverage information beyond linguistic cues and more accurately predict the use of hateful/non-hateful language from $p$ .", "In doing so, the framework learns low-dimensional embeddings for nodes in the graph. These embeddings can emphasize either their structural role or the local community they are a part of. This depends on the sampling strategies used to generate the neighborhood: if breadth-first sampling (bfs) is adopted, the model focuses on the immediate neighbors of a node; when depth-first sampling (dfs) is used, the model explores farther regions in the network, which results in embeddings that encode more information about the nodes' structural role (e.g., hub in a cluster, or peripheral node). The balance between these two ways of sampling the neighbors is directly controlled by two node2vec parameters, namely $p$ and $q$ . The default value for these is 1, which ensures a node representation that gives equal weight to both structural and community-oriented information. In our work, we use the default value for both $p$ and $q$ . Additionally, since node2vec does not produce embeddings for solitary authors, we map these to a single zero embedding.", "In Table 2 , we further compare the performance of the different methods on the racism and sexism classes individually. As in the previous experiments, the scores are averaged over 10 folds of cv. Of particular interest are the scores for the sexism class where the f $_1$ increases by over 10 points upon the addition of author profiling features. Upon analysis, we find that such a substantial increase in performance stems from the fact that many of the 527 unique authors of the sexist tweets are closely connected in the community graph. This allows for their penchant for sexism to be expressed in their respective author profiles.", "In this paper, we explored the effectiveness of community-based information about authors for the purpose of identifying hate speech. Working with a dataset of $16k$ tweets annotated for racism and sexism, we first comprehensively replicated three established and currently best-performing hate speech detection methods based on character n-grams and recurrent neural networks as our baselines. We then constructed a graph of all the authors of tweets in our dataset and extracted community-based information in the form of dense low-dimensional embeddings for each of them using node2vec. We showed that the inclusion of author embeddings significantly improves system performance over the baselines and advances the state of the art in this task. Users prone to hate speech do tend to form social groups online, and this stresses the importance of utilizing community-based information for automatic hate speech detection. In the future, we wish to explore the effectiveness of community-based author profiling in other tasks such as stereotype identification and metaphor detection."]}
{"question_id": "6883767bbdf14e124c61df4f76335d3e91bfcb03", "predicted_answer": "", "predicted_evidence": ["However, a number of hateful tweets still remain misclassified despite the addition of author profiling features. According to our analysis, many of these tend to contain urls to hateful content, e.g., \u201c@salmonfarmer1: Logic in the world of Islam http://t.co/6nALv2HPc3\" and \u201c@juliarforster Yes. http://t.co/ixbt0uc7HN\". Since Twitter shortens all urls into a standard format, there is no indication of what they refer to. One way to deal with this limitation could be to additionally maintain a blacklist of links. Another source of system errors is the deliberate obfuscation of words by authors in order to evade detection, e.g., \u201cKat, a massive c*nt. The biggest ever on #mkr #cuntandandre\". Current hate speech detection methods, including ours, do not directly attempt to address this issue. While this is a challenge for bag-of-word based methods such as lr, we hypothesize that neural networks operating at the character level may be helpful in recognizing obfuscated words.", "However, a number of hateful tweets still remain misclassified despite the addition of author profiling features. According to our analysis, many of these tend to contain urls to hateful content, e.g., \u201c@salmonfarmer1: Logic in the world of Islam http://t.co/6nALv2HPc3\" and \u201c@juliarforster Yes. http://t.co/ixbt0uc7HN\". Since Twitter shortens all urls into a standard format, there is no indication of what they refer to. One way to deal with this limitation could be to additionally maintain a blacklist of links. Another source of system errors is the deliberate obfuscation of words by authors in order to evade detection, e.g., \u201cKat, a massive c*nt. The biggest ever on #mkr #cuntandandre\". Current hate speech detection methods, including ours, do not directly attempt to address this issue.", "Wulczyn et al. Wulczyn:2017:EMP:3038912.3052591 prepared three different datasets of comments collected from the English Wikipedia Talk page; one was annotated for personal attacks, another for toxicity and the third one for aggression. Their best performing model was a multi-layered perceptron (mlp) classifier trained on character n-gram features. Experimenting with the personal attack and toxicity datasets, Pavlopoulos et al. Pavlopoulos:17 improved the results of Wulczyn et al. by using a gated recurrent unit (gru) model to encode the comments into dense low-dimensional representations, followed by a lr layer to classify the comments based on those representations.", "$$\\nonumber \\sum _{v \\in V} \\log Pr\\,(N_s(v)\\, |\\, v)$$   (Eq. 6)", "Hidden-state (hs). As our second baseline, we take the \u201crnn\u201d method of Pavlopoulos et al. Pavlopoulos:17 which achieves state-of-the-art results on the Wikipedia datasets released by Wulczyn et al. Wulczyn:2017:EMP:3038912.3052591. The method comprises a 1-layer gated recurrent unit (gru) that takes a sequence $w_1$ , $\\dots $ , $w_n$ of words represented as $d$ -dimensional embeddings and encodes them into hidden states $h_1$ , $\\dots $ , $h_n$ . This is followed by an lr layer that uses the last hidden state $h_n$ to classify the tweet. We make two minor modifications to the authors' original architecture: we deepen the 1-layer gru to a 2-layer gru and use softmax instead of sigmoid in the lr layer."]}
{"question_id": "11679d1feba747c64bbbc62939a20fbb69ada0f3", "predicted_answer": "", "predicted_evidence": ["We were able to extract community-based information for 1,836 out of the 1,875 unique authors who posted the $16,202$ tweets, covering a cumulative of 16,124 of them; the remaining 39 authors have either deactivated their accounts or are facing suspension. Tweets in the racism class are from 5 of the 1,875 authors, while those in the sexism class are from 527 of them.", "Word-sum (ws). As a third baseline, we adopt the \u201clstm+glove+gbdt\" method of Badjatiya et al. Badjatiya:17, which achieves state-of-the-art results on the Twitter dataset we are using. The authors first utilize an lstm to task-tune glove-initialized word embeddings by propagating the error back from an lr layer. They then train a gradient boosted decision tree (gbdt) classifier to classify texts based on the average of the embeddings of constituent words. We make two minor modifications to this method: we use a 2-layer gru instead of the lstm to tune the embeddings, and we train the gbdt classifier on the l $_2$ -normalized sum of the embeddings instead of their average.", "Waseem and Hovy c53cecce142c48628b3883d13155261c created and experimented with a dataset of racist, sexist and clean tweets. Utilizing a logistic regression (lr) classifier to distinguish amongst them, they found that character n-grams coupled with gender information of users formed the optimal feature set; on the other hand, geographic and word-length distribution features provided little to no improvement. Working with the same dataset, Badjatiya et al. Badjatiya:17 improved on their results by training a gradient-boosted decision tree (gbdt) classifier on averaged word embeddings learnt using a long short-term memory (lstm) network that they initialized with random embeddings.", "Badjatiya:17, which achieves state-of-the-art results on the Twitter dataset we are using. The authors first utilize an lstm to task-tune glove-initialized word embeddings by propagating the error back from an lr layer. They then train a gradient boosted decision tree (gbdt) classifier to classify texts based on the average of the embeddings of constituent words. We make two minor modifications to this method: we use a 2-layer gru instead of the lstm to tune the embeddings, and we train the gbdt classifier on the l $_2$ -normalized sum of the embeddings instead of their average. Although the authors achieved state-of-the-art results on Twitter by initializing embeddings randomly rather than with glove (which is what we do here), we found the opposite when performing a 10-fold stratified cross-validation (cv). A possible explanation of this lies in the authors' decision to not use stratification, which for such a highly imbalanced dataset can lead to unexpected outcomes BIBREF18 .", "Char n-grams + author profile (lr + auth). This method builds upon the lr baseline by appending author profile vectors on to the character n-gram count vectors for training the lr classifier."]}
{"question_id": "e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "predicted_answer": "", "predicted_evidence": ["For the history encoder, we empirically found that single mean pooling layer over the set of all document embeddings outperformed other more complicated architectures, and so that is what we use in our experiments. Finally, the classifier is a 3-layer feed-forward network with and INLINEFORM2 for the hidden layers, followed by a softmax over the INLINEFORM3 -dimensional output. We use Adam BIBREF26 as our optimizer, set the maximum number of epochs to 100, and shuffle the order of the training data at each epoch. During each training step, we represent each user's history as a new random sample of INLINEFORM4 documents if there are more than INLINEFORM5 documents available for the user, and we use a batch size of 32 users. Since there is a class imbalance in our data, we use sample weighting in order to prevent the model from converging to a solution that simply predicts the most common classes present in the training data. Each sample is weighted according to its class, INLINEFORM6 , using the following formula: INLINEFORM7", "We consider two variations on our dataset: the first is a simplified, 50-class classification problem. We choose the 50 most common clusters out of our full set of INLINEFORM0 and only make predictions about users who have reportedly performed an activity in one of these clusters. The second variation uses the entire dataset, but rather than making predictions about all INLINEFORM1 classes, we only make fine-grained predictions about those classes for which INLINEFORM2 . We do this under the assumption that training an adequate classifier for a given class requires at least INLINEFORM3 examples. All classes for which INLINEFORM4 are assigned an \u201cother\u201d label. In this way, we still make a prediction for every instance in the dataset, but we avoid allowing the model to try to fit to a huge landscape of outputs when the training data for some of these outputs is insufficient. By setting INLINEFORM5 to 100, we are left with 805 out of 1024 classes, and an 806th \u201cother\u201d class for our 806-class setup.", "We then use several simple rules to convert the Event2Mind instances into first-person past-tense activities. Since all events were already filtered so that they begin with \u201cPersonX\u201d, we replace the first occurrence of \u201cPersonX\u201d in each event with \u201cI\u201d and all subsequent occurrences with \u201cme\u201d. All occurrences of \u201cPersonX's\u201d become \u201cmy\u201d, and the main verb in each phrase is conjugated to its past-tense form using the Pattern python module. For example, the event \u201cPersonX teaches PersonX's son\u201d becomes the query \u201cI taught my son\u201d. Since Event2Mind also contains wildcard placeholders that can match any span of text within the same phrase (e.g., \u201cPersonX buys INLINEFORM0 at the store\u201d) but the Twitter API doesn't provide a mechanism for wildcard search, we split the event on the string INLINEFORM1 and generate a query that requires all substrings to appear in the tweet. We then check all candidate tweets after retrieval and remove any for which the substrings do not appear in the same order as the original pattern.", "In this paper, we explore the task of predicting human activities from user-generated text data, which will allow us to gain a deeper understanding of the kinds of everyday activities that people discuss online with one another. Throughout the paper, we use the word \u201cactivity\u201d to refer to what an individual user does or has done in their daily life. Unlike the typical use of this term in the computer vision community BIBREF12 , BIBREF13 , in this paper we use it in a broad sense, to also encompass non-visual activities such as \u201cmake vacation plans\" or \u201chave a dream\u201d We do not focus on fine-grained sequences actions such as \u201cpick up a camera\u201d, \u201chold a camera to one's face\u201d, \u201cpress the shutter release button\u201d, and others. Rather, we focus on the high-level activity as a person would report to others: \u201ctake a picture\u201d. Additionally, we specifically focus on everyday human activities done by the users themselves, rather than larger-scale events BIBREF14 , which are typically characterized by the involvement or interest of many users, often at a specific time and location.", "Second, we test how well the model is able to sort users by their likelihood of having reported to do an activity from a cluster. This average comparison rank (ACR) score is computed as follows: for each user in the test set, we sample INLINEFORM0 other users who do not have the same activity label. Then, we use the probabilities assigned by the model to rank all INLINEFORM1 users by their likelihood of being assigned INLINEFORM3 , and the comparison rank score is the percentage of users who were ranked ahead of the target user (lower is better). We then average this comparison rank across all users in the test set to get the ACR."]}
{"question_id": "7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "predicted_answer": "", "predicted_evidence": ["While the attributes vector INLINEFORM0 can be used to encode any information of interest about a user, we choose to experiment with the use of personal values because of their theoretical connection to human activities BIBREF6 . In order to get a representation of a user's values, we turn to the hierarchical personal values lexicon from BIBREF24 . In this lexicon, there are 50 value dimensions, represented as sets of words and phrases that characterize that value. Since users' profiles often contain value-related content, we use the Distributed Dictionary Representations (DDR) method BIBREF25 to compute a score, INLINEFORM1 for each value dimension, INLINEFORM2 , using cosine similarity as follows: INLINEFORM3", "While the relationship between two activity phrases can be defined in a number of ways BIBREF10 , we we chose a model that was optimized to capture relatedness so that our clusters would contain groups of related activities without enforcing that they are strictly the same activity. Since the model that we employed was trained on activity phrases in the infinitive form, we again use the Pattern python library, this time to convert all of our past-tense activities to this form. We also omit the leading first person pronoun from each phrase, and remove user mentions (@<user>), hashtags, and URLs. We then define the distance between any two vectors using cosine distance, i.e., INLINEFORM0 , for vectors INLINEFORM1 and INLINEFORM2 .", "We evaluate our activity prediction models using a number of metrics that consider not only the most likely cluster, but also the set of INLINEFORM0 most likely clusters. First, we evaluate the average per-class accuracy of the model's ability to rank INLINEFORM1 , the target cluster, within the top INLINEFORM2 clusters. These scores tell us how well the model is able to make predictions about the kinds of activities that each user is likely to do.", "The ability to predict the exact activity cluster correctly is an extremely difficult task, and in fact, achieving that alone would be a less informative result than producing predictions about the likelihood of all clusters. Further, in our setup, we only have knowledge about a sample of activities that people actually have done. In reality, it is very likely that users have participated in activities that belong to a huge variety of clusters, regardless of which activities were actually reported on social media. Therefore, it should be sufficient for a model to give a relatively high probability to any activity that has been reported by a user, even if there is no report of the user having performed an activity from the cluster with the highest probability for that user.", "If a model is able to accurately predict the target cluster, then it is able to estimate the general type of activity that the user is likely to write about doing in the future given some set of information about the user and what they have written in the past. By also generating a probability distribution over the clusters, we can assign a likelihood that each user will write about performing each group of activities in the future. For example, such a model could predict the likelihood that a person will claim to engage in a \u201cCooking\u201d activity or a \u201cPet/Animal related\u201d activity."]}
{"question_id": "cab082973e1648b0f0cc651ab4e0298a5ca012b5", "predicted_answer": "", "predicted_evidence": ["While our models are able to make predictions indicating that learning has taken place, it is clear that this prediction task is difficult. In the 50-class setup, the INLINEFORM0 model consistently had the strongest average per-class accuracy for all values of INLINEFORM1 and the lowest (best) ACR score (Table TABREF31 ). The INLINEFORM2 model performed nearly as well, showing that using only the human-activity relevant content from a user's history gives similar results to using the full set of content available. When including the attributes and profile for a user, the model typically overfits quickly and generalization deteriorates.", "In order to cluster our activity phrase instances, we need to define a notion of distance between any pair of instances. For this, we turn to prior work on models to determine semantic similarity between human activity phrases BIBREF16 in which the authors utilized transfer learning in order to fine-tune the Infersent BIBREF17 sentence similarity model to specifically capture relationships between human activity phrases. We use the authors' BiLSTM-max sentence encoder trained to capture the relatedness dimension of human activity phrases to obtain vector representations of each of our activity phrases. The measure of distance between vectors produced by this model was shown to be strongly correlated with human judgments of general activity relatedness (Spearman's INLINEFORM0 between the model and human ratings, while inter-annotator agreement is INLINEFORM1 ).", "A user's profile is a single document, also represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express self-identity such as \u201crepublican\u201d or \u201cathiest.\u201d", "For the history encoder, we empirically found that single mean pooling layer over the set of all document embeddings outperformed other more complicated architectures, and so that is what we use in our experiments. Finally, the classifier is a 3-layer feed-forward network with and INLINEFORM2 for the hidden layers, followed by a softmax over the INLINEFORM3 -dimensional output. We use Adam BIBREF26 as our optimizer, set the maximum number of epochs to 100, and shuffle the order of the training data at each epoch. During each training step, we represent each user's history as a new random sample of INLINEFORM4 documents if there are more than INLINEFORM5 documents available for the user, and we use a batch size of 32 users. Since there is a class imbalance in our data, we use sample weighting in order to prevent the model from converging to a solution that simply predicts the most common classes present in the training data. Each sample is weighted according to its class, INLINEFORM6 , using the following formula: INLINEFORM7", "where INLINEFORM0 is a set of activity clusters, INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 are vectors that represent the user's history, profile, and attributes, respectively, and INLINEFORM4 is the target cluster. The target cluster is the cluster label of an activity cluster that contains an activity that is known to have been performed by the user."]}
{"question_id": "1cc394bdfdfd187fc0af28500ad47a0a764d5645", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the number of training instances belonging to class INLINEFORM1 . We evaluate our model on the development data after each epoch and save the model with the highest per-class accuracy. Finally, we compute the results on the test data using this model, and report these results.", "While the relationship between two activity phrases can be defined in a number of ways BIBREF10 , we we chose a model that was optimized to capture relatedness so that our clusters would contain groups of related activities without enforcing that they are strictly the same activity. Since the model that we employed was trained on activity phrases in the infinitive form, we again use the Pattern python library, this time to convert all of our past-tense activities to this form. We also omit the leading first person pronoun from each phrase, and remove user mentions (@<user>), hashtags, and URLs. We then define the distance between any two vectors using cosine distance, i.e., INLINEFORM0 , for vectors INLINEFORM1 and INLINEFORM2 .", "In order to get an even richer set of human activities, we also ask a set of 1,000 people across the United States to list any five activities that they had done in the past week. We collect our responses using Amazon Mechanical Turk, and manually verify that all responses are reasonable. We remove any duplicate strings and automatically convert them into first-person and past-tense (if they were not in that form already). For this set of queries, there are no wildcards and we only search for exact matches. Example queries obtained using this approach include \u201cI went to the gym\u201d and \u201cI watched a documentary\u201d.", "We choose the 50 most common clusters out of our full set of INLINEFORM0 and only make predictions about users who have reportedly performed an activity in one of these clusters. The second variation uses the entire dataset, but rather than making predictions about all INLINEFORM1 classes, we only make fine-grained predictions about those classes for which INLINEFORM2 . We do this under the assumption that training an adequate classifier for a given class requires at least INLINEFORM3 examples. All classes for which INLINEFORM4 are assigned an \u201cother\u201d label. In this way, we still make a prediction for every instance in the dataset, but we avoid allowing the model to try to fit to a huge landscape of outputs when the training data for some of these outputs is insufficient. By setting INLINEFORM5 to 100, we are left with 805 out of 1024 classes, and an 806th \u201cother\u201d class for our 806-class setup. Note that this version includes all activities from all 1024 clusters, it is just that the smallest clusters are grouped together with the \u201cother\u201d label.", "In the 806-class version of the task, we observe the effects of including a larger range of activities, including many that do not appear as often as others in the training data (Table TABREF34 ). This version of the task also simulates a more realistic scenario, since predictions can be made for the \u201cother\u201d class when the model does to expect the user to claim to do an activity from any of the known clusters. In this setting, we see that the INLINEFORM0 model works well for INLINEFORM1 , suggesting that the use of the INLINEFORM2 vectors helps, especially when predicting the correct cluster within the top 25 is important. For INLINEFORM3 , the same INLINEFORM4 model that worked best in the 50-class setup again outperforms the others. Here, in contrast to the 50-class setting, using the full set of tweets usually performs better than focusing only on the human activity content. Interestingly, the best ACR scores are even lower in the 806-class setup, showing that it is just as easy to rank users by their likelihood of writing about an activity, even when considering many more activity clusters."]}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "predicted_answer": "", "predicted_evidence": ["A user's profile is a single document, also represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express self-identity such as \u201crepublican\u201d or \u201cathiest.\u201d", "Further, we explore the types of activity clusters that contain activities reported by users with high scores for various value dimensions. For a given value, we compute a score for each cluster INLINEFORM0 by taking the average INLINEFORM1 of all users who tweeted about doing activities in the cluster. For each value INLINEFORM2 , we can then rank all clusters by their INLINEFORM3 score. Examples of those with the highest scores are presented in Table TABREF28 . We observe that users whose profiles had high scores for Family were likely to report doing activities including family members, those with high scores for Nature tweeted about travel, and those with high Work-Ethic scores reported performing writing related tasks.", "We consider events for which a person is the subject (e.g, \u201cPersonX listens to PersonX's music\u201d) to be human activities, and remove the rest (e.g., \u201cIt is Christmas morning\u201d). We then use several simple rules to convert the Event2Mind instances into first-person past-tense activities. Since all events were already filtered so that they begin with \u201cPersonX\u201d, we replace the first occurrence of \u201cPersonX\u201d in each event with \u201cI\u201d and all subsequent occurrences with \u201cme\u201d. All occurrences of \u201cPersonX's\u201d become \u201cmy\u201d, and the main verb in each phrase is conjugated to its past-tense form using the Pattern python module. For example, the event \u201cPersonX teaches PersonX's son\u201d becomes the query \u201cI taught my son\u201d.", "We train a deep neural model, summarized in Figure FIGREF21 , to take a user's history, profile, and attributes, and output a probability distribution over the set of INLINEFORM0 clusters of human activities, indicating the likelihood that the user has reported to have performed an activity in each cluster. There are four major components of our network:", "We use K-means clustering in order to find a set of INLINEFORM0 clusters that can be used to represent the semantic space in which the activity vectors lie. We experiment with INLINEFORM1 with INLINEFORM2 and evaluate the clustering results using several metrics that do not require supervision: within-cluster variance, silhouette coefficient BIBREF18 , Calinski-Harabaz criterion BIBREF19 , and Davies-Bouldin criterion BIBREF20 . In practice, however, we find that these metrics are strongly correlated (either positively or negatively) with the INLINEFORM3 , making it difficult to quantitatively compare the results of using a different number of clusters, and we therefore make a decision based on a qualitative analysis of the clusters. For the purpose of making these kinds of predictions about clusters, it is beneficial to have a smaller number of larger clusters, but clusters that are too large are no longer meaningful since they contain sets of activities that are less strongly related to one another."]}
{"question_id": "cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "predicted_answer": "", "predicted_evidence": ["In this paper, we addressed the task of predicting human activities from user-generated content. We collected a large Twitter dataset consisting of posts from more than 200,000 users mentioning at least one of the nearly 30,000 everyday activities that we explored. Using sentence embedding models, we projected activity instances into a vector space and perform clustering in order to learn about the high-level groups of behaviors that are commonly mentioned online. We trained predictive models to make inferences about the likelihood that a user had reported to have done activities across the range of clusters that we discovered, and found that these models were able to achieve results significantly higher than random guessing baselines for the metrics that we consider. While the overall prediction scores are not very high, the models that we trained do show that they are able to generalize findings from one set of users to another. This is evidence that the task is feasible, but very difficult, and it could benefit from further investigation.", "For the document encoder and profile encoder we use Bi-LSTMs with max pooling BIBREF17 , with INLINEFORM0 and INLINEFORM1 . For the history encoder, we empirically found that single mean pooling layer over the set of all document embeddings outperformed other more complicated architectures, and so that is what we use in our experiments. Finally, the classifier is a 3-layer feed-forward network with and INLINEFORM2 for the hidden layers, followed by a softmax over the INLINEFORM3 -dimensional output. We use Adam BIBREF26 as our optimizer, set the maximum number of epochs to 100, and shuffle the order of the training data at each epoch. During each training step, we represent each user's history as a new random sample of INLINEFORM4 documents if there are more than INLINEFORM5 documents available for the user, and we use a batch size of 32 users.", "This research was supported in part through computational resources and services provided by the Advanced Research Computing at the University of Michigan. This material is based in part upon work supported by the Michigan Institute for Data Science, by the National Science Foundation (grant #1815291), by the John Templeton Foundation (grant #61156), and by DARPA (grant #HR001117S0026-AIDA-FP-045). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the Michigan Institute for Data Science, the National Science Foundation, the John Templeton Foundation, or DARPA. Many thanks to the anonymous reviewers who provided helpful feedback.", "Takes each token in the user's profile as input and produces a single INLINEFORM0 dimensional vector, INLINEFORM1 as output.", "This layer takes the sequence INLINEFORM0 as input and produces a single INLINEFORM1 dimensional vector, INLINEFORM2 , as output, intended to represent high-level features extracted from the entire history of the user."]}
{"question_id": "101d7a355e8bf6d1860917876ee0b9971eae7a2f", "predicted_answer": "", "predicted_evidence": ["Tweet2Vec (T2V) [3] - This is a character composition model working directly on the character sequences to predict the user-annotated hashtags in a tweet. We use publicly available encoder, which was trained on 2 million tweets.", "Skip-Thought Vectors (STV) [6] - This is a GRU [16] encoder trained to predict adjacent sentences in a books corpus. We use the recommended combine-skip (4800-dimensional) vectors from the publicly available encoder.", "Fine-grained analysis of various supervised and unsupervised models discussed in Section SECREF3 , across various dimensions discussed in Section SECREF4 , is presented in Table TABREF30 . The codes used to conduct our experiments are publicly accessible at: https://github.com/ganeshjawahar/fine-tweet/.", "We summarize the results of property prediction tasks in Table TABREF31 . Length prediction turns out to be a difficult task for most of the models. Models which rely on the recurrent architectures such as LSTM, STV, T2V have sufficient capacity to perform well in modeling the tweet length. Also BLSTM is the best in modeling slang words. BLSTM outperforms the LSTM variant in all the tasks except `Content', which signifies the power of using the information flowing from both the directions of the tweet. T2V which is expected to perform well in this task because of its ability to work at a more fine level (i.e., characters) performs the worst. In fact T2V does not outperform other models in any task, which could be mainly due to the fact that the hashtags which are used for supervision in learning tweet representations reduces the generalization capability of the tweets beyond hashtag prediction.", "Models which rely on the recurrent architectures such as LSTM, STV, T2V have sufficient capacity to perform well in modeling the tweet length. Also BLSTM is the best in modeling slang words. BLSTM outperforms the LSTM variant in all the tasks except `Content', which signifies the power of using the information flowing from both the directions of the tweet. T2V which is expected to perform well in this task because of its ability to work at a more fine level (i.e., characters) performs the worst. In fact T2V does not outperform other models in any task, which could be mainly due to the fact that the hashtags which are used for supervision in learning tweet representations reduces the generalization capability of the tweets beyond hashtag prediction. Prediction tasks such as `Content' and `Hashtag' seem to be less difficult as all the models perform nearly optimal for them. The superior performance of all the models for the `Content' task in particular is unlike the relatively lower performance reported for in [5], mainly because of the short length of the tweets."]}
{"question_id": "4288621e960ffbfce59ef1c740d30baac1588b9b", "predicted_answer": "", "predicted_evidence": ["[17] Harris, Z. S.: Distributional structure. In: Word. (1954) 146-162", "[18] Blei, D. M., Ng, A. Y., & Jordan, M. I.: Latent dirichlet allocation. In: JMLR. (2003)", "[8] Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T.: Bag of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759. (2016)", "On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties. The most relevant work is that of Adi et al.", "[1]https://noisy-text.github.io/norm-shared-task.html"]}
{"question_id": "c3befe7006ca81ce64397df654c31c11482dafbe", "predicted_answer": "", "predicted_evidence": ["[18] Blei, D. M., Ng, A. Y., & Jordan, M. I.: Latent dirichlet allocation. In: JMLR. (2003)", "[17] Harris, Z. S.: Distributional structure. In: Word. (1954) 146-162", "[16] Cho, K., Van Merri\u00ebnboer, B., Bahdanau, D., & Bengio, Y.: On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259. (2014)", "[14] Graves, A., Mohamed, A. R., & Hinton, G.: Speech recognition with deep recurrent neural networks. In: ICASSP. (2013) 6645-6649", "[15] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J.: Distributed representations of words and phrases and their compositionality. In: NIPS. (2013) 3111-3119"]}
{"question_id": "5d0a3f8ca3882f87773cf8c2ef1b4f72b9cc241e", "predicted_answer": "", "predicted_evidence": ["To address the brevity problem, many designers of NMT systems add corrections to the model. These corrections are often presented as modifications to the search procedure. But, in our view, the brevity problem is essentially a modeling problem, and these corrections should be seen as modifications to the model (Section SECREF5 ). Furthermore, since the root of the problem is local normalization, our view is that these modifications should be trained as globally-normalized models (Section SECREF6 ).", "Length normalization divides the score by INLINEFORM0 BIBREF0 , BIBREF1 , BIBREF2 : INLINEFORM1", "Most of the experimental settings below follow the recommendations of BIBREF15 . Our high-resource, German\u2013English data is from the 2016 WMT shared task BIBREF16 . We use a bidirectional encoder-decoder model with attention BIBREF17 . Our word representation layer has 512 hidden units, while other hidden layers have 1024 nodes. Our model is trained using Adam with a learning rate of 0.0002. We use 32k byte-pair encoding (BPE) operations learned on the combined source and target training data BIBREF19 . We train on minibatches of size 2012 words and validate every 100k sentences, selecting the final model based on development perplexity. Our medium-resource, Russian\u2013English system uses data from the 2017 WMT translation task, which consists of roughly 1 million training sentences BIBREF20 .", "Another way to demonstrate that the beam problem is the same as the brevity problem is to look at the translations generated by baseline systems on shorter sentences. Figure FIGREF18 shows the BLEU scores of the Russian\u2013English system for beams of size 10 and 1000 on sentences of varying lengths, with and without correcting lengths. The x-axes of the figure are cumulative: length 20 includes sentences of length 0\u201320, while length 10 includes 0\u201310. It is worth noting that BLEU is a word-level metric, but the systems were built using BPE; so the sequences actually generated are longer than the x-axes would suggest.", "Google's NMT system BIBREF3 relies on a more complicated correction: INLINEFORM0"]}
{"question_id": "dce27c49b9bf1919ca545e04663507d83bb42dbe", "predicted_answer": "", "predicted_evidence": ["Finally, instead of tuning the word reward using grid search, we introduce a way to learn it using a perceptron-like tuning method. We show that the optimal value is sensitive both to task and beam size, implying that it is important to tune for every model trained. Fortunately, tuning is a quick post-training step.", "Although highly successful, neural machine translation (NMT) systems continue to be plagued by a number of problems. We focus on two here: the beam problem and the brevity problem.", "In this example, INLINEFORM0 , even though overestimated, is still lower than INLINEFORM1 , and wins only because its suffixes have higher probability. Greedy search would prune the incorrect prefix an and yield the correct output. In general, then, we might expect greedy or beam search to alleviate some symptoms of label bias. Namely, a prefix with a low-entropy suffix distribution can be pruned if its probability is, even though overestimated, not among the highest probabilities. Such an observation was made by BIBREF11 in the context of dependency parsing, and we will see next that precisely such a situation affects output length in NMT.", "First, machine translation systems rely on heuristics to search through the intractably large space of possible translations. Most commonly, beam search is used during the decoding process. Traditional statistical machine translation systems often rely on large beams to find good translations. However, in neural machine translation, increasing the beam size has been shown to degrade performance. This is the last of the six challenges identified by BIBREF0 .", "The interaction between the length problem and the beam problem can be visualized in the histograms of Figure FIGREF19 on the Russian\u2013English system. In the upper left plot, the uncorrected model with beam 10 has the majority of the generated sentences with a length ratio close to 1.0, the gold lengths. Going down the column, as the beam size increases, the distribution of length ratios skews closer to 0. By a beam size of 1000, 37% of the sentences have a length of 0. However, both the word reward and the normalized models remain very peaked around a length ratio of 1.0 even as the beam size increases."]}
{"question_id": "991ea04072b3412928be5e6e903cfa54eeac3951", "predicted_answer": "", "predicted_evidence": ["Although highly successful, neural machine translation (NMT) systems continue to be plagued by a number of problems. We focus on two here: the beam problem and the brevity problem.", "The second problem, noted by several authors, is that NMT tends to generate translations that are too short. BIBREF1 and BIBREF0 address this by dividing translation scores by their length, inspired by work on audio chords BIBREF2 . A similar method is also used by Google's production system BIBREF3 . A third simple method used by various authors BIBREF4 , BIBREF5 , BIBREF6 is a tunable reward added for each output word. BIBREF7 and BIBREF8 propose variations of this reward that enable better guarantees during search.", "The results of tuning the word reward, INLINEFORM0 , as described in Section SECREF6 , is shown in the second section of Tables TABREF10 , TABREF11 , and TABREF12 . In contrast to our baseline systems, our tuned word reward always fixes the brevity problem (length ratios are approximately 1.0), and generally fixes the beam problem. An optimized word reward score always leads to improvements in METEOR scores over any of the best baselines. Across all language pairs, reward and norm have close METEOR scores, though the reward method wins out slightly. BLEU scores for reward and norm also increase over the baseline in most cases, despite BLEU's inherent bias towards shorter sentences. Most notably, whereas the baseline Russian\u2013English system lost more than 20 BLEU points when the beam was increased to 1000, our tuned reward score resulted in a BLEU gain over any baseline beam size. Whereas in our baseline systems, the length ratio decreases with larger beam sizes, our tuned word reward results in length ratios of nearly 1.0 across all language pairs, mitigating many of the issues of the brevity problem.", "Tuning the word reward score generally had higher METEOR scores than length normalization across all of our settings. With BLEU, length normalization beat the word reward on German-English and French\u2013English, but tied on English-French and lost on Russian\u2013English. For the largest beam of 1000, the tuned word reward had a higher BLEU than length normalization. Overall, the two methods have relatively similar performance, but the tuned word reward has the more theoretically justified, globally-normalized derivation \u2013 especially in the context of label bias' influence on the brevity problem.", "As in our label-bias example, greedy search would prune the incorrect empty translation. More generally, consider beam search: at time step INLINEFORM0 , only the top INLINEFORM1 partial or complete translations are retained while the rest are pruned. (Implementations of beam search vary in the details, but this variant is simplest for the sake of argument.) Even if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem. Our claim, hinted at by BIBREF0 , is that the brevity problem and the beam problem are essentially the same, and that solving one will solve the other."]}
{"question_id": "a82a12a22a45d9507bc359635ffe9574f15e0810", "predicted_answer": "", "predicted_evidence": ["Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.", "When building conventional models, we developed our own feature extraction scripts and used the SKLL python package for building Random Forest models. When implementing CNN, we used the Keras Python package. Regarding hyper-parameter tweaking, we utilized the Tree Parzen Estimation (TPE) method as detailed in TPE. After running 200 iterations of tweaking, we ended up with the following selection: INLINEFORM0 is 6 (entailing that the various filter sizes are INLINEFORM1 ), INLINEFORM2 is 100, INLINEFORM3 is INLINEFORM4 and INLINEFORM5 is INLINEFORM6 , optimization uses Adam BIBREF13 . When training the CNN model, we randomly selected INLINEFORM7 of the training data as the validation set for using early stopping to avoid over-fitting.", "", "In our experiment, we firstly divided each corpus into two parts. The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers. The larger portion (the CV set) was then formulated as a 10-fold cross-validation setup for obtaining a stable and comprehensive model evaluation result. For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences. For the TED data, the Dev set contains 1046 utterances, while the CV set contains 8406 utterances. Note that, with a goal of building a speaker-independent humor detector, when partitioning our TED data set, we always kept all utterances of a single talk within the same partition. To our knowledge, this is the first time that such a strict experimental setup has been used in recognizing humor in conversations, and it makes the humor recognition task on the TED data quite challenging.", "Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter). Note that for INLINEFORM13 convolution and FC layer's input, we applied `dropout' BIBREF10 regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome over-fitting. By using cross-entropy as the learning metric, the whole sequential network (all weights and bias) could be optimized by using any SGD optimization, e.g., Adam BIBREF13 , Adadelta BIBREF14 , and so on."]}
{"question_id": "355cf303ba61f84b580e2016fcb24e438abeafa7", "predicted_answer": "", "predicted_evidence": ["In this study, we utilized the Word2Vec BIBREF4 embedding vectors ( INLINEFORM4 ) that were trained on 100 billion words of Google News. Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter). Note that for INLINEFORM13 convolution and FC layer's input, we applied `dropout' BIBREF10 regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome over-fitting.", "Our CNN-based text classification's setup follows Kim2014. Figure FIGREF17 depicts the model's details. From the left side's input texts to the right side's prediction labels, different shapes of tensors flow through the entire network for solving the classification task in an end-to-end mode.", "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.", "Firstly, tokenized text strings were converted to a INLINEFORM0 tensor with shape INLINEFORM1 , where INLINEFORM2 represents sentences' maximum length while INLINEFORM3 represents the word-embedding dimension. In this study, we utilized the Word2Vec BIBREF4 embedding vectors ( INLINEFORM4 ) that were trained on 100 billion words of Google News. Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs.", "The ability to make effective presentations has been found to be linked with success at school and in the workplace. Humor plays an important role in successful public speaking, e.g., helping to reduce public speaking anxiety often regarded as the most prevalent type of social phobia, generating shared amusement to boost persuasive power, and serving as a means to attract attention and reduce tension BIBREF0 ."]}
{"question_id": "88757bc49ccab76e587fba7521f0981d6a1af2f7", "predicted_answer": "", "predicted_evidence": ["In this study, we utilized the Word2Vec BIBREF4 embedding vectors ( INLINEFORM4 ) that were trained on 100 billion words of Google News. Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter). Note that for INLINEFORM13 convolution and FC layer's input, we applied `dropout' BIBREF10 regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome over-fitting.", "Firstly, tokenized text strings were converted to a INLINEFORM0 tensor with shape INLINEFORM1 , where INLINEFORM2 represents sentences' maximum length while INLINEFORM3 represents the word-embedding dimension. In this study, we utilized the Word2Vec BIBREF4 embedding vectors ( INLINEFORM4 ) that were trained on 100 billion words of Google News. Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs.", "When building conventional models, we developed our own feature extraction scripts and used the SKLL python package for building Random Forest models. When implementing CNN, we used the Keras Python package. Regarding hyper-parameter tweaking, we utilized the Tree Parzen Estimation (TPE) method as detailed in TPE. After running 200 iterations of tweaking, we ended up with the following selection: INLINEFORM0 is 6 (entailing that the various filter sizes are INLINEFORM1 ), INLINEFORM2 is 100, INLINEFORM3 is INLINEFORM4 and INLINEFORM5 is INLINEFORM6 , optimization uses Adam BIBREF13 . When training the CNN model, we randomly selected INLINEFORM7 of the training data as the validation set for using early stopping to avoid over-fitting.", "For the purpose of monitoring how well speakers can use humor during their presentations, we have created a corpus from TED talks. Compared to the existing (albeit limited) corpora for humor recognition research, ours has the following advantages: (a) it was collected from authentic talks, rather than from TV shows performed by professional actors based on scripts; (b) it contains about 100 times more speakers compared to the limited number of actors in existing corpora. We compared two types of leading text-based humor recognition methods: a conventional classifier (e.g., Random Forest) based on human-engineered features vs. an end-to-end CNN method, which relies on its inherent representation learning. We found that the CNN method has better performance. More importantly, the representation learning of the CNN method makes it very efficient when facing new data sets.", "In this study, we utilized the Word2Vec BIBREF4 embedding vectors ( INLINEFORM4 ) that were trained on 100 billion words of Google News. Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter)."]}
{"question_id": "2f9a31f5a2b668acf3bce8958f5daa67ab8b2c83", "predicted_answer": "", "predicted_evidence": ["We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in yang-EtAl:2015:EMNLP2.", "Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter). Note that for INLINEFORM13 convolution and FC layer's input, we applied `dropout' BIBREF10 regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome over-fitting. By using cross-entropy as the learning metric, the whole sequential network (all weights and bias) could be optimized by using any SGD optimization, e.g., Adam BIBREF13 , Adadelta BIBREF14 , and so on.", "In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.", "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website.", "Automatically simulating an audience's reactions to humor will not only be useful for presentation training, but also improve conversational systems by giving machines more empathetic power. The present study reports our efforts in recognizing utterances that cause laughter in presentations. These include building a corpus from TED talks and using Convolutional Neural Networks (CNNs) in the recognition."]}
{"question_id": "4830459e3d1d204e431025ce7e596ef3f8d757d2", "predicted_answer": "", "predicted_evidence": ["Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website.", "Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues BIBREF2 , BIBREF5 . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases.", "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.", "We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'.", "Following yang-EtAl:2015:EMNLP2, we applied Random Forest BIBREF12 to perform humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories: Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations ( INLINEFORM0 ). More details can be found in BIBREF3 ."]}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "predicted_answer": "", "predicted_evidence": ["TED Talks are recordings from TED conferences and other special TED programs. In the present study, we focused on the transcripts of the talks. Most transcripts of the talks contain the markup `(Laughter)', which represents where audiences laughed aloud during the talks. This special markup was used to determine utterance labels.", "When building conventional models, we developed our own feature extraction scripts and used the SKLL python package for building Random Forest models. When implementing CNN, we used the Keras Python package. Regarding hyper-parameter tweaking, we utilized the Tree Parzen Estimation (TPE) method as detailed in TPE. After running 200 iterations of tweaking, we ended up with the following selection: INLINEFORM0 is 6 (entailing that the various filter sizes are INLINEFORM1 ), INLINEFORM2 is 100, INLINEFORM3 is INLINEFORM4 and INLINEFORM5 is INLINEFORM6 , optimization uses Adam BIBREF13 . When training the CNN model, we randomly selected INLINEFORM7 of the training data as the validation set for using early stopping to avoid over-fitting.", "In our experiment, we firstly divided each corpus into two parts. The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers. The larger portion (the CV set) was then formulated as a 10-fold cross-validation setup for obtaining a stable and comprehensive model evaluation result. For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences. For the TED data, the Dev set contains 1046 utterances, while the CV set contains 8406 utterances. Note that, with a goal of building a speaker-independent humor detector, when partitioning our TED data set, we always kept all utterances of a single talk within the same partition. To our knowledge, this is the first time that such a strict experimental setup has been used in recognizing humor in conversations, and it makes the humor recognition task on the TED data quite challenging.", "Firstly, tokenized text strings were converted to a INLINEFORM0 tensor with shape INLINEFORM1 , where INLINEFORM2 represents sentences' maximum length while INLINEFORM3 represents the word-embedding dimension. In this study, we utilized the Word2Vec BIBREF4 embedding vectors ( INLINEFORM4 ) that were trained on 100 billion words of Google News. Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution.", ""]}
{"question_id": "3a01dc85ac983002fd631f1c28fc1cbe16094c24", "predicted_answer": "", "predicted_evidence": ["In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion.", "$$f(x,y) = \\sigma (\\vec{x}^{T}W\\vec{y} + m(A_x,y)),$$   (Eq. 18)", "In this paper, we assume that a commonsense knowledge base is composed of assertions $A$ about concepts $C$ . Each assertion $a \\in A$ takes the form of a triple $<c_1,r,c_2 >$ , where $r \\in R$ is a relation between $c_1$ and $c_2$ , such as IsA, CapableOf, etc. $c_1,c_2$ are concepts in $C$ . The relation set $R$ is typically much smaller than $C$0 . $C$1 can either be a single word (e.g., \u201cdog\u201d and \u201cbook\u201d) or a multi-word expression (e.g., \u201ctake_a_stand\u201d and \u201cgo_shopping\u201d). We build a dictionary $C$2 out of $C$3 where every concept $C$4 is a key and a list of all assertions in $C$5 concerning $C$6 , i.e., $C$7 or $C$8 , is the value.", "i.e., we use simple addition to supplement $x$ with $A_x$ , without introducing a mechanism for any further interaction between $x$ and $A_x$ . This simple approach is suitable for response selection and proves effective in practice.", "The use of an external memory module in natural language processing (NLP) tasks has received considerable attention recently, such as in question answering BIBREF12 and language modeling BIBREF13 . It has also been employed in dialogue modeling in several limited settings. With memory networks, BIBREF14 used a set of fact triples about movies as long-term memory when modeling reddit dialogues, movie recommendation and factoid question answering. Similarly in a restaurant reservation setting, BIBREF2 provided local restaurant information to the conversational model."]}
{"question_id": "00ffe2c59a3ba18d6d2b353d6ab062a152c88526", "predicted_answer": "", "predicted_evidence": ["We gratefully acknowledge the help of Alan Ritter for sharing the twitter dialogue dataset and the NTU PDCC center for providing computing resources.", "Preprocessing of the dataset includes normalizing hashtags, \u201c@User\u201d, URLs, emoticons. Vocabulary $V$ is built out of the training set with 5 as minimum word frequency, containing 62535 words and an extra $<UNK >$ token representing all unknown words.", "Researchers have also proposed several methods to incorporate knowledge as external memory into the Seq2Seq framework. BIBREF15 incorporated the topic words of the message obtained from a pre-trained latent Dirichlet allocation (LDA) model into the context vector through a joint attention mechanism. BIBREF1 mined FoodSquare tips to be searched by an input message in the food domain and encoded such tips into the context vector through one-turn hop. The model we propose in this work shares similarities with BIBREF16 , which encoded unstructured textual knowledge with a recurrent neural network (RNN). Our work distinguishes itself from previous research in that we consider a large heterogeneous commonsense knowledge base in an open-domain retrieval-based dialogue setting.", "(1) LSTMs perform better at modeling dialogues than word embeddings on our dataset, as shown by the comparison between Tri-LSTM and word embeddings.", "This linear model differs from Tri-LSTM encoder in that it represents an utterance with its bag-of-words embedding instead of RNNs."]}
{"question_id": "042800c3336ed5f4826203616a39747c61382ba6", "predicted_answer": "", "predicted_evidence": ["Our assumption is that $A_x$ is helpful in selecting an appropriate response $y$ . However, usually very few assertions in $A_x$ are related to a particular response $y$ in the open-domain setting. As a result, we define the match score of $A_x$ and $y$ as", "For tuning and evaluation, we use 20K <message, response $>$ pairs that constitute the validation set (10K) and test set (10K). They are selected by a criterion that encourages interestingness and relevance: both the message and response have to be at least 3 tokens long and contain at least one non-stopword. For every message, at least one concept has to be found in the commonsense knowledge base. For each instance, we collect another 9 random responses from elsewhere to constitute the response candidates.", "Informally speaking, such cases suggest that to some extent, Dual-LSTM (models with no memory) is able to encode certain commonsense knowledge in model parameters (e.g., word embeddings) in an implicit way. In other cases, e.g., instance 4, the message itself is enough for the selection of the correct response, where both models do equally well.", "We also analyze samples from the test set to gain an insight on how commonsense knowledge supplements the message itself in response selection by comparing Tri-LSTM encoder and Dual-LSTM encoder.", "Commonsense assertions $A_x$ associated with a message is usually large ( $>$ 100 in our experiment). We observe that in a lot of cases of open-domain conversation, response $y$ can be seen as triggered by certain perception of message $x$ defined by one or more assertions in $A_x$ , as illustrated in Figure 4 . We can see the difference between message and response pair when commonsense knowledge is used. For example, the word `Insomnia' in the message is mapped to the commonsense assertion `Insomnia, IsA, sleep $\\_$ problem'. The appropriate response is then matched to `sleep $\\_$ problem' that is `go to bed'. Similarly, the word `Hawaii' in the message is mapped to the commonsense assertion `Hawaii, UsedFor, tourism'. The appropriate response is then matched to `tourism' that is `enjoy vacation'. In this way, new words can be mapped to the commonly used vocabulary and improve response accuracy."]}
{"question_id": "52868394eb2b3b37eb5f47f51c06ad53061f4495", "predicted_answer": "", "predicted_evidence": ["Interestingly, the Mean model obtains the best performance on the 20-core subset, while HFT achieves the best performance on the 5-core subset. We hypothesize that HFT and TransNet(-Ext) models perform better on the 5-core than 20-core subset, because of the number of data. More specifically, HFT employs Latent Dirichlet Allocation BIBREF27 to approximate topic and word distributions. Thus, the probabilities are more accurate with a text corpus approximately ten times larger.", "HFT BIBREF14: A latent-factor approach combined with a topic model that aims to find topics in the review text that correlate with latent factors of the users and the items;", "We analyze in Table TABREF20 the distribution of the reviews with fine-grained and Overall ratings. Unsurprisingly, the Overall rating is always available as it is mandatory. In terms of aspects, there is a group of six that are majorly predominant (following the observation in Table TABREF19), and two that are rarely rated: Check-In and Business Service. Surprisingly, these two aspects are not sharing similar rating averages and percentiles than the others. We explain this difference due to the small number of reviews rating them (approximately $2\\%$). Furthermore, most ratings across aspects are positive: the 25th percentile is 4, with an average of $4.23$ and a median of 5.", "HotelRec includes $50\\,264\\,531$ hotel reviews from TripAdvisor in a period of nineteen years (from February 1, 2001 to May 14, 2019). The distribution of reviews over the years is available in Figure FIGREF13. There is a significant activity increase of users from 2001 to 2010. After this period, the number of reviews per year grows slowly and oscillates between one to ten million.", "TransNet(-Ext): The model is based on zheng2017joint, which learns a user and item profile based on former reviews using convolutional neural networks, and predicts the ratings using matrix factorization methods afterward. They added a regularizer network to improve performance. TransNet-Ext is an extension of TransNet by using a collaborative-filtering component in addition to user and item reviews history."]}
{"question_id": "59dc6b1d3da74a2e67a6fb1ce940b28d9e3d8de0", "predicted_answer": "", "predicted_evidence": ["Interestingly, the Mean model obtains the best performance on the 20-core subset, while HFT achieves the best performance on the 5-core subset. We hypothesize that HFT and TransNet(-Ext) models perform better on the 5-core than 20-core subset, because of the number of data. More specifically, HFT employs Latent Dirichlet Allocation BIBREF27 to approximate topic and word distributions. Thus, the probabilities are more accurate with a text corpus approximately ten times larger.", "HotelRec includes $50\\,264\\,531$ hotel reviews from TripAdvisor in a period of nineteen years (from February 1, 2001 to May 14, 2019). The distribution of reviews over the years is available in Figure FIGREF13. There is a significant activity increase of users from 2001 to 2010. After this period, the number of reviews per year grows slowly and oscillates between one to ten million.", "In this section, we first describe two different $k$-core subsets of the HotelRec dataset that we used to evaluate multiple baselines on two tasks: rating prediction and recommendation performance. We then detail the models we employed, and discuss their results.", "Mean: A simple model that predicts a rating by the mean ratings of the desired item. It is a good baseline in recommendation BIBREF13;", "Everyday a large number of people write hotel reviews on on-line platforms (e.g., Booking, TripAdvisor) to share their opinions toward multiple aspects, such as their Overall experience, the Service, or the Location. Among the most popular platforms, we selected TripAdvisor: according to their third quarterly report of November 2019, on the U.S. Securities and Exchange Commission website, TripAdvisor is the world's largest online travel site with approximately $1.4$ million hotels. Consequently, we created our dataset HotelRec based on TripAdvisor hotel reviews. The statistics of the HotelRec dataset, the 5-core, and 20-core versions are shown in Table TABREF2; each contains at least $k$ reviews for each user or item."]}
{"question_id": "713e1c7b0ab17759ba85d7cd2041e387831661df", "predicted_answer": "", "predicted_evidence": ["Finally, in Figure FIGREF21, we computed the Pearson correlation of ratings between all pairs of aspects, including fine-grained and Overall ones. Interesting, all aspect-pairs have a correlation between $0.46$ and $0.83$. We observe that Service, Value, and Rooms correlate the most with the Overall ratings. Unsurprisingly, the aspect pair Service-Check In and Rooms-Cleanliness have a correlation of $0.80$, because people often evaluate them together in a similar fashion. Interestingly, Location is the aspect that correlates the least with the others, followed by Business Service, and Check-In.", "RAND: A simple model recommending random items;", "POP BIBREF24: Another non-personalized recommender method, where items are recommended based on their popularity (i.e., the number of interactions with users). It is a common baseline to benchmark the recommendation performance;", "ItemKNN/UserKNN BIBREF25: Two standard item-based (respectively user-based) collaborative filtering methods, using $k$ nearest neighbors;", "MLP BIBREF8: Similar than GMF, but it models the interaction of latent features with a neural network instead of a linear kernel;"]}
{"question_id": "00db191facf903cef18fb1727d1cab638c277e0a", "predicted_answer": "", "predicted_evidence": ["We conduct experiments on the well-known benchmark datasets: Penn Treebank, WikiText-2, and WikiText-103. Our experiments indicate that the proposed method outperforms neural language models trained with well-tuned hyperparameters and achieves state-of-the-art scores on each dataset. In addition, we incorporate our proposed method into a standard neural encoder-decoder model and investigate its effect on machine translation and headline generation. We indicate that the proposed method also has a positive effect on such tasks.", "For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 . The training set contains about 3.8M sentence-headline pairs. For evaluation, we exclude the test set constructed by BIBREF2 because it contains some invalid instances, as reported in BIBREF33 . We instead used the test sets constructed by BIBREF33 and BIBREF34 .", "BIBREF14 and BIBREF15 proposed a word tying method (WT) that shares the word embedding matrix ( INLINEFORM0 in Equation ) with the weight matrix to compute probability distributions ( INLINEFORM1 in Equation EQREF3 ). They demonstrated that WT significantly improves the performance of RNN language models.", "In these experiments, we only applied char3-MS-vec to EncDec but BIBREF38 indicated that combining multiple kinds of subword units can improve the performance. We will investigate the effect of combining several character INLINEFORM0 -gram embeddings in future work.", "In this paper, we incorporated character information with RNN language models. Based on the research in the field of word embedding construction BIBREF0 , we focused on character INLINEFORM0 -gram embeddings to construct word embeddings. We used multi-dimensional self-attention BIBREF11 to encode character INLINEFORM1 -gram embeddings. Our proposed char INLINEFORM2 -MS-vec improved the performance of state-of-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103. Moreover, we investigated the effect of char INLINEFORM3 -MS-vec on application tasks, specifically, machine translation and headline generation. Our experiments show that char INLINEFORM4 -MS-vec also improved the performance of a neural encoder-decoder on both tasks."]}
{"question_id": "1edfe390828f02a2db9a88454421c7f3d4cdd611", "predicted_answer": "", "predicted_evidence": ["Table TABREF24 also shows that excluding INLINEFORM0 from word tying (\u201cExclude INLINEFORM1 from word tying\u201d) achieved almost the same score as the baseline. Moreover, this table indicates that performance fails as the the number of parameters is increased. Thus, we need to assign INLINEFORM2 to word tying to prevent over-fitting for the PTB dataset. In addition, this result implies that the performance of WT103 in Table TABREF15 might be raised if we can apply word tying to WT103.", "We explored the effectiveness of multi-dimensional self-attention for word embedding construction. Table TABREF24 shows perplexities of using several encoders on the PTB dataset. As in BIBREF8 , we applied CNN to construct word embeddings (charCNN in Table TABREF24 ). Moreover, we applied the summation and standard self-attention, which computes the scalar value as a weight for a character INLINEFORM0 -gram embedding, to construct word embeddings (char INLINEFORM1 -Sum-vec and char INLINEFORM2 -SS-vec, respectively). For CNN, we used hyperparameters identical to BIBREF8 (\u201cOriginal Settings\u201d in Table TABREF24 ) but the setting has two differences from other architectures: 1. The dimension of the computed vectors is much larger than the dimension of the baseline word embeddings and 2. The dimension of the input character embeddings is much smaller than the dimension of the baseline word embeddings.", "Neural language models have played a crucial role in recent advances of neural network based methods in natural language processing (NLP). For example, neural encoder-decoder models, which are becoming the de facto standard for various natural language generation tasks including machine translation BIBREF1 , summarization BIBREF2 , dialogue BIBREF3 , and caption generation BIBREF4 can be interpreted as conditional neural language models. Moreover, neural language models can be used for rescoring outputs from traditional methods, and they significantly improve the performance of automatic speech recognition BIBREF5 . This implies that better neural language models improve the performance of application tasks.", "For machine translation, we used two kinds of language pairs: English-French and English-German sentences in the IWSLT 2016 dataset. The dataset contains about 208K English-French pairs and 189K English-German pairs. We conducted four translation tasks: from English to each language (En-Fr and En-De), and their reverses (Fr-En and De-En).", "As described in Section SECREF1 , neural encoder-decoder models can be interpreted as conditional neural language models. Therefore, to investigate if the proposed method contributes to encoder-decoder models, we conduct experiments on machine translation and headline generation tasks."]}
{"question_id": "3dad6b792044018bb968ac0d0fd4628653f9e4b7", "predicted_answer": "", "predicted_evidence": ["Table TABREF15 shows perplexities of the baselines and the proposed method. We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance.", "Table TABREF24 also shows that excluding INLINEFORM0 from word tying (\u201cExclude INLINEFORM1 from word tying\u201d) achieved almost the same score as the baseline. Moreover, this table indicates that performance fails as the the number of parameters is increased. Thus, we need to assign INLINEFORM2 to word tying to prevent over-fitting for the PTB dataset. In addition, this result implies that the performance of WT103 in Table TABREF15 might be raised if we can apply word tying to WT103.", "To compute INLINEFORM0 , we apply an encoder to character INLINEFORM1 -gram embeddings. Previous studies demonstrated that additive composition, which computes the (weighted) sum of embeddings, is a suitable method for embedding construction BIBREF13 , BIBREF0 . Thus, we adopt (simplified) multi-dimensional self-attention BIBREF11 , which computes weights for each dimension of given embeddings and sums up the weighted embeddings (i.e., element-wise weighted sum) as an encoder. Let INLINEFORM2 be the character INLINEFORM3 -gram embeddings of an input word, let INLINEFORM4 be the number of character INLINEFORM5 -grams extracted from the word, and let INLINEFORM6 be the matrix whose INLINEFORM7 -th column corresponds to INLINEFORM8 , that is, INLINEFORM9 . The multi-dimensional self-attention constructs the word embedding INLINEFORM10 by the following equations: DISPLAYFORM0", "We employed the neural encoder-decoder with attention mechanism described in BIBREF34 as the base model. Its encoder consists of a 2-layer bidirectional LSTM and its decoder consists of a 2-layer LSTM with attention mechanism proposed by BIBREF36 . We refer to this neural encoder-decoder as EncDec. To investigate the effect of the proposed method, we introduced char3-MS-vec into EncDec. Here, we applied char3-MS-vec to both the encoder and decoder. Moreover, we did not apply word tying technique to EncDec because it is default setting in the widely-used encoder-decoder implementation.", "In general, neural language models require word embeddings as an input BIBREF6 . However, as described by BIBREF7 , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word. For example, we can comprehend that the word `causal' is related to `cause' immediately because both words include the same character sequence `caus'. Thus, if we incorporate a method that handles the internal structure such as character information, we can improve the quality of neural language models and probably make them robust to infrequent words."]}
{"question_id": "a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "predicted_answer": "", "predicted_evidence": ["Let us consider the case where an input word is `the' and we use character 3-gram in Figure FIGREF4 . We prepare special characters `' and `$' to represent the beginning and end of the word, respectively. Then, `the' is composed of three character 3-grams: `th', `the', and `he$'. We multiply the embeddings of these 3-grams by transformation matrix INLINEFORM0 and apply the softmax function to each row as in Equation . As a result of the softmax, we obtain a matrix that contains weights for each embedding. The size of the computed matrix is identical to the input embedding matrix: INLINEFORM1 . We then compute Equation EQREF7 , i.e., the weighted sum of the embeddings, and add the resulting vector to the word embedding of `the'. Finally, we input the vector into an RNN to predict the next word.", "Table TABREF17 shows perplexities on the PTB dataset where the frequency of an input word is lower than 2,000 in the training data. This table indicates that the proposed method can improve the performance even if an input word is infrequent. In other words, char INLINEFORM0 -MS-vec helps represent the meanings of infrequent words. Therefore, we answer yes to the second research question in the case of our experimental settings.", "On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM).", "In this paper, we incorporated character information with RNN language models. Based on the research in the field of word embedding construction BIBREF0 , we focused on character INLINEFORM0 -gram embeddings to construct word embeddings. We used multi-dimensional self-attention BIBREF11 to encode character INLINEFORM1 -gram embeddings. Our proposed char INLINEFORM2 -MS-vec improved the performance of state-of-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103. Moreover, we investigated the effect of char INLINEFORM3 -MS-vec on application tasks, specifically, machine translation and headline generation. Our experiments show that char INLINEFORM4 -MS-vec also improved the performance of a neural encoder-decoder on both tasks.", "Neural language models have played a crucial role in recent advances of neural network based methods in natural language processing (NLP). For example, neural encoder-decoder models, which are becoming the de facto standard for various natural language generation tasks including machine translation BIBREF1 , summarization BIBREF2 , dialogue BIBREF3 , and caption generation BIBREF4 can be interpreted as conditional neural language models. Moreover, neural language models can be used for rescoring outputs from traditional methods, and they significantly improve the performance of automatic speech recognition BIBREF5 . This implies that better neural language models improve the performance of application tasks."]}
{"question_id": "5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is a weight matrix, INLINEFORM1 is a bias term, and INLINEFORM2 is a word embedding matrix. INLINEFORM3 and INLINEFORM4 are a one-hot vector of an input word INLINEFORM5 and the hidden state of the RNN at timestep INLINEFORM6 , respectively. We define INLINEFORM7 at timestep INLINEFORM8 as a zero vector, that is, INLINEFORM9 . Let INLINEFORM10 represent an abstract function of an RNN, which might be the LSTM, the Quasi-Recurrent Neural Network (QRNN) BIBREF12 , or any other RNN variants.", "BIBREF14 and BIBREF15 proposed a word tying method (WT) that shares the word embedding matrix ( INLINEFORM0 in Equation ) with the weight matrix to compute probability distributions ( INLINEFORM1 in Equation EQREF3 ). They demonstrated that WT significantly improves the performance of RNN language models.", "Neural language models have played a crucial role in recent advances of neural network based methods in natural language processing (NLP). For example, neural encoder-decoder models, which are becoming the de facto standard for various natural language generation tasks including machine translation BIBREF1 , summarization BIBREF2 , dialogue BIBREF3 , and caption generation BIBREF4 can be interpreted as conditional neural language models. Moreover, neural language models can be used for rescoring outputs from traditional methods, and they significantly improve the performance of automatic speech recognition BIBREF5 . This implies that better neural language models improve the performance of application tasks.", "In this study, we adopt char INLINEFORM0 -MS-vec as the weight matrix in language modeling. Concretely, we use INLINEFORM1 instead of INLINEFORM2 in Equation EQREF3 , where INLINEFORM3 contains char INLINEFORM4 -MS-vec for all words in the vocabulary.", "Table TABREF24 also shows that excluding INLINEFORM0 from word tying (\u201cExclude INLINEFORM1 from word tying\u201d) achieved almost the same score as the baseline. Moreover, this table indicates that performance fails as the the number of parameters is increased. Thus, we need to assign INLINEFORM2 to word tying to prevent over-fitting for the PTB dataset. In addition, this result implies that the performance of WT103 in Table TABREF15 might be raised if we can apply word tying to WT103."]}
{"question_id": "8e26c471ca0ee1b9779da04c0b81918fd310d0f3", "predicted_answer": "", "predicted_evidence": ["In addition to the results of our implementations, the lower portion of Table TABREF32 contains results reported in previous studies. Table TABREF32 shows that EncDec+char3-MS-vec also outperformed the methods proposed in previous studies. Therefore, EncDec+char3-MS-vec achieved the top scores in the test sets constructed by BIBREF33 and BIBREF34 even though it does not have a task-specific architecture such as the selective gate proposed by BIBREF33 .", "To estimate the conditional probability INLINEFORM0 , RNN language models encode sequence INLINEFORM1 into a fixed-length vector and compute the probability distribution of each word from this fixed-length vector. Let INLINEFORM2 be the vocabulary size and let INLINEFORM3 be the probability distribution of the vocabulary at timestep INLINEFORM4 . Moreover, let INLINEFORM5 be the dimension of the hidden state of an RNN and let INLINEFORM6 be the dimensions of embedding vectors. Then, RNN language models predict the probability distribution INLINEFORM7 by the following equation: DISPLAYFORM0", "In this paper, we incorporated character information with RNN language models. Based on the research in the field of word embedding construction BIBREF0 , we focused on character INLINEFORM0 -gram embeddings to construct word embeddings. We used multi-dimensional self-attention BIBREF11 to encode character INLINEFORM1 -gram embeddings. Our proposed char INLINEFORM2 -MS-vec improved the performance of state-of-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103. Moreover, we investigated the effect of char INLINEFORM3 -MS-vec on application tasks, specifically, machine translation and headline generation. Our experiments show that char INLINEFORM4 -MS-vec also improved the performance of a neural encoder-decoder on both tasks.", "We used the standard benchmark datasets for the word-level language modeling: Penn Treebank (PTB) BIBREF16 , WikiText-2 (WT2), and WikiText-103 (WT103) BIBREF17 . BIBREF18 and BIBREF17 published pre-processed PTB, WT2, and WT103. Following the previous studies, we used these pre-processed datasets for our experiments.", "We explored the effectiveness of multi-dimensional self-attention for word embedding construction. Table TABREF24 shows perplexities of using several encoders on the PTB dataset. As in BIBREF8 , we applied CNN to construct word embeddings (charCNN in Table TABREF24 ). Moreover, we applied the summation and standard self-attention, which computes the scalar value as a weight for a character INLINEFORM0 -gram embedding, to construct word embeddings (char INLINEFORM1 -Sum-vec and char INLINEFORM2 -SS-vec, respectively). For CNN, we used hyperparameters identical to BIBREF8 (\u201cOriginal Settings\u201d in Table TABREF24 ) but the setting has two differences from other architectures: 1. The dimension of the computed vectors is much larger than the dimension of the baseline word embeddings and 2. The dimension of the input character embeddings is much smaller than the dimension of the baseline word embeddings. Therefore, we added two configurations: assigning the dimension of the computed vectors and input character embeddings a value identical to the baseline word embeddings (in Table TABREF24 , \u201cSmall CNN result dims\u201d and \u201cLarge embedding dims\u201d, respectively)."]}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "predicted_answer": "", "predicted_evidence": ["Each record in the dataset has a target description attached with it. This is the entire text of the article whose title has been given. By definition, clickbait articles differ from the content described in their headline. We generate document embeddings for both the title and the article text and perform element wise multiplication over the two. This allows us to capture the interaction between the two, something which has not been used before. Since the title is supposed to mislead the reader with respect to the content, modeling this interaction in terms of their similarity gives an added dimenstion to our approach. It augments the output obtained from the first component.", "Keeping up with the times, news agencies have expanded their digital presence, increasing their reach exponentially. They generate revenue by (1) advertisements on their websites, or (2) a subscription based model for articles that might interest users. Since multiple agencies offer similar content, the user has his pick. To lure in more readers and increase the number of clicks on their content, subsequently enhancing their agency's revenue, writers have begun adopting a new technique - clickbait.", "Merriam-Webster defines clickbait as something (such as a headline) to encourage readers to click on hyperlinks based on snippets of information accompanying it, especially when those links lead to content of dubious value or interest. It is built to create, and consequently capitalise, on the Loewenstein information gap BIBREF0 by purposefully misrepresenting or promising what can be expected while reading a story on the web, be it through a headline, image or related text.", "INLINEFORM0 represent forward states of the LSTM and its state updates satisfy the following equations: DISPLAYFORM0 DISPLAYFORM1", "We use binary cross-entropy as the loss optimization function for our model. The cross-entropy method BIBREF18 is an iterative procedure where each iteration can be divided into two stages:"]}
{"question_id": "dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "predicted_answer": "", "predicted_evidence": ["We propose a two-pronged approach to detect such headlines. The first component leverages distributional semantics of the title text and models its temporal and sequential properties. The article title is represented as a concatenation of its sub-word level embeddings. The sub-word representation serves as input to a bidirectional LSTM network. The contribution of a sub-word towards the clickbait nature of the headline is calculated in a differential manner since the output of the LSTM is passed into an attention layer BIBREF1 , following which it goes through a dense layer. The second component focuses on Doc2Vec embeddings of the title and article content, performing an element wise multiplication of the two. This is concatenated with the dense layer output from the previous component. The obtained output is then passed through multiple hidden layers which performs the final classification.", "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers. A standard RNN has an internal state whose output at every time-step which can be expressed in terms of that of previous time-steps. However, it has been seen that standard RNNs suffer from a problem of vanishing gradients BIBREF17 . This means it will not be able to efficiently model dependencies and interactions between sub-word representations that are a few steps apart. LSTMs are able to tackle this issue by their use of gating mechanisms. We convert each article headline into its corresponding sub-word level representation to act as input to our bidirectional LSTMs.", "BIBREF6 used the same collection of headlines as BIBREF5 and proposed the first neural network based approach in the field. They employed various recurrent neural network architectures to model sequential data and its dependencies, taking as its inputs a concatenation of the word and character-level embeddings of the headline. Their experiments yielded that bidirectional LSTMs BIBREF7 were best suited for the same. BIBREF8 built BiLSTMs to model each textual attribute of the post (post-text, target-title, target-paragraphs, target-description, target-keywords, post-time) available in the corpus BIBREF4 , concatenating their outputs and feeding it to a fully connected layer to classify the post. Attention mechanisms BIBREF1 have grown popular for various text classification tasks, like aspect based sentiment analysis. Utilising this technique, BIBREF9 deployed a self-attentive bidirectional GRU to infer the importance of each tweet token and model the annotation distribution of headlines in the corpus.", "We now describe our approach to clickbait detection and the reasons behind devising such a model. Our approach is a fusion of multiple components, each exploiting a particular type of embedding: (1) BiLSTM with attention, and (2) Doc2Vec enrichment. Figure FIGREF14 lays out our proposed architecture.", "Merriam-Webster defines clickbait as something (such as a headline) to encourage readers to click on hyperlinks based on snippets of information accompanying it, especially when those links lead to content of dubious value or interest. It is built to create, and consequently capitalise, on the Loewenstein information gap BIBREF0 by purposefully misrepresenting or promising what can be expected while reading a story on the web, be it through a headline, image or related text."]}
{"question_id": "e9b6b14b8061b71d73a73d8138c8dab8eda4ba3f", "predicted_answer": "", "predicted_evidence": ["Doc2Vec BIBREF15 is an unsupervised approach to generate vector representations for slightly larger bodies of text, such as sentences, paragraphs and documents. It has been adapted from Word2Vec BIBREF10 which is used to generate vectors for words in large unlabeled corpora. The vectors generated by this approach come handy in tasks like calculating similarity metrics for sentences, paragraphs and documents. In sequential models like RNNs, the word sequence is captured in the generated sentence vectors. However, in Doc2Vec, the representations are order independent. We use GenSim BIBREF16 to learn 300 dimensional Doc2Vec embeddings for each target description and post title available.", "The number of bidirectional LSTM units is set to a constant K, which is the maximum length of all title lengths of records used in training. The forward and backward states are then concatenated to obtain INLINEFORM0 , where DISPLAYFORM0", "BIBREF11 proposed a convolutional neural network architecture to generate subword-level representations of words in order to capture word orthography. Sub-word level embeddings learn representations for character n-grams and represent words as the sum of the n-gram vectors BIBREF12 . Such representations also take into account word roots and inflections, rather than just word context. They work well even with highly noisy text with containing misspellings due to the model learning morpheme-level feature maps. They have proven to be extremely useful in tasks such as sentiment analysis BIBREF13 , PoS tagging BIBREF14 and language modeling BIBREF11 . These intermediate sub-word feature representations are learned by the filters during the convolution operation. We generate such an embedding by passing the characters of a sentence individually into 3 layer 1D convolutional neural network. Each filter then acts as a learned sub-word level feature.", "here INLINEFORM0 is the logistic sigmoid function, INLINEFORM1 , INLINEFORM2 , INLINEFORM3 represent the forget, input and output gates respectively. INLINEFORM4 denotes the input at time INLINEFORM5 and INLINEFORM6 denotes the latent state, INLINEFORM7 and INLINEFORM8 represent the bias terms. The forget, input and output gates control the flow of information throughout the sequence. INLINEFORM9 and INLINEFORM10 are matrices which represent the weights associated with the connections.", "In Table 1, we evaluate our model against the existing state-of-the-art for the dataset used and other models which have employed similar techniques to accomplish the task. It is clear that our proposed model outperforms the previous feature engineering benchmark and other work done in the field both in terms of F1 score and accuracy of detection. Feature engineering models rely on a selection of handcrafted attributes which may not be able to consider all the factors involved in making a post clickbait. The approach proposed in BIBREF8 takes into account each of the textual features available in an individual fashion, considering them to be independent of each other, which is not the case since, by definition of clickbait, the content of the article title and text are not mutually exclusive. BIBREF21 proposed the integration of multimodal embeddings. BIBREF6 utilise word and character embeddings which do not capture morpheme-level information that may incorporate a surprise element."]}
{"question_id": "76e17e648a4d1f386eb6bf61b0c24f134af872be", "predicted_answer": "", "predicted_evidence": ["After identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove final softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset.", "This phenomenon, called false positive bias, has been reported by BIBREF1 . They further defined this model bias as unintended, \u201ca model contains unintended bias if it performs better for comments containing some particular identity terms than for comments containing others.\u201d", "Debiased word2vec BIBREF9 is compared with the original word2vec BIBREF10 for evaluation. For gender swapping data augmentation, we use pairs identified through crowd-sourcing by BIBREF13 .", "Debiased Word Embeddings (DE) BIBREF9 proposed an algorithm to correct word embeddings by removing gender stereotypical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their effectiveness in our task.", "Recently, BIBREF4 has published a large scale crowdsourced abusive tweet dataset with 60K tweets. Their work incrementally and iteratively investigated methods such as boosted sampling and exploratory rounds, to effectively annotate tweets through crowdsourcing. Through such systematic processes, they identify the most relevant label set in identifying abusive behaviors in Twitter as INLINEFORM0 resulting in 11% as 'Abusive,' 7.5% as 'Hateful', 22.5% as 'Spam', and 59% as 'None'. We transform this dataset for a binary classification problem by concatenating 'None'/'Spam' together, and 'Abusive'/'Hateful' together."]}
{"question_id": "7572f6e68a2ed2c41b87c5088ba8680afa0c0a0b", "predicted_answer": "", "predicted_evidence": ["Automatic detection of abusive language is an important task since such language in online space can lead to personal trauma, cyber-bullying, hate crime, and discrimination. As more and more people freely express their opinions in social media, the amount of textual contents produced every day grows almost exponentially, rendering it difficult to effectively moderate user content. For this reason, using machine learning and natural language processing (NLP) systems to automatically detect abusive language is useful for many websites or social media services.", "On the other hand, abt dataset showed significantly better results on the two equality difference scores, of at most 0.04. Performance in the generated test set was better because the models successfully classify abusive samples regardless of the gender identity terms used. Hence, we can assume that abt dataset is less gender-biased than the st dataset, presumably due to its larger size, balance in classes, and systematic collection method.", "However, the equality difference scores tended to be larger when pre-trained embeddings were used, especially in the st dataset. This confirms the result of BIBREF9 . In all experiments, direction of the gender bias was towards female identity words. We can infer that this is due to the more frequent appearances of female identities in \u201csexist\u201d tweets and lack of negative samples, similar to the reports of BIBREF1 . This is problematic since not many NLP datasets are large enough to reflect the true data distribution, more prominent in tasks like abusive language where data collection and annotation are difficult.", "This phenomenon, called false positive bias, has been reported by BIBREF1 . They further defined this model bias as unintended, \u201ca model contains unintended bias if it performs better for comments containing some particular identity terms than for comments containing others.\u201d", "We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged."]}
{"question_id": "5d2bbcc3aa769e639dc21893890bc36b76597a33", "predicted_answer": "", "predicted_evidence": ["Recently, BIBREF4 has published a large scale crowdsourced abusive tweet dataset with 60K tweets. Their work incrementally and iteratively investigated methods such as boosted sampling and exploratory rounds, to effectively annotate tweets through crowdsourcing. Through such systematic processes, they identify the most relevant label set in identifying abusive behaviors in Twitter as INLINEFORM0 resulting in 11% as 'Abusive,' 7.5% as 'Hateful', 22.5% as 'Spam', and 59% as 'None'. We transform this dataset for a binary classification problem by concatenating 'None'/'Spam' together, and 'Abusive'/'Hateful' together.", "We focus on the fact that the representations of abusive language learned in only supervised learning setting may not be able to generalize well enough for practical use since they tend to overfit to certain words that are neutral but occur frequently in the training samples. To such classifiers, sentences like \u201cYou are a good woman\u201d are considered \u201csexist\u201d probably because of the word \u201cwoman.\u201d", "Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. BIBREF9 is one of the first works to point out the gender stereotypes inside word2vec BIBREF10 and propose an algorithm to correct them. BIBREF11 also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. BIBREF1 is one of the first works that point out existing \u201cunintended\u201d bias in abusive language detection models. BIBREF12 compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems. BIBREF13 shows the effectiveness of measuring and correcting gender biases in co-reference resolution tasks.", "So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd-sourcing or user moderation BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system BIBREF6 , BIBREF7 , BIBREF8 . However, these works do not explicitly address any model bias in their models.", "After identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove final softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset."]}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "predicted_answer": "", "predicted_evidence": ["Automatic detection of abusive language is an important task since such language in online space can lead to personal trauma, cyber-bullying, hate crime, and discrimination. As more and more people freely express their opinions in social media, the amount of textual contents produced every day grows almost exponentially, rendering it difficult to effectively moderate user content. For this reason, using machine learning and natural language processing (NLP) systems to automatically detect abusive language is useful for many websites or social media services.", "We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:", "Since the classifiers output probabilities, equal error rate thresholds are used for prediction decision.", "Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments.", "INLINEFORM0 -GRU: hidden dimension=256 (bidirectional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embedding Size=300, Dropout=0.3"]}
{"question_id": "5d93245832d90b31aee42ea2bf1e7704c22ebeca", "predicted_answer": "", "predicted_evidence": ["All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \u201cunbiased\u201d features.", "Since the classifiers output probabilities, equal error rate thresholds are used for prediction decision.", "Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. BIBREF9 is one of the first works to point out the gender stereotypes inside word2vec BIBREF10 and propose an algorithm to correct them. BIBREF11 also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. BIBREF1 is one of the first works that point out existing \u201cunintended\u201d bias in abusive language detection models. BIBREF12 compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems.", "Debiased Word Embeddings (DE) BIBREF9 proposed an algorithm to correct word embeddings by removing gender stereotypical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their effectiveness in our task.", "Table TABREF16 shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbiased test set. The latter improvement shows that the model significantly reduced errors on the unbiased set in general."]}
{"question_id": "c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "predicted_answer": "", "predicted_evidence": ["Such model bias is important but often unmeasurable in the usual experiment settings since the validation/test sets we use for evaluation are already biased. For this reason, we tackle the issue of measuring and mitigating unintended bias. Without achieving certain level of generalization ability, abusive language detection models may not be suitable for real-life situations.", "So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd-sourcing or user moderation BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system BIBREF6 , BIBREF7 , BIBREF8 . However, these works do not explicitly address any model bias in their models.", "Tables TABREF12 and TABREF14 show the bias measurement experiment results for st and abt, respectively. As expected, pre-trained embeddings improved task performance. The score on the unbiased generated test set (Gen. ROC) also improved since word embeddings can provide prior knowledge of words.", "The intuition of this template method is that given a pair of sentences with only the identity terms different (ex. \u201cHe is happy\u201d & \u201cShe is happy\u201d), the model should be able to generalize well and output same prediction for abusive language. This kind of evaluation has also been performed in SemEval 2018: Task 1 Affect In Tweets BIBREF12 to measure the gender and race bias among the competing systems for sentiment/emotion analysis.", "Since the classifiers output probabilities, equal error rate thresholds are used for prediction decision."]}
{"question_id": "ed7ce13cd95f7664a5e4fc530dcf72dc3808dced", "predicted_answer": "", "predicted_evidence": ["There also exist some papers for dynamical computational graph construction. At the lower level, pointer-switch networks BIBREF18 are a kind of dynamic differentiable neural model. At the higher level, some architecture search models BIBREF19 , BIBREF20 construct new differentiable computational graphs dynamically at every iteration.", "Conventionally, neural methodology aligns the sentence pair and then generates a matching score for paraphrase identification, BIBREF4 , BIBREF2 . Regarding the alignment, we conjecture that the aligned unmatched parts are semantically critical, where we define the corresponded word pairs with low similarity as aligned unmatched parts. For an example: \u201cOn Sunday, the boy runs in the yard\u201d and \u201cThe child runs inside at the weekend\u201d, the matched parts (i.e. (Sunday, weekend), (boy, child), run) barely make contribution to the semantic sentence similarity, but the unmatched parts (i.e. \u201cyard\u201d and \u201cinside\u201d) determine these two sentences are semantically dissimilar. For another example: \u201cOn Sunday, the boy runs in the yard\u201d and \u201cThe child runs outside at the weekend\u201d, the aligned unmatched parts (i.e.", "As Figure FIGREF13 shows, in the forward propagation, Hungarian algorithm works out the aligned position pairs, according to which, neural components are dynamically connected to the next layer. For the example of Figure FIGREF13 , the 1st source and 2nd target word representations are jointly linked to the 1st aligned position of concatenation layer. Once the computational graph has been dynamically constructed in the forward pass, the backward process could propagate through the dynamically constructed links between layers, without any branching and non-differentiated issues. For the example in Figure FIGREF13 , the backward pass firstly propagates to the 1st aligned position of concatenation layer, then respectively propagates to 1st source and 2nd target word representations. In this way, the optimization framework could still adjust the parameters of neural architectures in an end-to-end manner.", "where INLINEFORM0 is the matching score, INLINEFORM1 is the length of vector and INLINEFORM2 / INLINEFORM3 is the corresponding source/target part of the final sentence representation INLINEFORM4 . Thus, our output ranges in INLINEFORM5 , where INLINEFORM6 means the two sentences are similar/paraphrase, and INLINEFORM7 means otherwise. For further evaluation of accuracy, we also apply a threshold learned in the development dataset to binary the cosine similarity as paraphrase/non-paraphrase. Notably, the introduction of concatenation layer facilitates the inference and training of Hungarian layer.", "In summary, the advantage of this branch, which roots the foundation in linguistics, is semantically interpretable, while the disadvantage is too simple to understand complex language phenomenon."]}
{"question_id": "26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "predicted_answer": "", "predicted_evidence": ["Conventionally, neural methodology aligns the sentence pair and then generates a matching score for paraphrase identification, BIBREF4 , BIBREF2 . Regarding the alignment, we conjecture that the aligned unmatched parts are semantically critical, where we define the corresponded word pairs with low similarity as aligned unmatched parts. For an example: \u201cOn Sunday, the boy runs in the yard\u201d and \u201cThe child runs inside at the weekend\u201d, the matched parts (i.e. (Sunday, weekend), (boy, child), run) barely make contribution to the semantic sentence similarity, but the unmatched parts (i.e. \u201cyard\u201d and \u201cinside\u201d) determine these two sentences are semantically dissimilar. For another example: \u201cOn Sunday, the boy runs in the yard\u201d and \u201cThe child runs outside at the weekend\u201d, the aligned unmatched parts (i.e. \u201cyard\u201d and \u201coutside\u201d) are semantically similar, which makes the two sentences paraphrase.", "Cosine Similarity. Last, we average the concatenated hidden representations as the final sentence representation INLINEFORM0 , which is a conventional procedure in neural natural language processing, BIBREF4 . Then, we employ a cosine similarity as the output: DISPLAYFORM0", "Compared with Siamese LSTM BIBREF24 , which lacks the matching layer, our model could precisely align the input sentences. Thus, our method promotes the performance.", "First, we introduce the basic components of our neural architecture. Then, we analyze the training process of Hungarian layer, that how to dynamically construct the computational graph.", "In summary, this case study justifies our assumption that \u201cthe aligned unmatched parts are semantically critical\u201d."]}
{"question_id": "ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "predicted_answer": "", "predicted_evidence": ["Previously discussed, Hungarian algorithm is embedded into neural architecture, making a challenge for learning process. We tackle this issue by modifying the back-propagation algorithm in a dynamically graph-constructing manner. In the forward pass, we dynamically construct the links between Hungarian layer and the next layer, according to the aligned position pairs, while in the backward process, the back-propagation is performed through the dynamically constructed links. Next, we illustratively exemplify how the computational graph is dynamically constructed in Hungarian layer as Figure FIGREF13 shows.", "Algorithm SECREF7 demonstrates the first stage. The objective of this stage is to align the source and target hidden representations. The inputs of this stage are INLINEFORM0 source hidden representation vectors INLINEFORM1 and INLINEFORM2 target hidden representation vectors INLINEFORM3 , while the outputs of this stage are INLINEFORM4 aligned hidden representation vector pairs INLINEFORM5 , assuming INLINEFORM6 , where INLINEFORM7 corresponds to the INLINEFORM8 -th aligned source/target hidden representation vector, respectively.", "Conventionally, neural methodology aligns the sentence pair and then generates a matching score for paraphrase identification, BIBREF4 , BIBREF2 . Regarding the alignment, we conjecture that the aligned unmatched parts are semantically critical, where we define the corresponded word pairs with low similarity as aligned unmatched parts. For an example: \u201cOn Sunday, the boy runs in the yard\u201d and \u201cThe child runs inside at the weekend\u201d, the matched parts (i.e. (Sunday, weekend), (boy, child), run) barely make contribution to the semantic sentence similarity, but the unmatched parts (i.e. \u201cyard\u201d and \u201cinside\u201d) determine these two sentences are semantically dissimilar. For another example: \u201cOn Sunday, the boy runs in the yard\u201d and \u201cThe child runs outside at the weekend\u201d, the aligned unmatched parts (i.e. \u201cyard\u201d and \u201coutside\u201d) are semantically similar, which makes the two sentences paraphrase.", "[t] Hungarian Layer: First Stage [1] Source and target sentence hidden representations: INLINEFORM0 and INLINEFORM1 . INLINEFORM2 , where INLINEFORM3 and INLINEFORM4 mean the INLINEFORM5 -th aligned hidden representations for source and target respectively, and INLINEFORM6 means the corresponding similarity. Generate the pairwise similarity matrix: INLINEFORM7", "where INLINEFORM0 is the INLINEFORM1 -th hidden representation and INLINEFORM2 corresponds to the INLINEFORM3 -th word embedding in the source/target sentence or INLINEFORM4 / INLINEFORM5 ."]}
{"question_id": "ee19fd54997f2eec7c87c7d4a2169026fe208285", "predicted_answer": "", "predicted_evidence": ["These models are usually tested on domains where the input is either one document or a small set of documents. However, the number of opinions tends to be very large (150 for the example in Figure FIGREF1). It is therefore practically unfeasible to train a model in an end-to-end fashion, given the memory limitations of modern hardware. As a result, current approaches BIBREF16, BIBREF17, BIBREF18, BIBREF19 sacrifice end-to-end elegance in favor of a two-stage framework which we call Extract-Abstract: an extractive model first selects a subset of opinions and an abstractive model then generates the summary while conditioning on the extracted subset (see Figure FIGREF5). The extractive pass unfortunately has two drawbacks. Firstly, on account of having access to a subset of opinions, the summaries can be less informative and inaccurate, as shown in Figure FIGREF1. And secondly, user preferences cannot be easily taken into account (e.g., the reader may wish to obtain a summary focusing on the acting or plot of a movie as opposed to a general-purpose summary) since more specialized information might have been removed.", "We considered two evaluation metrics which are also reported in BIBREF16: METEOR BIBREF40, a recall-oriented metric that rewards matching stems, synonyms, and paraphrases, and ROUGE-SU4 BIBREF41 which is calculated as the recall of unigrams and skip-bigrams up to four words. We also report F1 for ROUGE-1, ROUGE-2, and ROUGE-L, which are widely used in summarization BIBREF41. They respectively measure word-overlap, bigram-overlap, and the longest common subsequence between the reference and system summaries. Our results are presented in Table TABREF28. The first block shows one-pass systems, both supervised (SubModular, SummaRunner) and unsupervised (LexRank, Opinosis). We can see that SummaRunner is the best performing system in this block; despite being extractive, it benefits from training data and the ability of neural models to learn task-specific representations.", "We also report F1 for ROUGE-1, ROUGE-2, and ROUGE-L, which are widely used in summarization BIBREF41. They respectively measure word-overlap, bigram-overlap, and the longest common subsequence between the reference and system summaries. Our results are presented in Table TABREF28. The first block shows one-pass systems, both supervised (SubModular, SummaRunner) and unsupervised (LexRank, Opinosis). We can see that SummaRunner is the best performing system in this block; despite being extractive, it benefits from training data and the ability of neural models to learn task-specific representations. The second block in Table TABREF28 shows several two-pass abstractive systems based on the EA framework.", "In this paper, we propose Condense-Abstract, an alternative two-stage framework which uses all input documents when generating the summary (see Figure FIGREF5). We view the opinion summarization problem as an instance of multi-source transduction BIBREF20; we first represent the input documents as multiple encodings, aiming to condense their meaning and distill information relating to sentiment and various aspects of the target being reviewed. These condensed representations are then aggregated using a multi-source fusion module based on which an opinion summary is generated using an abstractive model. We also introduce a zero-shot customization technique allowing users to control important aspects of the generated summary at test time. Our approach enables controllable generation while leveraging the full spectrum of opinions available for a specific target. We perform experiments on a dataset consisting of movie reviews and opinion summaries elicited from the Rotten Tomatoes website (BIBREF16; see Figure FIGREF1).", "Specifically, they use a ridge regression model with hand-engineered features such as TF-IDF scores and word counts, to estimate the importance of a document relative to its cluster (see also BIBREF17 for a survey of additional document selection methods). The extracted documents are then concatenated into a long sequence and fed to an encoder-decoder model. Our proposed framework eliminates the need to pre-select salient documents which we argue leads to information loss and less flexible generation capability. Instead, a separate model first condenses the source documents into multiple dense vectors which serve as input to a decoder to generate an abstractive summary. Beyond producing more informative summaries, we demonstrate that our approach allows to customize them. Recent conditional generation models have focused on controlling various aspects of the output such as politeness BIBREF26, length BIBREF27, BIBREF28, content BIBREF28, or style BIBREF29. In contrast to these approaches, our customization technique requires neither training examples of documents and corresponding (customized) summaries nor specialized pre-processing to encode which tokens in the input might give rise to customization."]}
{"question_id": "74fcb741d29892918903702dbb145fef372d1de3", "predicted_answer": "", "predicted_evidence": ["More concretely, in the movie review domain, we assume that users might wish to obtain a summary that focuses on the positive or negative aspects of a movie, the quality of the acting, or the plot. In a different domain, users might care about the price of a product, its comfort, and so on. We undertake such customization without requiring access to need-specific summaries at training time. Instead, at test time, we assume access to background reviews to represent the user need. For example, if we wish to generate a positive summary, our method requires a set of reviews with positive sentiment which approximately provide some background on how sentiment is communicated in a review. We use these background reviews conveying a user need $x$ (e.g., acting, plot, positive or negative sentiment) during fusion to attend more to input reviews related to $x$. Let $C_x$ denote the set of background reviews. We obtain a new query vector $\\hat{d} = \\sum _{c=1}^{|C_x|} d_c / |C_x|$, where $d_c$ is the document encoding of the $c$'th review in $C_x$, calculated using the Condense model.", "The decoder generates summaries conditioned on the reduced document encoding $d^{\\prime }$ and reduced word-level encodings $h^{\\prime }_1,h^{\\prime }_2,...,h^{\\prime }_V$. We use a simple LSTM decoder enhanced with attention BIBREF14 and copy mechanisms BIBREF32. We set the first hidden state $s_0$ to $d^{\\prime }$, and run an LSTM to calculate the current hidden state using the previous hidden state $s_{t-1}$ and word $y^{\\prime }_{t-1}$ at time step $t$:", "We considered two evaluation metrics which are also reported in BIBREF16: METEOR BIBREF40, a recall-oriented metric that rewards matching stems, synonyms, and paraphrases, and ROUGE-SU4 BIBREF41 which is calculated as the recall of unigrams and skip-bigrams up to four words. We also report F1 for ROUGE-1, ROUGE-2, and ROUGE-L, which are widely used in summarization BIBREF41. They respectively measure word-overlap, bigram-overlap, and the longest common subsequence between the reference and system summaries. Our results are presented in Table TABREF28. The first block shows one-pass systems, both supervised (SubModular, SummaRunner) and unsupervised (LexRank, Opinosis).", "We further assessed the ability of CA-based systems to generate customized summaries at test time. As discussed earlier, customization at test time is not trivially possible for EA-based systems and as a result we cannot compare against them. Instead, we evaluate two CA-based systems, namely AE+Att+Copy and AE+Att+Copy+Salient. Similar to EA-based systems, the latter biases summary generation towards the $k$ most salient extracted opinions using an additional extractive module, which may not contain information relevant to the user's need (we set $k=5$ in our experiments). We thus expect this model to be less effective for customization than AE+Att+Copy which makes no assumptions regarding which summaries to consider. In this experiment, we assume users may wish to control the output summaries in four ways focusing on acting- and plot-related aspects of a movie review, as well as its sentiment, which may be positive or negative.", "Textual summaries are created following mostly extractive methods which select representative segments (usually sentences) from the source text BIBREF2, BIBREF3, BIBREF4, BIBREF5. Despite being less popular, abstractive approaches seem more appropriate for the task at hand as they attempt to generate summaries which are maximally informative and minimally redundant without simply rearranging passages from the original opinions BIBREF6, BIBREF7, BIBREF8, BIBREF9. General-purpose summarization approaches have recently shown promising results with end-to-end models which are data-driven and take advantage of the success of sequence-to-sequence neural network architectures. Most approaches BIBREF10, BIBREF11, BIBREF12, BIBREF13 encode documents and then decode the learned representations into an abstractive summary, often by attending to the source input BIBREF14 and copying words from it BIBREF15. Under this modeling paradigm, it is no longer necessary to identify aspects and their sentiment for the opinion summarization task, as these are learned indirectly from training data (i.e., sets of opinions and their corresponding summaries)."]}
{"question_id": "de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "predicted_answer": "", "predicted_evidence": ["As an upper bound, we also included Gold standard summaries. The study was conducted on the Amazon Mechanical Turk platform using Best-Worst Scaling (BWS; BIBREF42), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales BIBREF43. Specifically, participants were shown the movie title and basic background information (i.e., synopsis, release year, genre, director, and cast). They were also presented with three system summaries and asked to select the best and worst among them according to Informativeness (i.e., does the summary convey opinions about specific aspects of the movie in a concise manner?), Correctness (i.e., is the information in the summary factually accurate and does it correspond to the information given about the movie?), and Grammaticality (i.e., is the summary fluent and grammatical?). Examples of system summaries are shown in Figure FIGREF1 and Figure FIGREF37. We randomly selected 50 movies from the test set and compared all possible combinations of summary triples for each movie.", "We use two objective functions to train the Abstract model. Firstly, we use a maximum likelihood loss to optimize the generation probability distribution $p(y^{\\prime }_t)$ based on gold summaries $Y=\\lbrace y_1,y_2,...,y_L\\rbrace $ provided at training time:", "The model presented so far treats all documents as equally important and has no specific mechanism to encourage saliency and eliminate redundancy. In order to encourage the decoder to focus on salient content, we can straightforwardly incorporate information from an extractive step. In experiments, we select $k$ documents using SummaRunner BIBREF33, a state-of-the-art neural extractive model where each document is classified as to whether it should be part of the summary or not. We concatenate $k$ preselected documents into a long sequence and encode it using a separate BiLSTM encoder. The encoded sequence serves as input to an LSTM decoder which generates a salience-biased hidden state $r_t$. We then update hidden state $s_t$ in Equation (DISPLAY_FORM19) as $s_t = [s_t; r_t]$. Notice that we still take all input documents into account, while acknowledging that some might be more descriptive than others.", "An advantage of using a separate encoder is increased training data, since we treat a single target with $N$ input documents as $N$ different instances. Once training has taken place, we use the Condense model to obtain $N$ pairs of document encodings $\\lbrace d_i\\rbrace $ and word-level encodings $\\lbrace h_{i,1}, h_{i,2}, ..., h_{i,M}\\rbrace $, $1 \\le i \\le N$ as representations for the documents in $\\mathcal {D}$.", "where the mean encoding $\\bar{d}$ is used as the query vector, and $W_p \\in \\mathbb {R}^{D_d \\times D_d \\times D_d}$ is a learned tensor. We also fuse word-level encodings, since the same words may appear in multiple documents. To do this, we simply average all encodings of the same word, if multiple tokens of the word exist:"]}
{"question_id": "6a20a3220c4edad758b912e2d3e5b99b0b295d96", "predicted_answer": "", "predicted_evidence": ["Replace entries corresponding to column and row INLINEFORM0 by zeros.", "We save INLINEFORM0 and INLINEFORM1 corresponding to each corpus.", "INLINEFORM0", "We formulate the problem of capturing semantic similarity between sentences as the problem of computing a maximum total matching weight of a bipartite graph, where X and Y are two sets of disjoint nodes. We use the Hungarian method BIBREF21 to solve this problem. Finally we get bipartite matching matrix INLINEFORM0 with entry INLINEFORM1 denoting matching between INLINEFORM2 and INLINEFORM3 . To obtain the overall similarity, we use Dice coefficient, INLINEFORM4", "Some easy additions that can be worked on are:"]}
{"question_id": "c2745e44ebe7dd57126b784ac065f0b7fc2630f1", "predicted_answer": "", "predicted_evidence": ["Some easy additions that can be worked on are:", "Unnecessary parts of the sentence can be trimmed to improve summary further.", "For generating summary through model overlaps, we can also try Graph-based methods or different Clustering techniques.", "We save INLINEFORM0 and INLINEFORM1 corresponding to each corpus.", "We can see that using a mixture of Semantic and Statistical models offers an improvement over stand-alone models. Given better training data, results can be further improved. Using domain-specific labeled data can provide a further increase in performances of Glove and WordNet Models."]}
{"question_id": "d5dcc89a08924bed9772bc431090cbb52fb7836f", "predicted_answer": "", "predicted_evidence": ["In the Table 1, we try different model pairs with weights trained on corpus for Task 2. We have displayed mean ROUGE-2 scores for base Models. We have calculated final scores taking into consideration all normalizations, stemming, lemmatizing and clustering techniques, and the ones providing best results were used. We generally expected WordNet, Glove based semantic models to perform better given they better capture crux of the sentence and compute similarity using the same, but instead, they performed average. This is attributed to the fact they assigned high similarity scores to not so semantically related sentences. We also observe that combinations with TF/IDF and Similarity Matrices(Jaccard/Cosine) offer nearly same results. The InferSent based Summarizer performed exceptionally well. We initially used pre-trained features to generate sentence vectors through InferSent.", "We save INLINEFORM0 and INLINEFORM1 corresponding to each corpus.", "We can see that using a mixture of Semantic and Statistical models offers an improvement over stand-alone models. Given better training data, results can be further improved. Using domain-specific labeled data can provide a further increase in performances of Glove and WordNet Models.", "where, INLINEFORM0 is total training set size, INLINEFORM1 is number of features in document vector.", "As we discussed earlier, summarization models are field selective. Some models tend to perform remarkably better than others in certain fields. So, instead of assigning uniform weights to all models we can go by the following approach."]}
{"question_id": "d418bf6595b1b51a114f28ac8a6909c278838aeb", "predicted_answer": "", "predicted_evidence": ["Other works suggest humans-in-the-loop for improving QA systems. Savenkov and Agichtein use crowdsourcing for re-ranking retrieved answer candidates in a real-time QA framework BIBREF17 . In Guardian, crowdworkers prepare a dialogue system based on a certain web API and, after deployment, manage actual conversations with users BIBREF18 . EVORUS learns to select answers from multiple chatbots via crowdsourcing BIBREF19 . The result is a chatbot ensemble excels the performance of each individual chatbot. Williams et al. present a dialogue architecture that continuously learns from user interaction and feedback BIBREF20 .", "Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ). Nevertheless, as can be seen in examples 1, 2 and 4 in Table TABREF1 , high-ranked but incorrect answers are often meaningful with respect to the query: the setting in our evaluation is overcritical, because we count incorrect, but meaningful answers as negative result. A major limitation is that the re-ranking algorithm cannot choose answer candidates beyond the top-10 results. It would be interesting to classify whether an answer is present in the top-10 or not. If not, the algorithm could search outside the top-10 results. Such a meta-model can also be used to estimate weaknesses of the QA model: it can determine topics that regularly fail, for instance, to guide data labelling for a targeted improvement of the model, also known as active learning BIBREF27 , and in combination with techniques from semi-supervised learning BIBREF5 , BIBREF28 .", "Our results indicate that the accuracy of the described QA system benefits from our re-ranking approach. Hence, it can be applied to improve the performance of already deployed QA systems that provide a top-10 ranking with confidences as output. However, the performance gain is small, which might have several reasons. For example, we did not integrate spell-checking in our re-ranking method which proved to be effective in our baseline evaluation. Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ).", "In this work, we include two corpora: one for training the baseline system and another for evaluating the performance of the QA pipeline and our re-ranking approach. In the following, we describe the creation of the training corpus and the structure of the test corpus. Both corpora have been anonymised.", "The training and testing procedure per data split of the cross-validation is shown in Algorithm SECREF5 . For each sample query INLINEFORM0 in the train set INLINEFORM1 , we include the correct answer INLINEFORM2 and one randomly selected negative answer candidate INLINEFORM3 for a balanced model training. We skip a sample, if the correct answer is not contained in the top-10 results: we include INLINEFORM4 of the data (see top-10 accuracy of the baseline QA model in Figure FIGREF11 ). The baseline QA model INLINEFORM5 and the trained re-ranking method INLINEFORM6 are applied to all sample queries in the test set INLINEFORM7 . Considered performance metrics are computed using the re-ranked top-10 INLINEFORM8 . We repeat the cross-validation 5 times to reduce effects introduced by the random selection of negative samples. We report the average metrics from 10 cross-validation folds and the 5 repetitions of the evaluation procedure."]}
{"question_id": "6d6b0628d8a942c57d7af1447a563021be79bc64", "predicted_answer": "", "predicted_evidence": ["The main difference is that spacy_sklearn uses Spacy for feature extraction with pre-trained word embedding models and Scikit-learn BIBREF22 for text classification. In contrast, the tensorflow_embedding pipeline trains custom word embeddings for text similarity estimation using TensorFlow BIBREF23 as machine learning backend. Figure FIGREF5 shows the general structure of both pipelines. We train QA models using both pipelines with the pre-defined set of hyper-parameters. For tensorflow_embedding, we additionally monitor changes in system performance using different epoch configurations. Further, we compare the performances of pipelines with or without a spellchecker and investigate whether model training benefits from additional user examples by training models with the three different versions of our training corpus including no additional samples (kw), samples from 1 user (kw+1u) or samples from 2 users (kw+2u) (see section Corpora). All training conditions are summarized in Table TABREF4 .", "In particular, we are interested in methods for improving a model after its deployment through re-ranking of the initial ranking results. In advance, we follow the steps of the CRISP cycle towards deployment for generating a state-of-the-art baseline QA model. First, we examine existing data (data understanding) and prepare a corpus for training (data preparation). Second, we implement and train a QA pipeline using state-of-the-art open source components (modelling). We perform an evaluation using different amounts of data and different pipeline configurations (evaluation), also to understand the nature of the data and the application (business understanding). Third, we investigate the effectiveness and efficiency of re-ranking in improving our QA pipeline after the deployment phase of CRISP. Adaptivity after deployment is modelled as (automatic) operationalisation step with external reflection based on, e.g., user feedback. This could be replaced by introspective meta-models that allow the system to enhance itself by metacognition BIBREF1 .", "Text Classification. The spacy_sklearn pipeline relies on Scikit-learn for text classification using a support vector classifier (SVC). The model confidences are used for ranking all answer candidates; the top-10 results are returned.", "Evaluation Corpus.", "First, we examine existing data (data understanding) and prepare a corpus for training (data preparation). Second, we implement and train a QA pipeline using state-of-the-art open source components (modelling). We perform an evaluation using different amounts of data and different pipeline configurations (evaluation), also to understand the nature of the data and the application (business understanding). Third, we investigate the effectiveness and efficiency of re-ranking in improving our QA pipeline after the deployment phase of CRISP. Adaptivity after deployment is modelled as (automatic) operationalisation step with external reflection based on, e.g., user feedback. This could be replaced by introspective meta-models that allow the system to enhance itself by metacognition BIBREF1 . The QA system and the re-ranking approach are evaluated using a separate test set that maps actual user queries from a chat-log to answers of the QA corpus. Sample queries from the evaluation set with one correct and one incorrect sample are shown in Table TABREF1 ."]}
{"question_id": "b21245212244ad7adf7d321420f2239a0f0fe56b", "predicted_answer": "", "predicted_evidence": ["We propose a re-ranking algorithm similar to BIBREF17 : we train a similarity model using n-gram based features of QA pairs for improving the answer selection of a retrieval-based QA system.", "In contrast, the tensorflow_embedding pipeline trains custom word embeddings for text similarity estimation using TensorFlow BIBREF23 as machine learning backend. Figure FIGREF5 shows the general structure of both pipelines. We train QA models using both pipelines with the pre-defined set of hyper-parameters. For tensorflow_embedding, we additionally monitor changes in system performance using different epoch configurations. Further, we compare the performances of pipelines with or without a spellchecker and investigate whether model training benefits from additional user examples by training models with the three different versions of our training corpus including no additional samples (kw), samples from 1 user (kw+1u) or samples from 2 users (kw+2u) (see section Corpora). All training conditions are summarized in Table TABREF4 . Next, we describe the implementation details of our QA system as shown in Figure FIGREF5 : the spellchecker module, the subsequent pre-processing and feature encoding, and the text classification. We include descriptions for both pipelines.", "With this work, we want to answer the question whether a deployed QA system that is difficult to adapt and that provides a top-10 ranking of answer candidates, can be improved by an additional re-ranking step that corresponds to the operationalisation step of the augmented CRISP cycle. It is also important to know the potential gain and the limitations of such a method that works on top of an existing system. We hypothesise that our proposed re-ranking approach can effectively improve ranking-based QA systems.", "Text classification for tensorflow_embedding is done using TensorFlow with an implementation of the StarSpace algorithm BIBREF24 . This component learns (and later applies) one embedding model for user queries and one for the answer id. It minimizes the distance between embeddings of QA training samples. The distances between a query and all answer ids are used for ranking.", "We implement our question answering system using state-of-the-art open source components. Our pipeline is based on the Rasa natural language understanding (NLU) framework BIBREF21 which offers two standard pipelines for text classification: spacy_sklearn and tensorflow_embedding. The main difference is that spacy_sklearn uses Spacy for feature extraction with pre-trained word embedding models and Scikit-learn BIBREF22 for text classification. In contrast, the tensorflow_embedding pipeline trains custom word embeddings for text similarity estimation using TensorFlow BIBREF23 as machine learning backend. Figure FIGREF5 shows the general structure of both pipelines. We train QA models using both pipelines with the pre-defined set of hyper-parameters. For tensorflow_embedding, we additionally monitor changes in system performance using different epoch configurations. Further, we compare the performances of pipelines with or without a spellchecker and investigate whether model training benefits from additional user examples by training models with the three different versions of our training corpus including no additional samples (kw), samples from 1 user (kw+1u) or samples from 2 users (kw+2u) (see section Corpora)."]}
{"question_id": "4a201b8b9cc566b56aedb5ab45335f202bc41845", "predicted_answer": "", "predicted_evidence": ["This study addresses the following research questions:", "In 2016, BIBREF3 proposed HolE, which relies on holographic models of associative memory by employing circular correlation to create compositional representations. HolE can capture rich interactions by using correlation as the compositional operator but it simultaneously remains efficient to compute, easy to train, and scalable to large datasets. In the same year, BIBREF4 presented RDF2Vec which uses language modeling approaches for unsupervised feature extraction from sequences of words and adapts them to RDF graphs. After generating sequences by leveraging local information from graph substructures by random walks, RDF2Vec learns latent numerical representations of entities in RDF graphs. The algorithm has been extended in order to reduce the computational time and the biased regarded the random walking BIBREF5 . More recently, BIBREF18 exploited the Global Vectors algorithm to compute embeddings from the co-occurrence matrix of entities and relations without generating the random walks. In following research, the authors refer to their algorithm as KGloVe.", "Formally, let $t = (s,p,o)$ be a triple containing a subject, a predicate, and an object in a knowledge base $K$ . For any triple, $(s,p,o) \\subseteq E \\times R \\times (E \\cap L)$ , where $E$ is the set of all entities, $R$ is the set of all relations, and $L$ is the set of all literals (i.e., string or numerical values). A representation function $F$ defined as", "As recently highlighted by several members of the ML and NLP communities, KGEs are rarely evaluated on downstream tasks different from link prediction (also known as knowledge base completion). Achieving high performances on link prediction does not necessarily mean that the generated embeddings are good, since the inference task is often carried out in combination with an external algorithm such as a neural network or a scoring function. The complexity is thus approach-dependent and distributed between the latent structure in the vector model and the parameters (if any) of the inference algorithm. For instance, a translational model such as TransE BIBREF10 would likely feature very complex embeddings, since in most approaches the inference function is a simple addition. On the other hand, we may find less structure in a tensor factorization model such as RESCAL BIBREF7 , as the inference is performed by a feed-forward neural network which extrapolates the hidden semantics layer by layer.", "assigns a vector of dimensionality $d$ to an entity, a relation, or a literal. However, some approaches consider only the vector representations of entities or subjects (i.e, $\\lbrace s \\in E : \\exists (s, p, o) \\in K \\rbrace $ ). For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity BIBREF9 , BIBREF12 , BIBREF2 . The reduction is performed through an embedding map $\\Phi : \\mathbb {R}^d \\rightarrow \\mathbb {R}^k$ , which is a homomorphism that maps the initial vector space into a smaller, reduced space."]}
{"question_id": "6a90135bd001be69a888076aff1b149b78adf443", "predicted_answer": "", "predicted_evidence": ["$$F : (E \\cap R \\cap L) \\rightarrow \\mathbb {R}^d$$   (Eq. 7)", "Existing KGE approaches based on the skip-gram model such as RDF2Vec BIBREF4 submit paths built using random walks to a Word2Vec algorithm. Instead, we preprocess the input knowledge base by converting each triple into a small sentence of three words. Our method is faster as it allows us to avoid the path generation step. The generated text corpus is thus processed by the skip-gram model as follows.", "As recently highlighted by several members of the ML and NLP communities, KGEs are rarely evaluated on downstream tasks different from link prediction (also known as knowledge base completion). Achieving high performances on link prediction does not necessarily mean that the generated embeddings are good, since the inference task is often carried out in combination with an external algorithm such as a neural network or a scoring function. The complexity is thus approach-dependent and distributed between the latent structure in the vector model and the parameters (if any) of the inference algorithm. For instance, a translational model such as TransE BIBREF10 would likely feature very complex embeddings, since in most approaches the inference function is a simple addition. On the other hand, we may find less structure in a tensor factorization model such as RESCAL BIBREF7 , as the inference is performed by a feed-forward neural network which extrapolates the hidden semantics layer by layer.", "$$ \nNST(\\tilde{E},N,K) = \\frac{1}{N \\vert \\tilde{E} \\vert } \\sum _{e \\in \\tilde{E}} \\sum _{j=1}^N \\frac{\\vert C_K(e) \\cap C_K(n_j^{(e)}) \\vert }{\\vert C_K(e) \\cup C_K(n_j^{(e)}) \\vert }$$   (Eq. 19)", "which means, in other words, to adopt a context window of 2, since the sequence size is always $|T|=3$ . The probability above is theoretically defined as:"]}
{"question_id": "1f40adc719d8ccda81e7e90525b577f5698b5aad", "predicted_answer": "", "predicted_evidence": ["As recently highlighted by several members of the ML and NLP communities, KGEs are rarely evaluated on downstream tasks different from link prediction (also known as knowledge base completion). Achieving high performances on link prediction does not necessarily mean that the generated embeddings are good, since the inference task is often carried out in combination with an external algorithm such as a neural network or a scoring function. The complexity is thus approach-dependent and distributed between the latent structure in the vector model and the parameters (if any) of the inference algorithm. For instance, a translational model such as TransE BIBREF10 would likely feature very complex embeddings, since in most approaches the inference function is a simple addition. On the other hand, we may find less structure in a tensor factorization model such as RESCAL BIBREF7 , as the inference is performed by a feed-forward neural network which extrapolates the hidden semantics layer by layer.", "The second metric is the Type and Category Test (TCT), based on the assumption that two entities which share types and categories should be close in the vector space. This assumption is suggested by the human bias for which rdf:type and dct:subject would be predicates with a higher weight than the others. Although this does not happen, we compute it for a mere sake of comparison with the NST metric. The TCT formula is equal to Equation 19 except for sets $C_K(e)$ , which are replaced by sets of types and categories $TC_K(e)$ .", "where $n_j^{(e)}$ is the $j$ th nearest neighbour of $e$ in the vector space.", "Several methods have been proposed to evaluate word embeddings. The most common ones are based on analogies BIBREF22 , BIBREF23 , where word vectors are summed up together, e.g.:", "$$score(\\bar{s},\\bar{p},\\bar{o}) = \\frac{1}{\\left|\\lbrace  (s,\\bar{p},o) \\in K \\rbrace  \\right|} \\sum _{(s,\\bar{p},o) \\in K} {\n{\\left\\lbrace \\begin{array}{ll}\n1 & \\text{if } \\left\\Vert v_{\\bar{s}} + v_o - v_s - v_{\\bar{o}} \\right\\Vert \\le \\epsilon \\\\\n0 & \\text{otherwise}\n\\end{array}\\right.}\n}$$   (Eq. 14)"]}
{"question_id": "f92c344e9b1a986754277fd0f08a47dc3e5f9feb", "predicted_answer": "", "predicted_evidence": ["In a different approach, Ghazvininejad et al BIBREF22 propose a knowledge grounded approach which infuses the output utterance with factual information relevant to the conversational context. Their architecture is shown in figure FIGREF7 . They use an external collection of world facts which is a large collection of raw text entries (e.g., Foursquare, Wikipedia, or Amazon reviews) indexed by named entities as keys. Then, given a conversational history or source sequence S, they identify the \u201cfocus\u201d in S, which is the text span (one or more entities) based on which they form a query to link to the facts. The query is then used to retrieve all contextually relevant facts. Finally, both conversation history and relevant facts are fed into a neural architecture that features distinct encoders for conversation history and facts.", "They get the external knowledge using a search engine. Then a knowledge enhanced sequence-to-sequence framework is designed to model multi-turn dialogs on external knowledge conditionally. For this purpose, their model extends the simple sequence-to-sequence model by augmenting the input with the knowledge vector so as to take account of the knowledge in the procedure of response generation into the decoder of the sequence-to-sequence model. Both the encoder and the decoder are composed of LSTM.", "The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. The policy gradient methods are used to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator is used as a reward for the generator, which is trained to maximize the expected reward of generated utterances using the REINFORCE algorithm.", "The first approach we discuss is the Dynamic Knowledge Graph Network (DynoNet) proposed by He et al BIBREF20 , in which the dialogue state is modeled as a knowledge graph with an embedding for each node. To model both structured and open-ended context they model two agents, each with a private list of items with attributes, that must communicate to identify the unique shared item. They structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph. An attention-based mechanism over the node embeddings drives generation of new utterances. The model is best explained by the example used in the paper which is as follows: The knowledge graph represents entities and relations in the agent\u2019s private KB, e.g., item-1\u2019s company is google. As the conversation unfolds, utterances are embedded and incorporated into node embeddings of mentioned entities.", "In 1995, two researchers (Ball et al, 1995 BIBREF4 ) at Microsoft developed a conversational assistant called Persona which was one of the first true personal assistant similar to what we have in recent times (like Siri, etc). It allowed users the maximum flexibility to express their requests in whatever syntax they found most natural and the interface was based on a broad-coverage NLP system unlike the system discussed in the previous paragraph. In this, a labelled semantic graph is generated from the speech input which encodes case frames or thematic roles. After this, a sequence of graph transformations is applied on it using the knowledge of interaction scenario and application domain. This results into a normalized application specific structure called as task graph which is then matched against the templates (in the application) which represent the normalized task graphs corresponding to all the possible user statements that the assistant understands and the action is then executed. The accuracy was not that good and they did not bother to calculate it."]}
{"question_id": "b10388e343868ca8e5c7c601ebb903f52e756e61", "predicted_answer": "", "predicted_evidence": ["Even though most of the modern work in the field is built on this approach there is a significant drawback to this idea. This model can theoretically never solve the problem of modelling dialogues due to various simplifications, the most important of them being the objective function that is being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. It is important to see that optimizing an agent to generate text based on what it sees in the two-turn conversation dataset that it is trained on does not mean that the agent would be able to generalize to human level conversation across contexts. Nevertheless in absence of a better way to capture human communication this approach laid the foundation of most of the modern advances in the field. Another problem that plagues this paper and the field in general is Evaluation. As there can be multiple correct output utterances for a given input utterance there is no quantitative way to evaluate how well a model is performing.", "Their system returns a success rate of 0.66 for small knowledge bases and a great success rate of 0.83 for medium and large knowledge bases. As the user interacts with the agent, the collected data can be used to train the end-to-end agent which we see has a strong learning capability. Gradually, as more experience is collected, the system can switch from Reinforcement Learning-Soft to the personalized end-to-end agent. Effective implementation of this requires such personalized end-to-end agents to learn quickly which should be explored in the future.", "They structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph. An attention-based mechanism over the node embeddings drives generation of new utterances. The model is best explained by the example used in the paper which is as follows: The knowledge graph represents entities and relations in the agent\u2019s private KB, e.g., item-1\u2019s company is google. As the conversation unfolds, utterances are embedded and incorporated into node embeddings of mentioned entities. For instance, in Figure FIGREF6 , \u201canyone went to columbia\u201d updates the embedding of columbia. Next, each node recursively passes its embedding to neighboring nodes so that related entities (e.g., those in the same row or column) also receive information from the most recent utterance. In this example, jessica and josh both receive new context when columbia is mentioned. Finally, the utterance generator, an LSTM, produces the next utterance by attending to the node embeddings.", "A lack of a coherent personality in conversational agents that most of these models propose has been identified as one of the primary reasons that these agents have not been able to pass the Turing test BIBREF0 BIBREF2 . Aside from such academic motivations, making conversational agents more like their human interlocutors which posses both a persona and are capable of parsing emotions is of great practical and commercial use. Consequently in the last couple of years different approaches have been tried to achieve this goal.", "Another feature that has been traditionally lacking in conversation agents is a personality. O Vinayal et al BIBREF2 hypothesis that not having a consistent personality is one of the main reasons that is stopping us from passing the turing test. Conversational agents also lack emotional consistency in their responses. These features are vital if we want humans to trust conversational agents. In section SECREF7 we discuss state of the art approaches to overcome these problems."]}
{"question_id": "e8cdeb3a081d51cc143c7090a54c82d393f1a2ca", "predicted_answer": "", "predicted_evidence": ["The average objective function score for the case of learned policies was 44.90. One of the main reasons for the low accuracy (which is also a limitation of this paper) was that there were a number of aspects of dialog that they had not modeled such as non-understandings, misunderstandings, and even parsing sentences into the action specification and generating sentences from the action specification. But the paper set the pavement of the reinforcement learning methods into the area of dialog and personal agents.", "In this, the authors used an RNN to allow the network to maintain an internal state of dialogue history. Specifically, they used a Gated Recurrent Unit followed by a fully-connected layer and softmax non-linearity to model the policy \u03c0 over the actions. During training, the agent samples its actions from this policy to encourage exploration. Parameters of the neural components were trained using the REINFORCE algorithm. For end-to-end training they updated both the dialogue policy and the belief trackers using the reinforcement signal. While testing, the dialogue is regarded as a success if the user target is in top five results returned by the agent and the reward is accordingly calculated that helps the agent take the next action.", "Due to their large parameter space, the estimation of neural conversation models requires considerable amounts of dialogue data. Large online corpora are helpful for this. However several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification.The neural conversation model may therefore inadvertently learn responses that remain within the same dialogue turn instead of starting a new turn. Lison et al BIBREF17 overcome these limitations by introduce a weighting model into the neural architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimized. The purpose of this model is to associate each \u27e8context, response\u27e9 example pair to a numerical weight that reflects the intrinsic \u201cquality\u201d of each example.", "We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years. The results have been impressive enough to find their way into commercial applications such that these agents have become truly ubiquitous. We attempt to present a broad view of these advancements with a focus on the main challenges encountered by the conversational agents and how these new approaches are trying to mitigate them.", "Under this architecture, different spoken dialogue agents handling different domains can be developed independently and cooperate with one another to respond to the user\u2019s requests. While a user interface agent can access the correct spoken dialogue agent through a domain switching protocol, and carry over the dialogue state and history so as to keep the knowledge processed persistently and consistently across different domains. Figure FIGREF1 shows the agent society for spoken dialogue for tour information service."]}
{"question_id": "833d3ae7613500f2867ed8b33d233d71781014e7", "predicted_answer": "", "predicted_evidence": ["They consider a persona to be composite of elements of identity (background facts or user profile), language behavior, and interaction style. They also account for a persona to be adaptive since an agent may need to present different facets to different human interlocutors depending on the interaction. Ultimately these personas are incorporated into the model as embeddings. Adding a persona not only improves the human interaction but also improves BLeU score and perplexity over the baseline sequence to sequence models. The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone.", "The goal of capturing emotions and having consistent personalities for a conversational agent is an important one. The field is still nascent but advances in the domain will have far reaching consequences for conversational models in general. People tend to trust agents that are emotionally consistent, and in the long term trust is what will decide the fate of large scale adoption of conversational agents.", "After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.", "Even though most of the modern work in the field is built on this approach there is a significant drawback to this idea. This model can theoretically never solve the problem of modelling dialogues due to various simplifications, the most important of them being the objective function that is being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. It is important to see that optimizing an agent to generate text based on what it sees in the two-turn conversation dataset that it is trained on does not mean that the agent would be able to generalize to human level conversation across contexts. Nevertheless in absence of a better way to capture human communication this approach laid the foundation of most of the modern advances in the field. Another problem that plagues this paper and the field in general is Evaluation.", "A lack of a coherent personality in conversational agents that most of these models propose has been identified as one of the primary reasons that these agents have not been able to pass the Turing test BIBREF0 BIBREF2 . Aside from such academic motivations, making conversational agents more like their human interlocutors which posses both a persona and are capable of parsing emotions is of great practical and commercial use. Consequently in the last couple of years different approaches have been tried to achieve this goal."]}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "predicted_answer": "", "predicted_evidence": ["The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone. The model first encodes message INLINEFORM0 into a vector representation INLINEFORM1 using the source LSTM. Then for each step in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step, the word representations at the current time step, and the speaker embedding INLINEFORM2 . In this way, speaker information is encoded and injected into the hidden layer at each time step and thus helps predict personalized responses throughout the generation process. The process described here is visualizes in figure FIGREF13 below.", "Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English.", "They also account for a persona to be adaptive since an agent may need to present different facets to different human interlocutors depending on the interaction. Ultimately these personas are incorporated into the model as embeddings. Adding a persona not only improves the human interaction but also improves BLeU score and perplexity over the baseline sequence to sequence models. The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone. The model first encodes message INLINEFORM0 into a vector representation INLINEFORM1 using the source LSTM. Then for each step in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step, the word representations at the current time step, and the speaker embedding INLINEFORM2 .", "The researchers thought that if they can create assistant models specific to the corresponding models, they can achieve better accuracy for those applications instead of creating a common unified personal assistant which at that time performed quite poorly. There was a surge in application-specific assistants like in-car intelligent personal assistant (Schillo et al, 1996 BIBREF5 ), spoken-language interface to execute military exercises (Stent et al, 1999 BIBREF6 ), etc. Since it was difficult to develop systems with high domain extensibility, the researchers came up with a distributed architecture for cooperative spoken dialogue agents (Lin et al, 1999 BIBREF7 ).", "Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail."]}
{"question_id": "64f7337970e8d1989b2e1f7106d86f73c4a3d0af", "predicted_answer": "", "predicted_evidence": ["To speed up the learning process, they presented two sample-efficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER). Both models employ off-policy learning with experience replay to improve sample-efficiency. For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes. For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence.", "Although they did not evaluate their model on some standard metric, they showed that their model can generate responses appropriate not only in content but also in emotion. In the future, instead of specifying an emotion class, the model should decide the most appropriate emotion category for the response. However, this may be challenging since such a task depends on the topic, context or the mood of the user.", "One of the first main papers that thought of using reinforcement learning for this came in 2005 by English et al BIBREF25 . They used an on-policy Monte Carlo method and the objective function they used was a linear combination of the solution quality (S) and the dialog length (L), taking the form: o(S,I) = INLINEFORM0 - INLINEFORM1 .", "Building on works like this the Emotional Chatting Machine model proposed by Zhou et al BIBREF30 is a model which generates responses that are not only grammatically consistent but also emotionally consistent. To achieve this their approach models the high-level abstraction of emotion expressions by embedding emotion categories. They also capture the change of implicit internal emotion states and use explicit emotion expressions with an external emotion vocabulary.", "In this, the authors used an RNN to allow the network to maintain an internal state of dialogue history. Specifically, they used a Gated Recurrent Unit followed by a fully-connected layer and softmax non-linearity to model the policy \u03c0 over the actions. During training, the agent samples its actions from this policy to encourage exploration. Parameters of the neural components were trained using the REINFORCE algorithm. For end-to-end training they updated both the dialogue policy and the belief trackers using the reinforcement signal. While testing, the dialogue is regarded as a success if the user target is in top five results returned by the agent and the reward is accordingly calculated that helps the agent take the next action."]}
{"question_id": "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550", "predicted_answer": "", "predicted_evidence": ["Their model achieved a machine vs random accuracy score of 0.952 out of 1. However, on applying the same training paradigm to machine translation in preliminary experiments, the authors did not find a clear performance boost. They thought that it may be because the adversarial training strategy is more beneficial to tasks in which there is a big discrepancy between the distributions of the generated sequences and the reference target sequences (that is, the adversarial approach may be more beneficial on tasks in which entropy of the targets is high). In the future, this relationship can be further explored.", "Although they did not evaluate their model on some standard metric, they showed that their model can generate responses appropriate not only in content but also in emotion. In the future, instead of specifying an emotion class, the model should decide the most appropriate emotion category for the response. However, this may be challenging since such a task depends on the topic, context or the mood of the user.", "They also account for a persona to be adaptive since an agent may need to present different facets to different human interlocutors depending on the interaction. Ultimately these personas are incorporated into the model as embeddings. Adding a persona not only improves the human interaction but also improves BLeU score and perplexity over the baseline sequence to sequence models. The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone. The model first encodes message INLINEFORM0 into a vector representation INLINEFORM1 using the source LSTM. Then for each step in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step, the word representations at the current time step, and the speaker embedding INLINEFORM2 .", "According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue.", "In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years."]}
{"question_id": "a0d45b71feb74774cfdc0d5c6e23cd41bc6bc1f2", "predicted_answer": "", "predicted_evidence": ["They consider a persona to be composite of elements of identity (background facts or user profile), language behavior, and interaction style. They also account for a persona to be adaptive since an agent may need to present different facets to different human interlocutors depending on the interaction. Ultimately these personas are incorporated into the model as embeddings. Adding a persona not only improves the human interaction but also improves BLeU score and perplexity over the baseline sequence to sequence models. The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone.", "They structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph. An attention-based mechanism over the node embeddings drives generation of new utterances. The model is best explained by the example used in the paper which is as follows: The knowledge graph represents entities and relations in the agent\u2019s private KB, e.g., item-1\u2019s company is google. As the conversation unfolds, utterances are embedded and incorporated into node embeddings of mentioned entities. For instance, in Figure FIGREF6 , \u201canyone went to columbia\u201d updates the embedding of columbia. Next, each node recursively passes its embedding to neighboring nodes so that related entities (e.g., those in the same row or column) also receive information from the most recent utterance. In this example, jessica and josh both receive new context when columbia is mentioned. Finally, the utterance generator, an LSTM, produces the next utterance by attending to the node embeddings.", "They also account for a persona to be adaptive since an agent may need to present different facets to different human interlocutors depending on the interaction. Ultimately these personas are incorporated into the model as embeddings. Adding a persona not only improves the human interaction but also improves BLeU score and perplexity over the baseline sequence to sequence models. The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone. The model first encodes message INLINEFORM0 into a vector representation INLINEFORM1 using the source LSTM. Then for each step in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step, the word representations at the current time step, and the speaker embedding INLINEFORM2 .", "The average objective function score for the case of learned policies was 44.90. One of the main reasons for the low accuracy (which is also a limitation of this paper) was that there were a number of aspects of dialog that they had not modeled such as non-understandings, misunderstandings, and even parsing sentences into the action specification and generating sentences from the action specification. But the paper set the pavement of the reinforcement learning methods into the area of dialog and personal agents.", "However, the system has a few limitations. The accuracy is not enough for using for the practical applications. The agent suffers from the cold start issue. In the case of end-to-end learning, they found that for a moderately sized knowledge base, the agent almost always fails if starting from random initialization."]}
{"question_id": "89414ef7fcb2709c47827f30a556f543b9a9e6e0", "predicted_answer": "", "predicted_evidence": ["\u010cOVJEKU - Locative singular", "The dictionary that was mentioned before is, in fact, the intermediary language, and all the necessary knowledge should be placed in this dictionary, the keys should ideally be just abstract codes, and everything else would reside and be accessible as values next to the keys BIBREF12. Petrovi\u0107, when discussing the translation of poetry BIBREF18, noted that ideally, machine translation should be from one language to another, without the use of an intermediate language of meanings.", "Next, the most frequently occurring meaning would be kept, but only if it grammatically fits the final sentence. One can extrapolate that it is tacitly assumed that the grammatical structure of the source language matches the target language, and to do this, a kind of categorical grammar similar to Lambek calculus BIBREF19 would have to be used. It seems that the Croatian group was not aware of the paper by Lambek (but only of Bar-Hillel's papers), so they did not elaborate this part.", "Laszlo and Petrovi\u0107 BIBREF11 argued that, in order to trim the search space, the words would have to be coded so as to retain their information value but to rid the representations of needless redundancies. This was based on previous calculations of language entropy by Matkovi\u0107, and Matkovi\u0107's idea was simple: conduct a statistical analysis to determine the most frequent letters and assign them the shortest binary code. So A would get 101, while F would get 11010011 BIBREF11. Building on that, Laszlo suggested that, when making an efficient machine translation system, one has to take into account not just the letter frequencies but also the redundancies of some of the letters in a word BIBREF16.", "After all the lemmas comprising the sentence have been looked up in this dictionary, the next step is to keep only the inner values and discard the inner keys, thus collapsing the list, so that the example above would become:"]}
{"question_id": "faffcc6ef27c1441e6528f924e320368430d8da3", "predicted_answer": "", "predicted_evidence": ["\u010cOVJEKU - Dative singular", "The thesaurus would have multiple entries for each lemma, and they would be ordered by descending frequency (if the group actually made a prototype, they would have realized that this simple frequency count was not enough to avoid only the first meaning to be used). The dictionary entry for \u010cOVJE- (using modern JSON notation) is:", "Use context to determine the meaning of polysemous words.", "The meaning of the numbers used is never explained, but they would probably be used for cross-referencing word categories.", "Separation of the dictionary from the MT algorithm"]}
{"question_id": "afad388a0141bdda5ca9586803ac53d5f10f41f6", "predicted_answer": "", "predicted_evidence": ["\"\u010cOVJE-\": \"mankind\": 193.5: \"LITTLENESS\", 690.2: \"AGENT\", \"man\": 554.4: \"REPRESENTATION\", 372.1: \"MANKIND\", 372.3: \"MANKIND\" ..., ...", "Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107.", "But what are those meanings? The algorithm to be used was a simple statistical alignment algorithm (in hopes of capturing semantics) described in BIBREF12 on a short Croatian sentence \"\u010dovjek [noun-subject] pu\u0161i [verb-predicate] lulu [noun-objective]\" (A man is smoking a pipe). The first step would be to parse and lemmatize. Nouns in Croatian have seven cases just in the singular, with different suffixes, for example:", "Unfortunately, this idea of using machine learning was never fully developed, and the Croatian group followed the Soviet approach(es) closely. Pranji\u0107 BIBREF17 analyses and extrapolates five basic ideas in the Soviet Machine Translation program, which were the basis for the Croatian approach:", "Finka and Laszlo envisioned three main data preparation tasks that are needed before prototype development could commence BIBREF10. The first task is to compile a dictionary of words sorted from the end of the word to the beginning. This would enable the development of what is now called stemming and lemmatization modules: a knowledge base with suffixes so they can be trimmed, but also a systematic way to find the base of the word (lemmatization) (p. 121). The second task would be to make a word frequency table. This would enable focusing on a few thousand most frequent words and dropping the rest. This is currently a good industrial practice for building efficient natural language processing systems, and in 1962, it was a computational necessity. The last task was to create a good thesaurus, but such a thesaurus where every data point has a \"meaning\" as the key, and words (synonyms) as values."]}
{"question_id": "baaa6ad7148b785429a20f38786cd03ab9a2646e", "predicted_answer": "", "predicted_evidence": ["After all the lemmas comprising the sentence have been looked up in this dictionary, the next step is to keep only the inner values and discard the inner keys, thus collapsing the list, so that the example above would become:", "\"\u010cOVJE-\": \"mankind\": 193.5: \"LITTLENESS\", 690.2: \"AGENT\", \"man\": 554.4: \"REPRESENTATION\", 372.1: \"MANKIND\", 372.3: \"MANKIND\" ..., ...", "\u010cOVJE\u010cE - Vocative singular", "The thesaurus would have multiple entries for each lemma, and they would be ordered by descending frequency (if the group actually made a prototype, they would have realized that this simple frequency count was not enough to avoid only the first meaning to be used). The dictionary entry for \u010cOVJE- (using modern JSON notation) is:", "\u010cOVJEKA - Genitive singular"]}
{"question_id": "de346decb1fbca8746b72c78ea9d1208902f5e0a", "predicted_answer": "", "predicted_evidence": ["\u010cOVJEK - Nominative singular", "Several remarks are in order. First, the group seemed to think that encodings would be needed, but it seems that entropy-based encodings and calculations added no real benefits (i.e. added no benefit that would not be offset by the cost of calculating the codes). In addition, Finka and Laszlo BIBREF10 seem to place great emphasis on lemmatization instead of stemming, which, if they had constructed a prototype, they would have noticed it to be very hard to tackle with the technology of the age. Nevertheless, the idea of proper lemmatization would probably be replaced with moderately precise hard-coded stemming, made with the help of the \"inverse dictionary\", which Finka and Laszlo proposed as one of the key tasks in their 1962 paper. This paper also highlights the need for a frequency count and taking only the most frequent words, which is an approach that later became widely used in the natural language processing community.", "Several remarks are in order. First, the group seemed to think that encodings would be needed, but it seems that entropy-based encodings and calculations added no real benefits (i.e. added no benefit that would not be offset by the cost of calculating the codes). In addition, Finka and Laszlo BIBREF10 seem to place great emphasis on lemmatization instead of stemming, which, if they had constructed a prototype, they would have noticed it to be very hard to tackle with the technology of the age. Nevertheless, the idea of proper lemmatization would probably be replaced with moderately precise hard-coded stemming, made with the help of the \"inverse dictionary\", which Finka and Laszlo proposed as one of the key tasks in their 1962 paper. This paper also highlights the need for a frequency count and taking only the most frequent words, which is an approach that later became widely used in the natural language processing community. Sentential alignment coupled with part-of-speech tagging was correctly identified as one of the key aspects of machine translation, but its complexity was severely underestimated by the group.", "added no benefit that would not be offset by the cost of calculating the codes). In addition, Finka and Laszlo BIBREF10 seem to place great emphasis on lemmatization instead of stemming, which, if they had constructed a prototype, they would have noticed it to be very hard to tackle with the technology of the age. Nevertheless, the idea of proper lemmatization would probably be replaced with moderately precise hard-coded stemming, made with the help of the \"inverse dictionary\", which Finka and Laszlo proposed as one of the key tasks in their 1962 paper. This paper also highlights the need for a frequency count and taking only the most frequent words, which is an approach that later became widely used in the natural language processing community. Sentential alignment coupled with part-of-speech tagging was correctly identified as one of the key aspects of machine translation, but its complexity was severely underestimated by the group. One might argue that these two modules are actually everything that is needed for a successful machine translation system, which shows the complexity of the task.", "As noted earlier, the group had no computer available to build a prototype, and subsequently, they have underestimated the complexity of determining sentential alignment. Sentential alignment seems rather trivial from a theoretical standpoint, but it could be argued that machine translation can be reduced to sentential alignment. This reduction vividly suggests the full complexity of sentential alignment. But the complexity of alignment was not evident at the time, and only several decades after the Croatian group's dissolution, in the late 1990s, did the group centered around Tillmann and Ney start to experiment with statistical models using (non-trivial) alignment modules, and producing state-of-the-art results (cf. BIBREF24) and BIBREF25. However, this was statistical learning, and it would take another two decades for sentential alignment to be implemented in cybernetic models, by then known under a new name, deep learning. Alignment was implemented in deep neural networks by BIBREF26 and BIBREF27, but a better approach, called attention, which is a trainable alignment module, was being developed in parallel, starting with the seminal paper on attention in computer vision by BIBREF28."]}
{"question_id": "0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "predicted_answer": "", "predicted_evidence": ["An RNN allows us to compute a hidden state INLINEFORM0 of each word summarizing the preceding words INLINEFORM1 , but not considering the following words INLINEFORM2 that might also be useful for simplification. An alternative approach is to use a bidirectional-RNN BIBREF20 . Here, we propose to use Neural Semantic Encoders BIBREF21 . During each encoding time step INLINEFORM3 , we compute a memory matrix INLINEFORM4 where INLINEFORM5 is the dimensionality of the word vectors. This matrix is initialized with the word vectors and is refined over time through NSE's functions to gain a better understanding of the input sequence. Concretely, NSE sequentially reads the tokens INLINEFORM6 with its read function:", "As shown in Table TABREF16 , neural models often outperformed traditional systems (Pbmt-R, Hybrid, Sbmt-Sari) on Fluency. This is not surprising given the recent success of neural Seq2seq models in language modeling and neural machine translation BIBREF30 , BIBREF27 . On the downside, our manual inspection reveals that neural models learn to perform copying very well in terms of rewrite operations (e.g., copying, deletion, reordering, substitution), often outputting the same or parts of the input sentence.", "INLINEFORM0 .", "INLINEFORM0 , INLINEFORM1 ,", "INLINEFORM0 ,"]}
{"question_id": "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "predicted_answer": "", "predicted_evidence": ["INLINEFORM0 ,", "The goal of sentence simplification is to compose complex sentences into simpler ones so that they are more comprehensible and accessible, while still retaining the original information content and meaning. Sentence simplification has a number of practical applications. On one hand, it provides reading aids for people with limited language proficiency BIBREF1 , BIBREF2 , or for patients with linguistic and cognitive disabilities BIBREF3 . On the other hand, it can improve the performance of other NLP tasks BIBREF4 , BIBREF5 , BIBREF6 . Prior work has explored monolingual machine translation (MT) approaches, utilizing corpora of simplified texts, e.g., Simple English Wikipedia (SEW), and making use of statistical MT models, such as phrase-based MT (PBMT) BIBREF7 , BIBREF8 , BIBREF9 , tree-based MT (TBMT) BIBREF10 , BIBREF11 , or syntax-based MT (SBMT) BIBREF12 .", "Nisioi et al. Nisioi:17 implemented a standard LSTM-based Seq2seq model and found that they outperform PBMT, SBMT, and unsupervised lexical simplification approaches. Zhang and Lapata BIBREF15 viewed the encoder-decoder model as an agent and employed a deep reinforcement learning framework in which the reward has three components capturing key aspects of the target output: simplicity, relevance, and fluency. The common practice for Seq2seq models is to use recurrent neural networks (RNNs) with Long Short-Term Memory BIBREF16 or Gated Recurrent Unit BIBREF17 for the encoder and decoder BIBREF18 , BIBREF15 . These architectures were designed to be capable of memorizing long-term dependencies across sequences. Nevertheless, their memory is typically small and might not be enough for the simplification task, where one is confronted with long and complicated sentences. In this study, we go beyond the conventional LSTM/GRU-based Seq2seq models and propose to use a memory-augmented RNN architecture called Neural Semantic Encoders (NSE).", "(1) First, we present a novel simplification model which is, to the best of our knowledge, the first model that use memory-augmented RNN for the task. We investigate the effectiveness of neural Seq2seq models when different neural architectures for the encoder are considered. Our experiments reveal that the NseLstm model that uses an NSE as the encoder and an LSTM as the decoder performed the best among these models, improving over strong simplification systems. (2) Second, we perform an extensive evaluation of various approaches proposed in the literature on different datasets. Results of both automatic and human evaluation show that our approach is remarkably effective for the task, significantly reducing the reading difficulty of the input, while preserving grammaticality and the original meaning. We further discuss some advantages and disadvantages of these approaches.", "We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 ."]}
{"question_id": "051034cc94f2c02d3041575c53f969b3311c9ea1", "predicted_answer": "", "predicted_evidence": ["Nisioi et al. Nisioi:17 implemented a standard LSTM-based Seq2seq model and found that they outperform PBMT, SBMT, and unsupervised lexical simplification approaches. Zhang and Lapata BIBREF15 viewed the encoder-decoder model as an agent and employed a deep reinforcement learning framework in which the reward has three components capturing key aspects of the target output: simplicity, relevance, and fluency. The common practice for Seq2seq models is to use recurrent neural networks (RNNs) with Long Short-Term Memory BIBREF16 or Gated Recurrent Unit BIBREF17 for the encoder and decoder BIBREF18 , BIBREF15 . These architectures were designed to be capable of memorizing long-term dependencies across sequences. Nevertheless, their memory is typically small and might not be enough for the simplification task, where one is confronted with long and complicated sentences.", "We implemented two attention-based Seq2seq models, namely: (1) LstmLstm: the encoder is implemented by two LSTM layers; (2) NseLstm: the encoder is implemented by NSE. The decoder in both cases is implemented by two LSTM layers. The computations for a single model are run on an NVIDIA Titan-X GPU. For all experiments, our models have 300-dimensional hidden states and 300-dimensional word embeddings. Parameters were initialized from a uniform distribution [-0.1, 0.1). We used the same hyperparameters across all datasets. Word embeddings were initialized either randomly or with Glove vectors BIBREF24 pre-trained on Common Crawl data (840B tokens), and fine-tuned during training. We used a vocabulary size of 20K for Newsela, and 30K for WikiSmall and WikiLarge. Our models were trained with a maximum number of 40 epochs using Adam optimizer BIBREF25 with step size INLINEFORM0 for LstmLstm, and INLINEFORM1 for NseLstm, the exponential decay rates INLINEFORM2 .", "Here, INLINEFORM0 is the INLINEFORM1 row of the memory matrix at time INLINEFORM2 , INLINEFORM3 . Next, a write function is used to map INLINEFORM4 to the encoder output space:", "where INLINEFORM0 is an LSTM, INLINEFORM1 is the hidden state at time INLINEFORM2 . Then, a compose function is used to compose INLINEFORM3 with relevant information retrieved from the memory at the previous time step, INLINEFORM4 :", "INLINEFORM0 , INLINEFORM1"]}
{"question_id": "511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "predicted_answer": "", "predicted_evidence": ["NSE gives us unrestricted access to the entire source sequence stored in the memory. As such, the encoder may attend to relevant words when encoding each word. The sequence INLINEFORM0 is then used as the sequence INLINEFORM1 in Section SECREF2 .", "The results of the automatic evaluation are displayed in Table TABREF15 . We first discuss the results on Newsela that contains high-quality simplifications composed by professional editors. In terms of BLEU, all neural models achieved much higher scores than Pbmt-R and Hybrid. NseLstm-B scored highest with a BLEU score of 26.31. With regard to SARI, NseLstm-S scored best among neural models (29.58) and came close to the performance of Hybrid (30.00). This indicates that NSE offers an effective means to better encode complex sentences for sentence simplification.", "INLINEFORM0 , INLINEFORM1 ,", "We differ from the approach of Zhang et al. Zhang:17 in the sense that we implement both a greedy strategy and a beam-search strategy to generate the target sentence. Whereas the greedy decoder always chooses the simplification candidate with the highest log-probability, the beam-search decoder keeps a fixed number (beam) of the highest scoring candidates at each time step. We report the best simplification among the outputs based on automatic evaluation measures.", "We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 ."]}
{"question_id": "6b4006a90aeaaff8914052d72d28851a9c0c0146", "predicted_answer": "", "predicted_evidence": ["Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test.", "Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 .", "Nisioi et al. Nisioi:17 implemented a standard LSTM-based Seq2seq model and found that they outperform PBMT, SBMT, and unsupervised lexical simplification approaches. Zhang and Lapata BIBREF15 viewed the encoder-decoder model as an agent and employed a deep reinforcement learning framework in which the reward has three components capturing key aspects of the target output: simplicity, relevance, and fluency. The common practice for Seq2seq models is to use recurrent neural networks (RNNs) with Long Short-Term Memory BIBREF16 or Gated Recurrent Unit BIBREF17 for the encoder and decoder BIBREF18 , BIBREF15 . These architectures were designed to be capable of memorizing long-term dependencies across sequences. Nevertheless, their memory is typically small and might not be enough for the simplification task, where one is confronted with long and complicated sentences.", "We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.", "The goal of sentence simplification is to compose complex sentences into simpler ones so that they are more comprehensible and accessible, while still retaining the original information content and meaning. Sentence simplification has a number of practical applications. On one hand, it provides reading aids for people with limited language proficiency BIBREF1 , BIBREF2 , or for patients with linguistic and cognitive disabilities BIBREF3 . On the other hand, it can improve the performance of other NLP tasks BIBREF4 , BIBREF5 , BIBREF6 . Prior work has explored monolingual machine translation (MT) approaches, utilizing corpora of simplified texts, e.g., Simple English Wikipedia (SEW), and making use of statistical MT models, such as phrase-based MT (PBMT) BIBREF7 , BIBREF8 , BIBREF9 , tree-based MT (TBMT) BIBREF10 , BIBREF11 , or syntax-based MT (SBMT) BIBREF12 ."]}
{"question_id": "eccbbe3684d0cf6b794cb4eef379bb1c8bcc33bf", "predicted_answer": "", "predicted_evidence": ["BIBREF37: measures the number of words edited by the user, normalized by the number of words in the final translation.", "Additionally, to evaluate the quality of the modernization and the difficulty of each task, we made use of the following well-known metrics:", "In recent years, awareness of the importance of preserving our cultural heritage has increased. Historical documents are an important part of that heritage. In order to preserve them, there is an increased need in creating digital text versions which can be search and automatically processed BIBREF0. However, their linguistic properties create additional difficulties: due to the lack of a spelling convention, orthography changes depending on the time period and author. Furthermore, human language evolves with the passage of time, increasing the difficulty of the document's comprehension. Thus, historical documents are mostly accessible to scholars specialized in the time period in which each document was written.", "Modernization tackles the language barrier in order to increase the accessibility of historical documents. To achieve this, it generates a new version of a historical document in the modern version of the language in which the document was originally written (fi:Shakespeare shows an example of modernizing a document). However, while modernization has been successful in order to increase the comprehension of historical documents BIBREF1, BIBREF2, it is still far from creating error-free modern versions. Therefore, this task still needs to be carried out by scholars.", "In order to measure the gains in human effort reduction, we made use of the following metrics:"]}
{"question_id": "a3705b53c6710b41154c65327b7bbec175bdfae7", "predicted_answer": "", "predicted_evidence": ["ta:quality presents the quality of the modernization. Both SMT and NMT approaches were able to significantly improved the baseline. That is, the modernized documents are easier to comprehend by a contemporary reader than the original documents. An exception to this is El Conde Lucanor. The SMT approach yielded significant improvements in terms of TER, but was worse in terms of BLEU. Moreover, the NMT approach yielded worst results in terms of both BLEU and TER. Most likely, this results are due to having used the systems trained with El Quijote for modernizing El Conde Lucanor (see se:corp).", "The prefix-based IMT protocol (see se:PBIMT) can be naturally included into NMT systems since sentences are generated from left to right. In order to take into account the user's feedback and generate compatible hypothesis, the search space must be constraint. Given a prefix $\\tilde{\\mathbf {y}}_p$, only a single path accounts for it. The branching of the search process starts once this path has been covered. Introducing the validated prefix $\\tilde{\\mathbf {y}}_p$, eq:NMT becomes:", "Interactive machine translation (IMT) fosters human\u2013computer collaborations to generate error-free translations in a productive way BIBREF4, BIBREF5. In this work, we proposed to apply one of these protocols to historical documents modernization. We strive for creating an error-free modern version of a historical document, decreasing the human effort needed to achieve this goal.", "We used sacreBLEU BIBREF40 for ensuring consistent BLEU scores. For determining whether two systems presented statistically significant differences, we applied approximate randomization tests BIBREF41, with $10,000$ repetitions and using a $p$-value of $0.05$.", "This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method."]}
{"question_id": "b62b7ec5128219f04be41854247d5af992797937", "predicted_answer": "", "predicted_evidence": ["As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction.", "Interactive machine translation (IMT) fosters human\u2013computer collaborations to generate error-free translations in a productive way BIBREF4, BIBREF5. In this work, we proposed to apply one of these protocols to historical documents modernization. We strive for creating an error-free modern version of a historical document, decreasing the human effort needed to achieve this goal.", "Modernization tackles the language barrier in order to increase the accessibility of historical documents. To achieve this, it generates a new version of a historical document in the modern version of the language in which the document was originally written (fi:Shakespeare shows an example of modernizing a document). However, while modernization has been successful in order to increase the comprehension of historical documents BIBREF1, BIBREF2, it is still far from creating error-free modern versions. Therefore, this task still needs to be carried out by scholars.", "which implies a search over the space of translations, but constrained by the validated prefix $\\tilde{\\mathbf {y}}_p$ BIBREF15.", "When comparing the SMT and NMT approaches, we observe that SMT yielded the best results in all cases. This behavior was already perceived by BIBREF2 and is, most likely, due to the small size of the training corpora\u2014a well-known problem in NMT. However, while the goal of modernization is making historical documents as easier to comprehend by contemporary people as possible, our goal is different. In this work, our goal is to obtain an error-free modern copy of a historical document. To achieve this, we proposed an interactive collaboration between a human expert and our modernizing system, in order to reduce the effort needed to generate such copy. ta:effort presents the experimental results."]}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "predicted_answer": "", "predicted_evidence": ["The rest of this document is structured as follows: se:work introduces the related work. Then, in se:IMT we present our protocol. se:exp describes the experiments conducted in order to assess our proposal. The results of those experiments are presented and discussed in se:res. Finally, in se:conc, conclusions are drawn.", "BIBREF37: measures the number of words edited by the user, normalized by the number of words in the final translation.", "ta:corp presents the corpora statistics.", "Regarding the performance of both approaches, SMT achieved the highest effort reduction. This was reasonably expected since its modernization quality was better. However, in past neural IMT works BIBREF15, the neural IMT approach was able to yield further improvements despite having a lower translation quality than its SMT counterpart. Most likely, the reason of this is that, due to the small training corpora, the neural model was not able to reach its best performance, Nonetheless, we should address this in a future work.", "In NMT, eq:SMT is modeled by a neural network with parameters $\\mathbf {\\Theta }$:"]}
{"question_id": "51e9f446d987219bc069222731dfc1081957ce1f", "predicted_answer": "", "predicted_evidence": ["This equation is very similar to eq:SMT: at each iteration, the process consists in a regular search in the translations space but constrained by the prefix $\\tilde{\\mathbf {y}}_p$.", "In this section, we present our experimental conditions, including translation systems, corpora and evaluation metrics.", "In NMT, eq:SMT is modeled by a neural network with parameters $\\mathbf {\\Theta }$:", "Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras.", "Additionally, to evaluate the quality of the modernization and the difficulty of each task, we made use of the following well-known metrics:"]}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "predicted_answer": "", "predicted_evidence": ["Event span identification is the task of extracting character offsets of the expression in raw clinical notes. This subtask is quite important due to the fact that the event span identification accuracy will affect the accuracy of attribute identification. We first run our neural network classifier to identify event spans. Then, given each span, our system tries to identify attribute values.", "Temporal Convolution applies one-dimensional convolution over the input sequence. The one-dimensional convolution is an operation between a vector of weights INLINEFORM0 and a vector of inputs viewed as a sequence INLINEFORM1 . The vector INLINEFORM2 is the filter of the convolution. Concretely, we think of INLINEFORM3 as the input sentence and INLINEFORM4 as a single feature value associated with the INLINEFORM5 -th word in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector INLINEFORM6 with each INLINEFORM7 -gram in the sentence INLINEFORM8 to obtain another sequence INLINEFORM9 : DISPLAYFORM0", "To address this challenge, we propose a deep neural networks based method, especially convolution neural network BIBREF0 , to learn hidden feature representations directly from raw clinical notes. More specifically, one method first extract a window of surrounding words for the candidate word. Then, we attach each word with their part-of-speech tag and shape information as extra features. Then our system deploys a temporal convolution neural network to learn hidden feature representations. Finally, our system uses Multilayer Perceptron (MLP) to predict event spans. Note that we use the same model to predict event attributes.", "After that, the objective function is the negative log-likelihood of the true class labels INLINEFORM0 : DISPLAYFORM0", "The input of our system consists of raw clinical notes or pathology reports like below:"]}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "predicted_answer": "", "predicted_evidence": ["In this work, we brought deep representation learning technologies to the clinical domain. Specifically, we focus on clinical information extraction, using clinical notes and pathology reports from the Mayo Clinic. Our system will identify event expressions consisting of the following components:", "After that, the objective function is the negative log-likelihood of the true class labels INLINEFORM0 : DISPLAYFORM0", "We also employ dropout on the penultimate layer with a constraint on INLINEFORM0 -norms of the weight vector. Dropout prevents co-adaptation of hidden units by randomly dropping out a proportion INLINEFORM1 of the hidden units during forward-backpropagation. That is, given the penultimate layer INLINEFORM2 , instead of using: DISPLAYFORM0", "To address this challenge, we propose a deep neural networks based method, especially convolution neural network BIBREF0 , to learn hidden feature representations directly from raw clinical notes. More specifically, one method first extract a window of surrounding words for the candidate word. Then, we attach each word with their part-of-speech tag and shape information as extra features. Then our system deploys a temporal convolution neural network to learn hidden feature representations. Finally, our system uses Multilayer Perceptron (MLP) to predict event spans. Note that we use the same model to predict event attributes.", "And output annotations over the text that capture the key information such as event mentions and attributes. Table TABREF7 illustrates the output of clinical information extraction in details."]}
{"question_id": "930733efb3b97e1634b4dcd77123d4d5731e8807", "predicted_answer": "", "predicted_evidence": ["We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.", "where INLINEFORM0 is the element-wise multiplication operator and INLINEFORM1 is a masking vector of Bernoulli random variables with probability INLINEFORM2 of being 1. Gradients are backpropagated only through the unmasked units. At test step, the learned weight vectors are scaled by INLINEFORM3 such that INLINEFORM4 , and INLINEFORM5 is used to score unseen sentences. We additionally constrain INLINEFORM6 -norms of the weight vectors by re-scaling INLINEFORM7 to have INLINEFORM8 whenever INLINEFORM9 after a gradient descent step.", "In this work, we brought deep representation learning technologies to the clinical domain. Specifically, we focus on clinical information extraction, using clinical notes and pathology reports from the Mayo Clinic. Our system will identify event expressions consisting of the following components:", "We want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label INLINEFORM0 given the inputs INLINEFORM1 is predicted by a softmax classifier that takes the hidden state INLINEFORM2 as input: DISPLAYFORM0", "In the past few years, there has been much interest in applying neural network based deep learning techniques to solve all kinds of natural language processing (NLP) tasks. From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling BIBREF0 , BIBREF1 , to high level tasks such as machine translation, information retrieval, semantic analysis BIBREF2 , BIBREF3 , BIBREF4 and sentence relation modeling tasks such as paraphrase identification and question answering BIBREF5 , BIBREF6 , BIBREF7 . Deep representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via learning either word level representations or sentence level representations."]}
{"question_id": "11f9c207476af75a9272105e646df02594059c3f", "predicted_answer": "", "predicted_evidence": ["We also employ dropout on the penultimate layer with a constraint on INLINEFORM0 -norms of the weight vector. Dropout prevents co-adaptation of hidden units by randomly dropping out a proportion INLINEFORM1 of the hidden units during forward-backpropagation. That is, given the penultimate layer INLINEFORM2 , instead of using: DISPLAYFORM0", "The input of our system consists of raw clinical notes or pathology reports like below:", "Event span identification is the task of extracting character offsets of the expression in raw clinical notes. This subtask is quite important due to the fact that the event span identification accuracy will affect the accuracy of attribute identification. We first run our neural network classifier to identify event spans. Then, given each span, our system tries to identify attribute values.", "We want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label INLINEFORM0 given the inputs INLINEFORM1 is predicted by a softmax classifier that takes the hidden state INLINEFORM2 as input: DISPLAYFORM0", "for output unit INLINEFORM0 in forward propagation, dropout uses: DISPLAYFORM0"]}
{"question_id": "b32de10d84b808886d7a91ab0c423d4fc751384c", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the element-wise multiplication operator and INLINEFORM1 is a masking vector of Bernoulli random variables with probability INLINEFORM2 of being 1. Gradients are backpropagated only through the unmasked units. At test step, the learned weight vectors are scaled by INLINEFORM3 such that INLINEFORM4 , and INLINEFORM5 is used to score unseen sentences. We additionally constrain INLINEFORM6 -norms of the weight vectors by re-scaling INLINEFORM7 to have INLINEFORM8 whenever INLINEFORM9 after a gradient descent step.", "Our research proved that we can get competitive results without the help of a domain specific feature extraction toolkit, such as cTAKES. Also we only leverage basic natural language processing modules such as tokenization and part-of-speech tagging. With the help of deep representation learning, we can dramatically reduce the cost of clinical information extraction system development.", "The major advantage of our system is that we only leverage NLTK tokenization and a POS tagger to preprocess our training dataset. When implementing our neural network based clinical information extraction system, we found it is not easy to construct high quality training data due to the noisy format of clinical notes. Choosing the proper tokenizer is quite important for span identification. After several experiments, we found \"RegexpTokenizer\" can match our needs. This tokenizer can generate spans for each token via sophisticated regular expression like below,", "where INLINEFORM0 is the number of training examples and the superscript INLINEFORM1 indicates the INLINEFORM2 th example.", "We want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label INLINEFORM0 given the inputs INLINEFORM1 is predicted by a softmax classifier that takes the hidden state INLINEFORM2 as input: DISPLAYFORM0"]}
{"question_id": "9ea3669528c2b295f21770cb7f70d0c4b4389223", "predicted_answer": "", "predicted_evidence": ["[id=lq]Most existing work does not consider the relation between an emotion word and the cause of such an emotion, or they simply use the emotion word as a feature in their model learning. Since emotion cause extraction requires an understanding of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems BIBREF26 , BIBREF27 . Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation BIBREF28 , sentiment analysis BIBREF29 or summarization BIBREF30 . To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction.", "Note that we obtain the attention for each position rather than each word. It means that the corresponding attention for the INLINEFORM0 -th word in the previous convolutional slot should be INLINEFORM1 . Hence, there are three prediction output vectors, namely, INLINEFORM2 , INLINEFORM3 , INLINEFORM4 : DISPLAYFORM0", "For the first and the last word in a clause, we use zero padding, INLINEFORM0 , where INLINEFORM1 is the length of a clause. Then, the attention [id=lq]weightsignal for each word position in the clause is [id=lq]now defined as: DISPLAYFORM0", "In this [id=lq]work, we [id=lq]treat emotion cause extraction as a QA task and propose a new model based on deep memory networks for identifying [id=lq]the emotion causes for an emotion expressed in text. [id=lq]The key property of this approach is the use of context information in the learning process which is ignored in the original memory network. Our new [id=lq]memory network architecture is able [id=lq]to store context in different memory slots to capture context information [id=lq]in proper sequence by convolutional operation. Our model achieves the state-of-the-art performance on a dataset for emotion cause detection when compared to a number of competitive baselines. In the future, we will explore effective ways [id=lq]to model discourse relations among clauses and develop a QA system which can directly output the cause of emotions as answers.", "Here, the size of INLINEFORM0 is INLINEFORM1 . Since the prediction vector is a concatenation of three outputs. We implement a concatenation operation rather than averaging or other operations because the parameters in different memory slots can be updated [id=lq]respectively in this way by back propagation. The concatenation of three output vectors forms a sequence-level feature which can be used in the training. Such a feature is important especially [id=lq]when the size of annotated training data is small."]}
{"question_id": "9863f5765ba70f7ff336a580346ef70205abbbd8", "predicted_answer": "", "predicted_evidence": ["Ex.3 45\u5929\uff0c\u5bf9\u4e8e\u5931\u53bb\u513f\u5b50\u7684\u4ed6\u4eec\u662f\u591a\u4e48\u7684\u6f2b\u957f\uff0c\u5b9d\u8d1d\u56de\u5bb6\u4e86\uff0c\u8fd9\u4e2a\u6625\u8282\u662f\u591a\u4e48\u5e78\u798f\u3002", "Ex.1 Because I lost my phone yesterday, I feel sad now.", "Existing approaches to emotion cause extraction mostly rely on methods typically used in information extraction, such as rule based template matching, sequence labeling and classification based methods. Most of them use linguistic rules or lexicon features, but do not consider the semantic information and ignore the relation between the emotion word and emotion cause. In this paper, we present a new method for emotion cause extraction. We consider emotion cause extraction as a question answering (QA) task. Given a text containing the description of an event which [id=lq]may or may not cause a certain emotion, we take [id=lq]an emotion word [id=lq]in context, such as \u201csad\u201d, as a query. The question to the QA system is: \u201cDoes the described event cause the emotion of sadness?\u201d. The [id=lq]expected answer [id=lq]is either \u201cyes\u201d or \u201cno\u201d. (see Figure FIGREF1 ). We build our QA system based on a deep memory network.", "where INLINEFORM0 is the length of the clause. [id=lq] INLINEFORM1 also serves as the size of the memory. Obviously, INLINEFORM2 and INLINEFORM3 . [id=lq] INLINEFORM4 can serve as an attention weight to measure the importance of each word in our model.", "Ex.3 45 days, it is long time for the parents who lost their baby. If the baby comes back home, they would become so happy in this Spring Festival."]}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "predicted_answer": "", "predicted_evidence": ["In our model, the training epochs are set to 20. In this section, we examine the testing error using a case study. Due to the page length limit, we only choose one example from the corpus. The text below has four clauses:", "SVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31", "CNN: The convolutional neural network for sentence classification BIBREF5 .", "Ex.1 \u6211\u7684\u624b\u673a\u6628\u5929\u4e22\u4e86\uff0c\u6211\u73b0\u5728\u5f88\u96be\u8fc7\u3002", "We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause."]}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "predicted_answer": "", "predicted_evidence": ["In this example, the cause of emotion \u201chappy\u201d is described in the third clause.", "Ex.3 45 days, it is long time for the parents who lost their baby. If the baby comes back home, they would become so happy in this Spring Festival.", "For hop 1, the query is INLINEFORM0 and the prediction vector is INLINEFORM1 ;", "RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .", "For the first and the last word in a clause, we use zero padding, INLINEFORM0 , where INLINEFORM1 is the length of a clause. Then, the attention [id=lq]weightsignal for each word position in the clause is [id=lq]now defined as: DISPLAYFORM0"]}
{"question_id": "67c16ba64fe27838b1034d15194c07a9c98cdebe", "predicted_answer": "", "predicted_evidence": ["Since the reference methods do not focus on the keywords level, we only compare the performance of Memnet and ConvMS-Memnet in Table 6. It can be observed that our proposed ConvMS-Memnet outperforms Memnet by 5.6% in F-measure. It shows that by capturing context features, ConvMS-Memnet is able to identify the word level emotion cause better compare to Memnet.", "Ex.2 \u5bb6\u4eba/family \u7684/'s \u575a\u6301/insistence \u66f4/more \u8ba9/makes \u4eba/people \u611f\u52a8/touched", "Existing approaches to emotion cause extraction mostly rely on methods typically used in information extraction, such as rule based template matching, sequence labeling and classification based methods. Most of them use linguistic rules or lexicon features, but do not consider the semantic information and ignore the relation between the emotion word and emotion cause. In this paper, we present a new method for emotion cause extraction. We consider emotion cause extraction as a question answering (QA) task. Given a text containing the description of an event which [id=lq]may or may not cause a certain emotion, we take [id=lq]an emotion word [id=lq]in context, such as \u201csad\u201d, as a query. The question to the QA system is: \u201cDoes the described event cause the emotion of sadness?\u201d. The [id=lq]expected answer [id=lq]is either \u201cyes\u201d or \u201cno\u201d. (see Figure FIGREF1 ). We build our QA system based on a deep memory network.", "In this example, the cause of the emotion \u201ctouched\u201d is \u201cinsistence\u201d. We show in Table 5 the distribution of word-level attention weights in different hops of memory network training. We can observe that in the first two hops, the highest attention weights centered on the word \u201cmore\". However, from the third hop onwards, the highest attention weight moves to the word sub-sequence centred on the word \u201cinsistence\u201d. This shows that our model is effective in identifying the most important keyword relating to the emotion cause. Also, better results are obtained using deep memory network trained with at least 3 hops. This is consistent with what we observed in Section UID45 .", "We then normalize the value of INLINEFORM0 to INLINEFORM1 using a softmax function, denoted by INLINEFORM2 [id=lq]as: DISPLAYFORM0"]}
{"question_id": "58a3cfbbf209174fcffe44ce99840c758b448364", "predicted_answer": "", "predicted_evidence": ["Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a significant margin. NAS is not quite up there with the LSTM suggesting its architecture might have overfitted to Penn Treebank, but data for deeper variants would be necessary to draw this conclusion.", "Naturally, NAS benefitted only to a limited degree from our tuning, since the numbers of BIBREF1 were already produced by employing similar regularisation methods and a grid search. The small edge can be attributed to the suboptimality of grid search (see Section SECREF23 ).", "different initialisation seeds,", "non-deterministic ordering of floating-point operations in optimised linear algebra routines,", "Although we can draw attention to this problem, this paper does not offer a practical methodological solution beyond establishing reliable baselines that can be the benchmarks for subsequent work. Still, we demonstrate how, with a huge amount of computation, noise levels of various origins can be carefully estimated and models meaningfully compared. This apparent tradeoff between the amount of computation and the reliability of results seems to lie at the heart of the matter. Solutions to the methodological challenges must therefore make model evaluation cheaper by, for instance, reducing the number of hyperparameters and the sensitivity of models to them, employing better hyperparameter optimisation strategies, or by defining \u201cleagues\u201d with predefined computational budgets for a single model representing different points on the tradeoff curve."]}
{"question_id": "6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "predicted_answer": "", "predicted_evidence": ["Naturally, NAS benefitted only to a limited degree from our tuning, since the numbers of BIBREF1 were already produced by employing similar regularisation methods and a grid search. The small edge can be attributed to the suboptimality of grid search (see Section SECREF23 ).", "Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model.", "Once hyperparameters have been properly controlled for, we find that LSTMs outperform the more recent models, contra the published claims. Our result is therefore a demonstration that replication failures can happen due to poorly controlled hyperparameter variation, and this paper joins other recent papers in warning of the under-acknowledged existence of replication failure in deep learning BIBREF2 , BIBREF3 . However, we do show that careful controls are possible, albeit at considerable computational cost.", "In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1 . We specify flexible, parameterised model families with the ability to adjust embedding and recurrent cell sizes for a given parameter budget and with fine grain control over regularisation and learning hyperparameters.", "On two of the three datasets, we improved previous results substantially by careful model specification and hyperparameter optimisation, but the improvement for RHNs is much smaller compared to that for LSTMs. While it cannot be ruled out that our particular setup somehow favours LSTMs, we believe it is more likely that this effect arises due to the original RHN experimental condition having been tuned more extensively (this is nearly unavoidable during model development)."]}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "predicted_answer": "", "predicted_evidence": ["Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a significant margin. NAS is not quite up there with the LSTM suggesting its architecture might have overfitted to Penn Treebank, but data for deeper variants would be necessary to draw this conclusion.", "Third, we were also interested in how recurrent dropout BIBREF12 would perform in lieu of variational dropout. Dropout masks were shared between time steps in both methods, and our results indicate no consistent advantage to either of them.", "For evaluation, the checkpoint with the best validation perplexity found by the tuner is loaded and the model is applied to the test set with a batch size of 1. For the word based datasets, using the training batch size makes results worse by 0.3 PPL while Enwik8 is practically unaffected due to its evaluation and training sets being much larger. Preliminary experiments indicate that MC averaging would bring a small improvement of about 0.4 in perplexity and 0.005 in bits per character, similar to the results of BIBREF11 , while being a 1000 times more expensive which is prohibitive on larger datasets. Therefore, throughout we use the mean-field approximation for dropout at test time.", "For Enwik8 there are relatively few parameters in the embeddings since the vocabulary size is only 205. Here we choose not to share embeddings and to omit the down-projection unconditionally.", "On two of the three datasets, we improved previous results substantially by careful model specification and hyperparameter optimisation, but the improvement for RHNs is much smaller compared to that for LSTMs. While it cannot be ruled out that our particular setup somehow favours LSTMs, we believe it is more likely that this effect arises due to the original RHN experimental condition having been tuned more extensively (this is nearly unavoidable during model development)."]}
{"question_id": "4f8b078b9f60be30520fd32a3d8601ab3babb5c0", "predicted_answer": "", "predicted_evidence": ["the validation and test sets being finite samples from a infinite population.", "Once hyperparameters have been properly controlled for, we find that LSTMs outperform the more recent models, contra the published claims. Our result is therefore a demonstration that replication failures can happen due to poorly controlled hyperparameter variation, and this paper joins other recent papers in warning of the under-acknowledged existence of replication failure in deep learning BIBREF2 , BIBREF3 . However, we do show that careful controls are possible, albeit at considerable computational cost.", "Dropout is applied to feedforward connections denoted by dashed arrows in the figure. From the bottom up: to embedded inputs (input dropout), to connections between layers (intra-layer dropout), to the combined and the down-projected outputs (output dropout). All these dropouts have random masks drawn independently per time step, in contrast to the dropout on recurrent states where the same mask is used for all time steps in the sequence.", "RHN based models are typically conceived of as a single horizontal \u201chighway\u201d to emphasise how the recurrent state is processed through time. In Fig. FIGREF1 , we choose to draw their schema in a way that makes the differences from LSTMs immediately apparent. In a nutshell, the RHN state is passed from the topmost layer to the lowest layer of the next time step. In contrast, each LSTM layer has its own recurrent connection and state.", "When training word level models we follow common practice and use a batch size of 64, truncated backpropagation with 35 time steps, and we feed the final states from the previous batch as the initial state of the subsequent one. At the beginning of training and test time, the model starts with a zero state. To bias the model towards being able to easily start from such a state at test time, during training, with probability 0.01 a constant zero state is provided as the initial state."]}
{"question_id": "54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "predicted_answer": "", "predicted_evidence": ["With a large number of hyperparameter combinations evaluated, the question of how much the tuner overfits arises. There are multiple sources of noise in play,", "We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16 . Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.", "Our aim is strictly to do better model comparisons for these architectures and we thus refrain from including techniques that are known to push perplexities even lower, but which are believed to be largely orthogonal to the question of the relative merits of these recurrent cells. In parallel work with a remarkable overlap with ours, BIBREF5 demonstrate the utility of adding a Neural Cache BIBREF6 . Building on their work, BIBREF7 show that Dynamic Evaluation BIBREF8 contributes similarly to the final perplexity.", "To assess the severity of these issues, we conducted the following experiment: models with the best hyperparameter settings for Penn Treebank and Wikitext-2 were retrained from scratch with various initialisation seeds and the validation and test scores were recorded. If during tuning, a model just got a lucky run due to a combination of UID19 and UID20 , then retraining with the same hyperparameters but with different seeds would fail to reproduce the same good results.", "To further verify that the best hyperparameter setting found by the tuner is not a fluke, we plotted the validation loss against the hyperparameter settings. Fig. FIGREF24 shows one such typical plot, for a 4-layer LSTM. We manually restricted the ranges around the best hyperparameter values to around 15\u201325% of the entire tuneable range, and observed that the vast majority of settings in that neighbourhood produced perplexities within 3.0 of the best value. Widening the ranges further leads to quickly deteriorating results."]}
{"question_id": "803babb71e1bdaf507847d6c712585f4128e9f47", "predicted_answer": "", "predicted_evidence": ["Machine Translation of such user-generated content can improve the situation and make the data available for direct display or for downstream NLP tasks (e.g., cross-lingual information retrieval, sentiment analysis, spam or fake review detection), provided its quality is sufficient.", "BIBREF8 show that doing back-translation with sampling instead of beam search brings large improvements due to increased diversity. Following this work, we test several settings:", "", "BIBREF2 propose another approach, inline casing, which does not require any change in the model. We insert the case as a regular token into the sequence right after the word. Special tokens <U>, <L> and <T> (upper, lower and title) are used for this purpose and appended to the vocabulary. Contrary to the previous solution, there is only one embedding matrix and one softmax.", "BIBREF3, BIBREF4 and BIBREF5 propose to improve robustness by training models on data-augmented corpora, containing noisy sources obtained by random word or character deletions, insertions, substitutions or swaps. Recently, BIBREF6 proposed to use a similar technique along with noise generation through replacement of a clean source by one obtained by back-translation."]}
{"question_id": "5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "predicted_answer": "", "predicted_evidence": ["For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27. Training is done on 8 GPUs, with accumulated gradients over 10 batches BIBREF27, and a max batch size of 3500 tokens per GPU. We train for 20 epochs, while saving a checkpoint every 2500 updates ($\\approx \\frac{2}{5}$ epoch on UGC) and average the 5 best checkpoints according to their perplexity on a validation set (a held-out subset of UGC).", "We use langid.py BIBREF25 to filter sentence pairs from UGC. We also remove duplicate sentence pairs, and lines longer than 175 words or with a length ratio greater than $1.5$ (see Table TABREF31). Then we apply SentencePiece and our rare character handling strategy (Section SECREF8). We use a joined BPE model of size 32k, trained on the concatenation of both sides of the corpus, and set SentencePiece's vocabulary threshold to 100. Finally, unless stated otherwise, we always use the inline casing approach (see Section SECREF10).", "Translating restaurant reviews written by casual customers presents several difficulties for NMT, in particular robustness to non-standard language and adaptation to a specific style or domain (see Section SECREF7 for details).", "Foursquare-HT was translated from scratch by the same translators who post-edited Foursquare-PE. While we did not use it in this work, it can be used as extra training or development data. We also release a human translation of the French-language test set (668 sentences) of the Aspect-Based Sentiment Analysis task at SemEval 2016 BIBREF14.", "We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric."]}
{"question_id": "eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "predicted_answer": "", "predicted_evidence": ["Table TABREF46 shows the results of the back-translation (BT) techniques. Surprisingly, BT with beam search (BT-B) deteriorates BLEU scores on Foursquare-test, while BT with sampling gives a consistent improvement. BLEU scores on newstest2014 are not significantly impacted, suggesting that BT can be used for domain adaptation without hurting quality on other domains.", "For sake of brevity, we only give the final BLEU scores on newstest2014 and Foursquare-test. Scores on Foursquare-valid, and MTNT-test (for comparison with BIBREF0, BIBREF2) are given in Appendix. We evaluate \u201cdetokenized\u201d MT outputs against raw references using SacreBLEU BIBREF29.", "Table TABREF41 compares the case handling techniques presented in Section SECREF10. To better evaluate the robustness of our models to changes of case, we built 3 synthetic test sets from Foursquare-test, with the same target, but all source words in upper, lower or title case.", "", "Foursquare-PE + tags is not as good as fine-tuning with Foursquare-PE. However, fine-tuned models get slightly worse results on news."]}
{"question_id": "290ebf0d1c49b67a6d1858366be751d89086a78b", "predicted_answer": "", "predicted_evidence": ["We select 4 MT models for evaluation (see Table TABREF63) and show their 4 outputs at once, sentence-by-sentence, to human judges, who are asked to rank them given the French source sentence in context (with the full review). For each pair of models, we count the number of wins, ties and losses, and apply the Wilcoxon signed-rank test.", "Inline and factored case perform equally well, significantly better than the default (cased) model, especially on all-uppercase inputs. Lowercasing the source is a good option, but gives a slightly lower score on regular Foursquare-test. Finally, synthetic case noise added to the source gives surprisingly good results. It could also be combined with factored or inline case.", "This in-domain data is concatenated to the out-of-domain parallel data and used for training.", "Machine Translation of such user-generated content can improve the situation and make the data available for direct display or for downstream NLP tasks (e.g., cross-lingual information retrieval, sentiment analysis, spam or fake review detection), provided its quality is sufficient.", "After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid."]}
{"question_id": "806fefe0e331ddb3c17245d6a9fa7433798e367f", "predicted_answer": "", "predicted_evidence": ["We propose solutions for dealing with non-standard case, emoticons, emojis and other issues.", "While our models are promising, they still show serious errors when applied to user-generated content: missing negations, hallucinations, unrecognized named entities, insensitivity to context. This suggests that this task is far from solved.", "We took the first 300 test sentences to create 6 tasks of 50 sentences each. Then we asked bilingual colleagues to rank the output of 4 models by their translation quality. They were asked to do one or more of these tasks. The judge did not know about the list of models, nor the model that produced any given translation. We got 12 answers. The inter-judge Kappa coefficient ranged from 0.29 to 0.63, with an average of 0.47, which is a good value given the difficulty of the task. Table TABREF63 gives the results of the evaluation, which confirm our observations with BLEU.", "With this dictionary, describing the real error distribution in Foursquare text, we take our large out-of-domain training corpus, and randomly replace source-side words with one of their variants (rules 1 to 6), while respecting the frequency of this variant in the real data. We also manually define regular expressions to randomly apply rules 7 to 11 (e.g., \"er \"$\\rightarrow $\"\u00e9 \").", "As shown in Table TABREF54, these techniques can be combined to achieve the best results. The natural noise does not have a significant effect on BLEU scores. Back-translation combined with fine-tuning gives the best performance on Foursquare (+4.5 BLEU vs UGC). However, using tags instead of fine-tuning strikes a better balance between general domain and in-domain performance."]}
{"question_id": "458e5ed506883bfec6623102ec9f43c071f0616f", "predicted_answer": "", "predicted_evidence": ["We propose a machine learning method to cyberbullying detection by making use of a linear SVM classifier BIBREF14 , BIBREF15 exploiting a varied set of features. To the best of our knowledge, this is the first approach to the annotation of fine-grained text categories related to cyberbullying and the detection of signals of cyberbullying events. It is also the first elaborate research on automatic cyberbullying detection on Dutch social media. For the present experiments, we focus on an English and Dutch ASKfm corpus, but the methodology adopted is language and genre independent, provided there is annotated data available.", "Similarly to traditional bullying, cyberbullying involves a number of participants that adopt well-defined roles. Researchers have identified several roles in (cyber)bullying interactions. Although traditional studies on bullying have mainly concentrated on bullies and victims BIBREF46 , the importance of bystanders in a bullying episode has been acknowledged BIBREF47 , BIBREF48 . Bystanders can support the victim and mitigate the negative effects caused by the bullying BIBREF48 , especially on social networking sites, where they hold higher intentions to help the victim than in real life conversations BIBREF49 . While BIBREF46 distinguish four different bystanders, BIBREF50 distinguish three main types: i) bystanders who participate in the bullying, ii) who help or support the victim and iii) those who ignore the bullying. Given that passive bystanders are hard to recognise in online text, only the former two are included in our annotation scheme.", "In spite of these efforts, a lot of undesirable and hurtful content remains online. BIBREF1 analysed a body of quantitative research on cyberbullying and observed cybervictimisation rates among teenagers between 20% and 40%. BIBREF5 focused on 12 to 17 year olds living in the United States and found that no less than 72% of them had encountered cyberbullying at least once within the year preceding the questionnaire. BIBREF6 surveyed 9 to 26 year olds in the United States, Canada, the United Kingdom and Australia, and found that 29% of the respondents had ever been victimised online. A study among 2,000 Flemish secondary school students (age 12 to 18) revealed that 11% of them had been bullied online at least once in the six months preceding the survey BIBREF7 .", "The present annotation scheme describes some specific textual categories related to cyberbullying, including threats, insults, defensive statements from a victim, encouragements to the harasser, etc. (see Section SECREF15 for a complete overview). All of these forms were inspired by social studies on cyberbullying BIBREF7 , BIBREF19 and manual inspection of cyberbullying examples.", "Parental control tools (e.g. NetNanny) already block unsuited or undesirable content and some social networks make use of keyword-based moderation tools (i.e., using lists of profane and insulting words to flag harmful content). However, such approaches typically fail to detect implicit or subtle forms of cyberbullying in which no explicit vocabulary is used. There is therefore a need for intelligent and self-learning systems that can go beyond keyword spotting and hence improve recall of cyberbullying detection."]}
{"question_id": "85ab5f773b297bcf48a274634d402a35e1d57446", "predicted_answer": "", "predicted_evidence": ["As mentioned earlier, data collection remains a bottleneck in cyberbullying research. Although cyberbullying has been recognised as a serious problem (cf. Section SECREF1 ), real-world examples are often hard to find in public platforms. Naturally, the vast majority of communications do not contain traces of verbal aggression or transgressive behaviour. When constructing a corpus for machine learning purposes, this results in imbalanced datasets, meaning that one class (e.g. cyberbullying posts) is much less represented in the corpus than the other (e.g. non-cyberbullying posts). To tackle this problem, several studies have adopted resampling techniques BIBREF35 , BIBREF41 , BIBREF31 that create synthetic minority class examples or reduce the number of negative class examples (i.e., minority class oversampling and majority class undersampling BIBREF44 ).", "Cyberbullying has been a widely covered research topic recently and studies have shed light on direct and indirect types of cyberbullying, implicit and explicit forms, verbal and non-verbal cyberbullying, and so on. This is important from a sociolinguistic point of view, but knowing what cyberbullying involves is also crucial to build models for automatic cyberbullying detection. In the following paragraphs, we present our data annotation guidelines BIBREF30 and focus on different types and roles related to the phenomenon.", "Parental control tools (e.g. NetNanny) already block unsuited or undesirable content and some social networks make use of keyword-based moderation tools (i.e., using lists of profane and insulting words to flag harmful content). However, such approaches typically fail to detect implicit or subtle forms of cyberbullying in which no explicit vocabulary is used. There is therefore a need for intelligent and self-learning systems that can go beyond keyword spotting and hence improve recall of cyberbullying detection.", "Other: expressions that contain any other form of cyberbullying-related behaviour than the ones described here.", "As mentioned earlier, although research on cyberbullying detection is more limited than social studies on the phenomenon, some important advances have been made in recent years. In what follows, we present a brief overview of the most important natural language processing approaches to cyberbullying detection."]}
{"question_id": "5154f63c50729b8ac04939588c2f5ffeb916e3df", "predicted_answer": "", "predicted_evidence": ["Many social and psychological studies have worked towards a definition of cyberbullying. A common starting point for conceptualising cyberbullying are definitions of traditional (or offline) bullying. Seminal work has been published by BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , who describe bullying based on three main criteria, including i) intention (i.e., a bully intends to inflict harm on the victim), ii) repetition (i.e., bullying acts take place repeatedly over time) and iii) a power imbalance between the bully and the victim (i.e., a more powerful bully attacks a less powerful victim). With respect to cyberbullying, a number of definitions are based on the above-mentioned criteria. A popular definition is that of BIBREF21 which describes cyberbullying as \u201can aggressive, intentional act carried out by a group or individual, using electronic forms of contact, repeatedly and over time, against a victim who cannot easily defend him or herself\u201d.", "`Some tweens got violent on the n train, the one boy got off after blows 2 the chest... Saw him cryin as he walkd away :( bullying not cool' BIBREF42", "Table TABREF9 presents a number of recent studies on cyberbullying detection, providing insight into the state of the art in cyberbullying research and the contribution of the current research to the domain.", "Essentially, the annotation scheme describes two levels of annotation. Firstly, the annotators were asked to indicate, at the post level, whether the post under investigation was related to cyberbullying. If the post was considered a signal of cyberbullying, annotators identified the author's role. Secondly, at the subsentence level, the annotators were tasked with the identification of a number of fine-grained text categories related to cyberbullying. More concretely, they identified all text spans corresponding to one of the categories described in the annotation scheme. To provide the annotators with some context, all posts were presented within their original conversation when possible. All annotations were done using the Brat rapid annotation tool BIBREF52 , some examples of which are presented in Table TABREF33 .", "In the research described in this paper, cyberbullying is considered a complex phenomenon consisting of different forms of harmful behaviour online, which are described in more detail in our annotation scheme BIBREF30 . Purposing to facilitate manual monitoring efforts on social networks, we develop a system that automatically detects signals of cyberbullying, including attacks from bullies, as well as victim and bystander reactions. Similarly, BIBREF42 investigated bullying traces posted by different author roles (accuser, bully, reporter, victim). However, they collected tweets by using specific keywords (i.e., bully, bullied and bullying). As a result, their corpus contains many reports or testimonials of a cyberbullying incident (example 1), instead of actual signals that cyberbullying is going on. Moreover, their method implies that cyberbullying-related content devoid of such keywords will not be part of the training corpus."]}
{"question_id": "2aeabec8a734a6e8ca9e7a308dd8c9a1011b3d6e", "predicted_answer": "", "predicted_evidence": ["Threat/Blackmail: expressions containing physical or psychological threats or indications of blackmail.", "`Some tweens got violent on the n train, the one boy got off after blows 2 the chest... Saw him cryin as he walkd away :( bullying not cool' BIBREF42", "Similarly to traditional bullying, cyberbullying involves a number of participants that adopt well-defined roles. Researchers have identified several roles in (cyber)bullying interactions. Although traditional studies on bullying have mainly concentrated on bullies and victims BIBREF46 , the importance of bystanders in a bullying episode has been acknowledged BIBREF47 , BIBREF48 . Bystanders can support the victim and mitigate the negative effects caused by the bullying BIBREF48 , especially on social networking sites, where they hold higher intentions to help the victim than in real life conversations BIBREF49 . While BIBREF46 distinguish four different bystanders, BIBREF50 distinguish three main types: i) bystanders who participate in the bullying, ii) who help or support the victim and iii) those who ignore the bullying. Given that passive bystanders are hard to recognise in online text, only the former two are included in our annotation scheme.", "Insult: expressions meant to hurt or offend the victim.", "font=footnotesize,sc,justification=centering,labelsep=period"]}
{"question_id": "f2b8a2ed5916d75cf568a931829a5a3cde2fc345", "predicted_answer": "", "predicted_evidence": ["To be able to build representative models for cyberbullying, a suitable dataset is required. This section describes the construction of two corpora, English and Dutch, containing social media posts that are manually annotated for cyberbullying according to our fine-grained annotation scheme. This allows us to develop a detection system covering different forms and participants (or roles) involved in a cyberbullying event.", "Bystander-assistant: person who does not initiate, but helps or encourages the harasser.", "Based on the literature on role-allocation in cyberbullying episodes BIBREF51 , BIBREF50 , four roles are distinguished, including victim, bully, and two types of bystanders.", "Parental control tools (e.g. NetNanny) already block unsuited or undesirable content and some social networks make use of keyword-based moderation tools (i.e., using lists of profane and insulting words to flag harmful content). However, such approaches typically fail to detect implicit or subtle forms of cyberbullying in which no explicit vocabulary is used. There is therefore a need for intelligent and self-learning systems that can go beyond keyword spotting and hence improve recall of cyberbullying detection.", "font=footnotesize,sc,justification=centering,labelsep=period"]}
{"question_id": "c0af44ebd7cd81270d9b5b54d4a40feed162fa54", "predicted_answer": "", "predicted_evidence": ["Table TABREF47 presents the scores of the (hyperparameter-optimised) single feature type systems, to gain insight into the performance of these feature types when used individually. Analysis of the combined and single feature type sets reveals that word INLINEFORM0 -grams, character INLINEFORM1 -grams, and subjectivity lexicons prove to be strong features for this task. In effect, adding character INLINEFORM2 -grams always improved classification performance for both languages. They likely provide robustness to lexical variation in social media text, as compared to word INLINEFORM3 -grams. While subjectivity lexicons appear to be discriminative features, term lists perform badly on their own as well as in combinations for both languages. This shows once again (cf. profanity baseline) that cyberbullying detection requires more sophisticated information sources than profanity lists. Topic models seem to do badly for both languages on their own, but in combination, they improve Dutch performance consistently. A possible explanation for their varying performance in both languages would be that the topic models trained on the Dutch background corpus are of better quality than the English ones.", "Harasser or Bully: person who initiates the bullying.", "Essentially, the annotation scheme describes two levels of annotation. Firstly, the annotators were asked to indicate, at the post level, whether the post under investigation was related to cyberbullying. If the post was considered a signal of cyberbullying, annotators identified the author's role. Secondly, at the subsentence level, the annotators were tasked with the identification of a number of fine-grained text categories related to cyberbullying. More concretely, they identified all text spans corresponding to one of the categories described in the annotation scheme. To provide the annotators with some context, all posts were presented within their original conversation when possible. All annotations were done using the Brat rapid annotation tool BIBREF52 , some examples of which are presented in Table TABREF33 .", "As mentioned earlier, data collection remains a bottleneck in cyberbullying research. Although cyberbullying has been recognised as a serious problem (cf. Section SECREF1 ), real-world examples are often hard to find in public platforms. Naturally, the vast majority of communications do not contain traces of verbal aggression or transgressive behaviour. When constructing a corpus for machine learning purposes, this results in imbalanced datasets, meaning that one class (e.g. cyberbullying posts) is much less represented in the corpus than the other (e.g. non-cyberbullying posts). To tackle this problem, several studies have adopted resampling techniques BIBREF35 , BIBREF41 , BIBREF31 that create synthetic minority class examples or reduce the number of negative class examples (i.e., minority class oversampling and majority class undersampling BIBREF44 ).", "Based on the literature on role-allocation in cyberbullying episodes BIBREF51 , BIBREF50 , four roles are distinguished, including victim, bully, and two types of bystanders."]}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "predicted_answer": "", "predicted_evidence": ["Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.", "However, one of the main limitations of their approach is that it is highly dependent on the data. Rowe and Saif BIBREF7 focused on studying Europe-based Twitter accounts in order to understand what happens before, during, and after they exhibit pro-ISIS behaviour. They define such behaviour as sharing of pro-ISIS content and/or using pro-ISIS terms. To achieve this, they use a term-based approach such that a user is considered to exhibit a radicalization behaviour if he/she uses more pro-ISIS terms than anti-ISIS terms. While such an approach seems effective in distinguishing radicalised users, it is unable to properly deal with lexical ambiguity (i.e., polysemy). Furthermore, in BIBREF11 the authors focused on detecting Twitter users who are involved with \u201cMedia Mujahideen\u201d, a Jihadist group who distribute propaganda content online. They used a machine learning approach using a combination of data-dependent and data-independent features. Similar to BIBREF7 they used textual features as well as temporal features to classify tweets and accounts.", "A third dataset is used which was acquired from Kaggle community. This dataset is created to be a counterpoise to the pro-ISIS dataset (our known-bad) as it consists of tweets talking about topics concerning ISIS without being radical. It contains INLINEFORM0 tweets from around INLINEFORM1 users collected on two separate days. We verify that this dataset is indeed non radical by checking the status of users in Twitter and found that a subset ( INLINEFORM2 users) was suspended. We remove those from the dataset and only keep users that are still active on Twitter. This dataset is labelled as counterpoise data.", "Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 .", "Ashcroft et al. BIBREF6 make an attempt to automatically detect Jihadist messages on Twitter. They adopt a machine-learning method to classify tweets as ISIS supporters or not. In the article, the authors focus on English tweets that contain a reference to a set of predefined English hashtags related to ISIS. Three different classes of features are used, including stylometric features, temporal features and sentiment features. However, one of the main limitations of their approach is that it is highly dependent on the data. Rowe and Saif BIBREF7 focused on studying Europe-based Twitter accounts in order to understand what happens before, during, and after they exhibit pro-ISIS behaviour. They define such behaviour as sharing of pro-ISIS content and/or using pro-ISIS terms. To achieve this, they use a term-based approach such that a user is considered to exhibit a radicalization behaviour if he/she uses more pro-ISIS terms than anti-ISIS terms. While such an approach seems effective in distinguishing radicalised users, it is unable to properly deal with lexical ambiguity (i.e., polysemy)."]}
{"question_id": "778c6a27182349dc5275282c3e9577bda2555c3d", "predicted_answer": "", "predicted_evidence": ["We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector.", "To model the normal behaviour, we collected a random sample of tweets from ten-trending topics in Twitter using the Twitter streaming API. These topics were related to news events and on-going social events (e.g., sports, music). We filter out any topics and keywords that may be connected to extremist views. This second dataset consists of around INLINEFORM0 tweets published by around INLINEFORM1 users. A random sample of 200 tweets was manually verified to ascertain it did not contain radical views. We label this dataset as our random-good data.", "Exp2: In this experiment, we tested the performance of our classifier in distinguishing between radical and normal tweets that discusses ISIS-related topics. Although this task is more challenging given the similarity of the topic discussed in the two classes, we find that the model still achieves high performance. Table TABREF17 shows the different metrics obtained from each feature category. The INLINEFORM0 feature group obtains 80% accuracy, and 91%, 100% for INLINEFORM1 and INLINEFORM2 feature groups, respectively. The results are consistent with the ones obtained from the first experiment with the features from INLINEFORM3 group contributing to the high accuracy of the model. The area under the Receiver Operator Characteristic (ROC) curve, which measures accuracy based on TP, and FP rates, is shown in Fig. FIGREF18 for each classification model.", "Rowe and Saif BIBREF7 focused on studying Europe-based Twitter accounts in order to understand what happens before, during, and after they exhibit pro-ISIS behaviour. They define such behaviour as sharing of pro-ISIS content and/or using pro-ISIS terms. To achieve this, they use a term-based approach such that a user is considered to exhibit a radicalization behaviour if he/she uses more pro-ISIS terms than anti-ISIS terms. While such an approach seems effective in distinguishing radicalised users, it is unable to properly deal with lexical ambiguity (i.e., polysemy). Furthermore, in BIBREF11 the authors focused on detecting Twitter users who are involved with \u201cMedia Mujahideen\u201d, a Jihadist group who distribute propaganda content online. They used a machine learning approach using a combination of data-dependent and data-independent features. Similar to BIBREF7 they used textual features as well as temporal features to classify tweets and accounts. The experiment was based on a limited set of Twitter accounts, which makes it difficult to generalize the results for a more complex and realistic scenario.", "Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories."]}
{"question_id": "42dcf1bb19b8470993c05e55413eed487b0f2559", "predicted_answer": "", "predicted_evidence": ["This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub. Such properties have been adopted in the research literature to study properties of cyber-criminal networks and their behaviour BIBREF22 , BIBREF23 .", "Research in fields such as linguistics, social science, and psychology suggest that the use of language and the word choices we make in our daily communication, can act as a powerful signal to detect our emotional and psychological states BIBREF8 . Several psychological properties are unintentionally transmitted when we communicate. Additionally, literature from the fields of terrorism and psychology suggests that terrorists may differ from non-terrorists in their psychological profiles BIBREF19 . A number of studies looked at the motivating factors surrounding terrorism, radicalization, and recruitment tactics, and found that terrorist groups tend to target vulnerable individuals who have feelings of desperation and displaced aggression. In particular research into the recruiting tactics of ISIS groups, it was found that they focus on harnessing the individual's need for significance. They seek out vulnerable people and provide them with constant attention BIBREF20 .", "The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 .", "Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 . Moreover, since ISIS supporters typically advocate for violent behaviour and tend to use offensive curse words, we use dictionaries of violent words and curse words to record the ratio of such words in the tweet. We also count the frequency of words with all capital letters as they are traditionally used to convey yelling behaviour.", "The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 . Moreover, since ISIS supporters typically advocate for violent behaviour and tend to use offensive curse words, we use dictionaries of violent words and curse words to record the ratio of such words in the tweet."]}
{"question_id": "2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "predicted_answer": "", "predicted_evidence": ["To model the normal behaviour, we collected a random sample of tweets from ten-trending topics in Twitter using the Twitter streaming API. These topics were related to news events and on-going social events (e.g., sports, music). We filter out any topics and keywords that may be connected to extremist views. This second dataset consists of around INLINEFORM0 tweets published by around INLINEFORM1 users. A random sample of 200 tweets was manually verified to ascertain it did not contain radical views. We label this dataset as our random-good data.", "In recent years, there has been an increase in online accounts advocating and supporting terrorist groups such as ISIS BIBREF5 . This phenomenon has attracted researchers to study their online existence, and research ways to automatically detect these accounts and limit their spread. Ashcroft et al. BIBREF6 make an attempt to automatically detect Jihadist messages on Twitter. They adopt a machine-learning method to classify tweets as ISIS supporters or not. In the article, the authors focus on English tweets that contain a reference to a set of predefined English hashtags related to ISIS. Three different classes of features are used, including stylometric features, temporal features and sentiment features. However, one of the main limitations of their approach is that it is highly dependent on the data. Rowe and Saif BIBREF7 focused on studying Europe-based Twitter accounts in order to understand what happens before, during, and after they exhibit pro-ISIS behaviour.", "Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories.", "We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety).", "We performed a series of preprocessing steps to clean the complete dataset and prepare it for feature extraction. These steps are: (1) We remove any duplicates and re-tweets from the dataset in order to reduce noise. (2) We remove tweets that have been authored by verified users accounts, as they are typically accounts associated with known public figures. (3) All stop words (e.g., and, or, the) and punctuation marks are removed from the text of the tweet. (4) If the tweet text contains a URL, we record the existence of the URL in a new attribute, hasURL, and then remove it from the tweet text. (5) If the tweet text contains emojis (e.g., :-), :), :P), we record the existence of the emoji in a new attribute, hasEmj, and then remove it from the tweet text. (6) If the tweet text contains any words with all capital characters, we record its existence in a new attribute, allCaps, and then normalize the text to lower-case and filter out any non-alphabetic characters."]}
{"question_id": "824629b36a75753b1500d9dcaee0fc3c758297b1", "predicted_answer": "", "predicted_evidence": ["We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5.", "We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector.", "The classification task is binomial (binary) classification where the output of the model predicts whether the input tweet is considered radical or normal. In order to handle the imbalanced class problem in the dataset, there are multiple techniques suggested in the literature Oversampling or undersampling of the minority/majority classes are common techniques. Another technique that is more related to the classification algorithm is cost sensitive learning, which penalizes the classification model for making a mistake on the minority class. This is achieved by applying a weighted cost on misclassifying of the minority class BIBREF24 . We will use the last approach to avoid downsampling of our dataset.", "The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 .", "Research in fields such as linguistics, social science, and psychology suggest that the use of language and the word choices we make in our daily communication, can act as a powerful signal to detect our emotional and psychological states BIBREF8 . Several psychological properties are unintentionally transmitted when we communicate. Additionally, literature from the fields of terrorism and psychology suggests that terrorists may differ from non-terrorists in their psychological profiles BIBREF19 . A number of studies looked at the motivating factors surrounding terrorism, radicalization, and recruitment tactics, and found that terrorist groups tend to target vulnerable individuals who have feelings of desperation and displaced aggression. In particular research into the recruiting tactics of ISIS groups, it was found that they focus on harnessing the individual's need for significance. They seek out vulnerable people and provide them with constant attention BIBREF20 . Similarly, these groups create a dichotomy and promote the mentality of dividing the world into \u201cus\u201d versus \u201cthem\u201d BIBREF21 ."]}
{"question_id": "31894361833b3e329a1fb9ebf85a78841cff229f", "predicted_answer": "", "predicted_evidence": ["The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 .", "We performed a series of preprocessing steps to clean the complete dataset and prepare it for feature extraction. These steps are: (1) We remove any duplicates and re-tweets from the dataset in order to reduce noise. (2) We remove tweets that have been authored by verified users accounts, as they are typically accounts associated with known public figures. (3) All stop words (e.g., and, or, the) and punctuation marks are removed from the text of the tweet. (4) If the tweet text contains a URL, we record the existence of the URL in a new attribute, hasURL, and then remove it from the tweet text. (5) If the tweet text contains emojis (e.g., :-), :), :P), we record the existence of the emoji in a new attribute, hasEmj, and then remove it from the tweet text. (6) If the tweet text contains any words with all capital characters, we record its existence in a new attribute, allCaps, and then normalize the text to lower-case and filter out any non-alphabetic characters.", "The rise of Online Social Networks (OSN) has facilitated a wide application of its data as sensors for information to solve different problems. For example, Twitter data has been used for predicting election results, detecting the spread of flu epidemics, and a source for finding eye-witnesses during criminal incidents and crises BIBREF0 , BIBREF1 . This phenomenon is possible due to the great overlap between our online and offline worlds. Such seamless shift between both worlds has also affected the modus operandi of cyber-criminals and extremist groups BIBREF2 . They have benefited tremendously from the Internet and OSN platforms as it provides them with opportunities to spread their propaganda, widen their reach for victims, and facilitate potential recruitment opportunities. For instance, recent studies show that the Internet and social media played an important role in the increased amount of violent, right-wing extremism BIBREF3 . Similarly, radical groups such as Al-Qaeda and ISIS have used social media to spread their propaganda and promoted their digital magazine, which inspired the Boston Marathon bombers in 2010 BIBREF4 .", "A third dataset is used which was acquired from Kaggle community. This dataset is created to be a counterpoise to the pro-ISIS dataset (our known-bad) as it consists of tweets talking about topics concerning ISIS without being radical. It contains INLINEFORM0 tweets from around INLINEFORM1 users collected on two separate days. We verify that this dataset is indeed non radical by checking the status of users in Twitter and found that a subset ( INLINEFORM2 users) was suspended. We remove those from the dataset and only keep users that are still active on Twitter. This dataset is labelled as counterpoise data.", "In this paper, we identified different signals that can be utilized to detect evidence of online radicalization. We derived linguistic and psychological properties from propaganda published by ISIS for recruitment purposes. We utilize these properties to detect pro-ISIS tweets that are influenced by their ideology. Unlike previous efforts, these properties do not only focus on lexical keyword analysis of the messages, but also add a contextual and psychological dimension. We validated our approach in different experiments and the results show that this method is robust across multiple datasets. This system can aid law enforcement and OSN companies to better address such threats and help solve a challenging real-world problem. In future work, we aim to investigate if the model is resilient to different evasion techniques that users may adopt. We will also expand the analysis to other languages."]}
{"question_id": "cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "predicted_answer": "", "predicted_evidence": ["We expect hotels in the same market to be more similar to each other than to hotels in other markets. To evaluate how well this market-level information is encoded by the learned embeddings, we calculate the average similarity between pairs of markets, with the expectation that we should see a strong diagonal component in the similarity matrix. We note that our model is not explicitly trained to learn this kind of market information. However, it is able to learn this by combining the click sessions and hotel attribute information. Figure FIGREF13 shows the average similarity scores between hotels in multiple famous cities using two of the embedding vectors. As Figure FIGREF13 clearly depicts, there is a strong similarity between hotels of the same city. Also, markets that are closer to each other (all US cities vs European vs Asian), or for reasons other than geographic proximity are expected to be more similar (e.g., Las Vegas and Macao, or Tokyo and Paris) do indeed have a higher similarity. For comparison, Figure FIGREF13 shows the average cosine similarity between and within markets for the session-only model embeddings.", "Most of the prior work on item embedding exploit the co-occurrence of items in a sequence as the main signal for learning the representation. One disadvantage of this approach is that it fails to incorporate rich structured information associated with the embedded items. For example, in the travel domain, where we seek to embed hotels and other travel-related entities, it could be helpful to encode explicit information such as user ratings, star ratings, hotel amenities, and location in addition to implicit information encoded in the click-stream.", "Compared to previous work on item embeddings, the novel contributions of this paper are as follows:", "To further illuminate the nature of the embeddings learned by the hotel2vec model, we examine a low-dimensional projection of hotel embeddings in the Miami market (Figures FIGREF25 and FIGREF25). The colors signify the grouping of hotels into various competing subcategories (i.e., similar hotels), manually annotated by a human domain expert. The enriched model is significantly better at clustering similar hotels than the session-only model.", "The authors would like to thank Ion Lesan, Peter Barszczewski, Daniele Donghi, Ankur Aggrawal for helping us collecting hotel's attribute, click and geographical data. We would also like to thank Dan Friedman and Thomas Mulc for providing useful comments and feedback."]}
{"question_id": "636ac549cf4917c5922cd09a655abf278924c930", "predicted_answer": "", "predicted_evidence": ["We generate an interpretable embedding which can be decomposed into sub-embeddings for clicks, location, ratings, and attributes, and employed either as separate component embeddings or a single, unified embedding.", "In this section, rather than using the model's output probabilities to induce a ranking over hotels, we measure hits@k over the ranking induced using cosine similarity of the embedding vectors. This is useful in scenarios where it isn't feasible to directly use the model's probabilities. Table TABREF21 shows the results for various embeddings. We show that using the enriched vectors one achieves the highest performance.", "Figure FIGREF7 illustrates the proposed architecture for an enriched, hotel2vec model. As we can see, each aspect of the hotel is embedded separately, and these representations are later concatenated and further compressed before being used for context prediction.", "Recommendation is an inherently challenging task that requires learning user interests and behaviour. There has been a significant body of research on advancing it using various frameworks BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7. Learning a semantic representation/embedding of the items being recommended is a critical piece of most of these frameworks.", "Dealing with such hotels/items and choosing appropriate weights for them is referred to as the \"cold start problem.\" One of the main advantages of the enriched hotel2vec model over session-only approaches is its ability to better handle cold start cases. Although an item might lack sufficient prior user engagement, there are often other attributes available. For example, in our use case, thousands of new properties are added to the lodging platform's inventory each quarter. While we don't have prior user engagement data from which to learn a click embedding, we do have other attributes such as geographical location, star rating, amenities, etc. Hotel2vec can take advantage of this supplemental information to provide a better cold-start embedding."]}
{"question_id": "c61c0b25f9de4a7ca2013d2e4aba8a5047e14ce4", "predicted_answer": "", "predicted_evidence": ["The authors would like to thank Ion Lesan, Peter Barszczewski, Daniele Donghi, Ankur Aggrawal for helping us collecting hotel's attribute, click and geographical data. We would also like to thank Dan Friedman and Thomas Mulc for providing useful comments and feedback.", "In practice, many hotels/items appear infrequently or never in historical data. Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data. Apart from the obvious negative impacts on searchability and sales, neglecting these items can introduce a feedback loop. That is, the less these items are recommended, or the more they are recommended in inappropriate circumstances, the more the data reinforces their apparent lack of popularity.", "In the case of recommendation, there is typically a large inventory of items available to recommend to the user, and thus we train our skip-gram model using negative sampling. However, it is not uncommon that users frequently search exclusively within a particular subdomain. For example, in hotel search, a customer looking to stay in Miami will focus on that market and rarely across different markets. This motivates a more targeted strategy when selecting negative samples: we select half of our negative samples following the schema in BIBREF20, i.e., from the complete set of all hotels, and the other half uniformly at random from the same market as the clicked hotel. Throughout this paper, a market is defined as a set of similar hotels in the same geographic region. It's worth noting that there may be multiple markets in the same city or other geo region. In the experimental section, we show that this improves the model's within-market similarities and its predictions.", "Figure FIGREF35 shows the overall training progress of both the session-32 and enriched-32 models with their respective best hyperparameters. As shown in Figure FIGREF35, our model achieves similar performance with fewer data.", "In this work, we propose a framework to learn a semantic representation of hotels by jointly embedding hotel click data, geographic information, user rating, and attributes (such as stars, whether it has free breakfast, whether pets are allowed, etc.). Our neural network architecture extends the skip-gram model to accommodate multiple features and encode each one separately. We then fuse the sub-embeddings to predict hotels in the same session. Through experimental results, we show that enriching the neural network with supplemental, structured hotel information results in superior embeddings when compared to a model that relies solely on click information. Our final embedding can be decomposed into multiple sub-embeddings, each encoding the representation for a different hotel aspect, resulting in an interpretable representation. It is also dynamic, in a sense that if one of the attributes or user ratings changes for a hotel, we can feed the updated data to the model and easily obtain a new embedding."]}
{"question_id": "1d047286ac63e5dca1ab811172b89d7d125679e5", "predicted_answer": "", "predicted_evidence": ["We propose a novel framework for fusing multiple sources of information about an item (such as user click sequences and item-specific information) to learn item embeddings via self-supervised learning.", "It is well known BIBREF18, BIBREF0, BIBREF19 that using negative sampling, a version of noise contrastive estimation, significantly decreases the amount of time required to train a classifier with a large number of possible classes. In the case of recommendation, there is typically a large inventory of items available to recommend to the user, and thus we train our skip-gram model using negative sampling. However, it is not uncommon that users frequently search exclusively within a particular subdomain. For example, in hotel search, a customer looking to stay in Miami will focus on that market and rarely across different markets. This motivates a more targeted strategy when selecting negative samples: we select half of our negative samples following the schema in BIBREF20, i.e., from the complete set of all hotels, and the other half uniformly at random from the same market as the clicked hotel. Throughout this paper, a market is defined as a set of similar hotels in the same geographic region.", "Similar to our work on hotel2vec, there are also some works which attempt to include explicit item attributes (e.g., size, artist, model, color) within the sequence prediction framework using various strategies. In BIBREF13, the item metadata is injected into the model as side information to regularize the item embeddings. In their approach, they only use one feature (singer ID) in the experiments. In addition, their approach does not accommodate learning independent embedding vectors for each attribute group. Most recently, BIBREF14 propose a method where they train separate encoders for text data, click-stream session data, and product image data, and then use a simple weighted average to unify these embeddings. The weights are learned using grid search on the downstream task. While their approach allows for exploring independent embedding vectors, the sub-embeddings of different attribute groups are learned independently rather than jointly.", "A robust metric for evaluating a set of hotel embeddings (or, more generally, any set of items displayed to a user in response to an information need) is its ability to predict a user's next click/selection. In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions.", "In more recent work BIBREF2, the authors use the skip-gram framework to learn embeddings for vacation rental properties. They extend the ideas in BIBREF1 to take into account a user's click stream data during a session. A key contribution of their method is the modification of the skip-gram model to always include the booked hotels in the context of each target token, so that special attention is paid to bookings. They also improve negative sampling by sampling from the same market, which leads to better within-market listing similarities. Nevertheless, their model relies exclusively on large amounts of historical user engagement data, which is a major drawback when such data are sparse."]}
{"question_id": "6d17dc00f7e5331128b6b585e78cac0b9082e13d", "predicted_answer": "", "predicted_evidence": ["The slight intensity is infrequent, with 213 positive and 329 negative polar expressions with this label. This relative difference can be explained by the tendency to hedge negative statements more than positive ones BIBREF24. Strong negative is the minority class, with only 144 examples. Overall, the distribution of intensity scores in NoReC$_\\text{\\textit {fine}}$ is very similar to what is reported for other fine-grained sentiment datasets for English and Dutch BIBREF25.", "Finally, we note that there are 1118 examples where the target is further marked as Not-on-Topic and 213 where the holder is Not-First-Person.", "Returning to the token counts in Table TABREF31, we see that while references to holders are just one word on average (often just a pronoun), targets are two on average. However, not all targets and holders have a surface realization. There are 6314 polar expressions with an implicit holder and an additional 1660 with an implicit target.", "Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\text{\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.", "Figure FIGREF32 plots the distribution of polarity labels and their intensity scores. We see that the intensities are clearly dominated by standard strength, while there are also 627 strong labels for positive. Regardless of intensity, we see that positive valence is more prominent than negative, and this reflects a similar skew for the document-level ratings in this data BIBREF0."]}
{"question_id": "de0154affd86c608c457bf83d888bbd1f879df93", "predicted_answer": "", "predicted_evidence": ["State-of-the-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches BIBREF12, often using pre-trained language models BIBREF13, BIBREF14 to improve model performance BIBREF15. Additionally, approaches which attempt to incorporate document- and sentence-level supervision via multi-task learning often lead to improvements BIBREF16.", "We build on the sentence-level annotation of evaluative sentences in the NoReC$_\\text{\\textit {eval}}$ -corpus BIBREF20, where two types of evaluative sentences were annotated: simple evaluative sentences (labeled EVAL), or the special case of evaluative fact-implied non-personal (FACT-NP) sentences. The EVAL label roughly comprises the three opinion categories described by Liu:15 as emotional, rational and fact-implied personal. Sentences including emotional responses (arousal) are very often evaluative and involve emotion terms, e. g. elske `love', like `like', hate `hate'. Sentences that lack the arousal we find in emotional sentences may also be evaluative, for instance by indicating worth and utilitarian value, e. g. nyttig `useful', verdt (penger, tid) `worth (money, time)'. In NoReC$_\\text{\\textit {eval}}$, a sentence is labeled as FACT-NP when it is a fact or a descriptive sentence but evaluation is implied, and the sentence does not involve any personal experiences or judgments.", "We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks BIBREF26, BIBREF27, BIBREF28. We use the IOB2 label encoding for sources, targets, and polar expressions, including the polarity of the latter, giving us nine tags in total. This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored.", "Finally, we note that there are 1118 examples where the target is further marked as Not-on-Topic and 213 where the holder is Not-First-Person.", "Comparative sentences can pose certain challenges because they involve the same polar expression having relations to two different targets, usually (but not necessarily) with opposite polarities. Comparative sentences are indicated by the use of comparative adjectival forms, and commonly also by the use of the comparative subjunction enn `than'. In comparative sentences like X er bedre enn Y `X is better than Y', X and Y are entities, and bedre `better' is the polar expression. In general we annotate X er bedre `X is better' as a polar expression modifying Y, and bedre enn Y `better than Y' as a polar expression modifying X. Here there should be a difference in polarity as well, indicating that X is better than Y. The annotated examples in Figure FIGREF24 shows the two layers of annotation invoked by a comparative sentence."]}
{"question_id": "9887ca3d25e2109f41d1da80eeea05c465053fbc", "predicted_answer": "", "predicted_evidence": ["Finally, we note that there are 1118 examples where the target is further marked as Not-on-Topic and 213 where the holder is Not-First-Person.", "In terms of modeling, we also aim to investigate approaches that better integrate the various types of annotated information (targets, holders, polar expressions, and more) and the relations between them when making predictions, for example in the form of multi-task learning. Modeling techniques employing attention or aspect-specific gates that have provided state-of-the-art results for English provide an additional avenue for future experimentation.", "Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\text{\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.", "The slight intensity is infrequent, with 213 positive and 329 negative polar expressions with this label. This relative difference can be explained by the tendency to hedge negative statements more than positive ones BIBREF24. Strong negative is the minority class, with only 144 examples. Overall, the distribution of intensity scores in NoReC$_\\text{\\textit {fine}}$ is very similar to what is reported for other fine-grained sentiment datasets for English and Dutch BIBREF25.", "Comparative sentences can pose certain challenges because they involve the same polar expression having relations to two different targets, usually (but not necessarily) with opposite polarities. Comparative sentences are indicated by the use of comparative adjectival forms, and commonly also by the use of the comparative subjunction enn `than'. In comparative sentences like X er bedre enn Y `X is better than Y', X and Y are entities, and bedre `better' is the polar expression. In general we annotate X er bedre `X is better' as a polar expression modifying Y, and bedre enn Y `better than Y' as a polar expression modifying X. Here there should be a difference in polarity as well, indicating that X is better than Y. The annotated examples in Figure FIGREF24 shows the two layers of annotation invoked by a comparative sentence."]}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "predicted_answer": "", "predicted_evidence": ["Returning to the token counts in Table TABREF31, we see that while references to holders are just one word on average (often just a pronoun), targets are two on average. However, not all targets and holders have a surface realization. There are 6314 polar expressions with an implicit holder and an additional 1660 with an implicit target.", "Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution.", "The inter annotator agreement scores obtained in the first rounds of (double) annotation are reported in Table TABREF28. We find that even though annotators tend to agree on certain parts of the expressions, they agree less when it comes to exact spans. This reflects the annotators subjective experiences, and although an attempt has been made to follow the guidelines strictly, it seems to be difficult to reach high agreement scores. The binary polar expression score is the highest score (96% Binary $\\text{F}_1$). This is unsurprising, as we noted during annotation that there was strong agreement on the most central elements, even though there were certain disagreements when it comes to the exact span of a polar expression. As holder expressions tend to be short, the relatively low binary agreement might reflect the tendency of holder expressions to occur multiple times in the same sentence, creating some confusion over which of these expressions to choose.", "This paper has introduced a new dataset for fine-grained sentiment analysis, the first such dataset available for Norwegian. The data, dubbed NoReC$_\\text{\\textit {fine}}$, comprise a subset of documents in the Norwegian Review Corpus, a collection of professional reviews across multiple domains. The annotations mark polar expressions with positive/negative valence together with an intensity score, in addition to the holders and targets of the expressed opinion. Both subjective and objective expressions can be polar, and a special class of objective expressions called fact-implied non-personal expressions are given a separate label. The annotations also indicate whether holders are first-person (i.e. the author) and whether targets are on-topic. Beyond discussing the principles guiding the annotations and describing the resulting dataset, we have also presented a series of first classification results, providing benchmarks for further experiments. The dataset, including the annotation guidelines, are made publicly available.", "There are several constructions where targets and polar expressions coincide. Like most Germanic languages, nominal compounding is highly productive in Norwegian and compounds are mostly written as one token. Adjective-noun compounds are fairly frequent and these may sometimes express both polar expression and target in one and the same token, e.g. favorittfilm `favourite-movie'. Since our annotation does not operate over sub-word tokens, these types of examples are marked as polar expressions."]}
{"question_id": "075d6ab5dd132666e85d0b6ad238118271dfc147", "predicted_answer": "", "predicted_evidence": ["As shown in Table , editing the gold query consistently improves both question match and interaction match accuracy. This shows the editing approach is indeed helpful to improve the generation quality when the previous query is the oracle.", "To better understand how models perform as the interaction proceeds, Figure FIGREF30 (Left) shows the performance split by turns on the dev set. The questions asked in later turns are more difficult to answer given longer context history. While the baselines have lower performance as the turn number increases, our model still maintains 38%-48% accuracy for turn 2 and 3, and 20% at turn 4 or beyond. Similarly, Figure FIGREF30 (Right) shows the performance split by hardness levels with the frequency of examples. This also demonstrates our model is more competitive in answering hard and extra hard questions.", "where $i$ is the turn index, $j$ is the token index, and $\\mathbf {h}^{E}_{i,j}$ is the token embedding for the $j$-th token of $i$-th utterance. The context vector $\\mathbf {c}_k$ is a concatenation of the two:", "Furthermore, adding the utterance-table BERT embedding gives significant improvement, achieving 57.6% on dev set and 53.4% on test set, which is comparable to the state-of-the-art results from IRNet with BERT. We attribute our BERT model's high performance to (1) the empirically powerful text understanding ability of pretrained BERT model and (2) the early interaction between utterances and column headers when they are concatenated in a single sequence as the BERT input.", "Figure FIGREF7 shows the utterance encoder. For the user utterance at each turn, we first use a bi-LSTM to encode utterance tokens. The bi-LSTM hidden state is fed into a dot-product attention layer BIBREF5 over the column header embeddings. For each utterance token embedding, we get an attention weighted average of the column header embeddings to obtain the most relevant columns BIBREF6. We then concatenate the bi-LSTM hidden state and the column attention vector, and use a second layer bi-LSTM to generate the utterance token embedding $\\mathbf {h}^{E}$."]}
{"question_id": "f2b1e87f61c65aaa99bcf9825de11ae237260270", "predicted_answer": "", "predicted_evidence": ["Concurrent with our work, yu2019cosql introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting. Each dialogue in CoSQL simulates a DB querying scenario with a crowd worker as a user and a college computer science student who is familiar with SQL as an expert. Question-SQL pairs in CoSQL reflect greater diversity in user backgrounds compared to other corpora and involve frequent changes in user intent between pairs or ambiguous questions that require user clarification. These features pose new challenges for text-to-SQL systems.", "Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms BIBREF10, BIBREF11 and lambda calculus BIBREF12, BIBREF13, and using executable programs, such as SQL queries BIBREF14, BIBREF15 and other general-purpose programming languages BIBREF16, BIBREF17. Most of the early studies worked on a few domains and small datasets such as GeoQuery BIBREF10 and Overnight BIBREF18.", "where $i$ is the turn index, $j$ is the token index, and $\\mathbf {h}^{E}_{i,j}$ is the token embedding for the $j$-th token of $i$-th utterance. The context vector $\\mathbf {c}_k$ is a concatenation of the two:", "At each turn $t$, the goal is to generate $Y_t$ given the current utterance $X_t$ and the interaction history", "Then, we use a separate layer to score the query tokens at turn $t-1$, and the output distribution is modified as the following to take into account the editing probability:"]}
{"question_id": "78c7318b2218b906a67d8854f3e511034075f79a", "predicted_answer": "", "predicted_evidence": ["We do not compare across papers as evaluation setups differ. Values are provided in Tables TABREF19, TABREF26, TABREF24 and TABREF27. While the tests generally agree, Acute-eval can be a more sensitive test, which more often yields significance. On Wizard of Wikipedia where all Likert matchups are known, 8 of the pairwise matchups are significant for our test with human-model chats, while 6 are significant for Likert. On PersonaChat for the interestingness question, 6 of 10 matchups are significant for Acute-eval, including all known Likert matchups, which only has 2 of 3 that are significant. For the humanness question, 5 of 10 matchups are significant for Acute-eval, including all known Likert matchups, which only has 2 of 3 that are significant. For the engagingness question, 5 of the 9 Likert matchups are significant. All 9 are significant for Acute-eval when using self-chats; 3 are significant for human-model chats.", "PersonaChat was the subject of the NeurIPS 2018 ConvAI2 Challenge BIBREF8, in which competitor's models were first evaluated with respect to automatic metrics, and then with respect to human judgment via human-bot chats followed by the question \u201cHow much did you enjoy talking to this user?\" on a scale of 1\u20134. A total of 9 systems were evaluated using human annotators, 100 conversations for each. In this work, we leverage the human-model chat logs from the ConvAI2 competition for three models: Lost in Conversation (LIC), which won the competition, and Hugging Face (HF; BIBREF23, BIBREF23) which won the automatic evaluation track, and the KVMemNN BIBREF24 baseline released by the competition organizers (KV; BIBREF8, BIBREF8).", "We consider different approaches to step (1) and (2) below.", "We similarly compare all 4 models and humans on the optimized engaging and knowledge questions. The results are given in Tables TABREF27 and TABREF28. We again find retrieval models outperform generative models, with knowledge attention (GK) clearly helping the generative models, but with RU and RK very close.", "Evaluation of chitchat tasks with automatic metrics is difficult precisely because of their open-ended nature. For example, the answer to the question \u201cWhat are you doing tonight?\u201d has many possible answers, each with little word overlap. This means standard metrics for tasks like question-answering or machine translation do not work well, and have poor correlation with human judgments BIBREF0, BIBREF15. Nevertheless, a number of studies do report automatic metrics, without human studies BIBREF16, BIBREF17. Researchers have made attempts to improve automatic evaluation, trying methods such as adversarial evaluation BIBREF18, learning a scoring model BIBREF1, or a learnt ensemble of automatic metrics BIBREF19, but their value is as yet not fully understood."]}
{"question_id": "697c5d2ba7e019ddb91a1de5031a90fe741f2468", "predicted_answer": "", "predicted_evidence": ["The annotator is posed a question phrasing (e.g. \u201cwhich speaker is more knowledgeable\u201d or \u201cwhich speaker sounds more human?\u201d), and asked to make a binary choice between model $A$ and model $B$. They are strongly encouraged to provide a short text justification for their choice. We collect $N$ trials of such pairwise judgments, and use them to decide which model wins. Statistical significance can be computed using a binomial test.", "Unfortunately, human judgments are themselves difficult to measure. The two most used approaches, single-turn pairwise evaluation BIBREF2, BIBREF3, and multi-turn Likert scores BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8 have serious limitations. Single-turn pairwise evaluation provides the benefits and simplicity of an A/B test, allowing for cheap and fast annotations, with comparisons that are robust to annotator score bias, but fail to take into account the multi-turn aspect of conversations. To give a trivial example, such comparisons fail to capture whether the model would repeat itself in a multi-turn conversation because they only look at one turn; repetition is a known issue that humans dislike BIBREF6.", "Dialogue between human and machine is an important end-goal of natural language research. The open-ended nature of generating sequences in a multi-turn setup naturally makes the task difficult to evaluate \u2013 with full evaluation possessing many of the difficulties of the task itself as it requires deep understanding of the content of the conversation. As in many other natural language generation (NLG) tasks, automatic metrics have not been shown to have a clear correlation with human evaluations BIBREF0, BIBREF1. This means the current standard for all dialogue research involves human trials, which slows down research and greatly increases the cost of model development.", "We do not compare across papers as evaluation setups differ. Values are provided in Tables TABREF19, TABREF26, TABREF24 and TABREF27. While the tests generally agree, Acute-eval can be a more sensitive test, which more often yields significance. On Wizard of Wikipedia where all Likert matchups are known, 8 of the pairwise matchups are significant for our test with human-model chats, while 6 are significant for Likert. On PersonaChat for the interestingness question, 6 of 10 matchups are significant for Acute-eval, including all known Likert matchups, which only has 2 of 3 that are significant. For the humanness question, 5 of 10 matchups are significant for Acute-eval, including all known Likert matchups, which only has 2 of 3 that are significant. For the engagingness question, 5 of the 9 Likert matchups are significant.", "Multi-turn Likert scores require the annotator to have a multi-turn conversation and then provide an integer score, which is more costly and time-consuming to run but evaluates full conversations more accurately. The integer scores however suffer from differing bias and variance per annotator, which researchers have tried to mitigate BIBREF9, but nevertheless due to its lack of sensitivity often yields comparisons that are not statistically significant. Furthermore, due to strong anchoring effects during model evaluation, i.e. that annotators are affected by the first systems they evaluate, Likert comparisons are generally not comparable across multiple papers. This mandates that evaluations of new models be simultaneously collected with baselines, further increasing the cost of developing additional models BIBREF6."]}
{"question_id": "e25b73f700e8c958b64951f14a71bc60d225125c", "predicted_answer": "", "predicted_evidence": ["We plot a confusion matrix (Figure FIGREF20) to analyze this further based on the clustering with BERT-base and k=5. We first note that the outlier sentences are much shorter than the average sentence length in the corpus (11.62 tokens on average for outliers vs. 20.5 tokens on average in general). This makes sense as shorter sentences contain less information, making it harder to assign them to an appropriate cluster. Table TABREF19 shows examples of outlier sentences, assigned to clusters of domains different from their originating domain. We can see that in many cases the assignments are sensible \u2013 for example for sentences originating from the subtitles corpus, a sentence that mentions \u201cgreat priest\u201d is assigned to the Koran cluster, a sentence that mentions \u201cThe International Criminal Court in The Hague\u201d is assigned to the Law cluster, a sentence that mentions \u201cthe virus\u201d is assigned to the Medical cluster and so on. This strengthens our claim that defining domains based on the corpus they originated from may be over-simplistic, and using a more data-driven approach may enable to find better domain assignments across different corpora.", "While the method by BIBREF4 is tried-and-true, it is based on simple n-gram language models which cannot generalize beyond the n-grams that are seen in the in-domain set. In addition, it is restricted to the in-domain and general-domain datasets it is trained on, which are usually small. On the contrary, pre-trained language models are trained on massive amounts of text, and, as we showed through unsupervised clustering, learn representations with domain-relevant information. In the following sections, we investigate whether this property of pretrained language models makes them useful for domain data selection.", "Results The results for the cross-domain evaluation are available in Table TABREF28. In most cases, the best results for each domain are obtained by training on the in-domain data. Training on all the available data helped mostly for the Koran test set. This is expected as the training data for this domain is considerably smaller than the training data for rest of the domains (Table TABREF24). We can also see that more data is not necessarily better BIBREF37: while the subtitles corpus is the largest of all 5 and includes 500,000 sentence pairs, it is second to last in performance as measured by the average BLEU across all test sets.", "Our methods enable to select relevant data for the task while requiring only a small set of monolingual in-domain data. As they are based solely on the representations learned by self-supervised LMs, they do not require additional domain labels which are usually vague and over-simplify the notion of domain in textual data. We evaluate our method on data selection for neural machine translation (NMT) using the multi-domain German-English parallel corpus composed by BIBREF8. Our data selection methods enable to train NMT models that outperform those trained using the well-established cross-entropy difference method of BIBREF4 across five diverse domains, achieving a recall of more than 95% in all cases with respect to an oracle that selects the \u201ctrue\u201d in-domain data.", "The domain that attracted the largest number of outliers is the IT domain cluster, with 597 sentences assigned to it from other domains. Looking more closely we find that more than half of these sentences (340 out of 597) included numbers (e.g. \u201c34% 25% 34%\u201d (from medical), \u201c(b) reference number 20 is deleted;\u201d (from law), \u201c(Command of Prostration # 1)\u201d (from Koran) or \u201cThe message, R2.\u201d (from subtitles)). As numbers appear in many different contexts, they may be harder to assign to a specific domain by the context-aware language models in such short sentences. The second largest attractor of outliers is the Subtitles cluster, with 372 sentences assigned to it from other domains."]}
{"question_id": "908ba58d26d15c14600623498d4e86c9b73b14b2", "predicted_answer": "", "predicted_evidence": ["BIBREF6 proposed to replace n-gram models with RNN-based LMs with notable improvements. However, such methods do not capture the rich sentence-level global context as in the recent self-attention-based MLMs; as we showed in the clustering experiments, autoregressive neural LMs were inferior to masked LMs in clustering the data by domain. In addition, training very large neural LMs may be prohibitive without relying on pre-training.", "We propose two methods for domain data selection with pretrained language models.", "Our methods enable to select relevant data for the task while requiring only a small set of monolingual in-domain data. As they are based solely on the representations learned by self-supervised LMs, they do not require additional domain labels which are usually vague and over-simplify the notion of domain in textual data. We evaluate our method on data selection for neural machine translation (NMT) using the multi-domain German-English parallel corpus composed by BIBREF8. Our data selection methods enable to train NMT models that outperform those trained using the well-established cross-entropy difference method of BIBREF4 across five diverse domains, achieving a recall of more than 95% in all cases with respect to an oracle that selects the \u201ctrue\u201d in-domain data.", "We note that some classes of models did better than others: while all vector-based models did far better than the random and LDA baselines, the MLM-based models dominated in all cases over word2vec and the auto-regressive models. This may be explained by the fact that the MLM-based models use the entire sentence context when generating the representations for each token, while the auto-regressive models only use the past context, and word2vec uses a limited window context. Using PCA improved performance in most cases and especially for the auto-regressive models, although the results for the MLMs remain high in both cases \u2013 suggesting that these models encode the information very differently.", "We perform data selection experiments for each domain in the multi-domain dataset. As the small set of monolingual in-domain data we take the 2000 development sentences from each domain. For the general-domain corpus we concatenate the training data from all domains, resulting in 1,456,317 sentences. To enable faster experimentation we used DistilBERT BIBREF18 for the Domain-Cosine and Domain-Finetune methods. More technical details are available in the supplementary material. We compare our methods to four approches: (1) The established method by BIBREF4, (2) a random selection baseline, (3) an oracle which is trained on all the available in-domain data, and (4) the model we train on all the domains concatenated. We select the top 500k examples to cover the size of every specific in-domain dataset. We train Transformer NMT models on the selected data with a similar configuration to the ones trained in the cross-domain evaluation."]}
{"question_id": "3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "predicted_answer": "", "predicted_evidence": ["While previous work made important contributions to domain data selection, our work is the first to explore massive pretrained language models for both unsupervised domain clustering and for data selection in NMT.", "For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair. This is shown in Figure FIGREF29. We can see the general trend where the closer the domain centroids are (with a similarity of 1 for training and evaluating on the same domain), the higher the cross-domain BLEU is between those domains, resulting in a Pearson's correlation of 0.81 (strong correlation).", "To simulate a diverse multi-domain setting we use the dataset proposed in BIBREF8, as it was recently adopted for domain adaptation research in NMT BIBREF28, BIBREF29, BIBREF30, BIBREF31. The dataset includes parallel text in German and English from five diverse domains (Medical, Law, Koran, IT, Subtitles; as discussed in Section SECREF2), available via OPUS BIBREF32, BIBREF33.", "Figure FIGREF46 shows visualizations of the multi-domain dataset from additional pre-trained masked language models (BERT large and RoBERTa), and Figure FIGREF47 shows the same visualization for autoregressive models (XLNet and GPT2).", "Table TABREF44 shows details about the overlap between the training, development and test sets for the different data splits of the multi-domain dataset. The overlap was computed using the English part of the corpus."]}
{"question_id": "c0847af3958d791beaa14c4040ada2d364251c4d", "predicted_answer": "", "predicted_evidence": ["\u201c34% 25% 34%\u201d (from medical), \u201c(b) reference number 20 is deleted;\u201d (from law), \u201c(Command of Prostration # 1)\u201d (from Koran) or \u201cThe message, R2.\u201d (from subtitles)). As numbers appear in many different contexts, they may be harder to assign to a specific domain by the context-aware language models in such short sentences. The second largest attractor of outliers is the Subtitles cluster, with 372 sentences assigned to it from other domains. We find that most of these sentences contain personal pronouns or question marks (228 out of 372, 61.2%) while the ratio of such sentences in the entire corpus is only 40%. Examples include \u201cWhy did you choose the name & amarok;?\u201d (from IT), or \u201cWhat is Avonex?\u201d (from Medical). This may be expected as the subtitles corpus mainly includes transcriptions of spoken, conversational language, and \u201cconversation tends to have more verbs, more personal pronouns, and more questions\u201d BIBREF25.", "Regarding domain clustering for MT, BIBREF47 discovered topics using LDA instead of using domain labels. BIBREF48 induced latent subdomains from the training data using a dedicated probabilistic model.", "Unrelated to MT, BIBREF57 used BERT to select data for tasks from the GLUE benchmark BIBREF12. However, they assumed supervision for all the different tasks/domains, while we propose an unsupervised method requiring only a small set of in-domain data. Also in the context of pretrained language models, BIBREF58 show the importance of additional pretraining with in-domain data to improve the down-stream task-specific performance.", "As we showed that pre-trained language models are indeed very useful in clustering sentence representations by domains in an unsupervised manner, we now seek to harness this property for a down-stream task \u2013 domain data selection for machine translation. Domain data selection is the task of selecting examples from a large corpus which are as close as possible to the domain of interest, given a smaller set of in-domain examples. The selected examples can be used to either (1) train a domain-specific model from scratch BIBREF5, (2) fine-tune a pre-trained general-domain model BIBREF26, BIBREF7, or (3) prioritize data for annotation as in an Active-Learning framework, if only monolingual data is available BIBREF27. To demonstrate the need for domain data selection and set the stage for our data selection experiments, we perform preliminary experiments with NMT in a multi-domain scenario.", "We note that some classes of models did better than others: while all vector-based models did far better than the random and LDA baselines, the MLM-based models dominated in all cases over word2vec and the auto-regressive models. This may be explained by the fact that the MLM-based models use the entire sentence context when generating the representations for each token, while the auto-regressive models only use the past context, and word2vec uses a limited window context. Using PCA improved performance in most cases and especially for the auto-regressive models, although the results for the MLMs remain high in both cases \u2013 suggesting that these models encode the information very differently."]}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "predicted_answer": "", "predicted_evidence": ["MT-trans-A Unlike MT-trans-G, MT-trans-A replaces gated sharing cell with attention sharing cell for selecting shared features.", "We perform experiments on RumourEval and PHEME datasets to evaluate the performance of our method and the baselines. The experimental results are shown in Table TABREF27. We gain the following observations:", "where $\\odot $ denotes element-wise multiplication.", "Stance Detection The researches BIBREF22, BIBREF23 demonstrate that the stance detected from fake news can serve as an effective credibility indicator to improve the performance of fake news detection. The common way of stance detection in rumors is to catch deep semantics from text content based on neural networksBIBREF24. For instance, Kochkina et al.BIBREF25 project branch-nested LSTM model to encode text of each tweet considering the features and labels of the predicted tweets for stance detection, which reflects the best performance in RumourEval dataset. In this work, we utilize transformer encoder to acquire semantics from responses and forwarding of fake news for stance detection.", "In recent years, the proliferation of fake news with various content, high-speed spreading, and extensive influence has become an increasingly alarming issue. A concrete instance was cited by Time Magazine in 2013 when a false announcement of Barack Obama's injury in a White House explosion \u201cwiped off 130 Billion US Dollars in stock value in a matter of seconds\". Other examples, an analysis of the US Presidential Election in 2016 BIBREF0 revealed that fake news was widely shared during the three months prior to the election with 30 million total Facebook shares of 115 known pro-Trump fake stories and 7.6 million of 41 known pro-Clinton fake stories. Therefore, automatically detecting fake news has attracted significant research attention in both industries and academia."]}
{"question_id": "ce23849e9e9a22626965f1ca8ca948a5c87280e9", "predicted_answer": "", "predicted_evidence": ["We explore a selected sharing layer relying on gate mechanism and attention mechanism, which can selectively capture valuable shared features between tasks of fake news detection and stance detection for respective tasks.", "Position embeddings refer to vectorization representations of position information of words in a sentence. We employ one-hot encoding to represent position embeddings $p_i$ of token $x_i$, where $p_i \\in \\mathbb {R}^{d_p}$, $d_p$ is the positional embedding dimension. Therefore, the embeddings of a sentence are represented as $ {\\rm \\textbf {E}}=\\lbrace [w_1;p_1 ], [w_2;p_2], ..., [w_l;p_l]\\rbrace , {\\rm \\textbf {E}}\\in \\mathbb {R}^{l \\times (d_p+d_w)}$. In particular, we adopt one-hot encoding to embed positions of tokens, rather than sinusoidal position encoding recommended in BERT model BIBREF31. The reason is that our experiments show that compared with one-hot encoding, sinusoidal position encoding not only increases the complexity of models but also performs poorly on relatively small datasets.", "Attention Sharing Cell To focus on helpful shared features that are beneficial to specific tasks from upstream shared layer, we devise an attention sharing cell based on attention mechanism. Specifically, this cell utilizes input embeddings of the specific task to weight shared features for paying more attention to helpful features. The inputs of this cell include two matrixes: the input embeddings of the specific task and the shared features of both tasks. The basic attention architecture of this cell, the same as shared-private feature extractor, also adopts transformer encoder (the details in subsection SECREF8).", "In consideration of the imbalance label distributions, in addition to accuracy (A) metric, we add Precision (P), Recall (R) and F1-score (F1) as complementary evaluation metrics for tasks. We hold out 10% of the instances in each dataset for model tuning, and the rest of the instances are performed 5-fold cross-validation throughout all experiments.", "where $\\odot $ denotes element-wise multiplication."]}
{"question_id": "d9a45fea8539aac01dec01f29b7d04b44b9c2ca6", "predicted_answer": "", "predicted_evidence": ["Experiments on two public, widely used fake news datasets demonstrate that our method significantly outperforms previous state-of-the-art methods.", "MTL-LSTM A multi-task learning model based on LSTM networks BIBREF14 trains jointly the tasks of veracity classification, rumor detection, and stance detection.", "MT-trans The only difference between MT-trans and MT-lstm is that encoder of MT-trans is composed of transformer encoder.", "Word embeddings $w_i$ of token $x_i$ are a $d_w$-dimensional vector obtained by pre-trained Word2Vec model BIBREF30, i.e., $w_i \\in \\mathbb {R}^{d_w}$. Position embeddings refer to vectorization representations of position information of words in a sentence. We employ one-hot encoding to represent position embeddings $p_i$ of token $x_i$, where $p_i \\in \\mathbb {R}^{d_p}$, $d_p$ is the positional embedding dimension. Therefore, the embeddings of a sentence are represented as $ {\\rm \\textbf {E}}=\\lbrace [w_1;p_1 ], [w_2;p_2], ..., [w_l;p_l]\\rbrace , {\\rm \\textbf {E}}\\in \\mathbb {R}^{l \\times (d_p+d_w)}$. In particular, we adopt one-hot encoding to embed positions of tokens, rather than sinusoidal position encoding recommended in BERT model BIBREF31.", "Multi-task Learning A collection of improved models BIBREF26, BIBREF27, BIBREF28 are developed based on multi-task learning. Especially, shared-private model, as a popular multi-task learning model, divides the features of different tasks into private and shared spaces, where shared features, i.e., task-irrelevant features in shared space, as supplementary features are used for different tasks. Nevertheless, the shared space usually mixes some task-relevant features, which makes the learning of different tasks introduce noise. To address this issue, Liu et al. BIBREF29 explore an adversarial shared-private model to alleviate the shared and private latent feature spaces from interfering with each other. However, these models transmit all shared features in the shared layer to related tasks without distillation, which disturb specific tasks due to some useless and even harmful shared features. How to solve this drawback is the main challenge of this work."]}
{"question_id": "246e924017c48fa1f069361c44133fdf4f0386e1", "predicted_answer": "", "predicted_evidence": ["Stance Detection The researches BIBREF22, BIBREF23 demonstrate that the stance detected from fake news can serve as an effective credibility indicator to improve the performance of fake news detection. The common way of stance detection in rumors is to catch deep semantics from text content based on neural networksBIBREF24. For instance, Kochkina et al.BIBREF25 project branch-nested LSTM model to encode text of each tweet considering the features and labels of the predicted tweets for stance detection, which reflects the best performance in RumourEval dataset. In this work, we utilize transformer encoder to acquire semantics from responses and forwarding of fake news for stance detection.", "Multi-task Learning A collection of improved models BIBREF26, BIBREF27, BIBREF28 are developed based on multi-task learning. Especially, shared-private model, as a popular multi-task learning model, divides the features of different tasks into private and shared spaces, where shared features, i.e., task-irrelevant features in shared space, as supplementary features are used for different tasks. Nevertheless, the shared space usually mixes some task-relevant features, which makes the learning of different tasks introduce noise. To address this issue, Liu et al. BIBREF29 explore an adversarial shared-private model to alleviate the shared and private latent feature spaces from interfering with each other. However, these models transmit all shared features in the shared layer to related tasks without distillation, which disturb specific tasks due to some useless and even harmful shared features. How to solve this drawback is the main challenge of this work.", "MT-trans-G On the basis of MT-trans, MT-trans-G adds gated sharing cell behind the shared layer of MT-trans to filter shared features.", "In summary, the contributions of this paper are as follows:", "Although the sifted multi-task learning method outperforms previous state-of-the-art methods on two datasets (From Table TABREF27), we observe that the proposed method achieves more remarkable performance boosts on PHEME than on RumourEval. There are two reasons for our analysis according to Table TABREF24 and Table TABREF27. One is that the number of training examples in RumourEval (including 5,568 tweets) is relatively limited as compared with PHEME (including 105,354 tweets), which is not enough to train deep neural networks. Another is that PHEME includes more threads (6,425 threads) than RumourEval (325 threads) so that PHEME can offer more rich credibility features to our proposed method."]}
{"question_id": "96459b02efa82993a0b413530ed0b517c6633eea", "predicted_answer": "", "predicted_evidence": ["The problem of empty translations is also visible in the histogram over length ratios (Fig. FIGREF13 ). Beam search \u2013 although still slightly too short \u2013 roughly follows the reference distribution, but exact search has an isolated peak in INLINEFORM0 from the empty translations.", "The NMT search space is vast as it grows exponentially with the sequence length. For example, for a common vocabulary size of INLINEFORM0 , there are already more possible translations with 20 words or less than atoms in the observable universe ( INLINEFORM1 ). Thus, complete enumeration of the search space is impossible. The size of the NMT search space is perhaps the main reason why \u2013 besides some preliminary studies BIBREF3 , BIBREF4 , BIBREF5 \u2013 analyzing search errors in NMT has received only limited attention. To the best of our knowledge, none of the previous studies were able to quantify the number of search errors in unconstrained NMT due to the lack of an exact inference scheme that \u2013 although too slow for practical MT \u2013 guarantees to find the global best model score for analysis purposes.", "[t!] DFS INLINEFORM0 [1] INLINEFORM1 : Source sentence", "Beam search is the ubiquitous decoding algorithm for NMT, but it is prone to search errors as the number of active hypotheses is limited by INLINEFORM0 . In particular, beam search never compares partial hypotheses of different lengths with each other. As we will see in later sections, this is one of the main sources of search errors. However, in many cases, the model score found by beam search is a reasonable approximation to the global best model score. Let INLINEFORM1 be the model score found by beam search ( INLINEFORM2 in line 12, Alg. SECREF1 ), which is a lower bound on the global best model score: INLINEFORM3 . Furthermore, since the conditionals INLINEFORM4 in Eq. EQREF1 are log-probabilities and thus non-positive, expanding a partial hypothesis is guaranteed to result in a lower model score, i.e.: DISPLAYFORM0", "The task of finding the most likely translation INLINEFORM0 for a given source sentence INLINEFORM1 is known as the decoding or inference problem: DISPLAYFORM0"]}
{"question_id": "6c1614991647705265fb348d28ba60dd3b63b799", "predicted_answer": "", "predicted_evidence": ["[0]Now at Google.", "A popular method to counter the length bias in NMT is length normalization BIBREF6 , BIBREF7 which simply divides the sentence score by the sentence length. We can find the global best translations under length normalization by generalizing our exact inference scheme to length dependent lower bounds INLINEFORM0 . The generalized scheme finds the best model scores for each translation length INLINEFORM1 in a certain range (e.g. zero to 1.2 times the source sentence length). The initial lower bounds are derived from the Beam-10 hypothesis INLINEFORM2 as follows: DISPLAYFORM0", "Neural machine translation BIBREF0 , BIBREF1 , BIBREF2 assigns the probability INLINEFORM0 of a translation INLINEFORM1 of length INLINEFORM2 over the target language vocabulary INLINEFORM3 for a source sentence INLINEFORM4 of length INLINEFORM5 over the source language vocabulary INLINEFORM6 via a left-to-right factorization using the chain rule: DISPLAYFORM0", "Tab. TABREF14 demonstrates that the problems of search errors and empty translations are not specific to the Transformer base model and also occur with other architectures. Even a highly optimized Transformer Big model from our WMT18 shared task submission BIBREF15 has 25.8% empty translations.", "Beam search is the ubiquitous decoding algorithm for NMT, but it is prone to search errors as the number of active hypotheses is limited by INLINEFORM0 . In particular, beam search never compares partial hypotheses of different lengths with each other. As we will see in later sections, this is one of the main sources of search errors. However, in many cases, the model score found by beam search is a reasonable approximation to the global best model score. Let INLINEFORM1 be the model score found by beam search ( INLINEFORM2 in line 12, Alg. SECREF1 ), which is a lower bound on the global best model score: INLINEFORM3 . Furthermore, since the conditionals INLINEFORM4 in Eq. EQREF1 are log-probabilities and thus non-positive, expanding a partial hypothesis is guaranteed to result in a lower model score, i.e.: DISPLAYFORM0"]}
{"question_id": "b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "predicted_answer": "", "predicted_evidence": ["For Meta-NLG, we set batch size to 5, and INLINEFORM0 and INLINEFORM1 . A single inner gradient update is used per meta update with Adam BIBREF25 . The size of a Meta NLG task is set to 400 with 200 samples assigned to INLINEFORM2 and INLINEFORM3 because the minimum amount of target low-resource samples is 200 in our later experiments. During fine-tuning on a low-resource target task, early-stop is conducted on a small validation set with size 200. The model is then evaluated on other DA-utterance pairs in the target task.", "", "", "Meta-NLG( INLINEFORM0 )", "In this section, we first describe the objective of fine-tuning a NLG model on a low-resource NLG task in Section 3.1. Then, we describe how our Meta-NLG algorithm encapsulates this objective into Meta NLG tasks and into the meta optimization algorithm to learn better low-resource NLG models."]}
{"question_id": "157284acedf13377cbc6d58c8f3648d3a62f5db5", "predicted_answer": "", "predicted_evidence": ["", "Near-domain Adaptation: Figure FIGREF25 and Table TABREF26 show that \u201cAttraction\u201d, \u201cHotel\u201d, \u201cRestaurant\u201d, and \u201cTaxi\u201d, are four near-domains compared to remainder domains. Only results for \u201cAttraction\u201d and \u201cHotel\u201d are included due to page limit. The other two domains are also simpler with only one domain-specific slot. Several observations can be noted from results in Table TABREF27 . First, Using only source or target domain samples does not produce competitive performance. Using only source domain samples (Zero-NLG) performs the worst. It obtains very low BLEU-4 scores, indicating that the sentences generated do not match the linguistic patterns in the target domain. Using only low-resource target domain samples (Scratch-NLG) performs slightly better, yet still much worse than MTL-NLG and Meta-NLG. Second, Meta-NLG shows a very strong performance for this near-domain adaptation setting.", "Initialize INLINEFORM0", "Annotation Statistics: Cases with identical utterances generated by two models were filtered out. We obtained in total 600 annotations on each individual metric for each target domain. We calculated the Fleiss\u2019 kappa BIBREF27 to measure inter-rater consistency. The overall Fleiss\u2019 kappa values for informativeness and naturalness are 0.475 and 0.562, indicating \u201cModerate Agreement\u201d, and 0.637 for pairwise preferences, indicating \u201cSubstantial Agreement\u201d.", "To further investigate the adaptation process, we presented in Figure FIGREF34 the performance curves of MTL-NLG and Meta-NLG as fine-tuning epoch proceeds on the most challenging \u201cTrain\u201d domain. The effect of meta-learning for low-resource NLG can be observed by comparing the two solid curves against the corresponding dashed curves. First, Meta-NLG adapts faster than MTL-NLG. We can see that the ERR of Meta-NLG (red-solid) decreases much more rapidly than that of MTL-NLG (red-dashed) , and the BLEU-4 score of Meta-NLG (purple-solid) also increases more quickly. The optimal BLEU-4 and ERR that MTL-NLG converges to can be obtained by Meta-NLG within 10 epochs. Second, Meta-NLG adapts better than MTL-NLG. As it can be seen, Meta-NLG achieves a much lower ERR and a higher BLEU-4 score when it converges, indicating that it found a better INLINEFORM0 of the base NLG model to generalize to the low-resource target domain."]}
{"question_id": "e4ea0569b637d5f56f63e933b8f269695fe1a926", "predicted_answer": "", "predicted_evidence": ["Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as \u201cSUPPORTS\u201d, \u201cREFUTES\u201d, or \u201cNOT ENOUGH INFO\u201d, based on the evidence set.", "In this paper, we propose an unsupervised question-answering based approach for solving the fact-checking problem. This approach is inspired from the memory-based reading comprehension task that humans perform at an early age. As we know that kids in schools, first read and learn the syllabus content so that they can answer the questions in the exam. Similarly, our model learns a language model and linguistics features in unsupervised fashion from the provided Wikipedia pages.", "The claims generally feature information about one or more entities. These entities can be of many types such as PERSON, CITY, DATE. Since the entities can be considered as the content words for the claim, we utilize these entities to generate the questions. Although function words such as conjunctions and prepositions form relationship between entities in the claims, we currently do not make use of such function words to avoid generating complex questions. The types of entities in a sentence can be recognized by using Stanford CoreNLP (BIBREF12) NER tagger.", "In this section, we explain the design and all the underlying methods that our system has adopted. Our system is a pipeline consisting of three stages: (1) Question Generation, (2) Question Answering, (3) Label Classification. The question generation stage attempts to convert the claims into appropriate questions and answers. It generates questions similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank. Question Answering stage predicts the masked blanks in an unsupervised manner. The respective predictions are then compared with the original answers and exported into a file for label classification. The label classifier calculates the predicted label based on a threshold.", "Here, the classification threshold ($\\phi $) is derived empirically based on the precision-recall curve."]}
{"question_id": "e3c44964eb6ddc554901244eb6595f26a9bae47e", "predicted_answer": "", "predicted_evidence": ["Trainable parameters: 110M", "Every day textual information is being added/updated on Wikipedia, as well as other social media platforms like Facebook, Twitter, etc. These platforms receive a huge amount of unverified textual data from all its users such as News Channels, Bloggers, Journalists, Field-Experts which ought to be verified before other users start consuming it. This information boom has increased the demand of information verification also known as Fact Checking. Apart from the encyclopedia and other platforms, domains like scientific publications and e-commerce also require information verification for reliability purposes. Generally, Wikipedia authors, bloggers, journalists and scientists provide references to support their claims. Providing referenced text against the claims makes the fact checking task a little easier as the verification system no longer needs to search for the relevant documents.", "Attention heads: 12", "In the literature, the shared task has been tackled using pipeline-based supervised models (BIBREF9; BIBREF10; BIBREF11). To our knowledge, only BIBREF10 has provided the confusion matrix for each of the labels for their supervised system. For the same reason, we are only providing the comparison of the label accuracy on the \u201cSUPPORTS\u201d label in the results section.", "Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as \u201cSUPPORTS\u201d, \u201cREFUTES\u201d, or \u201cNOT ENOUGH INFO\u201d, based on the evidence set."]}
{"question_id": "905a8d775973882227549e960c7028e4a3561752", "predicted_answer": "", "predicted_evidence": ["Table TABREF16 shows the performance of our Fact Checking system on the \u201cSUPPORTS\u201d label, the output of our system. We compare the results against two different classification thresholds. Table TABREF3 shows that on an average there are 3 questions generated per claim. Here, $\\phi $ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while $\\phi $ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as \u201cSUPPORTS\u201d.", "If only 1 question is generated, then it has to be answered correctly for the claim to be classified as \u201cSUPPORTS\u201d in case of both the thresholds.", "Trainable parameters: 110M", "In this paper, we propose an unsupervised question-answering based approach for solving the fact-checking problem. This approach is inspired from the memory-based reading comprehension task that humans perform at an early age. As we know that kids in schools, first read and learn the syllabus content so that they can answer the questions in the exam. Similarly, our model learns a language model and linguistics features in unsupervised fashion from the provided Wikipedia pages.", "Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as \u201cSUPPORTS\u201d, \u201cREFUTES\u201d, or \u201cNOT ENOUGH INFO\u201d, based on the evidence set."]}
{"question_id": "76f90c88926256e7f90d2104a88acfdd7fc5475e", "predicted_answer": "", "predicted_evidence": ["In this stage, we compute the final label based on the correctness score of the predictions that we received from the previous stage. The correctness score ($s$) is computed as:", "In this paper, we presented a transformer-based unsupervised question-answering pipeline to solve the fact checking task. The pipeline consisted of three stages: (1) Question Generation (similar to a Cloze-task), (2) Question Answering, (3) Label Classification. We use Stanford CoreNLP NER tagger to convert the claim into a Cloze-task by masking the named entities. The Question Generation task achieves almost 90% accuracy in transforming the FEVER dataset into a Cloze-task. To answer the questions generated, we utilize masked language modeling approach from the BERT model. We could achieve 80.2% label accuracy on \u201cSUPPORTS\u201d label. From the results, we conclude that it is possible to verify the facts with the right kind of factoid questions.", "In our case, FEVER claims are derived from Wikipedia. We first collect all the claims from the FEVER dataset along with \u201cid\u201d, \u201clabel\u201d and \u201cverifiable\u201d fields. We don't perform any normalization on the claims such as lowercasing, transforming the spaces to underscore or parenthesis to special characters as it may decrease the accuracy of the NER tagger. These claims are then processed by the NER tagger to identify the named entities and their type. The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "Here, the classification threshold ($\\phi $) is derived empirically based on the precision-recall curve.", "Trainable parameters: 110M"]}
{"question_id": "182eb91090017a7c8ea38a88b219b641842664e4", "predicted_answer": "", "predicted_evidence": ["The AttnCopy-S2S model only concerns about content fidelity, and achieves a high content precision score (but a low recall). However, its style BLEU is particularly low, which verifies the rich variation in language and that direct supervised learning is incapable of controlling the variation. We can see that the rule-based method achieves reasonably good precision and recall, setting a strong baseline for content fidelity. As discussed above, the rule-based method can reach the maximum BLEU (100) after masking out content tokens. To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well. In comparison, our proposed model achieves better content precision/recall, substantially improving over other methods (e.g., with a 15-point precision boost in comparison with the rule-based baseline) except for AttnCopy-S2S which has failed in style control.", "We have proposed a new and practical task of text content manipulation which aims to generate a sentence that describes desired content from a structured record (content fidelity) and meanwhile follows the writing style of a reference sentence (style preservation). To study the unsupervised problem, we derived a new dataset, and developed a method with competing learning objectives and an explicit coverage constraint. For empirical study, we devised two automatic metrics to measure different aspects of model performance. Both automatic and human evaluations showed superiority of the proposed approach.", "Table TABREF31 shows the human evaluation results. From the top block of the table, as expected and discussed above, the rule-based method sets the records of style preservation and fluency scores, as it only conducts lightweight token replacement on reference sentences. However, its content fidelity score is very low. In contrast, our model achieves a reasonably high content score of 3.88, which is much higher than those of other methods. The model is also more balanced across the three criteria, achieving reasonably high scores in both style preservation and language fluency. The fluency of the full model is slightly inferior to the variant without coverage constraint, which is not unexpected since the full model has modified more portions of reference sentence in order to better describe the desired content, which would tend to introduce more language mistakes as well.", "Adversarial Style Transfer (AdvST) BIBREF12 . As another latest style transfer approach capable of handling more than one attributes, the model also mixes back-translation with auto-encoding as the above method, and additionally uses adversarial training to disentangle content and style representations.", "Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes. To apply to our setting, we treat content record INLINEFORM0 as the attributes. The method is based on back-translation BIBREF23 that first generates a target sentence INLINEFORM1 conditioning on INLINEFORM2 , and then treat it as the reference to reconstruct INLINEFORM3 conditioning on INLINEFORM4 . Auxiliary sentence INLINEFORM5 is used in an extra auto-encoding loss."]}
{"question_id": "0ef114d24a7a32821967e912dff23c016c4eab41", "predicted_answer": "", "predicted_evidence": ["The main idea underlying those models is to learn disentangled representations of text so as modify textual attributes or style of interest. Those papers used different objectives to encourage learning disentangled representations. BIBREF9 used pre-trained classifiers as the supervision. BIBREF10 used a GAN-based approach in which binary classifiers were used as discriminators. BIBREF15 proposed to use more structured discriminators such as language models to provide better supervision to the generator. BIBREF16 , BIBREF11 further augmented prior work using back-translation technique to incorporate cycle-consistency loss. Both BIBREF11 and BIBREF12 generalized the task to controlling multiple categorical attributes at the same time. Our work differs from those in that we assume an existing sentence to provide the source of style and a structured record as the source of content.", "The first block shows the two baseline models providing reference performance. The AttnCopy-S2S model only concerns about content fidelity, and achieves a high content precision score (but a low recall). However, its style BLEU is particularly low, which verifies the rich variation in language and that direct supervised learning is incapable of controlling the variation. We can see that the rule-based method achieves reasonably good precision and recall, setting a strong baseline for content fidelity. As discussed above, the rule-based method can reach the maximum BLEU (100) after masking out content tokens. To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well.", "Content fidelity. Following the table-to-document task BIBREF0 where our dataset is derived from, we use an information extraction (IE) approach to measure content fidelity. That is, given a generated sentence INLINEFORM0 and the conditioning content record INLINEFORM1 , we extract data tuples from INLINEFORM2 with an IE tool, and compute the precision and recall against INLINEFORM3 . We use the IE model provided in BIBREF0 and re-train with INLINEFORM4 pairs in our dataset. The IE model achieves around 87% precision and 76% recall on the test set, which is comparable to the one used in BIBREF0 .", "Generating text conditioning on structured input has been widely studied in recent work, such as BIBREF3 , BIBREF1 , BIBREF4 , BIBREF0 . Those methods are based on neural sequence to sequence models and trained with supervised data. This line of work has focused primarily on generating more accurate description of the given data, while does not study the problem of controlling the writing style of outputs. Our task takes a step forward to simultaneously describing desired content and controlling stylistic properties. Furthermore, our task is challenging due to its unsupervised setting in practice.", "As no ground truth annotations are available, we first set up automatic metrics for quantitatively measuring the key aspects of model performance."]}
{"question_id": "67672648e7ebcbef18921006e2c8787966f8cdf2", "predicted_answer": "", "predicted_evidence": ["Table TABREF31 shows the human evaluation results. From the top block of the table, as expected and discussed above, the rule-based method sets the records of style preservation and fluency scores, as it only conducts lightweight token replacement on reference sentences. However, its content fidelity score is very low. In contrast, our model achieves a reasonably high content score of 3.88, which is much higher than those of other methods. The model is also more balanced across the three criteria, achieving reasonably high scores in both style preservation and language fluency. The fluency of the full model is slightly inferior to the variant without coverage constraint, which is not unexpected since the full model has modified more portions of reference sentence in order to better describe the desired content, which would tend to introduce more language mistakes as well.", "The main idea underlying those models is to learn disentangled representations of text so as modify textual attributes or style of interest. Those papers used different objectives to encourage learning disentangled representations. BIBREF9 used pre-trained classifiers as the supervision. BIBREF10 used a GAN-based approach in which binary classifiers were used as discriminators. BIBREF15 proposed to use more structured discriminators such as language models to provide better supervision to the generator. BIBREF16 , BIBREF11 further augmented prior work using back-translation technique to incorporate cycle-consistency loss. Both BIBREF11 and BIBREF12 generalized the task to controlling multiple categorical attributes at the same time. Our work differs from those in that we assume an existing sentence to provide the source of style and a structured record as the source of content.", "To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well. In comparison, our proposed model achieves better content precision/recall, substantially improving over other methods (e.g., with a 15-point precision boost in comparison with the rule-based baseline) except for AttnCopy-S2S which has failed in style control. Our method also manages to preserve a high BLEU score of over 80. The superior performance of the full model compared to the variant Ours-w/o-Coverage demonstrates the usefulness of the content coverage constraint (Eq. EQREF15 ).", "In this work, we study the new yet practical problem in which we aim to express given content with a sentence and mimic the writing style of a reference sentence (Table TABREF1 ). More specifically, we are given a structured data record containing the content to describe, along with a sentence about a similar but different matter. Our goal is to generate a new sentence that precisely depicts all content in the record, while at the same time using as much of the writing style of reference sentence as possible. As above, the problem differs critically from the supervised data-to-text BIBREF0 or retrieval-and-rewriting work BIBREF7 , BIBREF8 as we have imposed an additional goal of preserving the reference text style. The resulting problem is typically unsupervised due to lack of parallel data.", "We now present a dataset developed for the task. Our dataset is derived from a recent large table-to-document corpus BIBREF0 which consists of box-score tables of NBA basketball games and associated documents as game reports. The corpus is originally used for studying supervised game report generation which has attracted increasing research interest BIBREF18 , BIBREF0 ."]}
{"question_id": "c32fc488f0527f330273263fa8956788bd071efc", "predicted_answer": "", "predicted_evidence": ["Without loss of generality, consider a content record INLINEFORM0 , where each element INLINEFORM1 is a data tuple which typically includes a data type (e.g., points), a value (e.g., 32), and other information (such as the associated player, e.g., Lebron_James). INLINEFORM2 is the number of tuples in record INLINEFORM3 , which can vary across different records. We are also given a reference sentence INLINEFORM4 which is assumed to describe content that has a similar but not exact the same structure with that of the record INLINEFORM5 . For example, in Table TABREF1 , both the content record and the reference sentence involve two players, respectively, but the number of associated data tuples as well as the types are different (e.g., Lebron_James in the record has 3 box-score entries, while Jrue_Holiday in the reference has only 2).", "Beyond generating text from scratch, there is another line of work that first retrieves a similar sentence and then rewrites it to express desired information BIBREF8 , BIBREF7 , BIBREF13 , BIBREF14 . For example, BIBREF8 used the framework to generate response in dialogues, while BIBREF7 studied programming code generation. The goal of the work is to manifest useful information from neighbors, usually in a supervised context, without aiming at controlling writing characteristics, and thus has fundamentally different assumptions to ours.", "Those papers used different objectives to encourage learning disentangled representations. BIBREF9 used pre-trained classifiers as the supervision. BIBREF10 used a GAN-based approach in which binary classifiers were used as discriminators. BIBREF15 proposed to use more structured discriminators such as language models to provide better supervision to the generator. BIBREF16 , BIBREF11 further augmented prior work using back-translation technique to incorporate cycle-consistency loss. Both BIBREF11 and BIBREF12 generalized the task to controlling multiple categorical attributes at the same time. Our work differs from those in that we assume an existing sentence to provide the source of style and a structured record as the source of content. The input content record in our task is also more structured than the style attributes which are typically loosely connected and of a pre-fixed number. The resulting content manipulation setting poses unique challenges in controlling, as discussed more in the empirical study.", "We now present a dataset developed for the task. Our dataset is derived from a recent large table-to-document corpus BIBREF0 which consists of box-score tables of NBA basketball games and associated documents as game reports. The corpus is originally used for studying supervised game report generation which has attracted increasing research interest BIBREF18 , BIBREF0 .", "The goal of the task is to generate a new realistic sentence INLINEFORM0 that achieves (1) content fidelity by accurately describing the full content in INLINEFORM1 , and at the same time (2) style preservation by retaining as much of the writing style and characteristics of reference INLINEFORM2 as possible. The task is unsupervised as there is no ground-truth sentence for training."]}
{"question_id": "8908d1b865137bc309dde10a93735ec76037e5f9", "predicted_answer": "", "predicted_evidence": ["We set the network parameters as follows: SSG embbeding size d is chosen to be 200, the tweet max legnth maxl is 99. For convolutional layers, we set the number of feature maps f to 50 and used 8 filter sizes (1,2,3,4,5,2,3,4). The p value of Dropout layer is set to 0.3. We used Nadam optimizer BIBREF8 to update the weights of the network and back-propogation algorithm to compute the gradients. The batch size is set to be 50 and the training data is shuffled after each iteration.", "The architecture of our convolutional neural net- work for sentiment classification is shown on Fig. 1. Our network is composed of a single convolutional layer followed by a non-linearity, max pooling, Dropout, fully connected layer and a soft-max classification layer. Here we describe this architecture:", "Each term in the tweet is replaced by its SSG embedding which is a vector of d dimensions, all term vectors are concatenated to form the input matrix where the number of rows is d and the number of columns is set to be maxl: the max tweet length in the training dataset. This 2-dim matrix is the input layer for the neural network.", "The remaining of this paper is organized as follows: Section 2 describes the system architecture, Section 3 presents our experiments and results and Section 4 is devoted for the conclusion.", "This layer reduces the size of the output of activation layer, for each vector it selects the max value. Different variation of pooling layer can be used: average or k-max pooling."]}
{"question_id": "d207f78beb6cd754268881bf575c8f98000667ea", "predicted_answer": "", "predicted_evidence": ["We set the network parameters as follows: SSG embbeding size d is chosen to be 200, the tweet max legnth maxl is 99. For convolutional layers, we set the number of feature maps f to 50 and used 8 filter sizes (1,2,3,4,5,2,3,4). The p value of Dropout layer is set to 0.3. We used Nadam optimizer BIBREF8 to update the weights of the network and back-propogation algorithm to compute the gradients. The batch size is set to be 50 and the training data is shuffled after each iteration.", "The output of the fully connected layer is passed to a Softmax layer. It computes the probability distribution over the labels in order to decide the most probable label for a tweet.", "For training the network, we used about 30000 English tweets provided by SemEval organisers and the test set of 2016 which contains 12000 tweets as development set. The test set of 2017 is used to evaluate the system in SemEval-2017 competition. For implementing our system we used python and Keras.", "We can remark that in 2013, 2014 and 2015 most best systems were based on a rich feature extraction process with a traditional classifier such as SVM BIBREF1 or Logistic regression BIBREF2 . In 2014, kimconvolutional2014 proposed to use one convolutional neural network for sentence classification, he fixed the size of the input sentence and concatenated its word embeddings for representing the sentence, this architecture has been exploited in many later works. severynunitn:2015 adapted the convolutional network proposed by kimconvolutional2014 for sentiment analysis in Twitter, their system was ranked second in SemEval-2015 while the first system BIBREF3 combined four systems based on feature extraction and the third ranked system used logistic regression with different groups of features BIBREF2 .", "We presented our deep learning approach to Twitter sentiment analysis. We used ten convolutional neural network voters to get the polarity of a tweet, each voter has been trained on the same training data using the same word embeddings but different initial weights. The results demonstrate that our system is competitive as it is ranked forth in SemEval-2017 task 4-A."]}
{"question_id": "35c01dc0b50b73ee5ca7491d7d373f6e853933d2", "predicted_answer": "", "predicted_evidence": ["Firstly, to further incorporate fine-grained word-level representations into hidden features $h_\\text{last}$, we adopt the spatial attention and channel-wise attention introduced in BIBREF17 to generate spatial and channel-wise attention features $s \\in \\mathbb {R}^{C^{\\prime } \\times H^{\\prime } \\times D^{\\prime }}$ and $c \\in \\mathbb {R}^{C^{\\prime } \\times H^{\\prime } \\times D^{\\prime }}$, respectively, which are further concatenated with $h_\\text{last}$ to produce intermediate features $a$. The features $a$ can further aid the model to refine visual attributes that are relevant to the given text, contributing to a more accurate and effective modification of the contents corresponding to the given description.", "To train the network, we follow BIBREF17 and adopt adversarial training, where our network and the discriminators ($D_1$, $D_2$, $D_3$, $D_\\text{DCM}$) are alternatively optimised. Please see supplementary material for more details about training objectives. We only highlight some training differences compared with BIBREF17.", "Why does the detail correction module work? This module aims to refine the manipulated results by enhancing details and completing missing contents. On the one hand, the word-level spatial and channel-wise attentions closely correlate fine-grained word-level information with the intermediate feature maps, enhancing the detailed attribute modification. On the other hand, the shallow neural network layer is adopted to derive visual representations, which contain more detailed colour, texture, and edge information, contributing to missing detail construction. Finally, further benefiting from our ACM, the above fine-grained text-image representations collaborate to enhance the quality.", "Text-to-image generation has drawn much attention due to the success of GANs BIBREF12 in generating realistic images. Reed et al. BIBREF13 proposed to use conditional GANs to generate plausible images from given text descriptions. Zhang et al. BIBREF14, BIBREF15 stacked multiple GANs to generate high-resolution images from coarse- to fine-scale. Xu et at. BIBREF16 and Li et al. BIBREF17 implemented attention mechanisms to explore fine-grained information at the word-level. However, all aforementioned methods mainly focus on generating new photo-realistic images from texts, and not on manipulating specific visual attributes of given images using natural language descriptions.", "As shown in Fig. FIGREF4 (b), our detail correction module takes three inputs: (1) the last hidden features $h_\\text{last} \\in \\mathbb {R}^{{C}^{\\prime } \\times H^{\\prime } \\times D^{\\prime }}$ from the last affine combination module, (2) the word features encoded by a pretrained RNN following BIBREF16, where each word is associated with a feature vector, and (3) visual features ${v}^{\\prime } \\in \\mathbb {R}^{128 \\times 128 \\times 128}$ that are extracted from the input image $I$, which are the relu2_2 layer representations from a pretrained VGG-16 BIBREF33 network."]}
{"question_id": "c077519ea42c9649fb78da34485de2262a0df779", "predicted_answer": "", "predicted_evidence": ["The main module is trained for 600 epochs on the CUB and 120 epochs on the COCO using the Adam optimiser BIBREF34 with the learning rate 0.0002, and $\\beta _{1}=0.5$, $\\beta _{2}=0.999$. As for the detail correction module, there is a trade-off between the generation of new attributes corresponding to the given text and the reconstruction of text-irrelevant contents of the original image. Based on the manipulative precision (MP) values (see Fig. FIGREF12), we find that training 100 epochs for CUB, and 12 epochs for COCO to achieve an appropriate balance between generation and reconstruction. The other training setting is the same as in the main module. The hyperparameter controlling $\\mathcal {L}_\\text{reg}$ in Eq. (DISPLAY_FORM9) is set to 1 for CUB and 15 for COCO.", "To achieve effective image manipulation guided by text descriptions, the key is to exploit both text and image cross-modality information, generating new attributes matching the given text and also preserving text-irrelevant contents of the original image. To fuse text and image information, existing methods BIBREF8, BIBREF9 typically choose to directly concatenate image and global sentence features along the channel direction. Albeit simple, the above heuristic may suffer from some potential issues. Firstly, the model cannot precisely correlate fine-grained words with corresponding visual attributes that need to be modified, leading to inaccurate and coarse modification. For instance, shown in the first row of Fig. FIGREF1, both models cannot generate detailed visual attributes like black eye rings and a black bill. Secondly, the model cannot effectively identify text-irrelevant contents and thus fails to reconstruct them, resulting in undesirable modification of text-irrelevant parts in the image. For example, in Fig.", "Text-to-image generation has drawn much attention due to the success of GANs BIBREF12 in generating realistic images. Reed et al. BIBREF13 proposed to use conditional GANs to generate plausible images from given text descriptions. Zhang et al. BIBREF14, BIBREF15 stacked multiple GANs to generate high-resolution images from coarse- to fine-scale. Xu et at. BIBREF16 and Li et al. BIBREF17 implemented attention mechanisms to explore fine-grained information at the word-level. However, all aforementioned methods mainly focus on generating new photo-realistic images from texts, and not on manipulating specific visual attributes of given images using natural language descriptions.", "", "A natural question may arises: how does the model learn to modify the image $I$ if the input image $I$ and ground-truth image are the same, and the modified sentence $S^{\\prime }$ does not exist in the input? In theory, the optimal solution is that the network becomes an identity mapping from the input image to the output. The text-guided image manipulation model is required to jointly solve image generation from text descriptions ($S$ $\\rightarrow $ $I$), similarly to BIBREF17, and text-irrelevant contents reconstruction ($I$ $\\rightarrow $ $I$). Thanks to our proposed affine combination module, our model gains the capacity to disentangle regions required to be edited and regions needed to be preserved. Also, to generate new contents semantically matching the given text, the paired data $S$ and $I$ can serve as explicit supervision."]}
{"question_id": "a51c680a63ee393792d885f66de75484dc6bc9bc", "predicted_answer": "", "predicted_evidence": ["Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .", "Generally speaking, Group 2 performs better than Group 1. This is because Group 1 uses a traditional SVM with feature engineering BIBREF26 and Group 2 uses more advanced deep learning methods proven to be effective by recent studies BIBREF0 , BIBREF5 . However, some feature engineering methods are no worse than some deep learning methods. For example, the TextFeature model outperforms SSWE by a significant margin.", "The three benchmarking datasets include movie reviews from IMDB, restaurant reviews from Yelp13 and Yelp14 developed by Tang tang2015document. All datasets are tokenized using the Stanford NLP tool BIBREF22 . Table TABREF11 lists statistics of the datasets including the number of classes, number of documents, average length of sentences, the average number of documents per user, and the average number of documents per product. Since postings in social networks by both users and products follow the long tail distribution BIBREF23 , we only show the distribution of total number of posts for different products. For example, #p(0-50) means the number of products which have reviews between the size of 0 to 50. We split train/development/test sets at the rate of 8:1:1 following the same setting in BIBREF3 , BIBREF5 . The best configuration by the development dataset is used for the test set to obtain the final result.", "Written text is often meant to express sentiments of individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text.", "Most social network data follows the long tail distribution. If the memory size to represent the data is too small, some context information will be lost. On the other hand, too large memory size which requires more resources in computation and storage may not introduce much benefit. Thus, the fourth set of experiments evaluates the effect of dimension size INLINEFORM0 in the DUPMN memory networks. Figure FIGREF28 shows the result of the evaluation for 1 hop configuration with memory size starting at 1 with 10 points at each increment until size of 75, the increment set to 25 from 75 to 200 to cover most postings. Results show that when memory size increases from 10 to 100, the performance of DUPMN steadily increases. Once it goes beyond 100, DUPMN is no longer sensitive to memory size. This is related to the distribution of document frequency rated by user/product in Table TABREF11 as the average is around 50."]}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "predicted_answer": "", "predicted_evidence": ["For the DUPMN model, we also include two variations which use only one memory network. The first variation only includes user profiles in the memory network, denoted as DUPMN-U. The second variation only uses product information, denoted as DUPMN-P.", "Best results are marked in bold; second best are underlined in the table", "LSTM+UPA BIBREF5 \u2014 The state-of-the-art LSTM including both local context based attentions and user/product in the attention mechanism.", "Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) BIBREF0 , Recursive auto-encoders BIBREF1 , Long-Short Term Memory (LSTM) BIBREF2 , and many more.", "Most importantly, the DUPMN model with both user memory and product memory significantly outperforms all the baseline methods including the state-of-the-art LSTM+UPA model BIBREF5 . By using user profiles and product information in memory networks, DUPMN outperforms LSTM+UPA in all three datasets. In the IMDB dataset, our model makes 0.6 % improvement over LSTM+UPA in accuracy with INLINEFORM0 of 0.007. Our model also achieves lower RMSE value. In the Yelp review dataset, the improvement is even more significant. DUPMN achieves 1.2% improvement in accuracy in Yelp13 with INLINEFORM1 of 0.004 and 0.9% in Yelp14 with INLINEFORM2 of 0.001, and the lower RMSE obtained by DUPMN also indicates that the proposed model can predict review ratings more accurately."]}
{"question_id": "c79f168503a60d1b08bb2c9aac124199d210b06d", "predicted_answer": "", "predicted_evidence": ["However, for several datasets, it appears that the output of the third layer does not add value as it is a too abstract representation for the NLP tasks. The learned weighted average method presented by Peters et al. regularizes the three (softmax-normalized) weights $s_j$ . As a consequence, a zero or small $s_j$ value is not possible, and all three vectors are used even if one vector (e.g. the third layer output) decreases the performance. This explains why removing the (sometimes harmful) third layer can improve the model performance. Further, by removing the last layer, we observe a significant training speed-up. For the models included in AllenNLP, we observed a training speed-up of 19-44%, while improving the test performance in 3 out of 5 datasets. This speed-up can be crucial for cases that require fast training of inference speeds.", "For both experiments, we use the pre-trained ELMo 5.5B model, which was trained on a dataset of 5.5 billion tokens. We trained each setup with ten different random seed and report average test scores.", "The results for the second experiment, that uses AllenNLP and ELMo embeddings in combination with other input representations, are presented in the lower part of Table 1 .", "As it is not known in advance which layer produces the best input representation, learning a task-specific weighted average of the three layers appears advisable. However, for several datasets, it appears that the output of the third layer does not add value as it is a too abstract representation for the NLP tasks. The learned weighted average method presented by Peters et al. regularizes the three (softmax-normalized) weights $s_j$ . As a consequence, a zero or small $s_j$ value is not possible, and all three vectors are used even if one vector (e.g. the third layer output) decreases the performance. This explains why removing the (sometimes harmful) third layer can improve the model performance. Further, by removing the last layer, we observe a significant training speed-up. For the models included in AllenNLP, we observed a training speed-up of 19-44%, while improving the test performance in 3 out of 5 datasets.", "The weighting scheme appears especially important when these vectors are used as the only input representation for the task. In that case, we advise testing different weighting schemes. If ELMo is used in conjunction with other input representations, the weighting scheme was less critical."]}
{"question_id": "9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "predicted_answer": "", "predicted_evidence": ["They compare these two options in their paper and show a slight advantage for learning a weighted average. However, the evaluation is in our opinion insufficient. First, they evaluate both options on the development set, so it remains unclear if there are changes for unseen data (test set). Further, they evaluate it only with a single random seed. As shown in BIBREF3 , the performance of a neural network can change significantly with a different random seed. For example, we observe test score differences of up to 1.5 percentage points when the same model is trained with a different random seed with the AllenNLP model for the Stanford Sentiment Treebank (SST-5). The differences Peters et al. report between using the last layer and learning a task-specific weighting are rather small (0.4 - 0.7 percentage points). It is not clear if these differences are due to the effect of different random seeds or due to the weighting scheme.", "In contrast to our first experiment, we notice much smaller differences between different weighting schemes. In most cases, the differences are not statistically significant. When a network solely depends on ELMo embeddings as input, all relevant information to solve the task must be included. However, if ELMo is combined with other input representations, like GloVe embeddings, the dependency on the ELMo embeddings decreases. If ELMo does not capture critical word properties, the network can still fall back on one of the other input representations.", "Concatenation: All three vectors are concatenated.", "Surprisingly, using the output of the second layer of the biLM model yields a better performance than using the third (last) layer in many downstream NLP tasks. Using this insight, we present a weighting scheme that learns a weighted average of the first two layers of the biLM. This scheme outperforms the originally proposed weighting scheme by Peters et al. for several datasets. Further, it is computationally faster than the original method. For downstream tasks, we saw a training speed-up of 19-44%.", "We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 ."]}
{"question_id": "5cc5e2db82f5d40a5244224dad94da50b4f673db", "predicted_answer": "", "predicted_evidence": ["Occupation De-biasing is a first-of-a-kind tool to identify possibility of gender bias from occupation point of view, and to generate pieces of evidences by responding to different cultural contexts. Our future work would involve exploring other dimensions of biases and have a more sophisticated definition of bias in text.", "what he had been doing his whole", "Our system is represented in figure FIGREF7 . We have the following components in our system -", "Gender Tagging - We further use the names dataset to resolve the genders of the persons identified in the previous person tagging step.", "life."]}
{"question_id": "ab975efc916c34f55e1144b1d28e7dfdc257e371", "predicted_answer": "", "predicted_evidence": ["what she had been doing her whole", "Hence, the observation is that when we change the year and location parameters in the tool, the tool can automatically respond to the change. Therefore the system is sensitive to the subjectivity of bias in various cultural contexts and timeframes.", "The story-writer plans to write a story based in Russia between the timeframe 1980-2000. He/She uses the story and feeds it to the tool.", "This story interacts with our backend system and identifies if the story contains any occupational bias. Here, John is the named entity and doctor is the associated occupation. Furthermore, the system identifies John as a male character. It tries to search in backend if 'doctor' is a gender specific occupation or a gender neutral occupation. After detecting that it is a gender neutral occupation, the system checks the DBpedia corpus from 1980-2000 and fetches the instances of female doctors in the same timeframe in the United States. It displays the evidences for the user to go back and revisit and rewrite the story as below.", "The tool displays no evidences and shows the story free from bias with occupation point of view. The screen-shot of the interface is shown in FIGREF21"]}
{"question_id": "e7ce612f53e9be705cdb8daa775eae51778825ef", "predicted_answer": "", "predicted_evidence": ["As a future work, we are working on building reasoning systems which automatically regenerate an unbiased version of text.", "The tool displays no evidences and shows the story free from bias with occupation point of view. The screen-shot of the interface is shown in FIGREF21", "Hence, the observation is that when we change the year and location parameters in the tool, the tool can automatically respond to the change. Therefore the system is sensitive to the subjectivity of bias in various cultural contexts and timeframes.", "The screen-shots of the interface are represented in FIGREF18", "The story-writer plans to write a story based in United States of America between timeframe 1980-2000. The story-writer uses our system and types in the natural language story -"]}
{"question_id": "6c5a64b5150305c584326882d37af5b0e58de2fd", "predicted_answer": "", "predicted_evidence": ["Mary is a doctor. She treats her", "Occupation De-biasing is a first-of-a-kind tool to identify possibility of gender bias from occupation point of view, and to generate pieces of evidences by responding to different cultural contexts. Our future work would involve exploring other dimensions of biases and have a more sophisticated definition of bias in text.", "This story interacts with our backend system and identifies if the story contains any occupational bias. Here, John is the named entity and doctor is the associated occupation. Furthermore, the system identifies John as a male character. It tries to search in backend if 'doctor' is a gender specific occupation or a gender neutral occupation. After detecting that it is a gender neutral occupation, the system checks the DBpedia corpus from 1980-2000 and fetches the instances of female doctors in the same timeframe in the United States. It displays the evidences for the user to go back and revisit and rewrite the story as below.", "To further explain it more, given a sentence -", "The screen-shots of the interface are represented in FIGREF18"]}
{"question_id": "f7a27de3eb6447377eb48ef6d2201205ff943751", "predicted_answer": "", "predicted_evidence": ["The sentiment classifier attains 81.4% classification accuracy which is further used to annotate the OpenSubtitles dataset BIBREF36. The data statistic of the resulting sentiment-specific dialogue dataset is shown in Table TABREF21.", "We use a publicly available emotion-specific dataset BIBREF11 which contains responses with 6 different emotions including Like, Disgust, Happy, Anger, Sad and Other.", "Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.", "The quality of dialogue responses is known to be difficult to measure automatically BIBREF41; we therefore rely on human evaluation. To evaluate the responses, we hire five annotators from a commercial annotation company. To prevent introducing potential bias to the annotators, all results are randomly shuffled before being evaluated. All results are evaluated by the annotators following the metrics below.", "We use a publicly available gender-specific dialogue dataset BIBREF33. In this dataset, each response contains one specific gender preference including Female, Male and Neutral."]}
{"question_id": "2df3cd12937591481e85cf78c96a24190ad69e50", "predicted_answer": "", "predicted_evidence": ["An illustration of the proposed framework is shown in Figure FIGREF2, where a prototype is first extracted from the retrieved response. The stylistic response generator then takes the desired language style and the extracted prototype as additional input to obtain an adequate and stylistic response. The proposed stylistic response generator mainly inherits from the GPT-2 model BIBREF18 which is pre-trained with a large unlabeled text corpus. However, the GPT-2 model does not naturally fit the task of dialogue generation. To this end, we design various adaptations to the model architecture to extend the GPT-2 model to address the task of dialogue generation. Furthermore, in order to control the style of the generated responses, we train the model with a novel style-aware maximum likelihood estimation (MLE) objective that encodes additional style knowledge into the model's parameters.", "In this work, we propose a novel PS framework to tackle the task of stylistic dialogue generation. Additionally, we propose a new stylistic response generator which works coherently with the proposed framework. We conduct extensive experiments on three benchmark datasets from two languages. Results of human and automatic evaluation show that the proposed approach outperforms many strong baselines by a substantial margin.", "The proposed Stylistic Response Generator inherits from the GPT-2 BIBREF18 model which consists of a 12-layer decoder-only Transformer BIBREF30. To make use of the GPT-2 model, the input tokens must be a consecutive natural sequence (e.g. sentence, document). Based on the input sequence, the input representation is constructed by adding up the token embeddings and the corresponding position embeddings.", "Most early research on dialogue response generation focused on generating grammatical and contextually relevant responses BIBREF0, BIBREF1, BIBREF2. While promising results have been demonstrated BIBREF3, BIBREF4, syntactically coherent responses alone do not guarantee an engaging and attractive dialogue system. Expressing a unique and consistent speaking style has been shown to be crucial for increasing the user's engagement with dialogue systems BIBREF5. There are various definitions of language style BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. In this work, from a purely computational standpoint, we refer to language style as any characteristic style of expression. Hence, our work is in line with previous work on dialogue generation with emotion BIBREF11, BIBREF12, BIBREF13, BIBREF14; response attitude BIBREF15, and speaker personality BIBREF16.", "where $\\theta $ are the model parameters and $\\mathcal {SV}$ is the stylistic vocabulary introduced in SV. By increasing $\\alpha $, the proposed objective encodes more knowledge about stylistic expressions into the model parameters."]}
{"question_id": "fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "predicted_answer": "", "predicted_evidence": ["The sentiment classifier attains 81.4% classification accuracy which is further used to annotate the OpenSubtitles dataset BIBREF36. The data statistic of the resulting sentiment-specific dialogue dataset is shown in Table TABREF21.", "Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.", "The quality of dialogue responses is known to be difficult to measure automatically BIBREF41; we therefore rely on human evaluation. To evaluate the responses, we hire five annotators from a commercial annotation company. To prevent introducing potential bias to the annotators, all results are randomly shuffled before being evaluated. All results are evaluated by the annotators following the metrics below.", "In summary, the contributions of this work are: (1) We propose a novel framework that tackles the challenge of stylistic dialogue generation by leveraging useful information contained in the retrieved responses; (2) We propose a new stylistic response generator by making proper adaptations to a large-scale pre-trained language model. We train our model with a new style-aware learning objective in a de-noising manner. Experiments show that the proposed model outperforms many strong baselines on three benchmark datasets on both in-domain and cross-domain evaluations.", "In practice, a satisfactory stylistic dialogue system should express the desired style on the premise of the response quality. Based on the criterion of human evaluation metric, 3 is the marginal score of acceptance. So we deem a response as marginally acceptable by actual users when both quality and style expression scores are greater or equal to 3. On the other hand, 4 is the score that well satisfies the users, so responses with both scores greater or equal to 4 are deemed as satisfying to actual users."]}
{"question_id": "fc9aa04de4018b7d55e19a39663a2e9837328de7", "predicted_answer": "", "predicted_evidence": ["An illustration of the proposed framework is shown in Figure FIGREF2, where a prototype is first extracted from the retrieved response. The stylistic response generator then takes the desired language style and the extracted prototype as additional input to obtain an adequate and stylistic response. The proposed stylistic response generator mainly inherits from the GPT-2 model BIBREF18 which is pre-trained with a large unlabeled text corpus. However, the GPT-2 model does not naturally fit the task of dialogue generation. To this end, we design various adaptations to the model architecture to extend the GPT-2 model to address the task of dialogue generation. Furthermore, in order to control the style of the generated responses, we train the model with a novel style-aware maximum likelihood estimation (MLE) objective that encodes additional style knowledge into the model's parameters. Finally, to mitigate the possible effect that the retrieved response containing irrelevant and inappropriate information with respect to the input query, we adopt a de-noising learning strategy BIBREF19, BIBREF20 to prevent the model from uncritically copying the prototype.", "In this work, we propose a novel PS framework to tackle the task of stylistic dialogue generation. Additionally, we propose a new stylistic response generator which works coherently with the proposed framework. We conduct extensive experiments on three benchmark datasets from two languages. Results of human and automatic evaluation show that the proposed approach outperforms many strong baselines by a substantial margin.", "We find that including the language model as an auxiliary objective in addition to the supervised style-aware learning objective helps to improve generalization as well as accelerate convergence. This observation is in line with BIBREF31, BIBREF32. In this work, the language model objective is defined as the reconstruction loss of the input query based on itself:", "The aforementioned approaches explicitly incorporate the language style information into the model configuration either via embeddings or memory modules to control the process of response generation. In our replication experiments, we found that these approaches tend to overemphasise the importance of the language style. As a result, the generated responses tend to be generic and non-informative BIBREF17, but they do express a distinct style; e.g., they generate a generic response: \u201cI am happy to hear that.\" that conveys a `happy' emotion to different queries.", "Most early research on dialogue response generation focused on generating grammatical and contextually relevant responses BIBREF0, BIBREF1, BIBREF2. While promising results have been demonstrated BIBREF3, BIBREF4, syntactically coherent responses alone do not guarantee an engaging and attractive dialogue system. Expressing a unique and consistent speaking style has been shown to be crucial for increasing the user's engagement with dialogue systems BIBREF5. There are various definitions of language style BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. In this work, from a purely computational standpoint, we refer to language style as any characteristic style of expression. Hence, our work is in line with previous work on dialogue generation with emotion BIBREF11, BIBREF12, BIBREF13, BIBREF14; response attitude BIBREF15, and speaker personality BIBREF16."]}
{"question_id": "044cb5ef850c0a2073682bb31d919d504667f907", "predicted_answer": "", "predicted_evidence": ["According to all sets of models, scene 3.1 was written by Fletcher. All three sets of models indicate that the shift happened at the scene\u2019s end.", "The rolling attribution method suggests that particular scenes are indeed mostly a work of a single author and that their contributions roughly correspond to what has been proposed by James Spedding BIBREF3. The main differences between our results and Spedding\u2019s attribution are the ambivalent outputs of models for both scenes of act 4. However, it is worth noting that Spedding himself expressed some doubts about the authorship of these scenes. Other differences are rather marginal and usually support the modifications of Spedding\u2019s original attribution, as proposed by Thomas Merriam BIBREF25, BIBREF26, BIBREF27.", "For scenes 4.1 and 4.2 the rhythmic types indicate Shakespeare\u2019s authorship of the first (contrary to Spedding) and Fletcher\u2019s authorship of the latter. Location of the shift does not however fully correspond to the scene boundaries. Probabilities extracted from word-based models and combined models are close to 0.5 for both authors which may support Merriam\u2019s attribution (mixed authorship).", "While the stylistic dissimilarity of Henry VIII (henceforth H8) to Shakespeare\u2019s other plays had been pointed out before BIBREF2, it was not until the mid-nineteenth century that Shakespeare\u2019s sole authorship was called into question. In 1850 British scholar James Spedding published an article BIBREF3 attributing several scenes to John Fletcher. Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.", "Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding\u2019s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3."]}
{"question_id": "c845110efee2f633d47f5682573bc6091e8f5023", "predicted_answer": "", "predicted_evidence": ["Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work. The only place where Shakespeare comes closer to Fletcher\u2019s values is the first scene of act 5 of Bonduca. Having only 10 groups misattributed out of 4412 we may estimate the accuracy of rolling attribution to be as high as 0.9977 when distinguishing between Shakespeare and Fletcher.", "Since then many scholars have brought new evidence supporting Spedding\u2019s division of the play based both on versification and linguistic features. This includes e.g. frequencies of enjambment BIBREF4, frequencies of particular types of unstressed line endings BIBREF5, BIBREF6, frequencies of contractions BIBREF7, vocabulary richness BIBREF8, phrase length measured by the number of words BIBREF9, or complex versification analysis BIBREF10, BIBREF11. From the very beginning, beside advocates of Shakespeare\u2019s sole authorship (e.g. BIBREF13, BIBREF14), there were also those who supported alternative hypotheses concerning mixed authorship of either Shakespeare, Fletcher, and Philip Massinger BIBREF15, BIBREF16, BIBREF17, Fletcher and Massinger only BIBREF18, BIBREF19, Shakespeare and an unknown author BIBREF20, Shakespeare, Fletcher, Massinger, and an unknown author BIBREF21, BIBREF22 or Shakespeare and Fletcher with different shares than those proposed by Spedding BIBREF23.", "As shown in Table TABREF14, the versification-based models yield a very high accuracy with the recognition of Shakespeare and Fletcher (0.97 to 1 with the exception of Valentinian), yet slightly lower accuracy with the recognition of Massinger (0.81 to 0.88). The accuracy of words-based models remains very high across all three authors (0.95 to 1); in three cases it is nevertheless outperformed by the combined model. We thus may conclude that combined models provide a reliable discriminator between Shakespeare\u2019s, Fletcher\u2019s and Massinger\u2019s styles.", "According to all sets of models, scene 3.1 was written by Fletcher. All three sets of models indicate that the shift happened at the scene\u2019s end.", "After validation of the method we proceed to H8. Fig. FIGREF30 gives the results of rolling attribution based on a combined vector of most frequent types and most frequent words, and additionally for each of these feature subsets alone. Models were trained on all 8 plays in the training set with the same setting as above ($k = 100; d = 5$). It once again supports Spedding\u2019s attribution to a high extent:"]}
{"question_id": "2301424672cb79297cf7ad95f23b58515e4acce8", "predicted_answer": "", "predicted_evidence": ["Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.", "The rolling attribution method suggests that particular scenes are indeed mostly a work of a single author and that their contributions roughly correspond to what has been proposed by James Spedding BIBREF3. The main differences between our results and Spedding\u2019s attribution are the ambivalent outputs of models for both scenes of act 4. However, it is worth noting that Spedding himself expressed some doubts about the authorship of these scenes. Other differences are rather marginal and usually support the modifications of Spedding\u2019s original attribution, as proposed by Thomas Merriam BIBREF25, BIBREF26, BIBREF27.", "Since the training data are imbalanced (which may bias the results), we level the number of training samples per author by random selection.", "For scenes 4.1 and 4.2 the rhythmic types indicate Shakespeare\u2019s authorship of the first (contrary to Spedding) and Fletcher\u2019s authorship of the latter. Location of the shift does not however fully correspond to the scene boundaries. Probabilities extracted from word-based models and combined models are close to 0.5 for both authors which may support Merriam\u2019s attribution (mixed authorship).", "Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work. The only place where Shakespeare comes closer to Fletcher\u2019s values is the first scene of act 5 of Bonduca. Having only 10 groups misattributed out of 4412 we may estimate the accuracy of rolling attribution to be as high as 0.9977 when distinguishing between Shakespeare and Fletcher."]}
{"question_id": "6c05376cd0f011e00d1ada0254f6db808f33c3b7", "predicted_answer": "", "predicted_evidence": ["Scene 5.1 is according to all sets of models authored by Shakespeare. Rhythmic types and combined models locate the shift at its end; word-based models locate it a little later on.", "The rolling attribution method suggests that particular scenes are indeed mostly a work of a single author and that their contributions roughly correspond to what has been proposed by James Spedding BIBREF3. The main differences between our results and Spedding\u2019s attribution are the ambivalent outputs of models for both scenes of act 4. However, it is worth noting that Spedding himself expressed some doubts about the authorship of these scenes. Other differences are rather marginal and usually support the modifications of Spedding\u2019s original attribution, as proposed by Thomas Merriam BIBREF25, BIBREF26, BIBREF27.", "For scenes 1.3, 1.4, 2.1 and 2.2 all three sets of models indicate Fletcher to be the author. Rhythmic types indicate that the shift of authorship happened at the end of 2.2, while word-based models indicate that the shift happened before the end of the scene. (Recall that the shift of authorship within 2.2 is proposed also by Thomas Merriam (cf. Table TABREF3) even though a little bit further at line 1164.)", "After validation of the method we proceed to H8. Fig. FIGREF30 gives the results of rolling attribution based on a combined vector of most frequent types and most frequent words, and additionally for each of these feature subsets alone. Models were trained on all 8 plays in the training set with the same setting as above ($k = 100; d = 5$). It once again supports Spedding\u2019s attribution to a high extent:", "Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely."]}
{"question_id": "9925e7d8757e8fd7411bcb5250bc08158a244fb3", "predicted_answer": "", "predicted_evidence": ["Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.", "Scenes 2.3 and 2.4 are according to all sets of models authored by Shakespeare. All three sets of models indicate that the shift happened at the end of scene 2.4.", "Fig. FIGREF21 gives the results for each of the eight plays. Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work. The only place where Shakespeare comes closer to Fletcher\u2019s values is the first scene of act 5 of Bonduca.", "Altogether there are thus 53 training samples for Shakespeare, 90 training samples for Fletcher and 46 training samples for Massinger. In order to estimate the accuracy of the model, cross-validation is performed in the following way:", "In the first experiment we perform an attribution of individual scenes of H8 using the Support Vector Machine as a classifier and the frequencies of 500 most frequent rhythmic types and the frequencies of 500 most frequent words as a feature set. As training samples, individual scenes of plays written by Shakespeare, Fletcher, and Massinger are used that come roughly from the period when H8 was supposedly written, namely:"]}
{"question_id": "fa468c31dd0f9095d7cec010f2262eeed565a7d2", "predicted_answer": "", "predicted_evidence": ["Scene 3.2 is usually attributed to both Shakespeare and Fletcher. All three sets of models support this. While Spedding and other authors locate the shift to line 2081, all our sets of models indicate that it occurred later. Combined models locate it precisely at line 2200 (in agreement with earlier studies by Merriam BIBREF25, BIBREF26. A certain decrease in the probability of Shakespeare\u2019s authorship found in the neighborhood of line 2081 in word-based models and combined models may support Merriam\u2019s later attributions BIBREF27, i.e. mixed authorship even after the line 2081.", "Scenes 5.2, 5.3, 5.4 and 5.5 are Fletcher\u2019s according to word-based models and combined models. Rhythmic types indicate the possibility of Shakespeare\u2019s share in 5.4.", "are being classified but instead its overlapping parts of fixed length. Assume a text which one supposes to be a result of a collaboration between two (or more) authors consisting of $n$ lines $l_1, l_2, l_3, \\ldots , l_{n}$. Let $k$ and $d$ be arbitrarily chosen values so that $k \\in \\mathbb {N}$, $k < n$ and $d \\in \\mathbb {N}$, $d < n - k$, $d \\le k$. For each $i; i \\in \\lbrace 0, d, 2d, 3d, \\ldots \\rbrace , i < n - k$ a battery of attributions is performed of all the sections s consisting of lines $l_{i+1}, l_{i+2}, l_{i+3}, \\ldots , l_{i+k}$. To achieve a better sensitivity to authorship transitions Eder suggests not to work with simple predictions (labeling the section as being written by a single author) but\u2014if it\u2019s possible with a given classifier\u2014rather a probability distribution over candidate authors.", "Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work. The only place where Shakespeare comes closer to Fletcher\u2019s values is the first scene of act 5 of Bonduca. Having only 10 groups misattributed out of 4412 we may estimate the accuracy of rolling attribution to be as high as 0.9977 when distinguishing between Shakespeare and Fletcher.", "Scenes 2.3 and 2.4 are according to all sets of models authored by Shakespeare. All three sets of models indicate that the shift happened at the end of scene 2.4."]}
{"question_id": "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "predicted_answer": "", "predicted_evidence": ["The use of notes written by healthcare providers in the clinical settings has long been recognized to be a source of valuable information for clinical practice and medical research. Access to large quantities of clinical reports may help in identifying causes of diseases, establishing diagnoses, detecting side effects of beneficial treatments, and monitoring clinical outcomes BIBREF0 , BIBREF1 , BIBREF2 . The goal of clinical natural language processing (NLP) is to develop and apply computational methods for linguistic analysis and extraction of knowledge from free text reports BIBREF3 , BIBREF4 , BIBREF5 . But while the benefits of clinical NLP and data mining have been universally acknowledged, progress in the development of clinical NLP techniques has been slow. Several contributing factors have been identified, most notably difficult access to data, limited collaboration between researchers from different groups, and little sharing of implementations and trained models BIBREF6 . For comparison, in biomedical NLP, where the working data consist of biomedical research literature, these conditions have been present to a much lesser degree, and the progress has been more rapid BIBREF7 .", "The use of notes written by healthcare providers in the clinical settings has long been recognized to be a source of valuable information for clinical practice and medical research. Access to large quantities of clinical reports may help in identifying causes of diseases, establishing diagnoses, detecting side effects of beneficial treatments, and monitoring clinical outcomes BIBREF0 , BIBREF1 , BIBREF2 . The goal of clinical natural language processing (NLP) is to develop and apply computational methods for linguistic analysis and extraction of knowledge from free text reports BIBREF3 , BIBREF4 , BIBREF5 . But while the benefits of clinical NLP and data mining have been universally acknowledged, progress in the development of clinical NLP techniques has been slow. Several contributing factors have been identified, most notably difficult access to data, limited collaboration between researchers from different groups, and little sharing of implementations and trained models BIBREF6 .", "Because of legal and institutional concerns arising from the sensitivity of clinical data, it is difficult for the NLP community to gain access to relevant data BIBREF9 , BIBREF10 . This is especially true for the researchers not connected with a healthcare organization. Corpora with transparent access policies that are within reach of NLP researchers exist, but are few. An often used corpus is MIMICII(I) BIBREF11 , BIBREF12 . Despite its large size (covering over 58,000 hospital admissions), it is only representative of patients from a particular clinical domain (the intensive care in this case) and geographic location (a single hospital in the United States). Assuming that such a specific sample is representative of a larger population is an example of sampling bias (we discuss further sources of bias in section \"Social impact and biases\" ). Increasing the size of a sample without recognizing that this sample is atypical for the general population (e.g.", "Although it is a necessary first step in protecting the privacy of patients, sanitization has been criticized for several reasons. First, it affects the integrity of the data, and as a consequence, their utility BIBREF23 . Second, although sanitization in principle promotes data access and sharing, it may often not be sufficient to eliminate the need for consent. This is largely due to the well-known fact that original sensitive data can be re-identified through deductive disclosure BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 . Finally, sanitization focuses on protecting the individual, whereas ethical harms are still possible on the group level BIBREF30 , BIBREF31 . Instead of working towards increasingly restrictive sanitization and access measures, another course of action could be to work towards heightening the perception of scientific work, emphasizing professionalism and existence of punitive measures for illegal actions BIBREF32 , BIBREF33 .", "paragraph4 0.9ex plus1ex minus.2ex-1em Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports BIBREF13 . The bias of missing explanatory factors because they can not be identified within the given experimental setting is also known as the streetlight effect. In certain cases, we could obtain important prior knowledge (e.g. demographic characteristics) from data other than clinical notes."]}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "predicted_answer": "", "predicted_evidence": ["The ethics discussion is gaining momentum in general NLP BIBREF8 . We aim in this paper to gather the ethical challenges that are especially relevant for clinical NLP, and to stimulate discussion about those in the broader NLP community. Although enhancing privacy through restricted data access has been the norm, we do not only discuss the right to privacy, but also draw attention to the social impact and biases emanating from clinical notes and their processing. The challenges we describe here are in large part not unique to clinical NLP, and are applicable to general data science as well.", "paragraph4 0.9ex plus1ex minus.2ex-1em Consent Clinical NLP typically requires a large amount of clinical records describing cases of patients with a particular condition. Although obtaining consent is a necessary first step, obtaining explicit informed consent from each patient can also compromise the research in several ways. First, obtaining consent is time consuming by itself, and it results in financial and bureaucratic burdens. It can also be infeasible due to practical reasons such as a patient's death. Next, it can introduce bias as those willing to grant consent represent a skewed population BIBREF34 . Finally, it can be difficult to satisfy the informedness criterion: Information about the experiment sometimes can not be communicated in an unambiguous way, or experiments happen at speed that makes enacting informed consent extremely hard BIBREF35 .", "paragraph4 0.9ex plus1ex minus.2ex-1em Sanitization Sanitization techniques are often seen as the minimum requirement for protecting individuals' privacy when collecting data BIBREF21 , BIBREF22 . The goal is to apply a procedure that produces a new version of the dataset that looks like the original for the purposes of data analysis, but which maintains the privacy of those in the dataset to a certain degree, depending on the technique. Documents can be sanitized by replacing, removing or otherwise manipulating the sensitive mentions such as names and geographic locations. A distinction is normally drawn between anonymization, pseudonymization and de-identification. We refer the reader to Polonetsky et al. PolonetskyEtAl2016 for an excellent overview of these procedures.", "This is especially true for the researchers not connected with a healthcare organization. Corpora with transparent access policies that are within reach of NLP researchers exist, but are few. An often used corpus is MIMICII(I) BIBREF11 , BIBREF12 . Despite its large size (covering over 58,000 hospital admissions), it is only representative of patients from a particular clinical domain (the intensive care in this case) and geographic location (a single hospital in the United States). Assuming that such a specific sample is representative of a larger population is an example of sampling bias (we discuss further sources of bias in section \"Social impact and biases\" ). Increasing the size of a sample without recognizing that this sample is atypical for the general population (e.g. not all patients are critical care patients) could also increase sampling bias BIBREF13 . We need more large corpora for various medical specialties, narrative types, as well as languages and geographic areas.", "Although it is a necessary first step in protecting the privacy of patients, sanitization has been criticized for several reasons. First, it affects the integrity of the data, and as a consequence, their utility BIBREF23 . Second, although sanitization in principle promotes data access and sharing, it may often not be sufficient to eliminate the need for consent. This is largely due to the well-known fact that original sensitive data can be re-identified through deductive disclosure BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 . Finally, sanitization focuses on protecting the individual, whereas ethical harms are still possible on the group level BIBREF30 , BIBREF31 . Instead of working towards increasingly restrictive sanitization and access measures, another course of action could be to work towards heightening the perception of scientific work, emphasizing professionalism and existence of punitive measures for illegal actions BIBREF32 , BIBREF33 ."]}
{"question_id": "8126c6b8a0cab3e22661d3d71d96aa57360da65c", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 indicates how much the INLINEFORM1 -th paragraph INLINEFORM2 from the source section INLINEFORM3 contributes to generating the INLINEFORM4 -th word in target heading INLINEFORM5 , and is usually computed as: DISPLAYFORM0", "In this work, we formulate the OG task as a hierarchical structured prediction problem and introduce a novel hierarchical structured neural generation model, named HiStGen, to solve it. In this model, we view the section boundary prediction problem as a first-level sequential labeling process, and the section heading generation as a second-level structured prediction which depends on the predicted boundary labels from the lower level. For section identification, we employ a Markov paragraph dependency mechanism to model the coherence in adjacent paragraphs to help decide the section boundaries. For section heading generation, we leverage a section-aware attention mechanism BIBREF10 to allow the decoder to focus on the most informative content within a section for heading generation. Furthermore, we introduce a Markov heading dependency mechanism and a review mechanism BIBREF11 between context headings. The Markov heading dependency mechanism is used for modeling the consistency between adjacent headings, while the review mechanism is employed to avoid the repetition in the generated headings.", "All these methods employ the section-aware attention in generation. The performance under Rouge INLINEFORM7 is shown in Table TABREF68 . We can find that: (1) Hier performs worst among all the methods, showing that the independence between context headings is not good for section heading generation. (2) By incorporating all the previous generated headings to model the dependence between context headings, INLINEFORM8 shows slight improvements on the heading generation performance. It indicates that the global dependency may not be effective in heading generation by involving too much context information, and also leads to high computing complexity. (3) The improvements of INLINEFORM9 over INLINEFORM10 indicate that the dependency between adjacent headings is sufficient for generating good and consistent section headings. (4) The improvements of INLINEFORM11 over INLINEFORM12 demonstrate that the review mechanism is also helpful in improving the quality of section heading generation. All the improvements over the baselines are statistically significant (p-value INLINEFORM13 0.01).", "In this section, we conduct experiments to verify the effectiveness of our proposed model.", "Here we suppose that sections in an article are already given, and only need to predict the corresponding headings for each section. We consider two generative baselines INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 is an extension of INLINEFORM3 by employing a global heading dependency mechanism. We then introduce our Markov heading dependency mechanism based on the INLINEFORM4 , named Hier INLINEFORM5 , and further employ the review mechanism, named Hier INLINEFORM6 . All these methods employ the section-aware attention in generation. The performance under Rouge INLINEFORM7 is shown in Table TABREF68 . We can find that: (1) Hier performs worst among all the methods, showing that the independence between context headings is not good for section heading generation. (2) By incorporating all the previous generated headings to model the dependence between context headings, INLINEFORM8 shows slight improvements on the heading generation performance. It indicates that the global dependency may not be effective in heading generation by involving too much context information, and also leads to high computing complexity."]}
{"question_id": "2f01d3e5120d1fef4b01028536cb5fe0abad1968", "predicted_answer": "", "predicted_evidence": ["Given a multi-paragraph document, the OG task aims to unveil its inherent content structure, i.e., to identify the potential sections (i.e., sequential paragraphs with coherent information/topics) of the document, as well as to generate the section headings (i.e., a short description that summarizes the section) correctly. Specifically, headings over different sections should be consistent in style and exclusive on topics, i.e., they should cover different aspects in a similar style. For example, as shown in Figure FIGREF3 (b), headings in a research paper might include introduction, related work, method and so on. These headings are exclusive to each other and mainly describe the function of each section in the paper.", "Storyline generation aims to summarize the development of certain events and understand how events evolve over time. BIBREF32 BIBREF32 formalized different types of sub-events into local and global aspects. Some studies have been conducted in storyline generation with Bayesian networks to detect storylines BIBREF33 , BIBREF34 . BIBREF35 BIBREF35 firstly obtained relevant tweets and then generate storylines via graph optimization for the Tweets2011 corpus.", "The results are shown in Figure FIGREF65 . We can find that: (1) The improvements of INLINEFORM2 over INLINEFORM3 , showing that the consideration of the previous and successive paragraph is better than the consideration of all the paragraphs in a document for section boundary prediction. The reason might be by considering all the paragraphs, INLINEFORM4 tends to bring noisy information that may hurt the prediction on section boundaries. Moreover, INLINEFORM5 leads to much higher computing complexity than INLINEFORM6 (i.e., INLINEFORM7 ). (2) INLINEFORM8 performs better than INLINEFORM9 , demonstrating that depending on the semantic representations of the previous and successive paragraph is more beneficial than only depending on the labels of the previous and successive paragraph in section boundary prediction. All the improvements over the baselines are statistically significant (p-value < 0.01).", "where INLINEFORM0 is defined as DISPLAYFORM0", "Section-Aware Attention Mechanism. The key idea of the section-aware attention mechanism is to make the generation of a section heading focusing on the target section. Concretely, as shown in Figure FIGREF21 , we maintain a section-aware context vector INLINEFORM0 for generating the INLINEFORM1 -th word INLINEFORM2 in the INLINEFORM3 -th heading INLINEFORM4 . Based on the INLINEFORM5 -th section INLINEFORM6 , INLINEFORM7 is a weighted sum of the hidden representations of all the paragraphs in INLINEFORM8 : DISPLAYFORM0"]}
{"question_id": "b78bb6fe817c2d4bc69236df998f546e94c3ee21", "predicted_answer": "", "predicted_evidence": ["We assess Affect-LM's ability to generate emotionally colored text of varying degrees without severely deteriorating grammatical correctness, by conducting an extensive perception study on Amazon's Mechanical Turk (MTurk) platform. The MTurk platform has been successfully used in the past for a wide range of perception experiments and has been shown to be an excellent resource to collect human ratings for large studies BIBREF27 . Specifically, we generated more than 200 sentences for four sentence beginnings (namely the three sentence beginnings listed in Table 2 as well as an end of sentence token indicating that the model should generate a new sentence) in five affect categories happy(positive emotion), angry, sad, anxiety, and negative emotion. The Affect-LM model trained on the Fisher corpus was used for sentence generation. Each sentence was evaluated by two human raters that have a minimum approval rating of 98% and are located in the United States. The human raters were instructed that the sentences should be considered to be taken from a conversational rather than a written context: repetitions and pause fillers (e.g., um, uh) are common and no punctuation is provided.", "Negative Emotion Sentences. The multivariate result was significant for negative emotion generated sentences (Pillai's Trace $=$ .130, F(4,413) $=$ 2.30, p $<$ .0005). Follow up ANOVAs revealed significant results for affective valence and happy DVs with p $<$ .0005, indicating that the affective valence DV was successfully manipulated with $\\beta $ , as seen in Figure 2 (b). Further, as intended there were no significant differences for DVs angry, sad and anxious, indicating that the negative emotion DV refers to a more general affect related concept rather than a specific negative emotion. This finding is in concordance with the intended LIWC category of negative affect that forms a parent category above the more specific emotions, such as angry, sad, and anxious BIBREF11 .", "Angry Sentences. The multivariate result was significant for angry generated sentences (Pillai's Trace $=$ .199, F(4,433) $=$ 3.76, p $<$ .0001). Follow up ANOVAs revealed significant results for affective valence, happy, and angry DVs with p $<$ .0001, indicating that both affective valence and angry DVs were successfully manipulated with $\\beta $ , as seen in Figure 2 (c). Grammatical correctness was not significantly influenced by the affect strength parameter $\\beta $ , which indicates that angry sentences are highly stable across a wide range of $\\beta $ (see Figure 3 ). However, it seems that human raters could not successfully distinguish between angry, sad, and anxious affect categories, indicating that the generated sentences likely follow a general negative affect dimension.", "$\\mathbf {f(.)}$ is the output of an LSTM network which takes in the context words $w_1, w_2,...,w_{t-1}$ as inputs through one-hot representations, $\\mathbf {U}$ is a matrix of word representations which on visualization we have found to correspond to POS (Part of Speech) information, while $\\mathbf {b_i}$ is a bias term capturing the unigram occurrence of word $i$ . Equation 5 expresses the word $w_t$ as a function of its context for a LSTM language model which does not utilize any additional affective information.", "$$P(w_1, w_2,..., w_M) = \\prod _{t=1}^{t=M} P(w_t|w_1, w_2,...., w_{t-1})$$   (Eq. 4)"]}
{"question_id": "1a419468d255d40ae82ed7777618072a48f0091b", "predicted_answer": "", "predicted_evidence": ["For each target emotion (i.e., intended emotion of generated sentences) we conducted an initial MANOVA, with human ratings of affect categories the DVs (dependent variables) and the affect strength parameter $\\beta $ the IV (independent variable). We then conducted follow-up univariate ANOVAs to identify which DV changes significantly with $\\beta $ . In total we conducted 5 MANOVAs and 30 follow-up ANOVAs, which required us to update the significance level to p $<$ 0.001 following a Bonferroni correction.", "This material is based upon work supported by the U.S. Army Research Laboratory under contract number W911NF-14-D-0005. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Government, and no official endorsement should be inferred. Sayan Ghosh also acknowledges the Viterbi Graduate School Fellowship for funding his graduate studies.", "In contrast, previous literature on affective language generation has not focused sufficiently on customizable state-of-the-art neural network techniques to generate emotional text, nor have they quantitatively evaluated their models on multiple emotionally colored corpora. BIBREF16 mahamood2011generating use several NLG (natural language generation) strategies for producing affective medical reports for parents of neonatal infants undergoing healthcare. While they study the difference between affective and non-affective reports, their work is limited only to heuristic based systems and do not include conversational text. BIBREF17 mairesse2007personage developed PERSONAGE, a system for dialogue generation conditioned on extraversion dimensions. They trained regression models on ground truth judge's selections to automatically determine which of the sentences selected by their model exhibit appropriate extroversion attributes. In BIBREF18 keshtkar2011pattern, the authors use heuristics and rule-based approaches for emotional sentence generation. Their generation system is not training on large corpora and they use additional syntactic knowledge of parts of speech to create simple affective sentences.", "In Equation 7 , Affect-LM learns a weight matrix $\\mathbf {V}$ which captures the correlation between the predicted word $w_t$ , and the affect category $\\mathbf {e_{t-1}}$ . Thus, each row of the matrix $\\mathbf {V_i}$ is an emotionally meaningful embedding of the $i$ -th word in the vocabulary. In Figure 4 , we present a visualization of these embeddings, where each data point is a separate word, and words which appear in the LIWC dictionary are colored based on which affect category they belong to (we have labeled only words in categories positive emotion, negative emotion, anger, sad and anxiety since these categories contain the most frequent words). Words colored grey are those not in the LIWC dictionary. In Figure 4 , we observe that the embeddings contain affective information, where the positive emotion is highly separated from the negative emotions (sad, angry, anxiety) which are clustered together.", "Affect-LM can be used to generate sentences conditioned on the input affect category, the affect strength $\\beta $ , and the context words. For our experiments, we have chosen the following affect categories - positive emotion, anger, sad, anxiety, and negative emotion (which is a superclass of anger, sad and anxiety). As described in Section \"Conclusions and Future Work\" , the affect strength $\\beta $ defines the degree of dominance of the affect-dependent energy term on the word prediction in the language model, consequently after model training we can change $\\beta $ to control the degree of how \u201cemotionally colored\" a generated utterance is, varying from $\\beta =0$ (neutral; baseline model) to $\\beta =\\infty $ (the generated sentences only consist of emotionally colored words, with no grammatical structure). When Affect-LM is used for generation, the affect categories could be either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$ (this is obtained by setting $\\mathbf {e}$ to a binary vector encoding the desired emotion and works even for neutral sentence beginnings)."]}
{"question_id": "52f5249a9a2cb7210eeb8e52cb29d18912f6c3aa", "predicted_answer": "", "predicted_evidence": ["The total number of technique instances found in the articles, after the consolidation phase, is $7,485$, out of a total number of $21,230$ sentences (35.2%). The distribution of the techniques in the corpus is also uneven: while there are $2,547$ occurrences of loaded language, there are only 15 instances of straw man (more statistics about the corpus can be found in BIBREF10). We define two tasks based on the corpus described in Section SECREF2: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularity, $g_1$ and $g_2$, namely tokens for FLC and sentences for SLC.", "We now define variants of precision and recall able to account for the imbalance in the corpus:", "Let $s$ and $t$ be two fragments, i.e., sequences of characters. We measure the overlap of two annotated fragments as $ C(s,t,h) = \\frac{|(s\\cap t)|}{h}\\delta \\left(l(s), l(t) \\right)$, where $h$ is a normalizing factor, $l(a)$ is the labelling of fragment $a$, and $\\delta (a,b)=1$ if $a=b$, and 0 otherwise.", "Again, we use BERT BIBREF12 for the contextualized embedding layer and we place the multi-granularity network on top of it.", "In eq. (DISPLAY_FORM4), we define $P(S,T)$ to be zero if $|S|=0$ and $R(S,T)$ to be zero if $|T|=0$. Finally, we compute the harmonic mean of precision and recall in Eq. (DISPLAY_FORM4) and we obtain an F$_1$-measure. Having a separate function $C$ for comparing two annotations gives us additional flexibility compared to standard NER measures that operate at the token/character level, e.g., we can change the factor that gives credit for partial overlaps and be more forgiving when only a few characters are wrong."]}
{"question_id": "baad4b6f834d5944f61bd12f30908e3cf3739dcd", "predicted_answer": "", "predicted_evidence": ["If $w_{g_{k}}=0$ for a given example, the output of the next granularity task $o_{g_{k+1}}$ would be 0 as well. In our setting, this means that, if the sentence-level classifier is confident that the sentence does not contain propaganda, i.e., $w_{g_{k}}=0$, then $o_{g_{k+1}}=0$ and there would be no propagandistic technique predicted for any span within that sentence. Similarly, when back-propagating the error, if $w_{g_{k}}=0$ for a given example, the final entropy loss would become zero, i.e., the model would not get any information from that example. As a result, only examples strongly classified as negative in a lower-granularity task would be ignored in the high-granularity task. Having the lower-granularity as the main task means that higher-granularity information can be selectively used as additional information to improve the performance, but only if the example is not considered as highly negative.", "The total number of technique instances found in the articles, after the consolidation phase, is $7,485$, out of a total number of $21,230$ sentences (35.2%). The distribution of the techniques in the corpus is also uneven: while there are $2,547$ occurrences of loaded language, there are only 15 instances of straw man (more statistics about the corpus can be found in BIBREF10). We define two tasks based on the corpus described in Section SECREF2: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularity, $g_1$ and $g_2$, namely tokens for FLC and sentences for SLC.", "More generally, suppose there are $k$ tasks of increasing granularity, e.g., document-level, paragraph-level, sentence-level, word-level, subword-level, character-level. Each task has a separate classification layer $L_{g_k}$ that receives the feature representation of the specific level of granularity $g_k$ and outputs $o_{g_k}$. The dimension of the representation depends on the embedding layer, while the dimension of the output depends on the number of classes in the task. The output $o_{g_k}$ is used to generate a weight for the next granularity task $g_{k+1}$ through a trainable gate $f$:", "The total number of technique instances found in the articles, after the consolidation phase, is $7,485$, out of a total number of $21,230$ sentences (35.2%). The distribution of the techniques in the corpus is also uneven: while there are $2,547$ occurrences of loaded language, there are only 15 instances of straw man (more statistics about the corpus can be found in BIBREF10). We define two tasks based on the corpus described in Section SECREF2: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularity, $g_1$ and $g_2$, namely tokens for FLC and sentences for SLC. We split the corpus into training, development and test, each containing 293, 57, 101 articles and 14,857, 2,108, 4,265 sentences, respectively.", "Our corpus could enable research in propagandistic and non-objective news, including the development of explainable AI systems. A system that can detect instances of use of specific propagandistic techniques would be able to make it explicit to the users why a given article was predicted to be propagandistic. It could also help train the users to spot the use of such techniques in the news."]}
{"question_id": "37b972a3afae04193411dc569f672d802c16ad71", "predicted_answer": "", "predicted_evidence": ["We have argued for a new way to study propaganda in news media: by focusing on identifying the instances of use of specific propaganda techniques. Going at this fine-grained level can yield more reliable systems and it also makes it possible to explain to the user why an article was judged as propagandistic by an automatic system.", "We used the PyTorch framework and the pretrained BERT model, which we fine-tuned for our tasks. To deal with class imbalance, we give weight to the binary cross-entropy according to the proportion of positive samples. For the $\\alpha $ in the joint loss function, we use 0.9 for sentence classification, and 0.1 for word-level classification. In order to reduce the effect of random fluctuations for BERT, all the reported numbers are the average of three experimental runs with different random seeds. As it is standard, we tune our models on the dev partition and we report results on the test partition.", "The left side of Table TABREF12 shows the performance for the three baselines and for our multi-granularity network on the FLC task. For the latter, we vary the degree to which the gate function is applied: using ReLU is more aggressive compared to using the Sigmoid, as the ReLU outputs zero for a negative input. Table TABREF12 (right) shows that using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification.", "We experimented with a number of BERT-based models and devised a novel architecture which outperforms standard BERT-based baselines. Our fine-grained task can complement document-level judgments, both to come out with an aggregated decision and to explain why a document \u2014or an entire news outlet\u2014 has been flagged as potentially propagandistic by an automatic system.", "For the loss function, we use a cross-entropy loss with sigmoid activation for every layer, except for the highest-granularity layer $L_{g_K}$, which uses a cross-entropy loss with softmax activation. Unlike softmax, which normalizes over all dimensions, the sigmoid allows each output component of layer $L_{g_k}$ to be independent from the rest. Thus, the output of the sigmoid for the positive class increases the degree of freedom by not affecting the negative class, and vice versa. As we have two tasks, we use sigmoid activation for $L_{g_1}$ and softmax activation for $L_{g_2}$. Moreover, we use a weighted sum of losses with a hyper-parameter $\\alpha $:"]}
{"question_id": "a01af34c7f630ba0e79e0a0120d2e1c92d022df5", "predicted_answer": "", "predicted_evidence": ["This research is part of the Propaganda Analysis Project, which is framed within the Tanbih project. The Tanbih project aims to limit the effect of \u201cfake news\u201d, propaganda, and media bias by making users aware of what they are reading, thus promoting media literacy and critical thinking. The project is developed in collaboration between the Qatar Computing Research Institute (QCRI), HBKU and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).", "We used the PyTorch framework and the pretrained BERT model, which we fine-tuned for our tasks. To deal with class imbalance, we give weight to the binary cross-entropy according to the proportion of positive samples. For the $\\alpha $ in the joint loss function, we use 0.9 for sentence classification, and 0.1 for word-level classification. In order to reduce the effect of random fluctuations for BERT, all the reported numbers are the average of three experimental runs with different random seeds. As it is standard, we tune our models on the dev partition and we report results on the test partition.", "The left side of Table TABREF12 shows the performance for the three baselines and for our multi-granularity network on the FLC task. For the latter, we vary the degree to which the gate function is applied: using ReLU is more aggressive compared to using the Sigmoid, as the ReLU outputs zero for a negative input. Table TABREF12 (right) shows that using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification.", "Journalistic organisations, such as Media Bias/Fact Check, provide reports on news sources highlighting the ones that are propagandistic. Obviously, such analysis is time-consuming and possibly biased and it cannot be applied to the enormous amount of news that flood social media and the Internet. Research on detecting propaganda has focused primarily on classifying entire articles as propagandistic/non-propagandistic BIBREF0, BIBREF1, BIBREF2. Such learning systems are trained using gold labels obtained by transferring the label of the media source, as per Media Bias/Fact Check judgment, to each of its articles. Such distant supervision setting inevitably introduces noise in the learning process BIBREF3 and the resulting systems tend to lack explainability.", "Our corpus could enable research in propagandistic and non-objective news, including the development of explainable AI systems. A system that can detect instances of use of specific propagandistic techniques would be able to make it explicit to the users why a given article was predicted to be propagandistic. It could also help train the users to spot the use of such techniques in the news."]}
{"question_id": "0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "predicted_answer": "", "predicted_evidence": ["Our task requires specific evaluation measures that give credit for partial overlaps of fragments. Thus, in our precision and recall versions, we give partial credit to imperfect matches at the character level, as in plagiarism detection BIBREF11.", "We experimented with a number of BERT-based models and devised a novel architecture which outperforms standard BERT-based baselines. Our fine-grained task can complement document-level judgments, both to come out with an aggregated decision and to explain why a document \u2014or an entire news outlet\u2014 has been flagged as potentially propagandistic by an automatic system.", "BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.", "BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b).", "BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC. Instead of using only the $L_{g_2}$ layer for FLC, we concatenate $L_{g_1}$ and $L_{g_2}$, and we add an extra 19-dimensional classification layer $L_{g_{1,2}}$ on top of that concatenation to perform the prediction for FLC (cf. Figure FIGREF7-c)."]}
{"question_id": "7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "predicted_answer": "", "predicted_evidence": ["In a study on how phrasing affects memorability, BIBREF9 take a language model approach to measure the distinctiveness of memorable movie quotes. They do this by evaluating a quote with respect to a \u201ccommon language\u201d model built from the newswire sections of the Brown corpus BIBREF10 . They find that movie quotes which are less like \u201ccommon language\u201d are more distinctive and therefore more memorable. The intuition behind our approach is that humor should in some way be memorable or distinct, and so tweets that diverge from a \u201ccommon language\u201d model would be expected to be funnier.", "These results suggest that there are only slight differences between bigram and trigram models, and that the type and quantity of corpora used to train the models is what really determines the results.", "Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post\u2013evaluation runs than the tweet data.", "Tokenization: Text in all training data was split on white space and punctuation", "Our system estimated tweet probability using N-gram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:"]}
{"question_id": "8b1af67e3905244653b4cf66ba0acec8d6bff81f", "predicted_answer": "", "predicted_evidence": ["The tweet data was significantly smaller than the news data, and so certainly we believe that this was a factor in the performance during the evaluation stage, where the models built from the news data were significantly more effective. Going forward we plan to collect more tweet data, particularly those that participate in #HashtagWars. We also intend to do some experiments where we cut the amount of news data and then build models to see how those compare.", "The tweet data was provided by the task organizers. It consists of 106 hashtag files made up of about 21,000 tokens. The hashtag files were further divided into a development set trial_dir of 6 hashtags and a training set of 100 hashtags train_dir. We also obtained 6.2 GB of English news data with about two million tokens from the News Commentary Corpus and the News Crawl Corpus from 2008, 2010 and 2011. Each tweet and each sentence from the news data is found on a single line in their respective files.", "Our system estimated tweet probability using N-gram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:", "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps BIBREF7 . A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet", "We use KenLM BIBREF11 as our language modeling tool. Language models are estimated using modified Kneser-Ney smoothing without pruning. KenLM also implements a back-off technique so if an N-gram is not found, KenLM applies the lower order N-gram's probability along with its back-off weights."]}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "predicted_answer": "", "predicted_evidence": ["Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps BIBREF7 . A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet", "SemEval-2017 Task 6 BIBREF6 also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program.", "Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post\u2013evaluation runs than the tweet data.", "\u201ctears\u201d, \u201cin\u201d, \u201cRamen\u201d and \u201c#SingleLifeIn3Words\u201d are unigrams; \u201ctears in\u201d, \u201cin Ramen\u201d and \u201cRamen #SingleLifeIn3Words\u201d are bigrams and \u201ctears in Ramen\u201d and \u201cin Ramen #SingleLifeIn3Words\u201d are trigrams.", "Pre-processing consists of two steps: filtering and tokenization. The filtering step was only for the tweet training corpus. We experimented with various filtering and tokenziation combinations during the development stage to determine the best setting."]}
{"question_id": "3605ea281e72e9085a0ac0a7270cef25fc23063f", "predicted_answer": "", "predicted_evidence": ["We relied on bigram and trigram language models because tweets are short and concise, and often only consist of just a few words.", "tears in Ramen #SingleLifeIn3Words", "Our system estimated tweet probability using N-gram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:", "An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0", "For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a \u201c1\u201d. If the second tweet is funnier it outputs the tweet_ids followed by a \u201c0\u201d. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest."]}
{"question_id": "21f6cb3819c85312364dd17dd4091df946591ef0", "predicted_answer": "", "predicted_evidence": ["During the development of our system we trained our language models solely on the 100 hashtag files from train_dir and then evaluated our performance on the 6 hashtag files found in trial_dir. That data was formatted such that each tweet was found on a single line.", "In order to evaluate how funny a tweet is, we train language models on two datasets: the tweet data and the news data. Tweets that are more probable according to the tweet data language model are ranked as being funnier. However, tweets that have a lower probability according to the news language model are considered the funnier since they are the least like the (unfunny) news corpus. We relied on both bigrams and trigrams when training our models.", "Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post\u2013evaluation runs than the tweet data.", "SemEval-2017 Task 6 BIBREF6 also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program.", "For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a \u201c1\u201d. If the second tweet is funnier it outputs the tweet_ids followed by a \u201c0\u201d. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest."]}
{"question_id": "fd8a8eb69f07c584a76633f8802c2746f7236d64", "predicted_answer": "", "predicted_evidence": ["To our surprise, F1 score gap was lowest for hypernym, which we predicted would have a higher gap like that for spouse. Also surprisingly, F1 gap for birthPlace was almost as high as that for spouse. While all the model exhibited bias in predictions for all relations, we note that using gender-swapping and debiasing embeddings were able to significantly mitigate the gap in F1 scores for the model's predictions on male and female sentences. However, while the F1 score gap for birthPlace responded strongly to debiasing methods, spouse did not respond as strongly. Gender-swapping was able to bolster the model's absolute F1 scores as well. Thus, we note that mitigating context bias worked extremely well in this case. Name anonymization was as effective and actually increased gender bias for hypernym; it seems removing entity bias increased F1 score gap for hypernym. We note that the best combination for both bias mitigation and absolute model performance was using gender-swapping on its own.", "We also utilize Equality of Opportunity BIBREF7. In our case $A = \\lbrace male,female\\rbrace $, because gender is our protected attribute and we assume it to be binary. We evaluate EOP on a per-relation, one-versus-rest basis. Thus, we calculate one EOP where spouse is the positive class and all other classes are negative; in this case, $Y=1$ corresponds to the true-label being spouse and $Y=0$ corresponds to the true label being hypernym, birthDate, birthPlace, or NA. We then do another calculation for each relation where $Y=1$ corresponds to that relation being expressed and $Y=0$ corresponds to any other relation being expressed. Note that this is equivalent to measuring per-relation recall for each gender.", "We partition the test set into two subsets: one with sentences from female articles, and one with sentences from male articles (see Table TABREF6). We collect data using our variant of the distant supervision assumption (see Section SECREF7). However, as noted earlier, some sentences can be noisy. Evaluating models on noisy data is unfair since a model could be penalized for correctly predicting the relation is not expressed in the sentence. Thus, we had to obtain ground truth labels.", "Word embeddings can encode gender biases BIBREF8, BIBREF31, BIBREF32 and this can affect bias in downstream predictions for models using the embeddings BIBREF10. Hard-Debiasing mitigates gender bias in embeddings. Hard-Debiasing involves finding a direction representing gender in the vector space, then removing the component on that direction for all gender-neutral words, then equalizing the distance from that direction for all (masculine, feminine) word pairs BIBREF8. We applied hard-debiasing to Word2Vec embeddings BIBREF29 we trained on the sentences in WikiGenderBias. Every time we applied CDA or NA or some combination of the two, we trained a new embedding model on that debiased dataset as well.", "It is also worth noting that the average selector performed slightly better than the attention selector across the board, which is intriguing considering that the average selector is used as a baseline since it weights sentences in the training data equally for each relation."]}
{"question_id": "452e978bd597411b65be757bf47dc6a78f3c67c9", "predicted_answer": "", "predicted_evidence": ["Aggregate Results Thus, throughout all combinations of debiasing options, the PCNN with Attention model attains better F1 score for the spouse relation when predicting on male sentences than for female sentences. For birthplace, F1 score gap is far lower as we predicted. To our surprise, F1 score gap was lowest for hypernym, which we predicted would have a higher gap like that for spouse. Also surprisingly, F1 gap for birthPlace was almost as high as that for spouse. While all the model exhibited bias in predictions for all relations, we note that using gender-swapping and debiasing embeddings were able to significantly mitigate the gap in F1 scores for the model's predictions on male and female sentences. However, while the F1 score gap for birthPlace responded strongly to debiasing methods, spouse did not respond as strongly. Gender-swapping was able to bolster the model's absolute F1 scores as well.", "With the wealth of information being posted online daily, Relation Extraction (RE) has become increasingly important. RE aims specifically to extract relations from raw sentences and represent them as succinct relation tuples of the form (head, relation, tail). An example is (Barack Obama, spouse, Michelle Obama).", "Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions.", "While these findings will help future work avoid gender biases, this study is preliminary. We only consider binary gender, but future work should consider non-binary genders. Additionally, future work should further probe the source of gender bias in the model's predictions, perhaps by visualizing attention or looking more closely at the model's outputs.", "In RE, using supervised machine learning models has become popular. Training data for these models is typically obtained using the Distant Supervision or a variation: for a given relation (e1, r, e2) in a KB, assume any sentence that contains both e1 and e2 expresses r BIBREF20. Many NRE models focus on mitigating the effects of noise in the training data introduced by Distant Supervision to increase performance BIBREF21, BIBREF22, BIBREF2, BIBREF3, BIBREF5. Recent work uses KBs to further increase NRE performance BIBREF6, BIBREF23. Despite these significant efforts towards improving NRE performance, there are no studies on bias or ethics in NRE to our knowledge. We provide such a study."]}
{"question_id": "159025c44c0115ab4cdc253885384f72e592e83a", "predicted_answer": "", "predicted_evidence": ["With the wealth of information being posted online daily, Relation Extraction (RE) has become increasingly important. RE aims specifically to extract relations from raw sentences and represent them as succinct relation tuples of the form (head, relation, tail). An example is (Barack Obama, spouse, Michelle Obama).", "WikiGenderBias is the first dataset aimed at training and evaluating NRE systems for gender bias. It contains ground truth labels for the test set and about 45,000 sentences in total.", "Combinations Combining debiased embeddings and gender swapping turned out to have the highest relative difference in F1 score between male and female sentences for spouse while also reducing bias in other relations (see Figure FIGREF19). All models which use name anonymization (Models 1-4) have significantly higher F1 score gaps for the hypernym relation. While all combinations reduced gender bias to varying extents, gender bias in the spouse relation was mitigated to a similar extent by all combinations. Surprisingly, applying gender-swapping on its own reduces gender bias about as well or better as any combination of methods.", "We provide the first evaluation of NRE systems for gender bias and find that it exhibits gender bias.", "Aggregate Results Thus, throughout all combinations of debiasing options, the PCNN with Attention model attains better F1 score for the spouse relation when predicting on male sentences than for female sentences. For birthplace, F1 score gap is far lower as we predicted. To our surprise, F1 score gap was lowest for hypernym, which we predicted would have a higher gap like that for spouse. Also surprisingly, F1 gap for birthPlace was almost as high as that for spouse. While all the model exhibited bias in predictions for all relations, we note that using gender-swapping and debiasing embeddings were able to significantly mitigate the gap in F1 scores for the model's predictions on male and female sentences. However, while the F1 score gap for birthPlace responded strongly to debiasing methods, spouse did not respond as strongly. Gender-swapping was able to bolster the model's absolute F1 scores as well. Thus, we note that mitigating context bias worked extremely well in this case."]}
{"question_id": "6590055fb033cb32826f2afecb3d7f607dd97d57", "predicted_answer": "", "predicted_evidence": ["F1 scores between predictions on male and female sentences on all relations differ for every encoder selector combinations, although the difference is relatively small (see the leftmost column in FIGREF16). We find that predictions on spouse typically exhibit the highest difference in F1 score, as we predicted. However, surprisingly, predictions on hypernym exhibit the least gender bias and predictions on birthPlace exhibit more significant gender bias than predictions on spouse in some cases. Predictions on birthDate exhibited very little gender bias, as predicted.", "To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. For instance, if we know (Barack, spouse, Michelle) is a relation and we find the sentence He and Michelle were married in Barack's Wikipedia article, then we assume that sentence expresses the (Barack, spouse, Michelle) relation. This assumption is similar to that made by BIBREF20 and allows us to scalably create the dataset.", "We provide the first evaluation of NRE systems for gender bias and find that it exhibits gender bias.", "As mentioned in Section SECREF2, gender bias can be measured as the difference in a performance metric for a model when evaluated on male and female datapoints. We evaluate the effect these methods have on NRE models using this. We define male (female) datapoints to be relations for which the head entity is male (female), which means the distantly supervised sentence is taken from a male (female) article. Prior work has used area under the precision-recall curve and F1 score to measure NRE model performance BIBREF33, BIBREF27, BIBREF34; following prior work, we use F1 score as our performance metric.", "After discovering gender bias exists, prior work has developed methods to mitigate that bias. Debiasing methods can debias the training set, word embeddings, or the prediction or training algorithms. In the case of training set or training algorithm debiasing, the model must be retrained. We use two training set debiasing methods (Counterfactual Data Augmentation BIBREF10 and Name Anonymization BIBREF10) and a word embedding debiasing method (Hard Debiasing BIBREF8) and analyze their affect on bias in predictions of NRE models."]}
{"question_id": "3435e365adf7866e45670c865dc33bb7d2a6a0c6", "predicted_answer": "", "predicted_evidence": ["In this paper, we take the first step at understanding and evaluating gender bias in NRE systems. We analyze gender bias by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about males. Significant discrepancies in performance between genders could diminish the fairness of systems and distort outcomes in applications that use them. For example, if a model predicts the occupation relation for with higher recall for male entities, this could lead to KBs having more occupation information for males. Downstream search tasks using that KB could produce biased predictions, such as ranking articles about female computer scientists below articles about their male peers.", "After discovering gender bias exists, prior work has developed methods to mitigate that bias. Debiasing methods can debias the training set, word embeddings, or the prediction or training algorithms. In the case of training set or training algorithm debiasing, the model must be retrained. We use two training set debiasing methods (Counterfactual Data Augmentation BIBREF10 and Name Anonymization BIBREF10) and a word embedding debiasing method (Hard Debiasing BIBREF8) and analyze their affect on bias in predictions of NRE models.", "We evaluate NRE models from a popular open-source code repository called OpenNRE BIBREF27. OpenNRE models combine methods including usage of selective attention to add weight to sentences with relevant information BIBREF2 as well as methods to reduce noise at an entity-pair level BIBREF2 and innovations in adversarial training of NRE models BIBREF4. OpenNRE allows users to choose a selector (Attention or Average) and an encoder (PCNN, CNN, RNN, or Bi-RNN) for each model. Each of these models requires word embeddings to create distributed representations of sentences. It should be noted that a PCNN is simply a CNN which has a piecewise max-pooling operation, where the sentence is split into three sections based on the positions of the head and tail entities BIBREF28.", "WikiGenderBias is the first dataset aimed at training and evaluating NRE systems for gender bias. It contains ground truth labels for the test set and about 45,000 sentences in total.", "We demonstrate that using both gender-swapping and debiased embeddings effectively mitigates bias in the model's predictions and that using genderswapping improves the model's performance when the training data contains contextual biases."]}
{"question_id": "cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "predicted_answer": "", "predicted_evidence": ["We performed an error analysis for the best-scoring model BertEmb. The class distribution for claim validation is highly biased towards refuted (false) claims and, therefore, claims are frequently labeled as refuted even though they belong to one of the other two classes (see confusion matrix in the Appendix in Table TABREF45).", "The results in Table TABREF27 show that AtheneMLP scores highest. Similar to the outcome of the Fake News Challenge, feature-based models outperform neural networks based on word embeddings BIBREF19. As the comparison to the human agreement bound suggests, there is still substantial room for improvement.", "The results illustrated in Table TABREF36 show that BertEmb, USE+MLP, BiLSTM, and extendedESIM reach similar performance, with BertEmb being the best. However, compared to the FEVER claim validation problem, where systems reach up to 0.7 F1 macro, the scores are relatively low. Thus, there is ample opportunity for improvement by future systems.", "To evaluate the performance of the models in the ranking setup, we measure the precision and recall on five highest ranked ETS sentences (precision @5 and recall @5), similar to the evaluation procedure used in the FEVER shared task. Table TABREF31 summarizes the performance of several models on our corpus. The rankingESIM BIBREF23 was the best performing model on the FEVER evidence extraction task. The Tf-Idf model BIBREF1 served as a baseline in the FEVER shared task. We also evaluate the performance of DecompAttent and a simple BiLSTM BIBREF24 architecture. To adjust the latter two models to the ranking problem setting, we used the hinge loss objective function with negative sampling as implemented in the rankingESIM model. As in the FEVER shared task, we consider the recall @5 as a metric for the evaluation of the systems.", "We have also found that it is often difficult to classify the claims as the provided FGE in many cases are contradicting (e.g. Appendix SECREF44). Although the corpus is biased towards false claims (Table TABREF23), there is a large number of ETSs that support those false claims (Table TABREF22). As discussed in Section SECREF20, this is because many of the retrieved ETSs originate from false news websites."]}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "predicted_answer": "", "predicted_evidence": ["Below we give an instance of a misclassified ETS. Even though the ETS supports the claim, the lexical overlap is relatively low. Most likely, for this reason, the model predicts refute.", "We have also found that it is often difficult to classify the claims as the provided FGE in many cases are contradicting (e.g. Appendix SECREF44). Although the corpus is biased towards false claims (Table TABREF23), there is a large number of ETSs that support those false claims (Table TABREF22). As discussed in Section SECREF20, this is because many of the retrieved ETSs originate from false news websites.", "We formulate the claim validation problem in such a way that we can compare it to the FEVER recognizing textual entailment task. Thus, as illustrated in Table TABREF34, we compress the different verdicts present on the Snopes webpage into three categories of the FEVER shared task. In order to form the not enough information (NEI) class, we compress the three verdicts mixture, unproven, and undetermined. We entirely omit all the other verdicts like legend, outdated, miscaptioned, as these cases are ambiguous and difficult to classify. For the classification of the claims, we provide only the FGE as they contain the most important information from ETSs.", "FGE Annotation. Similar to the stance annotation, we used the approach of BIBREF12 to compute the agreement. The inter-annotator agreement between the crowd workers in this case is $\\kappa = 0.55$ Cohen's Kappa. We compared the annotations of FGE in 200 ETSs by experts with the annotations by crowd workers, reaching an agreement of $\\kappa = 0.56$. This is considered as moderate inter-annotator agreement BIBREF15.", "We define evidence extraction as the identification of fine-grained evidence (FGE) in the evidence text snippets (ETSs). The problem can be approached in two ways, either as a classification problem, where each sentence from the ETSs is classified as to whether it is an evidence for a given claim, or as a ranking problem, in the way defined in the FEVER shared task. For FEVER, sentences in introductory sections of Wikipedia articles need to be ranked according to their relevance for the validation of the claim and the 5 highest ranked sentences are taken as evidence."]}
{"question_id": "29794bda61665a1fbe736111e107fd181eacba1b", "predicted_answer": "", "predicted_evidence": ["Below we give an instance of a misclassified ETS. Even though the ETS supports the claim, the lexical overlap is relatively low. Most likely, for this reason, the model predicts refute.", "There are a number of experiments beyond the scope of this paper, which are left for future work: (1) retrieval of the original documents (ODCs) given a claim, (2) identification of ETSs in ODCs, and (3) prediction of a claim's verdict on the basis of FGE, the stance of FGE, and their sources.", "Based on our analysis, we conclude that heterogeneous data and FGE from unreliable sources, as found in our corpus and in the real world, make it difficult to correctly classify the claims. Thus, in future experiments, not just FGE need to be taken into account, but also additional information from our newly constructed corpus, that is, the stance of the FGE, FGE sources, and documents from the Snopes website which provide additional information about the claim. Taking all this information into account would enable the system to find a consistent configuration of these labels and thus potentially help to improve performance. For instance, a claim that is supported by evidence coming from an unreliable source is most likely false. In fact, we believe that modeling the meta-information about the evidence and the claim more explicitly represents an important step in making progress in automated fact-checking.", ".", "This work has been supported by the German Research Foundation as part of the Research Training Group \u201dAdaptive Preparation of Information from Heterogeneous Sources\u201d (AIPHES) at the Technische Universit\u00e4t Darmstadt under grant No. GRK 1994/1."]}
{"question_id": "dd80a38e578443496d3720d883ad194ce82c5f39", "predicted_answer": "", "predicted_evidence": ["Claim: As a teenager, U.S. Secretary of State Colin Powell learned to speak Yiddish while working in a Jewish-owned baby equipment store.", "", "We have also found that it is often difficult to classify the claims as the provided FGE in many cases are contradicting (e.g. Appendix SECREF44). Although the corpus is biased towards false claims (Table TABREF23), there is a large number of ETSs that support those false claims (Table TABREF22). As discussed in Section SECREF20, this is because many of the retrieved ETSs originate from false news websites.", "", "The FGE are contradicting and the classifier predicts refuted instead of supported."]}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "predicted_answer": "", "predicted_evidence": ["We performed an error analysis for the best-scoring model AtheneMLP. The error analysis has shown that supporting ETSs are mostly classified correctly if there is a significant lexical overlap between the claim and the ETS. If the claim and the ETSs use different wording, or if the ETS implies the validity of the claim without explicitly referring to it, the model often misclassifies the snippets (see example in the Appendix SECREF41). This is not surprising, as the model is based on bag-of-words, topic models, and lexica.", "We have described the structure and statistics of the corpus, as well as our methodology for the annotation of evidence and the stance of the evidence. We have also presented experiments for stance detection, evidence extraction, and claim validation with models that achieve high performance in similar problem settings. In order to support the development of machine learning approaches that go beyond the presented models, we provided an error analysis for each of the three tasks, identifying difficulties with each.", "For supporting and refuting ETSs annotators identified FGE sets for 8,291 out of 8,998 ETSs. ETSs with a stance but without FGE sets often miss a clear connection to the claim, so the annotators did not annotate any sentences in these cases. The class distribution of the FGE sets in Table TABREF23 shows that supporting ETSs are more dominant.", "I this subsection, we briefly discuss the differences of our corpus to the FEVER dataset as the most comprehensive dataset introduced so far. Due to the way the FEVER dataset was constructed, the claim validation problem defined by this corpus is different compared to the problem setting defined by our corpus. The verdict of a claim for FEVER depends on the stance of the evidence, that is, if the stance of the evidence is agree the claim is necessarily true, and if the stance is disagree the claim is necessarily false. As a result, the claim validation problem can be reduced to stance detection. Such a transformation is not possible for our corpus, as the evidence might originate from unreliable sources and a claim may have both supporting and refuting ETSs. The stance of ETSs is therefore not necessarily indicative of the veracity of the claim. In order to investigate how the stance is related to the verdict of the claim for our dataset, we computed their correlation.", "Claim: The Reuters news agency has proscribed the use of the word 'terrorists' to describe those who pulled off the September 11 terrorist attacks on America."]}
{"question_id": "4ed58d828cd6bb9beca1471a9fa9f5e77488b1d1", "predicted_answer": "", "predicted_evidence": ["We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$. The agreement between the experts' annotations and the computed gold annotations from the crowd workers is also substantial, $\\kappa = 0.683$.", "The ever-increasing role of the Internet as a primary communication channel is arguably the single most important development in the media over the past decades. While it has led to unprecedented growth in information coverage and distribution speed, it comes at a cost. False information can be shared through this channel reaching a much wider audience than traditional means of disinformation BIBREF0.", "As our analysis shows, while multiple fact-checking corpora are already available, no single existing resource provides full fact-checking sub-task coverage backed by a substantially-sized and validated dataset spanning across multiple domains. To eliminate this gap, we have created a new corpus as detailed in the following sections.", "Based on our analysis, we conclude that heterogeneous data and FGE from unreliable sources, as found in our corpus and in the real world, make it difficult to correctly classify the claims. Thus, in future experiments, not just FGE need to be taken into account, but also additional information from our newly constructed corpus, that is, the stance of the FGE, FGE sources, and documents from the Snopes website which provide additional information about the claim. Taking all this information into account would enable the system to find a consistent configuration of these labels and thus potentially help to improve performance. For instance, a claim that is supported by evidence coming from an unreliable source is most likely false. In fact, we believe that modeling the meta-information about the evidence and the claim more explicitly represents an important step in making progress in automated fact-checking.", "Moreover, as the distribution of the classes in Table TABREF23 shows, support and no stance are more dominant than the refute class. The model is therefore biased towards these classes and is less likely to predict refute (see confusion matrix in the Appendix Table TABREF42). An analysis of the misclassified refute ETSs has shown that the contradiction is often expressed in difficult terms, which the model could not detect, e.g. \u201cthe myth originated\u201d, \u201cno effect can be observed\u201d, \u201cThe short answer is no\u201d."]}
{"question_id": "de580e43614ee38d2d9fc6263ff96e6ca2b54eb5", "predicted_answer": "", "predicted_evidence": ["The results in Table TABREF27 show that AtheneMLP scores highest. Similar to the outcome of the Fake News Challenge, feature-based models outperform neural networks based on word embeddings BIBREF19. As the comparison to the human agreement bound suggests, there is still substantial room for improvement.", "FGE: As a boy whose friends and employers at the furniture store were Jewish, Powell picked up a smattering of Yiddish. He kept working at Sickser's through his teens, ... picking up a smattering of Yiddish ... A spokesman for Mr. Powell said he hadn't heard about the spoof ...", "Below we give an instance of a misclassified ETS. Even though the ETS supports the claim, the lexical overlap is relatively low. Most likely, for this reason, the model predicts refute.", "This work has been supported by the German Research Foundation as part of the Research Training Group \u201dAdaptive Preparation of Information from Heterogeneous Sources\u201d (AIPHES) at the Technische Universit\u00e4t Darmstadt under grant No. GRK 1994/1.", "In fact, the task is significantly more difficult than stance annotation as sentences may provide only partial evidence for or against the claim. In such cases, it is unclear how large the information overlap between sentence and claim should be for a sentence to be FGE. The sentence (1a) in Table TABREF18, for example, only refers to one part of the claim without mentioning the time of the shutdown. We can further modify the example in order to make the problem more obvious: (a) The channel announced today that it is planing a shutdown. (b) Fox News made an announcement today."]}
{"question_id": "ae89eed483c11ccd70a34795e9fe416af8a35da2", "predicted_answer": "", "predicted_evidence": ["We consider the task as a ranking problem, but also provide the human agreement bound, the random baseline and the majority vote for evidence extraction as a classification problem for future reference in Table TABREF39 in the Appendix.", "We performed an error analysis for the BiLSTM and the Tf-Idf model, as they reach the highest recall and precision, respectively. Tf-Idf achieves the best precision because it only predicts a small set of sentences, which have lexical overlap with the claim. The model therefore misses FGE that paraphrase the claim. The BiLSTM is better able to capture the semantics of the sentences. We believe that it was therefore able to take related word pairs, such as \u201cIsrael\u201d - \u201cJewish\u201d, \u201cprice\u201d-\u201csold\u201d, \u201cpointed\u201d-\u201cpointing\u201d, \u201cbroken\"-\"injured\u201d, into account during the ranking process. Nevertheless, the model fails when the relationship between the claim and the potential FGE is more elaborate, e.g. if the claim is not paraphrased, but reasons for it being true are provided. An example of a misclassified sentence is given in the Appendix SECREF43.", "", "Another possible reason for the lower performance is that our data is heterogeneous and, therefore, it is more challenging for a machine learning model to generalize. In fact, we have performed additional experiments in which we pre-trained a model on the FEVER corpus and fine-tuned the parameters on our corpus and vice versa. However, no significant performance gain could be observed in both experiments", "The annotation of the corpus described in the previous section provides supervision for different fact-checking sub-tasks. In this paper, we perform experiments for the following sub-tasks: (1) detection of the stance of the ETSs with respect to the claim, (2) identification of FGE in the ETSs, and (3) prediction of a claim's verdict given FGE."]}
{"question_id": "fc62549a8f0922c09996a119b2b6a8b5e829e989", "predicted_answer": "", "predicted_evidence": ["The model is similar to that of BIBREF3 , and we put the 1,000-dimension word-embedding layer right after the input layer. Then 3 deep LSTM layers with 100 LSTM cells each and without peephole connection are used for learning the sequential pattern of the sentences.", "As we are focusing on a personalized language modeling with the preservation of user data, we generate two types of language models. First is a sentence completion language model, which can complete sentences with a given n-many sequence of words. Second is a message-reply prediction language model, which can generate a response sentence for a given message. The output of both models implies user characteristics such as preferable vocabulary, sentence length, and other language-related patterns.", "$$\\begin{aligned}\n& p(Y|X)=\\prod _{t=1}^{T}p(y_t|x_{1:t-1}) \\\\\n& \\textit {L}= -\\dfrac{1}{|T|}\\sum \\limits _{t=1}^{T} x_{t+1}\\log p(y_t|x_{1:t-1}),\n\\end{aligned}$$   (Eq. 3)", "The contributions of this paper are as follows. First, we propose efficient transfer learning schemes for personalized language modeling, which is the first research on transfer learning for RNN based language models with privacy preserving. Second, we show the applicability of our research to the target scenario in the short message reply application by training the model in the similar environment to that of the mobile device, and highlight its test results.", "The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."]}
{"question_id": "e2a507749a4a3201edd6413c77ad0d4c23e9c6ce", "predicted_answer": "", "predicted_evidence": ["We also apply the transfer learning schemes with some of the English bible data. The same general language model, which involved previously training with the WMT'14 corpus for 10 days, is used. English bible data is added and employed in training for another 4 hours using proposed transfer learning schemes.", "In the message-reply prediction case, pairwise data is generated by extracting the drama corpus of each character and concatenating two consecutive sentences of different characters to form one single message-reply sentence data. We insert the word \u201c $<eos>$ \" between the message and reply to mark the border separating them. This pairwise data is used for the training, and only the message part of the pairwise data is used for the message-reply prediction. During implementation, it took about a day to train the general language model with the \u201cFriends\" corpus and another 4 hours to train the personalized language model with two main character corpora. The \u201ctitan-X GPU\" and the \u201cGeForce GT 730 GPU\" was used for these experiments. Validation messages-reply sentences of 1,281 are randomly sampled from the \u201cFriends\" corpus for tracking validation curve and another 753 test messages are prepared for predicting the responses. These data remained unseen from training phase.", "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery.", "The model is similar to that of BIBREF3 , and we put the 1,000-dimension word-embedding layer right after the input layer. Then 3 deep LSTM layers with 100 LSTM cells each and without peephole connection are used for learning the sequential pattern of the sentences.", "Scheme 1, relearn the whole layer: As a baseline, we retrain the whole model with private data only and compare the result with the two other schemes below. Because of the retraining of the LSTM layers in their entirety, this scheme requires more computing power than the other two schemes."]}
{"question_id": "a3a867f7b3557c168d05c517c468ff6c7337bff9", "predicted_answer": "", "predicted_evidence": ["A sentence completion model completes a sentence with the given word sequence $X= \\lbrace x_1,x_2, \\dots , x_T\\rbrace $ , where $x_N$ is a word ( $N=1, 2, \\dots , T$ ). The model can predict the next word $x_{N+1}$ with given word sequence $x_{1:N}$ . By repeating the prediction until the output word reaches the end-of-sentence signal, \u201c $<eos>$ ,\" the whole sentence can be generated.", "The \u201cpersonalized language model 2\" in Table 2 shows the sample output of the personalized language model trained with another style of document data, English bible data. As shown in Table 2, the output of the personalized language model contains more bible-like vocabulary and sentence styles.", "$$\\begin{aligned}\n& p(Y|X)=\\prod _{t=1}^{T}p(y_t|x_{1:t-1}) \\\\\n& \\textit {L}= -\\dfrac{1}{|T|}\\sum \\limits _{t=1}^{T} x_{t+1}\\log p(y_t|x_{1:t-1}),\n\\end{aligned}$$   (Eq. 3)", "The output probability to the input sequence $X$ and the training objective are", "A message-reply prediction model generates a response sentence for a given message. It is similar to the sentence completion language model except that the message sentence is encoded and used as a context information when the model generates a response word sequence. Our approach is inspired by the sequence-to-sequence learning research BIBREF0 that is successfully applied to a machine translation task. The message word sequence $X=\\lbrace x_1, x_2, \\dots , x_T\\rbrace $ is fed into the model, and the last hidden state is used as context information $c_T$ . With this context information, the next sequence word is predicted similarly to that in the sentence completion language model case. During implementation, we used 1,000-dimension word embedding and 3-deep LSTM layers with 100 LSTM cells in each layer. The output probability and the training objective are"]}
{"question_id": "8bb2280483af8013a32e0d294e97d44444f08ab0", "predicted_answer": "", "predicted_evidence": ["The contributions of this paper are as follows. First, we propose efficient transfer learning schemes for personalized language modeling, which is the first research on transfer learning for RNN based language models with privacy preserving. Second, we show the applicability of our research to the target scenario in the short message reply application by training the model in the similar environment to that of the mobile device, and highlight its test results.", "where $X$ is a word sequence in the sentence, $Y$ is a model output sequence $Y=\\lbrace y_1,y_2, \\dots , y_{T}\\rbrace $", "In the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, \u201cFriends,\" and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users' dialogues.", "The model is similar to that of BIBREF3 , and we put the 1,000-dimension word-embedding layer right after the input layer. Then 3 deep LSTM layers with 100 LSTM cells each and without peephole connection are used for learning the sequential pattern of the sentences.", "To resolve these limitations, we propose fast transfer learning schemes. It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are proposed for achieving the result. In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model. With this approach, the final model can mimic target user's language style with proper syntax."]}
{"question_id": "a68acd8364764d5601dc12e4b31d9102fb7d5f7e", "predicted_answer": "", "predicted_evidence": ["$$\\begin{aligned}\n& p(Y|X)=\\prod _{t=1}^{T}p(y_t|x_{1:t-1}) \\\\\n& \\textit {L}= -\\dfrac{1}{|T|}\\sum \\limits _{t=1}^{T} x_{t+1}\\log p(y_t|x_{1:t-1}),\n\\end{aligned}$$   (Eq. 3)", "where $X$ is a word sequence in the message sentence, $Z$ is a target word sequence in the response sentence $Z = \\lbrace z_1,z_2, \\dots , z_{T^{\\prime }}\\rbrace $ , $Y$ is a model output sequence $Y=\\lbrace y_1,y_2, \\dots , y_{T^{\\prime }}\\rbrace $ , $c_T$ is the encoding vector for the message sentence.", "To achieve this result, we trained the language model with a large amount of general data in powerful computing environments, and then applied the transfer learning in relatively small computing environments. We assume that this method would be applied to mobile devices. As we are taking the preservation of privacy into consideration, the transferred model is retrained within the local environments such as mobile devices, and no personal data is sent out of the devices. This could have been accomplished using the proposed transfer learning schemes in RNN-LSTM architecture.", "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently.", "A sentence completion model completes a sentence with the given word sequence $X= \\lbrace x_1,x_2, \\dots , x_T\\rbrace $ , where $x_N$ is a word ( $N=1, 2, \\dots , T$ ). The model can predict the next word $x_{N+1}$ with given word sequence $x_{1:N}$ . By repeating the prediction until the output word reaches the end-of-sentence signal, \u201c $<eos>$ ,\" the whole sentence can be generated."]}
{"question_id": "6d55e377335815b7ad134d1a2977d231ad34a25b", "predicted_answer": "", "predicted_evidence": ["The contributions of this paper are as follows. First, we propose efficient transfer learning schemes for personalized language modeling, which is the first research on transfer learning for RNN based language models with privacy preserving. Second, we show the applicability of our research to the target scenario in the short message reply application by training the model in the similar environment to that of the mobile device, and highlight its test results.", "The model is similar to that of BIBREF3 , and we put the 1,000-dimension word-embedding layer right after the input layer. Then 3 deep LSTM layers with 100 LSTM cells each and without peephole connection are used for learning the sequential pattern of the sentences.", "In the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, \u201cFriends,\" and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users' dialogues.", "A sentence completion model completes a sentence with the given word sequence $X= \\lbrace x_1,x_2, \\dots , x_T\\rbrace $ , where $x_N$ is a word ( $N=1, 2, \\dots , T$ ). The model can predict the next word $x_{N+1}$ with given word sequence $x_{1:N}$ . By repeating the prediction until the output word reaches the end-of-sentence signal, \u201c $<eos>$ ,\" the whole sentence can be generated.", "The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."]}
{"question_id": "0035b351df63971ec57e36d4bfc6f7594bed41ae", "predicted_answer": "", "predicted_evidence": ["Opinion lexicons play an important role in sentiment analysis systems, and the majority of the existing systems rely heavily on them BIBREF20 . For each of the seven chosen lexicons, a tweet is represented by calculating the following features: (1) tweet polarity, (2) the average polarity of the positive terms, (3) the average polarity of the negative terms, (4) the score of the last positive term, (5) the score of the last negative term, (6) the maximum positive score and (7) the minimum negative score.", "SiTAKA uses five types of features: basic text, syntactic, lexicon, cluster and Word Embeddings. These features are described in the following subsections:", "We used two pre-trained embedding models in En-SiTAKA. The first one is word2vec which is provided by Google. It is trained on part of the Google News dataset (about 100 billion words) and it contains 300-dimensional vectors for 3M words and phrases BIBREF11 . The second one is SSWEu, which has been trained to capture the sentiment information of sentences as well as the syntactic contexts of words BIBREF12 . The SSWEu model contains 50-dimensional vectors for 100K words.", "where INLINEFORM0 denotes the concatenation operation. The pooling function is an element-wise function, and it converts texts with various lengths into a fixed-length vector allowing to capture the information throughout the entire text.", "This work was partially supported by URV Research Support Funds (2015PFR-URV-B2-60, 2016PFR-URV-B2-60 and Mart\u00ed i Franqu\u00e9s PhD grant)."]}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 denotes the concatenation operation. The pooling function is an element-wise function, and it converts texts with various lengths into a fixed-length vector allowing to capture the information throughout the entire text.", "These basic features are extracted from the text. They are the following:", "The evaluation metrics used by the task organizers were the macroaveraged recall ( INLINEFORM0 ), the F1 averaged across the positives and the negatives INLINEFORM1 and the accuracy ( INLINEFORM2 ) BIBREF25 .", "Syntactic features are useful to discriminate between neutral and non-neutral texts.", "We used two pre-trained embedding models in En-SiTAKA. The first one is word2vec which is provided by Google. It is trained on part of the Google News dataset (about 100 billion words) and it contains 300-dimensional vectors for 3M words and phrases BIBREF11 . The second one is SSWEu, which has been trained to capture the sentiment information of sentences as well as the syntactic contexts of words BIBREF12 . The SSWEu model contains 50-dimensional vectors for 100K words."]}
{"question_id": "e801b6a6048175d3b1f3440852386adb220bcb36", "predicted_answer": "", "predicted_evidence": ["SiTAKA uses five types of features: basic text, syntactic, lexicon, cluster and Word Embeddings. These features are described in the following subsections:", "In this version of the SiTAKA system, we used four lexicons created by BIBREF10 . Arabic Hashtag Lexicon, Dialectal Arabic Hashtag Lexicon, Arabic Bing Liu Lexicon and Arabic Sentiment140 Lexicon. The first two were created manually, whereas the rest were translated to Arabic from the English version using Google Translator.", "In Ar-SiTAKA we used the model Arabic-SKIP-G300 provided by BIBREF13 . Arabic-SKIP-G300 has been trained on a large corpus of Arabic text collected from different sources such as Arabic Wikipedia, Arabic Gigaword Corpus, Ksucorpus, King Saud University Corpus, Microsoft crawled Arabic Corpus, etc. It contains 300-dimensional vectors for 6M words and phrases.", "Negation: A negated context can be defined as a segment of tweet that starts with a negation word (e.g. no, don't for English-language, \u0644\u0627 \u0648 \u0644\u064a\u0633 > for Arabic-language) and ends with a punctuation mark BIBREF0 . Each tweet is negated by adding a suffix (\"_NEG\" and \"_\u0645\u0646\u0641\u064a>\") to each word in the negated context.", "The polarity of a tweet T given a lexicon L is calculated using the equation (1). First, the tweet is tokenized. Then, the number of positive (P) and negative (N) tokens found in the lexicon are counted. Finally, the polarity measure is calculated as follows: DISPLAYFORM0"]}
{"question_id": "3699927c6c1146f5057576034d226a99946d52cb", "predicted_answer": "", "predicted_evidence": ["where $p(f|z)$ is learnt by BayesCat.", "We compared BCF against various models explained below. All experiments follow the same experimental protocol, i.e., we train separate instances of the same model on each language.", "Figure 5 depicts three English stimuli, together with concept predictions from BCF and the co-occurrence model. Table 4 shows quantitative results of the three models averaged over a corpus of 300 test stimuli for all languages. Both BCF and the co-occurrence model outperform the random baseline by a large margin, and BCF achieves consistently highest scores. Both Bayesian models (BCF and BayesCat) outperform the co-occurrence model across all metrics and conditions. We assume that plain concept-feature co-occurrence information might be too sparse to provide a strong signal of concept relevance given a set of features. The Bayesian models, on the other hand, learn complex correspondences between features and all concepts in a category. BayesCat and BCF perform comparably given that they exploit local co-occurrence relations in similar ways.", "A notational overview is provided in Table 1 . The generative story of our model is displayed in Figure 3 , and Figure 3 shows the plate diagram representation of BCF. The generative story proceeds as follows. We assume a global multinomial distribution over categories $Mult(\\theta )$ . Its parameter vector $\\theta $ is drawn from a symmetric Dirichlet distribution with hyperparameter $\\alpha $ . For each target concept $\\ell =[1...\\mathcal {L}]$ , we draw a category $k^\\ell $ from $Mult(\\theta )$ . For each category $k$ , we draw an independent set of multinomial parameters over feature types, $\\mu _k$ , from a symmetric Dirichlet distribution with hyperparameter $\\beta $ , reflecting the relative relevance of each feature type towards this category.", "Is the structure and architecture of BCF appropriate and necessary for category and structured feature learning? We answer this question by comparing BCF against a variety of related models. First, we report a random baseline which assigns concepts to categories at random. Secondly, we compare against a model entirely based on word co-occurrence. Unlike BCF, the co-occurrence model cannot learn categories and features jointly, and has no notion of feature structure. It uses $k$ -means clustering BIBREF66 to group concepts into categories, and, subsequently, group features into feature types for each category (see Section UID27 ). Finally, we compare BCF against BayesCat, a cognitively motivated Bayesian model of category acquisition BIBREF22 . Like BCF, it draws inspiration from topic modeling, however, BayesCat does not learn categories and features jointly, and does not acquire structured feature representations."]}
{"question_id": "6606160e210d05b94f7cbd9c5ff91947339f9d02", "predicted_answer": "", "predicted_evidence": ["Similar to BCF, BayesCat is a knowledge-lean acquisition model which can be straightforwardly applied to input from different languages. It induces categories $z$ which are represented through a distribution over target concepts $c$ , $p(c|z)$ , and a distribution over features $f$ (i.e., individual context words), $p(f|z)$ . BayesCat, like BCF, is a Bayesian model and its parameters are inferred using approximate MCMC inference, in the form of a Gibbs sampler. Unlike BCF, however, BayesCat does not induce structured feature representations, and comparing it to BCF allows us to evaluate the advantage of joint category and feature learning. BayesCat induces categories represented through unstructured bags-of-features. As such, the model structure of BayesCat is closely related to topic models such as Latent Dirichlet Allocation (LDA; BIBREF57 ). Comparing our proposed model against BayesCat allows us to shed light on the benefit of more sophisticated model structure which allows to learn features jointly with categories, compared to the information that can be captured in vanilla topic models.", "Table 6 shows the results of the feature coherence study, where the overall pattern of results is similar as above. We can see that participants are able to detect intruder features from the types learnt by BCF more reliably than from those learnt by all comparison models. Again, both Bayesian models outperform the count-based baselines both in terms of accuracy and inter annotator agreement. The superior performance of BCF compared to BayesCat indicates that its ability to learn structured features jointly with categories in a single process leads to higher quality feature representations. In particular, in addition to associating relevant feature types with categories, the feature types themselves are internally coherent, pertaining to different aspects or properties of the reference category.", "$$\\text{co} = \\frac{1}{N} \\sum _j \\max \\limits _i |C_i \\cap G_j|$$   (Eq. 35)", "Figures 2 and 21 qualitatively support the claim that BCF learns meaningful features across languages, which are overall coherent and relevant to their associated category. Some interesting cultural differences emerge, for example German is the only language for which a measurement feature type is induced for vegetables (Figure 21 ; de, 4th from left), while for clothing, a fashion industry feature emerges in French (Figure 21 ; fr, 3rd from left). For the same category, a feature type pertaining to colour emerges for all five languages ( 21 , bold margins). In addition, some features in other languages were not straightforwardly translatable into English. For example, the 3rd feature type for vegetables in Chinese (Figure 21 ) includes the word UTF8gbsn \u5206 which refers to the extent to which food is cooked and UTF8gbsn \u70c2 which is the stage when food starts to fall apart after cooking (stewing). In addition, the feature types induced for the Chinese clothing category include two words which both translate to the English word wear, but in Chinese are specific to wearing small items (e.g., jewelery; UTF8gbsn\u6234), and wearing clothes (UTF8gbsn\u7a7f), respectively.", "$$z(c) = \\max _z p(c|z) p(z|c).$$   (Eq. 31)"]}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "predicted_answer": "", "predicted_evidence": ["Comparing results across languages we observe that scores for English exceed scores for all other languages. At the same time, for almost all models and languages the IAA scores fall under the category of `fair agreement' ( $0.20 < \\kappa < 0.40$ ) indicating that the elicitation task was feasible for crowdworkers. This applies to both evaluations (Tables 5 and 6 ). We observed a similar pattern in the results of Experiment 1 (Table 3 ). We believe there are two reasons for this drop. Firstly, in order to perform cross-linguistic experiments, we translated English categories into other languages. As mentioned in Sections \"Results\" and \"Results\" , such a direct correspondence may not always exist. Consequently, annotators for languages other than English are faced with a noisier (and potentially harder) task.", "Figures 2 and 21 qualitatively support the claim that BCF learns meaningful features across languages, which are overall coherent and relevant to their associated category. Some interesting cultural differences emerge, for example German is the only language for which a measurement feature type is induced for vegetables (Figure 21 ; de, 4th from left), while for clothing, a fashion industry feature emerges in French (Figure 21 ; fr, 3rd from left). For the same category, a feature type pertaining to colour emerges for all five languages ( 21 , bold margins). In addition, some features in other languages were not straightforwardly translatable into English. For example, the 3rd feature type for vegetables in Chinese (Figure 21 ) includes the word UTF8gbsn \u5206 which refers to the extent to which food is cooked and UTF8gbsn \u70c2 which is the stage when food starts to fall apart after cooking (stewing).", "We showed that BCF learns meaningful categories across languages which are quantitatively better than those inferred by a simpler co-occurrence model. Although generally consistent, categories are sometimes influenced by characteristics of the respective training and test language. While the literature confirms an influence of language on categorization BIBREF4 , BIBREF5 , this effect is undoubtedly amplified through our experimental framework.", "$$Score(c|\\mathbf {f}) = \\sum _k P(c|k) P(\\mathbf {f}|k).$$   (Eq. 47)", "Comparing results across languages we observe that scores for English exceed scores for all other languages. At the same time, for almost all models and languages the IAA scores fall under the category of `fair agreement' ( $0.20 < \\kappa < 0.40$ ) indicating that the elicitation task was feasible for crowdworkers. This applies to both evaluations (Tables 5 and 6 ). We observed a similar pattern in the results of Experiment 1 (Table 3 ). We believe there are two reasons for this drop. Firstly, in order to perform cross-linguistic experiments, we translated English categories into other languages. As mentioned in Sections \"Results\" and \"Results\" , such a direct correspondence may not always exist. Consequently, annotators for languages other than English are faced with a noisier (and potentially harder) task. Secondly, while it is straightforward to recruit English native speakers on crowd sourcing platforms, it has proven more challenging for the other languages."]}
{"question_id": "4beb50ba020f624446ff1ef5bf4adca5ed318b98", "predicted_answer": "", "predicted_evidence": ["Textual Entailment (TE) is a task to determine the relationship between a hypothesis and a premise. The Stanford Natural Language Inference (SNLI) corpus BIBREF26 provides approximately 550K hypothesis/premise pairs. Our baseline adopts ESIM BIBREF27 which uses a Bi-LSTM encoder layer and a Bi-LSTM inference composition layer which are connected by an attention layer to model the relation between hypothesis and premise. Our ESuLMo outperforms ELMo by 0.8% in terms of accuracy. Though our performance does not reach the state-of-the-art, it is second-best in all single models according to the SNLI leaderboard .", "Recently, pre-trained language representation has shown to be useful for improving many NLP tasks BIBREF0, BIBREF1, BIBREF2, BIBREF3. Embeddings from Language Models (ELMo) BIBREF0 is one of the most outstanding works, which uses a character-aware language model to augment word representation.", "A highway network BIBREF16 is then applied to the output of CNN. A bidirectional long short-term memory network (Bi-LSTM) BIBREF17 generates the hidden states for the given sentence representation in forward and backward. Finally, the probability of each token is calculated by applying an affine transformation to all the hidden states followed by a $SoftMax$ function. During the training, our objective is to minimize the negative log-likelihood of all training samples.", "An essential challenge in training word-based language models is how to control vocabulary size for better rare word representation. No matter how large the vocabulary is, rare words are always insufficiently trained. Besides, an extensive vocabulary takes too much time and computational resource for the model to converge. Whereas, if the vocabulary is too small, the out-of-vocabulary (OOV) issue will harm the model performance heavily BIBREF4. To obtain effective word representation, BIBREF4 introduce character-driven word embedding using convolutional neural network (CNN) BIBREF5, following the language model in BIBREF6 for deep contextual representation.", "However, there is potential insufficiency when modeling word from characters which hold little linguistic sense, especially, the morphological source BIBREF7. Only 86 characters(also included some common punctuations) are adopted in English writing, making the input too coarse for embedding learning. As we argue that for better representation from a refined granularity, word is too large and character is too small, it is natural for us to consider subword unit between character and word levels."]}
{"question_id": "9bf60073fbb69fbf860196513fc6fd2f466535f6", "predicted_answer": "", "predicted_evidence": ["To segment subwords from a word, we adopt the generalized unsupervised segmentation framework proposed by BIBREF18. The generalized framework can be divided into two collocative parts, goodness measure (score), which evaluates how likely a subword is to be a \u2018proper\u2019 one, and a segmentation or decoding algorithm. For the sake of simplicity, we choose frequency as the goodness score and two representative decoding algorithms, byte pair encoding (BPE) BIBREF13 which uses a greedy decoding algorithm and unigram language model (ULM) BIBREF14 which adopts a Viterbi-style decoding algorithm.", "A highway network BIBREF16 is then applied to the output of CNN. A bidirectional long short-term memory network (Bi-LSTM) BIBREF17 generates the hidden states for the given sentence representation in forward and backward. Finally, the probability of each token is calculated by applying an affine transformation to all the hidden states followed by a $SoftMax$ function. During the training, our objective is to minimize the negative log-likelihood of all training samples.", "For a specific dataset, the BPE algorithm keeps the same segmentation for the same word in different sequences, whereas ULM cannot promise such segmentation. Both segmentation algorithms have their strengths, BIBREF13 show that BPE can fix the OOV issue well, and BIBREF14 proves that ULM is a subword regularization which is helpful in neural machine translation.", "Given a sentence $S = \\lbrace W_1, W_2, ... , W_n\\rbrace $, we first use a segmentation algorithm to divide each word into a sequence of subwords BIBREF13, BIBREF14.", "$\\bullet $ All the input sequences are tokenized into a sequence of single-character subwords."]}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "predicted_answer": "", "predicted_evidence": ["Semantic Role Labeling (SRL) is to model the predicate-argument structure of a sentence. BIBREF22 model SRL as a words pair classification problem and directly use a bi-affine scorer to predict the relation given two words in a sentence. By adding our ESuLMo to the baseline model BIBREF22, we can not only outperform the original ELMo by 0.5% F1-score but also outperform the state-of-the-art model BIBREF23 which has three times more parameters than our model in CoNLL 2009 benchmark dataset.", "Subword Vocabulary Size Tables TABREF5 and TABREF10 show the performance of ESuLMo drops with the vocabulary size increases . We explain the trend that neural network pipeline especially CNN would fail to capture necessary details of building word embeddings as more subwords are introduced.", "To segment subwords from a word, we adopt the generalized unsupervised segmentation framework proposed by BIBREF18. The generalized framework can be divided into two collocative parts, goodness measure (score), which evaluates how likely a subword is to be a \u2018proper\u2019 one, and a segmentation or decoding algorithm. For the sake of simplicity, we choose frequency as the goodness score and two representative decoding algorithms, byte pair encoding (BPE) BIBREF13 which uses a greedy decoding algorithm and unigram language model (ULM) BIBREF14 which adopts a Viterbi-style decoding algorithm.", "Word Sense Disambiguation To explore the word sense disambiguation capability of our ESuLMo, we isolate the representation encoded by our ESuLMo and use them to directly make predictions for a fine-grained word sense disambiguation (WSD) task. We choose the dataset and perform this experiment using the same setting as ELMo with only the last layer's representation. Table TABREF16 shows that our model can outperform the original ELMo.", "While applying our pre-trained ESuLMo to other NLP tasks, we have two different strategies: (1) Fine-tuning our ESuLMo while training other NLP tasks; (2) Fixing our ESuLMo while training other NLP tasks. During the experiment, we find there is no significant difference between these two strategies. However, the first strategy consumes much more resource than the second one. Therefore, we choose the second strategy to conduct all the remaining experiments."]}
{"question_id": "1c8958ec50976a9b1088c51e8f73a767fb3973fa", "predicted_answer": "", "predicted_evidence": ["The power of neural networks comes from their ability to find data representations that are useful for classification. Recurrent Neural Networks (RNN) are a special type of neural network, which can be thought of as the addition of loops to the architecture. RNNs use back propagation in the training process to update the network weights in every layer. In our experimentation we used a powerful type of RNN known as Long Short-Term Memory Network (LSTM). Inspired by the work by BIBREF15 , we experiment with combining various LSTM models enhanced with a number of novel features in an ensemble. More specifically we introduce:", "Although hate speech is protected under the free speech provisions in the United States, there are other countries, such as Canada, France, United Kingdom, and Germany, where there are laws prohibiting it as being promoting violence or social disorder. Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment. Moreover, a non-automated task by human annotators would have strong impact on system response times, since a computer-based solution can accomplish this task much faster than humans.", "Note that existing solutions for automatic detection are still falling short to effectively detect abusive messages. Therefore there is a need for new algorithms which would do the job of classification of such content more effectively and efficiently. Our work is towards that direction.", "Simple word-based approaches, if used for blocking the posting of text or blacklisting users, not only fail to identify subtle offensive content, but they also affect the freedom of speech and expression. The word ambiguity problem \u2013 that is, a word can have different meanings in different contexts \u2013 is mainly responsible for the high false positive rate in such approaches. Ordinary NLP approaches on the other hand, are ineffective to detect unusual spelling, experienced in user-generated comment text. This is best known as the spelling variation problem, and it is caused either by unintentional or intentional replacement of single characters in a token, aiming to obfuscate the detectors.", "Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment. Moreover, a non-automated task by human annotators would have strong impact on system response times, since a computer-based solution can accomplish this task much faster than humans. The massive rise in the user-generated content in the above social media services, with manual filtering not being scalable, highlights the need for automating the process of on-line hate-speech detection."]}
{"question_id": "363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "predicted_answer": "", "predicted_evidence": ["Although hate speech is protected under the free speech provisions in the United States, there are other countries, such as Canada, France, United Kingdom, and Germany, where there are laws prohibiting it as being promoting violence or social disorder. Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment.", "Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment. Moreover, a non-automated task by human annotators would have strong impact on system response times, since a computer-based solution can accomplish this task much faster than humans. The massive rise in the user-generated content in the above social media services, with manual filtering not being scalable, highlights the need for automating the process of on-line hate-speech detection.", "Although hate speech is protected under the free speech provisions in the United States, there are other countries, such as Canada, France, United Kingdom, and Germany, where there are laws prohibiting it as being promoting violence or social disorder. Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment. Moreover, a non-automated task by human annotators would have strong impact on system response times, since a computer-based solution can accomplish this task much faster than humans.", "Despite the fact that the majority of the solutions for automated detection of offensive text rely on Natural Language Processing (NLP) approaches, there is lately a tendency towards employing pure machine learning techniques like neural networks for that task. NLP approaches have the drawback of being complex, and to a large extent dependent on the language used in the text. This provides a strong motivation for employing alternative machine learning models for the classification task. Moreover, the majority of the existing automated approaches depend on using pre-trained vectors (e.g. Glove, Word2Vec) as word embeddings to achieve good performance from the classification model. That makes the detection of hatred content unfeasible in cases where users have deliberately obfuscated their offensive terms with short slang words.", "To improve classification ability we employ an ensemble of LSTM-based classifiers."]}
{"question_id": "cf0b7d8a2449d04078f69ec9717a547adfb67d17", "predicted_answer": "", "predicted_evidence": ["We also present the performance of each of the tested models per class label in Table TABREF25 . Results by other researchers have not been included, as these figures are not reported in the existing literature. As can be seen, sexism is quite easy to classify in hate-speech, while racism seems to be harder; similar results were reported by BIBREF7 . This result is consistent across all ensembles.", "The hidden layer. The sigmoid activation was selected for the the hidden LSTM layer. Based on preliminary experiments the dimensionality of the output space for this layer was set to 200. This layer is fully connected to both the Input and the subsequent layer.", "In this work we present an ensemble classifier that is detecting hate-speech in short text, such as tweets. The input to the base-classifiers consists of not only the standard word uni-grams, but also a set of features describing each user's historical tendency to post abusive messages. Our main innovations are: i) a deep learning architecture that uses word frequency vectorisation for implementing the above features, ii) an experimental evaluation of the above model on a public dataset of labeled tweets, iii) an open-sourced implementation built on top of Keras.", "Additionally, the F-score is the harmonic mean of precision and recall, expressed as INLINEFORM0 . For our particular case with three classes, P, R and F are computed for each class separately, with the final F value derived as the weighted mean of the separate INLINEFORM1 -scores: INLINEFORM2 ; recall that INLINEFORM3 , INLINEFORM4 and INLINEFORM5 . The results are shown in Table TABREF24 , along with the reported results from state of the art approaches proposed by other researchers in the field. Note that the performance numbers P,R and F of the other state of the art approaches are based on the authors' reported data in the cited works. Additionally, we report the performance of each individual LSTM classifier as if used alone over the same data (that is, without the ensemble logic).", "To avoid over-fitting, the model training was allowed to run for a maximum number of 100 epochs, out of which the optimally trained state was chosen for the model evaluation. An optimal epoch was identified so, such that the validation accuracy was maximized, while at the same time the error remained within INLINEFORM0 of the lowest ever figure within the current fold. Throughout the experiment we observed that the optimal epochs typically occurred after between the 30 and 40 epochs."]}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "predicted_answer": "", "predicted_evidence": ["The pruning algorithm is based on the depth-first search (DFS) traversal of the lattice $\\mathcal {L^{\\prime }}$ and marks each state in $\\mathcal {L^{\\prime }}$ as either new, visited, or pruned. Only new states are entered and each state is entered at most once. The FST arcs are only marked as either visited or pruned. Each FST state keeps track of whether an intent annotation parsing has begun (i.e., a begin symbol $\\iota _B$ has been encountered but the end symbol $\\iota _E$ has not appeared yet) and how many wildcard words have been matched so far.", "The naive implementation allowing for $n$ superfluous (non-intent) words appearing between intent-matching words would lead to a significant explosion of the annotations spans. Instead, we employ a post-processing filtering step to prune lattice paths where the number of allowed non-intent word is exceeded. Our filtering step has a computational complexity of $\\mathcal {O}(|S| + |A|)$, where $|S|$ is the number of states and $|A|$ is the number of arcs in the non-pruned annotated lattice $\\mathcal {L^{\\prime }}$.", "Each dialog act can be instantiated to form an intent, which is an expression of a particular intention. Intents are simply sets of utterances which exemplify the intention of the speaking party to perform a certain action, convey or obtain information, express opinion, etc. For instance, the intent Account Check can be expressed using examples such as \"let me go over your account\", \"found account under your name\", \"i have found your account\". At the same time, the intent Account Check would be an instance of the dialog act Statement. An important part of intent classification is entity recognition BIBREF2. An entity is a token which can either be labeled with a proper name, or assigned to a category with well defined semantics. The former leads to the concept of a named entity, where a token represents some real world object, such as a location (New York), a person (Lionel Messi), a brand (Apple). The latter can represent concepts such as money (one hundred dollars), date (first of May), duration (two hours), credit card number, etc.", "The intent symbol $\\iota $ is the delimiter of the intent annotation and it demarcates the words constituting the intent. The begin ($\\iota _B$) and continuation ($\\iota _C$) symbols are mapped onto arcs with words as input symbols, and the end ($\\iota _E$) symbol is inserted in an additional arc with an $\\epsilon $ input symbol after the annotated fragment of text. It is important that the begin symbol $\\iota _B$ does not introduce an extra input of $\\epsilon $. Otherwise, the FST composition is inefficient, as it tries to enter this path on every arc in the lattice $\\mathcal {L}$.", "The lattice $\\mathcal {L}$ is an acceptor, where each arc contains a symbol representing a single word in the current hypothesis (see Figure FIGREF12). We employ a closed ASR vocabulary assumption and operate on word-level, rather than character- or phoneme- level FST. Note that this assumption is not a limitation of our method. Should the ASR have an unlimited vocabulary (as some end-to-end ASR systems do), it is possible to dynamically construct the lattice symbol table and merge it with the symbol table of intent definitions."]}
{"question_id": "909ecf675f874421eecc926a9f7486475aa1423c", "predicted_answer": "", "predicted_evidence": ["Spoken language understanding is a challenging and difficult task BIBREF3. All problems present in natural language understanding are significantly exacerbated by several factors related to the characteristics of spoken language.", "This procedure successfully performs exact matching of the transcription to intent index, when all words present in the current lattice path are also found in the intent example. Unfortunately, this approach is highly impractical when real transcriptions are considered. Sequences of significant words are interwoven with filler phonemes and words, for instance the utterance \"I want uhm to order like um three yyh three tickets\" could not be matched with the intent example \"I want to order __NUMBER__ tickets\".", "The intent symbol $\\iota $ is the delimiter of the intent annotation and it demarcates the words constituting the intent. The begin ($\\iota _B$) and continuation ($\\iota _C$) symbols are mapped onto arcs with words as input symbols, and the end ($\\iota _E$) symbol is inserted in an additional arc with an $\\epsilon $ input symbol after the annotated fragment of text. It is important that the begin symbol $\\iota _B$ does not introduce an extra input of $\\epsilon $. Otherwise, the FST composition is inefficient, as it tries to enter this path on every arc in the lattice $\\mathcal {L}$.", "Firstly, each ASR engine introduces a mixture of systematic and stochastic errors which are intrinsic to the procedure of transcription of spoken audio. The quality of transcription, as measured by the popular word error rate (WER), attains the level of 5%-15% WER for high quality ASR systems for English BIBREF4, BIBREF5, BIBREF6, BIBREF7. The WER highly depends on the evaluation data difficulty and the speed to accuracy ratio. Importantly, errors in the transcription appear stochastically, both in audio segments which carry important semantic information, as well as in inessential parts of the conversation.", "Examples of dialog acts include statements, opinions, yes-no questions, backchannel utterances, response acknowledgements, etc BIBREF1. The recognition and classification of dialog acts is not sufficient for true spoken language understanding. Each dialog act can be instantiated to form an intent, which is an expression of a particular intention. Intents are simply sets of utterances which exemplify the intention of the speaking party to perform a certain action, convey or obtain information, express opinion, etc. For instance, the intent Account Check can be expressed using examples such as \"let me go over your account\", \"found account under your name\", \"i have found your account\". At the same time, the intent Account Check would be an instance of the dialog act Statement. An important part of intent classification is entity recognition BIBREF2. An entity is a token which can either be labeled with a proper name, or assigned to a category with well defined semantics. The former leads to the concept of a named entity, where a token represents some real world object, such as a location (New York), a person (Lionel Messi), a brand (Apple)."]}
{"question_id": "29477c8e28a703cacb716a272055b49e2439a695", "predicted_answer": "", "predicted_evidence": ["A word confusion network (WCN) BIBREF17 is a highly compact graph representation of possible confusion cases between words in the ASR lattice. The nodes in the network represent words and are weighted by the word's confidence score or its a posteriori probability. Two nodes (words) are connected when they appear in close time points and share a similar pronunciation, which merits suspecting they might get confused in recognition BIBREF18, BIBREF19. WCN may contain empty transitions which introduce paths through the graph that skip a particular word and its alternatives. An example of WCN is presented in Figure FIGREF5. Note that this seemingly small lattice encodes 46 080 possible transcription variants.", "This procedure successfully performs exact matching of the transcription to intent index, when all words present in the current lattice path are also found in the intent example. Unfortunately, this approach is highly impractical when real transcriptions are considered. Sequences of significant words are interwoven with filler phonemes and words, for instance the utterance \"I want uhm to order like um three yyh three tickets\" could not be matched with the intent example \"I want to order __NUMBER__ tickets\".", "A discussion of intent recognition in human-human conversations. While significant effort is being directed into human-machine conversation research, most of it is not directly applicable to human-human conversations. We highlight the issues frequently encountered in NLP applications dealing with the latter, and propose a framework for intent recognition aimed to address such problems.", "Another challenge stems from the fact that the structure of conversation changes dramatically when a human assumes an agency in the other party. When humans are aware that the other party is a machine (as is the case in dialogue chatbot interfaces), they tend to speak in short, well-structured turns following the subject-verb-object (SVO) sentence structure with a minimal use of relative clauses BIBREF8. This structure is virtually nonexistent in spontaneous speech, where speakers allow for a non-linear flow of conversation. This flow is further obscured by constant interruptions from backchannel or cross-talk utterances, repetitions, non-verbal signaling, phatic expressions, linguistic and non-linguistic fillers, restarts, and ungrammatical constructions.", "We identify the following as the key contributions of this paper:"]}
{"question_id": "9186b2c5b7000ab7f15a46a47da73ea45544bace", "predicted_answer": "", "predicted_evidence": ["In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold.", "We propose Morfessor EM+Prune, a new training algorithm for Morfessor Baseline. EM+Prune reduces search error during training, resulting in models with lower Morfessor costs. Lower costs also lead to improved accuracy when segmentation output is compared to linguistic morphological segmentation.", "EM,", "In this work we focused on model cost and linguistic segmentation. In future work the performance of Morfessor EM+Prune in applications will be evaluated. Also, a new frequency distribution prior, which is theoretically better motivated or has desirable properties, could be formulated.", "Additionally we apply the Bayesian EM implicit Dirichlet Process prior BIBREF15. We experiment with four variations of the prior:"]}
{"question_id": "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c", "predicted_answer": "", "predicted_evidence": ["The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21. The boundary $F_{1}$-score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score.", "($\\alpha $-weighted) MDL pruning,", "The prior for the morph form properties does not need to be modified. During the EM parameter estimation, the prior for the morph form properties is omitted as the morph lexicon remains constant. During pruning, the standard form prior is applicable.", "The prior must be slightly modified for use with the EM+Prune algorithm. The prior for the frequency distribution (DISPLAY_FORM10) is derived using combinatorics. When using real-valued expected counts, there are infinite assignments of counts to parameters. Despite not being theoretically motivated, it can still be desirable to compute an approximation of the Baseline frequency distribution prior, in order to use EM+Prune as an improved search to find more optimal parameters for the original cost. To do this, the real valued token count $\\nu $ is rounded to the nearest integer. Alternatively, the prior for the frequency distribution can be omitted, or a new prior with suitable properties could be formulated. We do not propose a completely new prior in this work, instead opting to remain as close as possible to Morfessor Baseline.", "Morphological surface segmentation is the task of splitting words into morphs, the surface forms of meaning-bearing sub-word units, morphemes. The concatenation of the morphs is the word, e.g."]}
{"question_id": "526dc757a686a1fe41e77f7e3848e3507940bfc4", "predicted_answer": "", "predicted_evidence": ["The seed lexicon consists of the one million most frequent substrings, with two restrictions on which substrings to include: pre-pruning of redundant subwords, and forcesplit. Truncating to the chosen size is performed after pre-pruning, which means that pre-pruning can make space for substrings that would otherwise have been below the threshold.", "the full EM+Prune prior,", "The prior can be further divided into two parts: the prior for the morph form properties and the usage properties. The form properties encode the string representation of the morphs, while the usage properties encode their frequencies. Morfessor Baseline applies a non-informative prior for the distribution of the morph frequencies. It is derived using combinatorics from the number of ways that the total token count $\\nu $ can be divided among the $\\mu $ lexicon items:", "While rule-based morphological segmentation systems can achieve high quality, the large amount of human effort needed makes the approach problematic, particularly for low-resource languages. The systems are language dependent, necessitating use of multiple tools in multilingual setups. As a fast, cheap and effective alternative, data-driven segmentation can be learned in a completely unsupervised manner from raw corpora. Unsupervised morphological segmentation saw much research interest until the early 2010's; for a survey on the methods, see hammarstrom2011unsupervised. Semi-supervised segmentation with already small amounts of annotated training data was found to improve the accuracy significantly when compared to a linguistic segmentation; see ruokolainen2016comparative for a survey. While this line of research has been continued in supervised and more grammatically oriented tasks BIBREF2, the more recent work on unsupervised segmentation is less focused on approximating a linguistically motivated segmentation. Instead, the aim has been to tune subword segmentations for particular applications.", "In the first phase, a character-level language model is trained. The initial probabilities of the subwords are computed using the language model. The probabilities are refined by EM, followed by hard-EM. During the hard-EM, frequency based pruning of subwords begins."]}
{"question_id": "2d91554c3f320a4bcfeb00aa466309074a206712", "predicted_answer": "", "predicted_evidence": ["The development of a match is performed by a matcher, which defines the process for finding the optimal match between two lists of entities. The most straightforward matcher is called the greedy matcher, which simply compares each pair of entities between the two datasets using a similarity function to compute the similarity between this pair. The similarity calculation is divided into three stages: firstly, the lens examines the entity pair and extracts information that can be more easily compared, for example a pair of labels one for each element. This is then fed into a feature extractor that analyses the facet to produce a numerical value that is assumed to be related to the similarity of this entity pair. Finally, a supervised similarity classifier is used to aggregate a number of features and is trained on existing training data. This then creates a single score between zero (totally dissimilar) and one (identical) for each pair of entities between the two datasets. The greedy matcher then proceeds by adding matches between the dataset that increase a topological constrained score (TCS), which is a function that both checks the validity and computes the score of a matching between two datasets.", "The type of an entity, in the case of wordnet this is a part-of-speech.", "The Princeton WordNet is a large, publicly available lexical semantic database of English nouns, verbs, adjectives and adverbs, grouped into synsets ( INLINEFORM0 117,000), which are aligned in terms of semantic and lexical relations. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches.", "The development of a match is performed by a matcher, which defines the process for finding the optimal match between two lists of entities. The most straightforward matcher is called the greedy matcher, which simply compares each pair of entities between the two datasets using a similarity function to compute the similarity between this pair. The similarity calculation is divided into three stages: firstly, the lens examines the entity pair and extracts information that can be more easily compared, for example a pair of labels one for each element. This is then fed into a feature extractor that analyses the facet to produce a numerical value that is assumed to be related to the similarity of this entity pair. Finally, a supervised similarity classifier is used to aggregate a number of features and is trained on existing training data. This then creates a single score between zero (totally dissimilar) and one (identical) for each pair of entities between the two datasets.", "Lithuanian BIBREF15"]}
{"question_id": "53362c2870cf76b7981c27b3520a71eb1e3e7965", "predicted_answer": "", "predicted_evidence": ["Polylingual WordNet is the largest multilingual resource released under an open license and the only resource that has been developed in a fully automatic manner. As such, the resource plays a number of useful roles in covering applications for languages or applications where the manually constructed wordnets do not have sufficient coverage. Moreover, this WordNet is intended to be a basis that can help in the translation of existing wordnets by providing a basis from which lexicographers can work.", "This wordnet is currently under active development and we plan to improve the quality of the resource over several iterations. The first release (1.0) was made in November 2016 and a second release has been made with this publication (1.1) in May 2017.", "The ratio of the number of tokens in each sentence. For symmetry this ratio is defined as INLINEFORM0 .", "Since only WordNet synsets are linked across different languages, we first align them with its translation equivalents, which is performed with their appearance within several million parallel sentences. In the next step we identify English sentences, which contain an English WordNet entry. Due to the multilingual nature of a parallel corpus, we identify the non-English Wordnet sense on the target side of the parallel corpus. Our approach is based on the assumption that a sentence shares the same semantic information as the WordNet entry sysnset if its translation, with the same mining or synset respectively, appears in the parallel target sentence. This disambiguation approach can be further strengthened, if translations of the targeted WordNet entry appear in several languages in the parallel corpus. Due to this assumption we use 16 different languages in our experiment, which requires 16 different non-English wordnets and parallel corpora. Besides the Princeton Wordnet, we engage wordnets, freely provided by the Open Multilingual Wordnet (OMW) web page, i.e.", "Croatian BIBREF8"]}
{"question_id": "5138121b9e9bd56962e69bfe49d5df5301cb7745", "predicted_answer": "", "predicted_evidence": ["In order to link Polylingual WordNet to other resources, we really on a dataset alignment system called NAISC (Nearly Automatic Alignment of SChema), which is a system designed to create linking between two resources. As such NAISC takes two lists of entities as input that may have the following:", "The type of an entity, in the case of wordnet this is a part-of-speech.", "For each word in each text we extract the GloVe vectors BIBREF41 and calculate the cosine similarity between these words, to give a value INLINEFORM0 between the INLINEFORM1 th and the INLINEFORM2 th word. We calculate a score as: INLINEFORM3", "We consider the two strings both as a set of words and a set of characters and compute the following functions INLINEFORM0 , INLINEFORM1 , INLINEFORM2 .", "One if all numbers (e.g, `6') in each text are found in the other, zero otherwise."]}
{"question_id": "25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "predicted_answer": "", "predicted_evidence": ["We compare the performance of state-of-the-art language models fine-tuned on Interview and other popular conversational datasets, demonstrating that Interview contains more complex dialog and better models the characteristics of natural spoken conversations. Our dataset is an order of magnitude larger than existing high-quality natural dialog datasets and contains speaker role annotations for each turn, facilitating the development of conversational agents and assistive systems for settings involving specific speaker roles, such as doctor-patient interviews or hosted talk shows.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog.", "The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "We use GPT2-small (Transformer with 12 layers, 768 hidden size, 12 heads, and 117M parameters) as the base architecture for all of our models. We perform BPE tokenization with the GPT2Tokenizer. We use the RAdam optimizer BIBREF30 with a learning rate of $10^{-6} \\times \\text{batch size} \\times \\text{no. of GPUs}$ to utilize linear scaling in multi-GPU training. Our models are trained to convergence on 8 NVIDIA Tesla V100 GPUs, with a batch size of 5 per GPU. We use teacher-forcing to calculate perplexity for all train/dev/test splits.", "We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service."]}
{"question_id": "d68cc9aaf0466b97354600a5646c3be4512fc096", "predicted_answer": "", "predicted_evidence": ["While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog.", "Dialog modeling of open-domain chit-chat predicts one turn of dialog from one or many context turn(s). Structured approaches for dialog modeling build on hierarchical RNNs BIBREF19, BIBREF20, BIBREF21, with recent work employing a simple concatenation of dialog history in a transformer-based architecture BIBREF22. We draw inspiration from recent works in dialog generation that model speakers via persistent `personas,' whose representations are learned from a set of grounding facts BIBREF23 or other non-conversational metadata BIBREF24. Our approach eschews external grounding and learns speaker embeddings via dialog modeling, similar to BIBREF25. We, however, propose to learn speaker embeddings for different roles and capture role-dependent lexical profiles in conversation.", "See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models.", "We generate a response conditioned on the host speaker role, to specifically model how an interview host speaks and inquires, contrary to speaker-agnostic dialog settings BIBREF28, BIBREF29. Individual guests appear sparsely and their utterances heavily rely on external world knowledge. Thus, we model host responses, which are generally aimed towards moderating the conversation via follow-up questions and acknowledgements. Role-specific generation like this can benefit the development of assistive technologies and role-dependent dialog systems.", "We decode the host response via top-$k$ sampling BIBREF27 with $k=5$. Results across all models on the test set are in tab:metrics."]}
{"question_id": "d038e5d2a6f85e68422caaf8b96cb046db6599fa", "predicted_answer": "", "predicted_evidence": ["See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models.", "We also investigate role change detection as a binary classification task for two-party dialogs. As a single turn of dialog may consist of multiple sentences, we aim to use a series of historical sentences and their speakers to classify whether a role change will occur in the next sentence of dialog. In contrast to previous textual speaker change detection tasks BIBREF33, we do not provide the target sentence for which we are predicting the role change. This setting is more realistic for a real-time assistive dialog system and online prediction in general.", "Dialog modeling of open-domain chit-chat predicts one turn of dialog from one or many context turn(s). Structured approaches for dialog modeling build on hierarchical RNNs BIBREF19, BIBREF20, BIBREF21, with recent work employing a simple concatenation of dialog history in a transformer-based architecture BIBREF22. We draw inspiration from recent works in dialog generation that model speakers via persistent `personas,' whose representations are learned from a set of grounding facts BIBREF23 or other non-conversational metadata BIBREF24. Our approach eschews external grounding and learns speaker embeddings via dialog modeling, similar to BIBREF25. We, however, propose to learn speaker embeddings for different roles and capture role-dependent lexical profiles in conversation.", "We fine-tune BERT BIBREF34 to encode the dialog history, classifying speaker changes with a linear layer over the [CLS] representation. To understand the role of contextual speaker information in this task, we investigate representing the dialog history with and without speaker labels for each turn. This is a difficult task on our dataset, as BERT obtains a 63.2 F1 score without speaker information, struggling to predict role changes substantially better than random. While the task remains difficult, the classifier benefits from the inclusion of speaker labels, learning speaker embeddings and achieving a 66.1 F1 score. We see the potential for further research toward learning speaker representations to predict role changes and infer the structure of dialogs.", "We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words."]}
{"question_id": "c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "predicted_answer": "", "predicted_evidence": ["We contribute a large-scale media dialog dataset that can act as a benchmark for complex open-domain, role-dependent grounded dialog. We present baseline model for role-conditioned dialog generation and show that they benefit from speaker information when added. In future work, we aim to perform temporal analyses of trends and biases within Interview and take advantage of the news setting to investigate external knowledge grounding in long natural conversations. These directions could potentially lead to more coherent free-form and assistive dialog systems.", "To measure the conditioning effect of speaker role profiles on host response generation, we generate a dialog turn with the gold host profile and a dialog history. We then compute the likelihood of generating that response conditioned on the same context but with the gold and nine randomly sampled hosts. As in BIBREF31, we rank the likelihoods for each host and report the host matching accuracy (HMA)\u2014proportion where the gold host is highest ranked\u2014and Mean Reciprocal Rank (MMR) BIBREF32 of the gold host. Our speaker-conditioned models achieve much higher HMA and MRR compared to strong speaker-agnostic baselines, indicating significant conditioning on host profiles.", "Dialog modeling of open-domain chit-chat predicts one turn of dialog from one or many context turn(s). Structured approaches for dialog modeling build on hierarchical RNNs BIBREF19, BIBREF20, BIBREF21, with recent work employing a simple concatenation of dialog history in a transformer-based architecture BIBREF22. We draw inspiration from recent works in dialog generation that model speakers via persistent `personas,' whose representations are learned from a set of grounding facts BIBREF23 or other non-conversational metadata BIBREF24. Our approach eschews external grounding and learns speaker embeddings via dialog modeling, similar to BIBREF25. We, however, propose to learn speaker embeddings for different roles and capture role-dependent lexical profiles in conversation.", "We fine-tune BERT BIBREF34 to encode the dialog history, classifying speaker changes with a linear layer over the [CLS] representation. To understand the role of contextual speaker information in this task, we investigate representing the dialog history with and without speaker labels for each turn. This is a difficult task on our dataset, as BERT obtains a 63.2 F1 score without speaker information, struggling to predict role changes substantially better than random. While the task remains difficult, the classifier benefits from the inclusion of speaker labels, learning speaker embeddings and achieving a 66.1 F1 score. We see the potential for further research toward learning speaker representations to predict role changes and infer the structure of dialogs.", "These role-specific speaker IDs are modeled by a speaker embedding layer of the same dimensions as the transformer hidden state, injected into the transformer input layer. We fine-tune GPT2 (Speaker GPT2) and DialoGPT (Speaker DialoGPT) on our dataset with speaker embeddings. We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation."]}
{"question_id": "369d7bc5351409910c7a5e05c0cbb5abab8e50ec", "predicted_answer": "", "predicted_evidence": ["See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models.", "For training and evaluation, we provide our model with up to 512 tokens of non-truncated historical turns. We use an 80-10-10 train/dev/test split with unique conversations in each split.", "We fine-tune BERT BIBREF34 to encode the dialog history, classifying speaker changes with a linear layer over the [CLS] representation. To understand the role of contextual speaker information in this task, we investigate representing the dialog history with and without speaker labels for each turn. This is a difficult task on our dataset, as BERT obtains a 63.2 F1 score without speaker information, struggling to predict role changes substantially better than random. While the task remains difficult, the classifier benefits from the inclusion of speaker labels, learning speaker embeddings and achieving a 66.1 F1 score. We see the potential for further research toward learning speaker representations to predict role changes and infer the structure of dialogs.", "We use GPT2-small (Transformer with 12 layers, 768 hidden size, 12 heads, and 117M parameters) as the base architecture for all of our models. We perform BPE tokenization with the GPT2Tokenizer. We use the RAdam optimizer BIBREF30 with a learning rate of $10^{-6} \\times \\text{batch size} \\times \\text{no. of GPUs}$ to utilize linear scaling in multi-GPU training. Our models are trained to convergence on 8 NVIDIA Tesla V100 GPUs, with a batch size of 5 per GPU. We use teacher-forcing to calculate perplexity for all train/dev/test splits. We avoid modeling salutations and sign-offs (which tend to be formulaic, speaker-independent, and specific to the radio station) by restricting the target turns to those with at least three prior turns and two following turns of conversation, resulting in a target training set of 87K host-only turns and 11K host-only turns for dev and test.", "We also investigate role change detection as a binary classification task for two-party dialogs. As a single turn of dialog may consist of multiple sentences, we aim to use a series of historical sentences and their speakers to classify whether a role change will occur in the next sentence of dialog. In contrast to previous textual speaker change detection tasks BIBREF33, we do not provide the target sentence for which we are predicting the role change. This setting is more realistic for a real-time assistive dialog system and online prediction in general."]}
{"question_id": "b9d9803ba24127f91ba4d7cff4da11492da20f09", "predicted_answer": "", "predicted_evidence": ["We compare the performance of state-of-the-art language models fine-tuned on Interview and other popular conversational datasets, demonstrating that Interview contains more complex dialog and better models the characteristics of natural spoken conversations. Our dataset is an order of magnitude larger than existing high-quality natural dialog datasets and contains speaker role annotations for each turn, facilitating the development of conversational agents and assistive systems for settings involving specific speaker roles, such as doctor-patient interviews or hosted talk shows.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog.", "The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "We use GPT2-small (Transformer with 12 layers, 768 hidden size, 12 heads, and 117M parameters) as the base architecture for all of our models. We perform BPE tokenization with the GPT2Tokenizer. We use the RAdam optimizer BIBREF30 with a learning rate of $10^{-6} \\times \\text{batch size} \\times \\text{no. of GPUs}$ to utilize linear scaling in multi-GPU training. Our models are trained to convergence on 8 NVIDIA Tesla V100 GPUs, with a batch size of 5 per GPU. We use teacher-forcing to calculate perplexity for all train/dev/test splits.", "We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service."]}
{"question_id": "7625068cc22a095109580b83eff48616387167c2", "predicted_answer": "", "predicted_evidence": ["We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.", "We use GPT2-small (Transformer with 12 layers, 768 hidden size, 12 heads, and 117M parameters) as the base architecture for all of our models. We perform BPE tokenization with the GPT2Tokenizer. We use the RAdam optimizer BIBREF30 with a learning rate of $10^{-6} \\times \\text{batch size} \\times \\text{no. of GPUs}$ to utilize linear scaling in multi-GPU training. Our models are trained to convergence on 8 NVIDIA Tesla V100 GPUs, with a batch size of 5 per GPU. We use teacher-forcing to calculate perplexity for all train/dev/test splits. We avoid modeling salutations and sign-offs (which tend to be formulaic, speaker-independent, and specific to the radio station) by restricting the target turns to those with at least three prior turns and two following turns of conversation, resulting in a target training set of 87K host-only turns and 11K host-only turns for dev and test.", "In these two-party conversations, each speaker takes an average of nine turns per dialog. Guests tend to speak longer on their turns, with 1.6x as many sentences spoken and 2x as many words per turn, and also use a more diverse vocabulary (1.6x size). Meanwhile, hosts ask five times as many questions as guests, with 40% of their dialog turns containing questions. When asking questions, hosts and guests use interrogative forms BIBREF26 at the same rate (65%). We note that the host and guest roles have differing discourse patterns, which support the notion of role modeling.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog."]}
{"question_id": "be0b438952048fe6bb91c61ba48e529d784bdcea", "predicted_answer": "", "predicted_evidence": ["We also investigate role change detection as a binary classification task for two-party dialogs. As a single turn of dialog may consist of multiple sentences, we aim to use a series of historical sentences and their speakers to classify whether a role change will occur in the next sentence of dialog. In contrast to previous textual speaker change detection tasks BIBREF33, we do not provide the target sentence for which we are predicting the role change. This setting is more realistic for a real-time assistive dialog system and online prediction in general.", "See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models.", "We fine-tune BERT BIBREF34 to encode the dialog history, classifying speaker changes with a linear layer over the [CLS] representation. To understand the role of contextual speaker information in this task, we investigate representing the dialog history with and without speaker labels for each turn. This is a difficult task on our dataset, as BERT obtains a 63.2 F1 score without speaker information, struggling to predict role changes substantially better than random. While the task remains difficult, the classifier benefits from the inclusion of speaker labels, learning speaker embeddings and achieving a 66.1 F1 score. We see the potential for further research toward learning speaker representations to predict role changes and infer the structure of dialogs.", "Speaker-conditioned models generate utterances closer to gold length than speaker-agnostic baselines, with significantly lower perplexity and higher BLEU scores. This indicates that including speaker information promotes the generation of higher fidelity responses. Our speaker models, especially Speaker GPT2, produce the most inquisitive responses (59.4% question-asking rate).", "This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog. We show statistics for Interview compared to other dialog datasets in tab:nprstats."]}
{"question_id": "a97137318025a6642ed0634f7159255270ba3d4f", "predicted_answer": "", "predicted_evidence": ["We also investigate role change detection as a binary classification task for two-party dialogs. As a single turn of dialog may consist of multiple sentences, we aim to use a series of historical sentences and their speakers to classify whether a role change will occur in the next sentence of dialog. In contrast to previous textual speaker change detection tasks BIBREF33, we do not provide the target sentence for which we are predicting the role change. This setting is more realistic for a real-time assistive dialog system and online prediction in general.", "See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models.", "We contribute a large-scale media dialog dataset that can act as a benchmark for complex open-domain, role-dependent grounded dialog. We present baseline model for role-conditioned dialog generation and show that they benefit from speaker information when added. In future work, we aim to perform temporal analyses of trends and biases within Interview and take advantage of the news setting to investigate external knowledge grounding in long natural conversations. These directions could potentially lead to more coherent free-form and assistive dialog systems.", "We fine-tune BERT BIBREF34 to encode the dialog history, classifying speaker changes with a linear layer over the [CLS] representation. To understand the role of contextual speaker information in this task, we investigate representing the dialog history with and without speaker labels for each turn. This is a difficult task on our dataset, as BERT obtains a 63.2 F1 score without speaker information, struggling to predict role changes substantially better than random. While the task remains difficult, the classifier benefits from the inclusion of speaker labels, learning speaker embeddings and achieving a 66.1 F1 score. We see the potential for further research toward learning speaker representations to predict role changes and infer the structure of dialogs.", "To measure the conditioning effect of speaker role profiles on host response generation, we generate a dialog turn with the gold host profile and a dialog history. We then compute the likelihood of generating that response conditioned on the same context but with the gold and nine randomly sampled hosts. As in BIBREF31, we rank the likelihoods for each host and report the host matching accuracy (HMA)\u2014proportion where the gold host is highest ranked\u2014and Mean Reciprocal Rank (MMR) BIBREF32 of the gold host. Our speaker-conditioned models achieve much higher HMA and MRR compared to strong speaker-agnostic baselines, indicating significant conditioning on host profiles."]}
{"question_id": "a24b2269b292fd0ee81d50303d1315383c594382", "predicted_answer": "", "predicted_evidence": ["Our models additionally exhibit several qualitative properties of high-quality and fluent conversation. We present a sample generation in tab:sampleconv (additional samples in the Appendix) that is indicative of broad trends across the test set. None of the models are able to introduce novel information (like Gold), but our speaker-conditioned models produce markedly better inquisitive responses. While GPT2 generates a natural-sounding short question with little relevance to the topic at hand, our Speaker DialoGPT model paraphrases previous turns and refers to existing entities to ask a substantial and coherent question. We further performed a human evaluation on a Likert scale to assess subjective dialog quality, with human raters preferring speaker model responses to speaker-agnostic models 62.5% of the time across 150 pairwise comparisons.", "To measure the conditioning effect of speaker role profiles on host response generation, we generate a dialog turn with the gold host profile and a dialog history. We then compute the likelihood of generating that response conditioned on the same context but with the gold and nine randomly sampled hosts. As in BIBREF31, we rank the likelihoods for each host and report the host matching accuracy (HMA)\u2014proportion where the gold host is highest ranked\u2014and Mean Reciprocal Rank (MMR) BIBREF32 of the gold host. Our speaker-conditioned models achieve much higher HMA and MRR compared to strong speaker-agnostic baselines, indicating significant conditioning on host profiles.", "We use GPT2-small (Transformer with 12 layers, 768 hidden size, 12 heads, and 117M parameters) as the base architecture for all of our models. We perform BPE tokenization with the GPT2Tokenizer. We use the RAdam optimizer BIBREF30 with a learning rate of $10^{-6} \\times \\text{batch size} \\times \\text{no. of GPUs}$ to utilize linear scaling in multi-GPU training. Our models are trained to convergence on 8 NVIDIA Tesla V100 GPUs, with a batch size of 5 per GPU. We use teacher-forcing to calculate perplexity for all train/dev/test splits. We avoid modeling salutations and sign-offs (which tend to be formulaic, speaker-independent, and specific to the radio station) by restricting the target turns to those with at least three prior turns and two following turns of conversation, resulting in a target training set of 87K host-only turns and 11K host-only turns for dev and test.", "Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "We contribute a large-scale media dialog dataset that can act as a benchmark for complex open-domain, role-dependent grounded dialog. We present baseline model for role-conditioned dialog generation and show that they benefit from speaker information when added. In future work, we aim to perform temporal analyses of trends and biases within Interview and take advantage of the news setting to investigate external knowledge grounding in long natural conversations. These directions could potentially lead to more coherent free-form and assistive dialog systems."]}
{"question_id": "7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "predicted_answer": "", "predicted_evidence": ["WS, Web Snippet, used in BIBREF7 , contains 12,237 web search snippets and each snippet belongs to one of 8 categories. The vocabulary contains 10,052 tokens and there are 15 words in one snippet on average.", "In this paper, we have presented a topic modelling framework named MetaLDA that can efficiently incorporate document and word meta information. This gains a significant improvement over others in terms of perplexity and topic quality. With two data augmentation techniques, MetaLDA enjoys full local conjugacy, allowing efficient Gibbs sampling, demonstrated by superiority in the per-iteration running time. Furthermore, without losing generality, MetaLDA can work with both regular texts and short texts. The improvement of MetaLDA over other models that also use meta information is more remarkable, particularly when the word-occurrence information is insufficient. As MetaLDA takes a particular approach for incorporating meta information on topic models, it is possible to apply the same approach to other Bayesian probabilistic models, where Dirichlet priors are used. Moreover, it would be interesting to extend our method to use real-valued meta information directly, which is the subject of future work.", "Specifically, the information of document INLINEFORM0 's labels is incorporated in INLINEFORM1 , the parameter of Dirichlet prior on INLINEFORM2 . As shown in Step UID12 , INLINEFORM3 is computed as a log linear combination of the labels INLINEFORM4 . Since INLINEFORM5 is binary, INLINEFORM6 is indeed the multiplication of INLINEFORM7 over all the active labels of document INLINEFORM8 , i.e., INLINEFORM9 . Drawn from the gamma distribution with mean 1, INLINEFORM10 controls the impact of label INLINEFORM11 on topic INLINEFORM12 . If label INLINEFORM13 has no or less impact on topic INLINEFORM14 , INLINEFORM15 is expected to be 1 or close to 1, and then INLINEFORM16 will have no or little influence on INLINEFORM17 and vice versa. The hyper-parameter INLINEFORM18 controls the variation of INLINEFORM19 .", "The intuition of our way of incorporating meta information is: At the document level, if two documents have more labels in common, their Dirichlet parameter INLINEFORM0 will be more similar, resulting in more similar topic distributions INLINEFORM1 ; At the word level, if two words have similar features, their INLINEFORM2 in topic INLINEFORM3 will be similar and then we can expect that their INLINEFORM4 could be more or less the same. Finally, the two words will have similar probabilities of showing up in topic INLINEFORM5 . In other words, if a topic \u201cprefers\u201d a certain word, we expect that it will also prefer other words with similar features to that word. Moreover, at both the document and the word level, different labels/features may have different impact on the topics ( INLINEFORM6 / INLINEFORM7 ), which is automatically learnt in MetaLDA.", "Gamma ratio 2 in Eq. ( SECREF17 ) is the Pochhammer symbol for a rising factorial, which can be augmented with an auxiliary variable INLINEFORM0 BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 as follows: +rCl+x* (d,k + md,k)(d,k)Gamma ratio 2 = td,k=0md,k Smd,ktd,k d,ktd,k where INLINEFORM1 indicates an unsigned Stirling number of the first kind. Gamma ratio 2 is a normalising constant for the probability of the number of tables in the Chinese Restaurant Process (CRP) BIBREF28 , INLINEFORM2 can be sampled by a CRP with INLINEFORM3 as the concentration and INLINEFORM4 as the number of customers: +rCl+x* td,k = i=1md,k Bern(d,kd,k+i) where INLINEFORM5 samples from the Bernoulli distribution."]}
{"question_id": "0fee37ebe0a010cf8bd665fa566306d8e7d12631", "predicted_answer": "", "predicted_evidence": ["The scores show that the improvement gained by MetaLDA over LDA on the short text datasets is larger than that on the regular text datasets. This is as expected because meta information serves as complementary information in MetaLDA and can have more significant impact when the data is sparser.", "The results show that MetaLDA outperformed all the competitors in terms of perplexity on nearly all the datasets, showing the benefit of using both document and word meta information. Specifically, we have the following remarks:", "At the document level, both MetaLDA-df-0.01 and DMR use priors to incorporate the document meta information and both of them were implemented in the SparseLDA framework. However, our variant is about 6 to 8 times faster than DMR on the Reuters dataset and more than 10 times faster on the WS dataset. Moreover, it can be seen that the larger the number of topics, the faster our variant is over DMR. At the word level, similar patterns can be observed: our MetaLDA-0.1-wf ran significantly faster than WF-LDA and LF-LDA especially when more topics are used (20-30 times faster on WS). It is not surprising that GPU-DMM has comparable running speed with our variant, because only one topic is allowed for each document in GPU-DMM. With both document and word meta information, MetaLDA still ran several times faster than DMR, LF-LDA, and WF-LDA.", "We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window. Those probabilities were computed on an external large corpus, i.e., a 5.48GB Wikipedia dump in our experiments. The NPMI score of each topic in the experiments is calculated with top 10 words ( INLINEFORM8 ) by the Palmetto package. Again, we report the average scores and the standard deviations over 5 random runs.", "In this section, we empirically study the efficiency of the models in term of per-iteration running time. The implementation details of our MetaLDA are as follows: (1) The SparseLDA framework BIBREF30 reduces the complexity of LDA to be sub-linear by breaking the conditional of LDA into three \u201cbuckets\u201d, where the \u201csmoothing only\u201d bucket is cached for all the documents and the \u201cdocument only\u201d bucket is cached for all the tokens in a document. We adopted a similar strategy when implementing MetaLDA. When only the document meta information is used, the Dirichlet parameters INLINEFORM0 for different documents in MetaLDA are different and asymmetric. Therefore, the \u201csmoothing only\u201d bucket has to be computed for each document, but we can cache it for all the tokens, which still gives us a considerable reduction in computing complexity."]}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "predicted_answer": "", "predicted_evidence": ["To test the hypothesis that the incorporation of meta information in MetaLDA can significantly improve the modelling accuracy in the cases where the corpus is sparse, we varied the proportion of documents used in training from 20% to 80% and used the remaining for testing. It is natural that when the proportion is small, the number of unseen words in testing documents will be large. Instead of simply excluding the unseen words in the previous experiments, here we compute the perplexity with unseen words for LDA, DMR, WF-LDA and the proposed MetaLDA. For perplexity calculation, INLINEFORM0 for each topic INLINEFORM1 and each token INLINEFORM2 in the test documents is needed. If INLINEFORM3 occurs in the training documents, INLINEFORM4 can be directly obtained. While if INLINEFORM5 is unseen, INLINEFORM6 can be estimated by the prior: INLINEFORM7 . For LDA and DMR which do not use word features, INLINEFORM8 ; For WF-LDA and MetaLDA which are with word features, INLINEFORM9 is computed with the features of the unseen token.", "In this section, we empirically study the efficiency of the models in term of per-iteration running time. The implementation details of our MetaLDA are as follows: (1) The SparseLDA framework BIBREF30 reduces the complexity of LDA to be sub-linear by breaking the conditional of LDA into three \u201cbuckets\u201d, where the \u201csmoothing only\u201d bucket is cached for all the documents and the \u201cdocument only\u201d bucket is cached for all the tokens in a document. We adopted a similar strategy when implementing MetaLDA. When only the document meta information is used, the Dirichlet parameters INLINEFORM0 for different documents in MetaLDA are different and asymmetric. Therefore, the \u201csmoothing only\u201d bucket has to be computed for each document, but we can cache it for all the tokens, which still gives us a considerable reduction in computing complexity.", "It is known that conventional topic models directly applied to short texts suffer from low quality topics, caused by the insufficient word co-occurrence information. Here we study whether or not the meta information helps MetaLDA improve topic quality, compared with other topic models that can also handle short texts. Table TABREF65 shows the NPMI scores on the three short text datasets. Higher scores indicate better topic coherence. All the models were trained with 100 topics. Besides the NPMI scores averaged over all the 100 topics, we also show the scores averaged over top 20 topics with highest NPMI, where \u201crubbish\u201d topics are eliminated, following BIBREF22 . It is clear that MetaLDA performed significantly better than all the other models in WS and AN dataset in terms of NPMI, which indicates that MetaLDA can discover more meaningful topics with the document and word meta information.", "The results show that MetaLDA outperformed all the competitors in terms of perplexity on nearly all the datasets, showing the benefit of using both document and word meta information. Specifically, we have the following remarks:", "In testing, we may encounter words that never occur in the training documents (a.k.a., unseen words or out-of-vocabulary words). There are two strategies for handling unseen words for calculating perplexity on test documents: ignoring them or keeping them in computing the perplexity. Here we investigate both strategies:"]}
{"question_id": "252599e53f52b3375b26d4e8e8b66322a42d2563", "predicted_answer": "", "predicted_evidence": ["Unlike most existing methods, our way of incorporating the meta information facilitates the derivation of an efficient Gibbs sampling algorithm. With two data augmentation techniques (i.e., the introduction of auxiliary variables), MetaLDA admits the local conjugacy and a close-form Gibbs sampling algorithm can be derived. Note that MetaLDA incorporates the meta information on the Dirichlet priors, so we can still use LDA's collapsed Gibbs sampling algorithm for the topic assignment INLINEFORM0 . Moreover, Step UID12 and UID9 show that one only needs to consider the non-zero entries of INLINEFORM1 and INLINEFORM2 in computing the full conditionals, which further reduces the inference complexity.", "In this section, we evaluate the proposed MetaLDA against several recent advances that also incorporate meta information on 6 real datasets including both regular and short texts. The goal of the experimental work is to evaluate the effectiveness and efficiency of MetaLDA's incorporation of document and word meta information both separately and jointly compared with other methods. We report the performance in terms of perplexity, topic coherence, and running time per iteration.", "Given a collection of INLINEFORM0 documents INLINEFORM1 , MetaLDA generates document INLINEFORM2 with a mixture of INLINEFORM3 topics and each topic INLINEFORM4 is a distribution over the vocabulary with INLINEFORM5 tokens, denoted by INLINEFORM6 . For document INLINEFORM7 with INLINEFORM8 words, to generate the INLINEFORM9 ( INLINEFORM10 ) word INLINEFORM11 , we first sample a topic INLINEFORM12 from the document's topic distribution INLINEFORM13 , and then sample INLINEFORM14 from INLINEFORM15 . Assume the labels of document INLINEFORM16 are encoded in a binary vector INLINEFORM17 where INLINEFORM18 is the total number of unique labels. INLINEFORM19 indicates label INLINEFORM20 is active in document INLINEFORM21 and vice versa. Similarly, the INLINEFORM22 features of token INLINEFORM23 are stored \u2202in a binary vector INLINEFORM24 .", "To sample/compute Eqs. ( SECREF17 )-( SECREF17 ), one only iterates over the documents where label INLINEFORM0 is active (i.e., INLINEFORM1 ). Thus, the sampling for all INLINEFORM2 takes INLINEFORM3 where INLINEFORM4 is the average number of documents where a label is active (i.e., the column-wise sparsity of INLINEFORM5 ). It is usually that INLINEFORM6 because if a label exists in nearly all the documents, it provides little discriminative information. This demonstrates how the sparsity of document meta information is leveraged. Moreover, sampling all the tables INLINEFORM7 takes INLINEFORM8 ( INLINEFORM9 is the total number of words in INLINEFORM10 ) which can be accelerated with the window sampling technique explained above.", "In this section, we empirically study the efficiency of the models in term of per-iteration running time. The implementation details of our MetaLDA are as follows: (1) The SparseLDA framework BIBREF30 reduces the complexity of LDA to be sub-linear by breaking the conditional of LDA into three \u201cbuckets\u201d, where the \u201csmoothing only\u201d bucket is cached for all the documents and the \u201cdocument only\u201d bucket is cached for all the tokens in a document. We adopted a similar strategy when implementing MetaLDA. When only the document meta information is used, the Dirichlet parameters INLINEFORM0 for different documents in MetaLDA are different and asymmetric. Therefore, the \u201csmoothing only\u201d bucket has to be computed for each document, but we can cache it for all the tokens, which still gives us a considerable reduction in computing complexity. However, when the word meta information is used, the SparseLDA framework no longer works in MetaLDA as the INLINEFORM1 parameters for each topic and each token are different."]}
{"question_id": "e12166fa9d6f63c4e92252c95c6a7bc96977ebf4", "predicted_answer": "", "predicted_evidence": ["Conversely, behavioral and mental problems greatly affect employee's productivity and loyalty. 70% of US workers are disengaged at work BIBREF3 . Each year lost productivity costs between 450 and 550 billion dollars. Disengaged workers are 87% more likely to leave their jobs than their more satisfied counterparts are BIBREF3 . The deaths by suicide among working age people (25-64 years old) costs more than $44 billion annually BIBREF4 . By contrast, behaviors such as helpfulness, kindness and optimism predict greater job satisfaction and positive or pleasurable engagement at work BIBREF5 .", "We observe in Table TABREF36 that annotators in R2 achieved the highest average inter-annotator agreements and the lowest standard deviations than the other two rounds, suggesting that tweets in R2 have the highest level of confidence being related to job/employment. As shown in Figure FIGREF4 , the annotated tweets in R1 are the outputs from INLINEFORM0 , the tweets in R2 are from INLINEFORM1 , and the tweets in R4 are from INLINEFORM2 . INLINEFORM3 is a supervised SVM classifier, while both INLINEFORM4 and INLINEFORM5 are rule-based classifiers. The higher agreement scores in R2 indicate that a trained SVM classifier can provide more reliable and less noisy predictions (i.e., labeled data). Further, higher agreement scores in R1 than R4 indicates that the rules in INLINEFORM6 are not intuitive as that in INLINEFORM7 and introduce ambiguities.", "We observe in Table TABREF36 that annotators in R2 achieved the highest average inter-annotator agreements and the lowest standard deviations than the other two rounds, suggesting that tweets in R2 have the highest level of confidence being related to job/employment. As shown in Figure FIGREF4 , the annotated tweets in R1 are the outputs from INLINEFORM0 , the tweets in R2 are from INLINEFORM1 , and the tweets in R4 are from INLINEFORM2 . INLINEFORM3 is a supervised SVM classifier, while both INLINEFORM4 and INLINEFORM5 are rule-based classifiers. The higher agreement scores in R2 indicate that a trained SVM classifier can provide more reliable and less noisy predictions (i.e., labeled data). Further, higher agreement scores in R1 than R4 indicates that the rules in INLINEFORM6 are not intuitive as that in INLINEFORM7 and introduce ambiguities. For example, tweets \u201cWhat a career from Vince young!\u201d and \u201cI hope Derrick Rose plays the best game of his career tonight\u201d both use career but convey different information: the first tweet was talking about this professional athlete's accomplishments while the second tweet was actually commenting on the game the user was watching.", "Working American adults spend more than one third of their daily time on job-related activities BIBREF0 \u2014more than on anything else. Any attempt to understand a working individual's experiences, state of mind, or motivations must take into account their life at work. In the extreme, job dissatisfaction poses serious health risks and even leads to suicide BIBREF1 , BIBREF2 .", "\"topic_human\":\"NA\","]}
{"question_id": "d4cb704e93086a2246a8caa5c1035e8297b8f4c0", "predicted_answer": "", "predicted_evidence": ["\"topic_human\":\"NA\",", "\"source_machine\":\"personal\",", "As shown in Figure FIGREF4 , the annotated tweets in R1 are the outputs from INLINEFORM0 , the tweets in R2 are from INLINEFORM1 , and the tweets in R4 are from INLINEFORM2 . INLINEFORM3 is a supervised SVM classifier, while both INLINEFORM4 and INLINEFORM5 are rule-based classifiers. The higher agreement scores in R2 indicate that a trained SVM classifier can provide more reliable and less noisy predictions (i.e., labeled data). Further, higher agreement scores in R1 than R4 indicates that the rules in INLINEFORM6 are not intuitive as that in INLINEFORM7 and introduce ambiguities. For example, tweets \u201cWhat a career from Vince young!\u201d and \u201cI hope Derrick Rose plays the best game of his career tonight\u201d both use career but convey different information: the first tweet was talking about this professional athlete's accomplishments while the second tweet was actually commenting on the game the user was watching. Hence crowdsourcing workers working on INLINEFORM8 tasks read more ambiguous tweets and solved more difficult problems than those in INLINEFORM9 tasks did.", "However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter. Their key strategy was to build a relatively clean training dataset from large volume of Twitter data with minimum human efforts. Their real world testing demonstrates the capability of their system to identify major life events accurately. The most parallel work that we can leverage here is the method and corpus developed by Liu et al. liu2016understanding, which is an effective supervised learning system to detect job-related tweets from individual and business accounts. To fully utilize the existing resources, we build upon the corpus by Liu et al.", "We trained our first SVM classification model INLINEFORM0 and then used it to label the remaining data in our data pool."]}
{"question_id": "a11b5eb928a6db9a0e3bb290ace468ff1685d253", "predicted_answer": "", "predicted_evidence": ["In order to identify probable job-related tweets which are talking about paid positions of regular employment while excluding noises (such as students discussing homework or school-related activities, or people complimenting others), we defined a simple term-matching classifier with inclusion and exclusion terms in the first step (see Table TABREF9 ).", "Classifier INLINEFORM0 consists of two rules: the matched tweet must contain at least one word in the Include lexicon and it cannot contain any word in the Exclude lexicon. Before applying filtering rules, we pre-processed each tweet by (1) converting all words to lower cases; (2) stripping out punctuation and special characters; and (3) normalizing the tweets by mapping out-of-vocabulary phrases (such as abbreviations and acronyms) to standard phrases using a dictionary of more than 5,400 slang terms in the Internet.", "Our conjecture about crowdsourced annotations, based on the experiments and conclusions from BIBREF17 , is that non-expert contributors could produce comparable quality of annotations when evaluating against those gold standard annotations from experts. And it is similarly effective to use the labeled tweets with high inter-annotator agreement among multiple non-expert annotators from crowdsourcing platforms to build robust models as doing so on expert-labeled data.", "However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter. Their key strategy was to build a relatively clean training dataset from large volume of Twitter data with minimum human efforts. Their real world testing demonstrates the capability of their system to identify major life events accurately. The most parallel work that we can leverage here is the method and corpus developed by Liu et al. liu2016understanding, which is an effective supervised learning system to detect job-related tweets from individual and business accounts. To fully utilize the existing resources, we build upon the corpus by Liu et al.", "Conversely, behavioral and mental problems greatly affect employee's productivity and loyalty. 70% of US workers are disengaged at work BIBREF3 . Each year lost productivity costs between 450 and 550 billion dollars. Disengaged workers are 87% more likely to leave their jobs than their more satisfied counterparts are BIBREF3 . The deaths by suicide among working age people (25-64 years old) costs more than $44 billion annually BIBREF4 . By contrast, behaviors such as helpfulness, kindness and optimism predict greater job satisfaction and positive or pleasurable engagement at work BIBREF5 ."]}
{"question_id": "275b2c22b6a733d2840324d61b5b101f2bbc5653", "predicted_answer": "", "predicted_evidence": ["We have our third part of human-annotated data (Part-3): tweets reviewed and corrected by the community annotators.", "We paid each worker $1.00 per HIT and gave extra bonuses to those who completed multiple HITs. We rejected workers who did not provide consistent answers to the duplicate tweets in each HIT. Before publishing the HITs to crowdsourcing workers, we consulted with Turker Nation to ensure that we treat and compensate workers fairly for their requested tasks.", "in the class of job-related tweets. Nearly every job-related tweet that contained at least one of the following hashtags: #veteranjob, #job, #jobs, #tweetmyjobs, #hiring, #retail, #realestate, #hr also had a URL embedded. We counted the tweets containing only the listed hashtags, and the tweets having both the queried hashtags and embedded URL, and summarized the statistics in Table TABREF34 . By spot checking we found such tweets always led to recruitment websites. This observation suggests that these tweets with similar \u201chashtags + URL\u201d patterns originated from business agencies or companies instead of personal accounts, because individuals by common sense are unlikely to post recruitment advertising.", "\"source_human\":\"NA\"", "As shown in Figure FIGREF4 , the annotated tweets in R1 are the outputs from INLINEFORM0 , the tweets in R2 are from INLINEFORM1 , and the tweets in R4 are from INLINEFORM2 . INLINEFORM3 is a supervised SVM classifier, while both INLINEFORM4 and INLINEFORM5 are rule-based classifiers. The higher agreement scores in R2 indicate that a trained SVM classifier can provide more reliable and less noisy predictions (i.e., labeled data). Further, higher agreement scores in R1 than R4 indicates that the rules in INLINEFORM6 are not intuitive as that in INLINEFORM7 and introduce ambiguities. For example, tweets \u201cWhat a career from Vince young!\u201d and \u201cI hope Derrick Rose plays the best game of his career tonight\u201d both use career but convey different information: the first tweet was talking about this professional athlete's accomplishments while the second tweet was actually commenting on the game the user was watching. Hence crowdsourcing workers working on INLINEFORM8 tasks read more ambiguous tweets and solved more difficult problems than those in INLINEFORM9 tasks did."]}
{"question_id": "f1f7a040545c9501215d3391e267c7874f9a6004", "predicted_answer": "", "predicted_evidence": ["'Reorganiza\u00e7\u00e3o','Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro',", "'Question\u00e1rio', 'Quadro', 'Relat\u00f3rio', 'Redu\u00e7\u00e3o', 'Reorganiza\u00e7\u00e3o','Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto',", "'Neste', 'Nesta', 'Nestas', 'Noutro', 'Outros', 'Outro', 'Outra', 'Outras', 'Onde', 'Poucos', 'Poucas', 'Perante', 'Pela', 'Rec\u00e9m', 'Tal', 'V\u00e1rios', 'V\u00e1rias', 'V\u00f3s', 'Aceite', 'Comprometo', 'Cabe', 'Coloca', 'Conhecemos', 'Casado', 'Considerava', 'Desejo', 'Dev\u00edamos', 'Escolhiam, 'Executa', 'Fa\u00e7a', 'Fica', 'Interrompidas', 'Indicar', 'Inclu\u00eddo', 'Leva', 'Morrer', 'Ouvistes', 'Prestaste', 'Praticou', 'Pressiona', 'Pensa', 'Poder', 'Podes', 'Revolta', 'Sabe',", "'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal', 'Minutos', 'Meses', 'Ano', 'Anos', 'Hoje'} INLINEFORM0 {Portuguese stopwords on R}", "'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal', 'Minutos', 'Meses',"]}
{"question_id": "b6f4fd6bc76bfcbc15724a546445908afa6d922c", "predicted_answer": "", "predicted_evidence": ["'Publica\u00e7\u00e3o', 'Question\u00e1rio', 'Quadro', 'Relat\u00f3rio', 'Redu\u00e7\u00e3o', 'Reorganiza\u00e7\u00e3o','Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho',", "'V\u00f3s', 'Aceite', 'Comprometo', 'Cabe', 'Coloca', 'Conhecemos', 'Casado', 'Considerava', 'Desejo', 'Dev\u00edamos', 'Escolhiam, 'Executa', 'Fa\u00e7a', 'Fica', 'Interrompidas', 'Indicar', 'Inclu\u00eddo', 'Leva', 'Morrer', 'Ouvistes', 'Prestaste', 'Praticou', 'Pressiona', 'Pensa', 'Poder', 'Podes', 'Revolta', 'Sabe', 'Ser', 'Ter', 'Toque', 'Toma', 'Trata', 'Vens', 'Verificou', 'Viver', 'Vivemos', 'Venho', 'Rea\u00e7\u00e3o', 'Sess\u00e3o', 'Testamento', 'Toler\u00e2ncia', 'T\u00e9rmino', 'Vit\u00f3ria', 'Visita',", "'Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio',", "'Irregularidades', 'Internet', 'Lda', 'Manuten\u00e7\u00e3o', 'Nomeado', 'Obedi\u00eancia', 'Peti\u00e7\u00e3o', 'Passaporte', 'Proposta', 'Programa', 'Proibi\u00e7\u00e3o', 'Paz', 'Publica\u00e7\u00e3o', 'Question\u00e1rio', 'Quadro', 'Relat\u00f3rio', 'Redu\u00e7\u00e3o', 'Reorganiza\u00e7\u00e3o','Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura',", "'Fica', 'Interrompidas', 'Indicar', 'Inclu\u00eddo', 'Leva', 'Morrer', 'Ouvistes', 'Prestaste', 'Praticou', 'Pressiona', 'Pensa', 'Poder', 'Podes', 'Revolta', 'Sabe', 'Ser', 'Ter', 'Toque', 'Toma', 'Trata', 'Vens', 'Verificou', 'Viver', 'Vivemos', 'Venho', 'Rea\u00e7\u00e3o', 'Sess\u00e3o', 'Testamento', 'Toler\u00e2ncia', 'T\u00e9rmino', 'Vit\u00f3ria', 'Visita', 'Harmonia', 'Iniciado', 'Instala\u00e7\u00e3o', 'Ibidem', 'Inventaria\u00e7\u00e3o', 'Irregularidades', 'Internet', 'Lda', 'Manuten\u00e7\u00e3o', 'Nomeado', 'Obedi\u00eancia', 'Peti\u00e7\u00e3o', 'Passaporte',"]}
{"question_id": "3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "predicted_answer": "", "predicted_evidence": ["'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal',", "'Quadro', 'Relat\u00f3rio', 'Redu\u00e7\u00e3o', 'Reorganiza\u00e7\u00e3o','Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro',", "'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal', 'Minutos', 'Meses', 'Ano', 'Anos',", "'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal', 'Minutos', 'Meses',", "'papa', 'duque', 'duquesa', 'conde', 'condessa', 'visconde', 'viscondessa', 'rei', 'ra\u00ednha', 'pr\u00edncipe', 'princesa', 'marqu\u00eas', 'marquesa', 'bar\u00e3o', 'baronesa', 'bispo', 'presidente', 'secret\u00e1rio', 'secret\u00e1ria', 'ministro', 'ministra', 'primeiro', 'primeira', 'deputado', 'deputada', 'general', 'tenente', 'capit\u00e3o', 'capit\u00e3', 'sargento', 'governador', 'governadora', 'diretor', 'director', 'diretora', 'directora', 'ex', 'filho', 'filha', irm\u00e3o', 'irm\u00e3', 'pai', 'm\u00e3e', 'tio', 'tia', 'padrinho',"]}
{"question_id": "c316d7d0c80b8f720ff90a8bb84a8b879a3ef7ea", "predicted_answer": "", "predicted_evidence": ["'Quadro', 'Relat\u00f3rio', 'Redu\u00e7\u00e3o', 'Reorganiza\u00e7\u00e3o','Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro',", "'Comprometo', 'Cabe', 'Coloca', 'Conhecemos', 'Casado', 'Considerava', 'Desejo', 'Dev\u00edamos', 'Escolhiam, 'Executa', 'Fa\u00e7a', 'Fica', 'Interrompidas', 'Indicar', 'Inclu\u00eddo', 'Leva', 'Morrer', 'Ouvistes', 'Prestaste', 'Praticou', 'Pressiona', 'Pensa', 'Poder', 'Podes', 'Revolta', 'Sabe', 'Ser', 'Ter', 'Toque', 'Toma', 'Trata', 'Vens', 'Verificou', 'Viver', 'Vivemos', 'Venho', 'Rea\u00e7\u00e3o', 'Sess\u00e3o', 'Testamento', 'Toler\u00e2ncia', 'T\u00e9rmino', 'Vit\u00f3ria', 'Visita', 'Harmonia', 'Iniciado',", "'V\u00e1rios', 'V\u00e1rias', 'V\u00f3s', 'Aceite', 'Comprometo', 'Cabe', 'Coloca', 'Conhecemos', 'Casado', 'Considerava', 'Desejo', 'Dev\u00edamos', 'Escolhiam, 'Executa', 'Fa\u00e7a', 'Fica', 'Interrompidas', 'Indicar', 'Inclu\u00eddo', 'Leva', 'Morrer', 'Ouvistes', 'Prestaste', 'Praticou', 'Pressiona', 'Pensa', 'Poder', 'Podes', 'Revolta', 'Sabe', 'Ser', 'Ter', 'Toque', 'Toma', 'Trata', 'Vens', 'Verificou', 'Viver', 'Vivemos', 'Venho', 'Rea\u00e7\u00e3o', 'Sess\u00e3o', 'Testamento', 'Toler\u00e2ncia', 'T\u00e9rmino', 'Vit\u00f3ria',", "'V\u00f3s', 'Aceite', 'Comprometo', 'Cabe', 'Coloca', 'Conhecemos', 'Casado', 'Considerava', 'Desejo', 'Dev\u00edamos', 'Escolhiam, 'Executa', 'Fa\u00e7a', 'Fica', 'Interrompidas', 'Indicar', 'Inclu\u00eddo', 'Leva', 'Morrer', 'Ouvistes', 'Prestaste', 'Praticou', 'Pressiona', 'Pensa', 'Poder', 'Podes', 'Revolta', 'Sabe', 'Ser', 'Ter', 'Toque', 'Toma', 'Trata', 'Vens', 'Verificou', 'Viver', 'Vivemos', 'Venho', 'Rea\u00e7\u00e3o', 'Sess\u00e3o', 'Testamento', 'Toler\u00e2ncia', 'T\u00e9rmino', 'Vit\u00f3ria', 'Visita',", "'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal', 'Minutos', 'Meses',"]}
{"question_id": "a786cceba4372f6041187c426432853eda03dca6", "predicted_answer": "", "predicted_evidence": ["Trolling behavior is present and has been studied in all kinds of online media: online magazines BIBREF28, social networking sites BIBREF20, online computer games BIBREF29, online encyclopedia BIBREF30, and online newspapers BIBREF31, among others.", "We performed different experiments with the hyper-parameters of the graph embeddings. With smaller dimensionality (i.e., using 16 dimensions instead of 128), we noticed 2\u20133 points of absolute decrease in accuracy across the board.", "Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018.", "Moreover, we found that using all of the data for learning the embeddings was better than focusing only on users that we target in this study, namely left, right, and news feed, i.e., using the rest of the data adds additional context to the embedding space, and makes the target labels more contextually distinguishable. Similarly, we observe 5\u20136 points of absolute drop in accuracy when training our embeddings on tweets by trolls labeled as left, right, and news feed.", "We build an undirected User-to-Mentioned-User (U2M) graph, where the nodes are users, and there is an edge $(u,v)$ between two nodes if user $u$ mentions user $v$ in their tweets (i.e., $u$ has authored a tweet that contains \u201c@$v$\u201d ). We run node2vec on this graph and we extract the embeddings for the users. As we are interested only in the troll users, we ignore the embeddings of users who are only mentioned by other trolls. We use 128 dimensions for the output embeddings. The embeddings extracted from this graph capture how similar troll users are according to the targets of their discussions on the social network."]}
{"question_id": "a837dcbd339e27a974e28944178c790a5b0b37c0", "predicted_answer": "", "predicted_evidence": ["Looking at individual features, for both T1 and T2, the embeddings from U2M outperform those from U2H and from BERT. One possible reason is that the U2M graph is larger, and thus contains more information. It is also possible that the social circle of a troll user is more indicative than the hashtags they used. Finally, the textual content on Twitter is quite noisy, and thus the BERT embeddings perform slightly worse when used alone.", "In mathematical terms, graph embeddings can be expressed as a function $f: V \\rightarrow R^d$ from the set of vertices $V$ to a set of embeddings, where $d$ is the dimensionality of the embeddings. The function $f$ can be represented as a matrix of dimensions $|V| \\times d$. In our experiments, we train Graph Embeddings in an unsupervised manner by using node2vec BIBREF67, which is based on random walks over the graph. Essentially, this is an application of the well-known skip-gram model BIBREF68 from word2vec to random walks on graphs.", "We build an undirected User-to-Mentioned-User (U2M) graph, where the nodes are users, and there is an edge $(u,v)$ between two nodes if user $u$ mentions user $v$ in their tweets (i.e., $u$ has authored a tweet that contains \u201c@$v$\u201d ). We run node2vec on this graph and we extract the embeddings for the users. As we are interested only in the troll users, we ignore the embeddings of users who are only mentioned by other trolls. We use 128 dimensions for the output embeddings. The embeddings extracted from this graph capture how similar troll users are according to the targets of their discussions on the social network.", "One advantage of using distant supervision is that we can get insights about the behavior of a newly-discovered troll farm quickly and effortlessly. Differently from troll accounts in social media, which usually have a high churn rate, news media accounts in social media are quite stable. Therefore, the latter can be used as an anchor point to understand the behavior of trolls, for which data may not be available.", "Such farms usually consist of state-sponsored agents who control a set of pseudonymous user accounts and personas, the so-called \u201csockpuppets\u201d, which disseminate misinformation and propaganda in order to sway opinions, destabilize the society, and even influence elections BIBREF0."]}
{"question_id": "c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "predicted_answer": "", "predicted_evidence": ["Next, we compared to the work of BIBREF2, who had a fully supervised learning scenario, based on Tarde's Actor-Network Theory. They paid more attention to the content of the tweet by applying a text-distance metric in order to capture the semantic distance between two sequences. In contrast, we focus on critical elements of information that are salient in Twitter: hashtags and user mentions. By building a connection between users, hashtags, and user mentions, we effectively filtered out the noise and we focused only on the most sensitive type of context, thus automatically capturing features from this network via graph embeddings.", "Gianmarco De Francisci Morales acknowledges support from Intesa Sanpaolo Innovation Center. The funder had no role in the study design, in the data collection and analysis, in the decision to publish, or in the preparation of the manuscript.", "Label Propagation ($b$) is a transductive, graph-based, semi-supervised machine learning algorithm that, given a small set of labeled examples, assigns labels to previously unlabeled examples. The labels of each example change in relationship to the labels of neighboring ones in a properly-defined graph.", "In this case, we use the same representation for the media as in the proxy model case above, as described by Equation DISPLAY_FORM17. Then, we build a similarity graph among media and users based on their embeddings. For each pair $x,y \\in U \\cup M$ there is an edge in the similarity graph $(x,y) \\in E$ iff", "In the first case, the set of edges among users $U$ in the similarity graph $G$ consists of the logical OR between the 2-hop closure of the U2H and the U2M graph. That is, for each two users $u, v \\in U$, there is an edge in the similarity graph $(u,v) \\in E$ if $u$ and $v$ share a common hashtag or a common user mention"]}
{"question_id": "16a10c1681dc5a399b6d34b4eed7bb1fef816dd0", "predicted_answer": "", "predicted_evidence": ["Finally, we can train a LR model that uses $R(m)$ as features and the label for the medium $l(m)$. This model can be applied to predict the label of a user $u$ by using the same type of representation $R(u)$, and the label mapping in Equation DISPLAY_FORM16.", "Label Propagation ($b$) is a transductive, graph-based, semi-supervised machine learning algorithm that, given a small set of labeled examples, assigns labels to previously unlabeled examples. The labels of each example change in relationship to the labels of neighboring ones in a properly-defined graph.", "We consider two possible scenarios. The first, prototypical ML scenario is supervised learning, where we want to learn a function from users to categories {left, right, news feed}, and the ground truth labels for the troll users are available. This scenario has been considered previously in the literature by BIBREF2. Unfortunately, a solution for such a scenario is not directly applicable to a real-world use case. Suppose a new troll farm trying to sway the upcoming European or US elections has just been discovered. While the identities of the accounts might be available, the labels to learn from would not be present. Thus, any supervised machine learning approach would fall short of being a fully automated solution to our initial problem.", "The behavior of political trolls has been analyzed in different recent circumstances, such as the 2016 US Presidential Elections and the Brexit referendum in UK BIBREF0, BIBREF1. However, this kind of analysis requires painstaking and time-consuming manual labor to sift through the data and to categorize the trolls according to their actions. Our goal in the current paper is to automate this process with the help of machine learning (ML). In particular, we focus on the case of the 2016 US Presidential Elections, for which a public dataset from Twitter is available. For this case, we consider only accounts that post content in English, and we wish to divide the trolls into some of the functional categories identified by BIBREF0: left troll, right troll, and news feed.", "We can therefore create a representation for a medium by aggregating the embeddings of the users that mention the target medium. Such a representation is convenient as it lies in the same space as the user representation. In particular, given a medium $m \\in M$, we compute its representation $R(m)$ as"]}
{"question_id": "2ca3ca39d59f448e30be6798514709be7e3c62d8", "predicted_answer": "", "predicted_evidence": ["The task consists of answering a cloze-style question, the answer to which depends on the understanding of a context document provided with the question. The model is also provided with a set of possible answers from which the correct one is to be selected. This can be formalized as follows:", "Our model architecture was inspired by ptrnet BIBREF9 in using an attention mechanism to select the answer in the context rather than to blend words from the context into an answer representation. While a ptrnet consists of an encoder as well as a decoder, which uses the attention to select the output at each step, our model outputs the answer in a single step. Furthermore, the pointer networks assume that no input in the sequence appears more than once, which is not the case in our settings.", "To train the model we used stochastic gradient descent with the ADAM update rule BIBREF14 and learning rate of INLINEFORM0 or INLINEFORM1 . During training we minimized the following negative log-likelihood with respect to INLINEFORM2 : DISPLAYFORM0", "Cloze-style questions BIBREF2 , i.e. questions formed by removing a phrase from a sentence, are an appealing form of such questions (for example see Figure FIGREF1 ). While the task is easy to evaluate, one can vary the context, the question sentence or the specific phrase missing in the question to dramatically change the task structure and difficulty.", "In this section we introduce the task that we are seeking to solve and relevant large-scale datasets that have recently been introduced for this task."]}
{"question_id": "df7fb8e6e44c9c5af3f19dde762c75cbf2f8452f", "predicted_answer": "", "predicted_evidence": ["Most of the information humanity has gathered up to this point is stored in the form of plain text. Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence. One way of testing the level of text understanding is simply to ask the system questions for which the answer can be inferred from the text. A well-known example of a system that could make use of a huge collection of unstructured documents to answer questions is for instance IBM's Watson system used for the Jeopardy challenge BIBREF0 .", "In the first part of this article we introduce the task at hand and the main aspects of the relevant datasets. Then we present our own model to tackle the problem. Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the performance of our model.", "The key difference between the Attentive Reader and our model is that the Attentive Reader uses attention to compute a fixed length representation INLINEFORM0 of the document INLINEFORM1 that is equal to a weighted sum of contextual embeddings of words in INLINEFORM2 , that is INLINEFORM3 . A joint query and document embedding INLINEFORM4 is then a non-linear function of INLINEFORM5 and the query embedding INLINEFORM6 . This joint embedding INLINEFORM7 is in the end compared against all candidate answers INLINEFORM8 using the dot product INLINEFORM9 , in the end the scores are normalized by INLINEFORM10 . That is: INLINEFORM11 .", "The effect of increasing number of candidate answers on the model's accuracy can be seen in Figure FIGREF38 . We can clearly see that as the number of candidate answers increases, the accuracy drops. On the other hand, the amount of examples with large number of candidate answers is quite small (Figure FIGREF38 ).", "Our model architecture was inspired by ptrnet BIBREF9 in using an attention mechanism to select the answer in the context rather than to blend words from the context into an answer representation. While a ptrnet consists of an encoder as well as a decoder, which uses the attention to select the output at each step, our model outputs the answer in a single step. Furthermore, the pointer networks assume that no input in the sequence appears more than once, which is not the case in our settings."]}
{"question_id": "20e2b517fddb0350f5099c39b16c2ca66186d09b", "predicted_answer": "", "predicted_evidence": ["A model presented in BIBREF7 is inspired by the Attentive Reader. One difference is that the attention weights are computed with a bilinear term instead of simple dot-product, that is INLINEFORM0 . The document embedding INLINEFORM1 is computed using a weighted sum as in the Attentive Reader, INLINEFORM2 . In the end INLINEFORM3 , where INLINEFORM4 is a new embedding function.", "The Dynamic Entity Representation model BIBREF12 has a complex architecture also based on the weighted attention mechanism and max-pooling over contextual embeddings of vectors for each named entity.", "Finally, since the summation of attention in our model inherently favours frequently occurring tokens, we also visualize how the accuracy depends on the frequency of the correct answer in the document. Figure FIGREF41 shows that the accuracy significantly drops as the correct answer gets less and less frequent in the document compared to other candidate answers. On the other hand, the correct answer is likely to occur frequently (Fig. FIGREF41 ).", "To improve on the initial accuracy, a heuristic approach called self supervision is used in BIBREF3 to help the network to select the right supporting \u201cmemories\u201d using an attention mechanism showing similarities to the ours. Plain MenNN without this heuristic are not competitive on these machine reading tasks. Our model does not need any similar heuristics.", "Our model architecture was inspired by ptrnet BIBREF9 in using an attention mechanism to select the answer in the context rather than to blend words from the context into an answer representation. While a ptrnet consists of an encoder as well as a decoder, which uses the attention to select the output at each step, our model outputs the answer in a single step. Furthermore, the pointer networks assume that no input in the sequence appears more than once, which is not the case in our settings."]}
{"question_id": "70512cc9dcd45157e40c8d1f85e82d21ade7645b", "predicted_answer": "", "predicted_evidence": ["A model presented in BIBREF7 is inspired by the Attentive Reader. One difference is that the attention weights are computed with a bilinear term instead of simple dot-product, that is INLINEFORM0 . The document embedding INLINEFORM1 is computed using a weighted sum as in the Attentive Reader, INLINEFORM2 . In the end INLINEFORM3 , where INLINEFORM4 is a new embedding function.", "On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%. The average performance of the top 20% models according to validation accuracy is 69.9% which is even 0.5% better than the single best-validation model. This shows that there were many models that performed better on test set than the best-validation model. Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets..", "Qualitative analysis of reasoning patterns needed to answer questions in the CNN dataset together with human performance on this task are provided in BIBREF7 .", "One way of altering the task difficulty is to vary the word type being replaced, as in BIBREF3 . The complexity of such variation comes from the fact that the level of context understanding needed in order to correctly predict different types of words varies greatly. While predicting prepositions can easily be done using relatively simple models with very little context knowledge, predicting named entities requires a deeper understanding of the context.", "To train the model we used stochastic gradient descent with the ADAM update rule BIBREF14 and learning rate of INLINEFORM0 or INLINEFORM1 . During training we minimized the following negative log-likelihood with respect to INLINEFORM2 : DISPLAYFORM0"]}
{"question_id": "fd556a038c36abc88a800d9d4f2cfa0aef6f5aba", "predicted_answer": "", "predicted_evidence": ["Among neural networks, LSTMs BIBREF0 are commonly used for language modeling. Although new architectures BIBREF1, BIBREF2 challenge this standard, LSTMs remain competitive for language modeling BIBREF3. However, despite the success of LM LSTMs, it is not clear what makes them so effective. In particular, are representations derived through language modeling able to effectively encode syntactic structures and relations? Do they encode them in a reliable and systematic way?", "Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans.", "For grammatical sentences only, we also conduct meaningfulness evaluations. Similarly to our grammaticality experiment, users are asked to grade 50 sentences from 1 to 10, where 1 is meaningless and 10 is meaningful. They were also shown examples of meaningful and meaningless grammaticality correct German sentences before starting the evaluations. Constraints are the same as above, except that all sentences are grammatical and that there are thus only 36 possibilities per template.", "Each of these log probabilities can be read from the softmax outputs of the LSTM, or directly estimated in the case of the unigram and bigram models. We also tried normalizing for unigram frequency as proposed by BIBREF20 but found like BIBREF6 that it did not improve results for the LSTM.", "We depart from these approaches because our test set encompasses whole sentence variations, such as argument reordering. Word probes are therefore less apt to capture such changes. Instead, we choose to follow BIBREF6 and BIBREF7 in taking the more general approach of comparing whole sentence probabilities as our grammaticality probe. This method, which also corresponds to the sentence-level LogProb acceptability measure of BIBREF14, evaluates whether the model assigns a higher log probability to sentences which are grammatical than to sentences which are not."]}
{"question_id": "9119fbfba84d298014d1b74e0e3d30330320002c", "predicted_answer": "", "predicted_evidence": ["We use the pre-trained word-level language model (German WordNLM) described and trained by BIBREF8. The model is a two-layer LSTM without attention, a hidden dimension of 1,204, and word embeddings of dimension 200 for the 50,000 most frequent words. It was trained on a corpus from German Wikipedia, totalling 819 million words. The 50,000 most-frequent words in this corpus are used as the vocabulary and embedded in 200-dimensional vector space. The model reaches a perplexity of 37.96 on this dataset. We use unigram and bigram language models that use the same corpus with Laplace smoothing as baselines. The probability of test sentences according to the language models is computed using the chain rule:", "For each case violation, we generated 36 sentences containing a case violation from every template. Thus, from each of our 50 templates, we generated 36 valid grammatical variations and 108 ungrammatical variations. Note also that throughout the number of words in our dataset stays constant (11 words per sentence), so that log probabilities are more comparable. Overall, our dataset comprises 7,200 sentences, of which 1,800 are grammatical and 5,400 are ungrammatical.", "Verb argument structure is typically correlated to sentence position in many languages like English. But in other languages with relatively free word order, it is indicated by morphological markers. Here, we study German, where the arguments of a verb can occur in any position (when occurring within a relative clause), and is indicated by the case of the noun phrase (nominative, accusative, etc).", "To control for all possible argument orders and words syntactic roles, for each template, we change (i) the positions of the three verb arguments in the subordinate clause and (ii) the case assignments of each noun group. There are three verb arguments, leading to six different position permutations. Similarly, they are three unique case assignments, leading to six possible case assignments. By generating all such permutations, we create $6 \\times 6 = 36$ grammatical sentences for each template, yielding 1800 grammatical sentences in total. In Figure FIGREF3, we show an example where only the positions of the subject and the indirect object are switched, which does not alter the meaning. We also show an example where only the case assignments of the subject and the indirect object are switched: \u201cThe Senate\u201d becomes the subject and \u201cthe minister\u201d the indirect object. The case permutations were done by retrieving the desired case markings (nominative, accusative or dative) from a dictionary mapping the vocabulary's nouns to their morphological forms.", "We asked Amazon Mechanical turkers to assess the sentence grammaticality on a scale from 1 to 10, where 1 means grammatically incorrect and 10 means grammatically correct. Before the task started, respondents were shown examples of grammatical sentences and ungrammatical sentences. Importantly, it was indicated that grammatical sentences were not necessarily meaningful. As an example, we translated to German Chomsky's famous quote: \u201cColorless green ideas sleep furiously\u201d BIBREF19. Each respondent graded 50 sentences, with the following constraints: (i) each sentence comes from a different template, to avoid that past sentences impact future ratings; (ii) twenty-five percent of the sentences shown are grammatical, mirroring the construction of the dataset; (iii) sentences selected are randomly chosen among the 144 possibilities for each template, so that each user is exposed to a wide variety of case assignments, argument orders and grammatical violations; (iv) no sentence is annotated twice."]}
{"question_id": "058b6e3fdbb607fa7dbfc688628b3e13e130c35a", "predicted_answer": "", "predicted_evidence": ["We asked Amazon Mechanical turkers to assess the sentence grammaticality on a scale from 1 to 10, where 1 means grammatically incorrect and 10 means grammatically correct. Before the task started, respondents were shown examples of grammatical sentences and ungrammatical sentences. Importantly, it was indicated that grammatical sentences were not necessarily meaningful. As an example, we translated to German Chomsky's famous quote: \u201cColorless green ideas sleep furiously\u201d BIBREF19. Each respondent graded 50 sentences, with the following constraints: (i) each sentence comes from a different template, to avoid that past sentences impact future ratings; (ii) twenty-five percent of the sentences shown are grammatical, mirroring the construction of the dataset; (iii) sentences selected are randomly chosen among the 144 possibilities for each template, so that each user is exposed to a wide variety of case assignments, argument orders and grammatical violations; (iv) no sentence is annotated twice.", "We use the pre-trained word-level language model (German WordNLM) described and trained by BIBREF8. The model is a two-layer LSTM without attention, a hidden dimension of 1,204, and word embeddings of dimension 200 for the 50,000 most frequent words. It was trained on a corpus from German Wikipedia, totalling 819 million words. The 50,000 most-frequent words in this corpus are used as the vocabulary and embedded in 200-dimensional vector space. The model reaches a perplexity of 37.96 on this dataset. We use unigram and bigram language models that use the same corpus with Laplace smoothing as baselines. The probability of test sentences according to the language models is computed using the chain rule:", "In Figure FIGREF20, we explore the effect of argument order. Despite the fact that all argument orderings should be equally valid from a grammatical perspective, we find that humans tend to favour more 'canonical' orders, with nominative-accusative-dative being the preferred order. Models also assign higher log probability scores to the canonical order compared to others. It is likely that some orders occur more frequently than others in German, thus leading to a frequency bias for both models and humans. Although sentences with shuffled argument order have the same meaning as those without shuffled order, we find a similar bias for the meaningfulness scores.", "Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans. We suspect that these are cases of inattention, as in our test set the distinction between grammatical and ungrammatical rest on a single word, and even a single character (the distinction between 'der' and 'den', for instance).", "We constructed a dataset designed to expose impossible verb argument structures by manipulating the arguments' case assignments. We introduced these changes within subordinate clauses rather than main clauses, because German subordinate clauses have a more flexible noun phrases order than main clauses. This specificity allows us to test whether models are able to capture syntactic dependencies when the arguments' positions vary."]}
{"question_id": "5b95665d44666a1dc9e568d2471e5edf8614859f", "predicted_answer": "", "predicted_evidence": ["Another point of convergence is found with regards to the association between case and semantic features: humans prefer that nominative phrases are animate, and accusative inanimate, a pattern also found in neural networks. This shows that humans have difficulties in judging grammaticality as separate from other factors like frequency and meaningfulness, especially when sentences are presented independently instead of in minimal pairs. In this respect, humans are quite comparable to neural models.", "To control for all possible argument orders and words syntactic roles, for each template, we change (i) the positions of the three verb arguments in the subordinate clause and (ii) the case assignments of each noun group. There are three verb arguments, leading to six different position permutations. Similarly, they are three unique case assignments, leading to six possible case assignments. By generating all such permutations, we create $6 \\times 6 = 36$ grammatical sentences for each template, yielding 1800 grammatical sentences in total. In Figure FIGREF3, we show an example where only the positions of the subject and the indirect object are switched, which does not alter the meaning. We also show an example where only the case assignments of the subject and the indirect object are switched: \u201cThe Senate\u201d becomes the subject and \u201cthe minister\u201d the indirect object. The case permutations were done by retrieving the desired case markings (nominative, accusative or dative) from a dictionary mapping the vocabulary's nouns to their morphological forms.", "In Table TABREF26, we show correlations between human judgments of grammaticality, meaningfulness and LSTM log probabilities. Unsurprisingly, all variables are positively correlated, which supports our earlier findings. More surprising is that the LSTM is more correlated with both grammaticality and meaningfulness than meaningfulness is with grammaticality. Note that meaningfulness and grammaticality have been annotated by different annotators, which might help explain this finding.", "In parallel to work focusing on word probe probabilities, another closely related line of inquiry has investigated surprisal, the inverse log probability assigned to a specific prediction by a model. For instance, BIBREF12 and BIBREF13 examine many syntactic phenomena, including filler gap dependencies and garden path effects.", "Grammaticality judgments for recurrent networks have been investigated since BIBREF9, who use closely matched pairs of sentences to investigate grammatical correctness. This approach has been adopted recently to assess the abilities of RNNs, and LSTMs in particular, to capture syntactic structures. For instance, BIBREF4 and BIBREF5 use word probes in minimally different pairs of English sentences to study number agreement. To discriminate grammatical sentences from ungrammatical ones, they retrieve the probabilities of the possible morphological forms of a target word, given the probability of the previous words in the sentence. Practically, in the sentence \u201cthe boy is sleeping\u201d, the network has detected number agreement if $\\mathbf {P}(w = is) > \\mathbf {P}(w = are)$. This methodology has also been adapted by BIBREF10 to models trained with a masked language-modeling objective."]}
{"question_id": "b9686a168366aafbab1737df426e031ad74a6284", "predicted_answer": "", "predicted_evidence": ["Language identification (LID) is important because we need to identify as many languages as possible while using the small samples provided by social media. We use a multi-layer perceptron with character trigrams, trained using samples of 50 characters from a range of datasets. The focus here is to evaluate the performance of the LID component within the context of language mapping. Previous work on LID either required large, homogenous samples BIBREF22 or covered only about one hundred languages BIBREF23, BIBREF24. The goal here is to evaluate the LID component on as many samples as possible from as many domains as possible. The model covers 464 languages drawn from the datasets represented in Table 2; this can be seen as a compromise between the sample size required and the number of languages covered. Table 2 shows the performance of the model on a held-out test set; the number of languages for each section of the test set is shown (listed as N. Langs) as well as the number of unique test samples for that section (listed as Test Samples).", "Data comes from two sources of digital texts: web pages from the Common Crawl and social media from Twitter. Starting with the web-crawled data, we can compare this dataset to previous georeferenced web corpora BIBREF12, BIBREF13. The basic pipeline is to process all text within $<p>$ tags, removing boilerplate content, navigation content, and noisy text. We view each web page as a document containing the remaining material. Documents are then deduplicated by site, by time, and by location.", "Each of these datasets rests on different assumptions and, as a result, is subject to different confounds. For example, using top-level domains to determine the country of origin for web pages is likely to over-represent countries like Estonia that use their TLD extensively and to under-represent countries like the United States that do not traditionally use their TLD. The Twitter dataset relies on georeferenced tweets, but not all users have GPS-enabled devices. For example, we might expect that countries with a lower per capita GDP have a lower percent of georeferenced tweets, in addition to having fewer tweets overall. The goal here is to establish a baseline of how well these corpora represent actual populations.", "We use a multi-layer perceptron with character trigrams, trained using samples of 50 characters from a range of datasets. The focus here is to evaluate the performance of the LID component within the context of language mapping. Previous work on LID either required large, homogenous samples BIBREF22 or covered only about one hundred languages BIBREF23, BIBREF24. The goal here is to evaluate the LID component on as many samples as possible from as many domains as possible. The model covers 464 languages drawn from the datasets represented in Table 2; this can be seen as a compromise between the sample size required and the number of languages covered. Table 2 shows the performance of the model on a held-out test set; the number of languages for each section of the test set is shown (listed as N. Langs) as well as the number of unique test samples for that section (listed as Test Samples). The model covers 464 languages because, after enforcing a threshold of the number of samples required for the development/training/testing datsets, this is how many languages remain.", "If the result is negative, then a particular country is under-represented. For example, the share of the corpus from India has values of -0.1730 (CC) and -0.1421 (TW). This means that language from India is under-represented given what we would expect its population to produce. On the other hand, if the result is positive, then a particular country is over-represented. For example, Estonia is over-represented in the web-crawled data (0.0290) as is Australia in the Twitter data (0.0226) These numbers mean that there is 2.9% more language data from Estonia on the web than expected given the population of Estonia; and there is 17.3% less language data from India on the web than expected given the population of India."]}
{"question_id": "740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "predicted_answer": "", "predicted_evidence": ["Each of these datasets rests on different assumptions and, as a result, is subject to different confounds. For example, using top-level domains to determine the country of origin for web pages is likely to over-represent countries like Estonia that use their TLD extensively and to under-represent countries like the United States that do not traditionally use their TLD. The Twitter dataset relies on georeferenced tweets, but not all users have GPS-enabled devices. For example, we might expect that countries with a lower per capita GDP have a lower percent of georeferenced tweets, in addition to having fewer tweets overall. The goal here is to establish a baseline of how well these corpora represent actual populations.", "Some countries are not available because their top-level domains are used for other purposes (i.e., .ai, .fm, .io, .ly, .ag, .tv). Domains that do not contain geographic information are also removed from consideration (e.g., .com sites). The Common Crawl dataset covers 2014 through the end of 2017, totalling 81.5 billion web pages. As shown in Table 1, after processing this produces a corpus of 16.65 billion words. Table 1 also shows the number of countries represented in the web corpus against the number of countries in the ground-truth UN dataset and in the collected Twitter corpus. Countries may be missing from the web dataset (i) because their domains are used for a different purpose or (ii) their domains are not widely used or the country does not produce a significant amount of data on the open internet.", "We begin by describing the corpora and how they were collected (Section 2) and the language identification model that is used to label them with language codes (Section 3). After looking at the frequency distribution of languages across the entire dataset (Section 4), we undertake a country-level evaluation of the datasets, first against population-density baselines (Section 5) and then against language-use baselines (Section 6).", "Can we use georeferenced corpora to determine what languages are used in a particular country? We use as ground-truth the UN aggregated census data for each country and, in countries where this is not available, fall back on the CIA World Factbook data. Instead of trying to match up exactly how much of the population uses a specific language, we instead say that a language is used in a country if at least 5% of the observation is in that language. For example, if over 5% of the population in India uses Hindi, then we expect to see Hindi make up at least 5% of the corpora from India. This threshold allows us to evaluate the corpora without expecting that they will predict precisely the estimated figures of speakers per language. If there are ten languages in India that are used by over 5% of the population, then we expect all ten languages to be present in the corpora from India.", "As shown in Figure 6, the web-crawled corpus has very few false positive outside of Russia and eastern Europe. The situation on Twitter is similar: most false positives are in Russia and Eastern Europe, but in Twitter there are also over-predicted languages in the US and Canada, South Africa, France, India, and Australia. This is important because it shows that relying on Twitter alone would indicate that there are more diverse language speaking populations in these countries. As shown in Table 1, Eastern Europe accounts for 2.4% of the world's population but 27.4% of the web corpus; this explains the region's general false positive rate. For Russia, on the other hand, which is not included in Eastern Europe, the false positive rate cannot be explained in reference to general over-representation. In this case, the false positives are other European languages: French, German, Spanish, Italian. More research is needed to distinguish between immigration, tourism, and business as alternate sources of false positive languages appearing in digital data sets."]}
{"question_id": "845bdcd900c0f96b2ae091d086fb1ab8bb1063f0", "predicted_answer": "", "predicted_evidence": ["For web-crawled data, internet access provides a much better population weighting ($r=0.49$). This is perhaps not surprising because the internet usage statistics are directly relevant to the production of websites. But it is surprising that general internet access is not a good predictor of Twitter usage. Overall, we see that there is a definite relationship between populations and the amount of digital text produced per country, but there are clear regional biases.", "As shown in Figure 6, the web-crawled corpus has very few false positive outside of Russia and eastern Europe. The situation on Twitter is similar: most false positives are in Russia and Eastern Europe, but in Twitter there are also over-predicted languages in the US and Canada, South Africa, France, India, and Australia. This is important because it shows that relying on Twitter alone would indicate that there are more diverse language speaking populations in these countries. As shown in Table 1, Eastern Europe accounts for 2.4% of the world's population but 27.4% of the web corpus; this explains the region's general false positive rate. For Russia, on the other hand, which is not included in Eastern Europe, the false positive rate cannot be explained in reference to general over-representation. In this case, the false positives are other European languages: French, German, Spanish, Italian. More research is needed to distinguish between immigration, tourism, and business as alternate sources of false positive languages appearing in digital data sets.", "But the third important finding is that, given what ground-truth language-use data is available, there remain a number of countries where these corpora do not represent all the language produced by the local populations: not all languages from censuses are found in digital texts. In this case Twitter has fewer missing languages.", "What countries are specifically over-represented and under-represented in the two datasets? We first find the relative share of each dataset for each country. For example, what percentage of the web-corpus is from India? This assumes the total world population and the total corpus size as constants and finds the share of that total from each country. We then subtract the population estimates from the corpus-based estimates. In other words, we first normalize each representation (corpus size and population) and then find the difference between the normalized measures. This allows us to take into account the very different counts (words vs. persons).", "Analyses and models based on digital texts, especially from Twitter, often come with uncertainty about the underlying populations that those texts represent. This paper has systematically collected Twitter and web-data from locations around the world without language-specific searches that would bias the collection. The purpose is to understand how well these data sets correspond with what we know about global populations from ground-truth sources, providing a method for evaluating different data collection techniques."]}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "predicted_answer": "", "predicted_evidence": ["Previous work on LID either required large, homogenous samples BIBREF22 or covered only about one hundred languages BIBREF23, BIBREF24. The goal here is to evaluate the LID component on as many samples as possible from as many domains as possible. The model covers 464 languages drawn from the datasets represented in Table 2; this can be seen as a compromise between the sample size required and the number of languages covered. Table 2 shows the performance of the model on a held-out test set; the number of languages for each section of the test set is shown (listed as N. Langs) as well as the number of unique test samples for that section (listed as Test Samples). The model covers 464 languages because, after enforcing a threshold of the number of samples required for the development/training/testing datsets, this is how many languages remain. It is a purely practical number that is limited by the datasets that are drawn from to create the LID model.", "Figures 4 and 5 show the true positive rate: what percent of the actual census-based languages for each country are found using text data? Figures 6 and 7, on the other hand, show the false positive rate: how many languages do the text datasets say are used in a country but are not found in census data? These are two simple methods for comparing the relationship between the corpora and the underlying populations. If we predicted that any language that makes up at least 5% of the corpus from a country is, in fact, used by the population of that country, how often would be correct? There are many countries for which these two ground-truth datasets have no language information. For example, the UN language dataset has no information for Brazil. The ground-truth for language-use is much more sparse than that for population because many countries have no recent and reliable ground-truth data for the languages used by their populations.", "Language samples are geo-located using country-specific top-level domains: the assumption is that a language sample from a web-site under the .ca domain originated from Canada. This approach to regionalization does not assume that whoever produced that language sample was born in Canada or represents a traditional Canadian dialect group. Rather, the assumption is only that the sample represents someone in Canada who is producing language data. Previous work has shown that there is a significant relationship between domain-level georeferencing and traditionally-collected linguistic data BIBREF14.", "Figures 4 and 5 show the true positive rate: what percent of the actual census-based languages for each country are found using text data? Figures 6 and 7, on the other hand, show the false positive rate: how many languages do the text datasets say are used in a country but are not found in census data? These are two simple methods for comparing the relationship between the corpora and the underlying populations. If we predicted that any language that makes up at least 5% of the corpus from a country is, in fact, used by the population of that country, how often would be correct? There are many countries for which these two ground-truth datasets have no language information. For example, the UN language dataset has no information for Brazil. The ground-truth for language-use is much more sparse than that for population because many countries have no recent and reliable ground-truth data for the languages used by their populations. This lack of ground-truth data is not a problem.", "The dataset used for training and evaluating the LID component contains several independent sources of data: The first, more formal, set of domains comes from a traditional LID source: religious texts. Bibles are taken from BIBREF25 and from BIBREF22 (this is listed as LTI in Table 2); translations of the Quran are taken from the Tanzil corpus BIBREF26. The second set of domains contains official government and legislative texts: the European parliament, the JRC-Acquis corpus of European Union texts, and the United Nations BIBREF26. The third set contains non-official formal texts: the EU Bookshop corpus BIBREF27, newspapers and commentary from GlobalVoices, NewsCommentary, and Setimes BIBREF26, and Wikipedia BIBREF28. The fourth set contains documentation from open source software packages: Ubuntu and Gnome BIBREF26."]}
{"question_id": "bc05503eef25c732f1785e29d59b6022f12ba094", "predicted_answer": "", "predicted_evidence": ["Let $d$ denote a document containing several sentences $[sent_1, sent_2, \\cdots , sent_m]$ , where $sent_i$ is the $i$ -th sentence in the document. Extractive summarization can be defined as the task of assigning a label $y_i \\in \\lbrace 0, 1\\rbrace $ to each $sent_i$ , indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document.", "$$\\tilde{h}^l=\\mathrm {LN}(h^{l-1}+\\mathrm {MHAtt}(h^{l-1}))\\\\\nh^l=\\mathrm {LN}(\\tilde{h}^l+\\mathrm {FFN}(\\tilde{h}^l))$$   (Eq. 9)", "In this paper, we focus on designing different variants of using BERT on the extractive summarization task and showing their results on CNN/Dailymail and NYT datasets. We found that a flat architecture with inter-sentence Transformer layers performs the best, achieving the state-of-the-art results on this task.", "As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol.", "The task is often divided into two paradigms, abstractive summarization and extractive summarization. In abstractive summarization, target summaries contains words or phrases that were not in the original text and usually require various text rewriting operations to generate, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document. In this paper, we focus on extractive summarization."]}
{"question_id": "a6603305f4fd3dd0010ac31243c40999a116537e", "predicted_answer": "", "predicted_evidence": ["During the predicting process, Trigram Blocking is used to reduce redundancy. Given selected summary $S$ and a candidate sentence $c$ , we will skip $c$ is there exists a trigram overlapping between $c$ and $S$ . This is similar to the Maximal Marginal Relevance (MMR) BIBREF11 but much simpler.", "The experimental results on NYT datasets are shown in Table 3. Different from CNN/Dailymail, we use the limited-length recall evaluation, following BIBREF15 . We truncate the predicted summaries to the lengths of the gold summaries and evaluate summarization quality with ROUGE Recall. Compared baselines are (1) First- $k$ words, which is a simple baseline by extracting first $k$ words of the input article; (2) Full is the best-performed extractive model in BIBREF15 ; (3) Deep Reinforced BIBREF18 is an abstractive model, using reinforce learning and encoder-decoder structure. The Bertsum+Classifier can achieve the state-of-the-art results on this dataset.", "The task is often divided into two paradigms, abstractive summarization and extractive summarization. In abstractive summarization, target summaries contains words or phrases that were not in the original text and usually require various text rewriting operations to generate, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document. In this paper, we focus on extractive summarization.", "$$\\tilde{h}^l=\\mathrm {LN}(h^{l-1}+\\mathrm {MHAtt}(h^{l-1}))\\\\\nh^l=\\mathrm {LN}(\\tilde{h}^l+\\mathrm {FFN}(\\tilde{h}^l))$$   (Eq. 9)", "As illustrated in the table, all BERT-based models outperformed previous state-of-the-art models by a large margin. Bertsum with Transformer achieved the best performance on all three metrics. The Bertsum with LSTM model does not have an obvious influence on the summarization performance compared to the Classifier model."]}
{"question_id": "2ba4477d597b1fd123d14be07a7780ccb5c4819b", "predicted_answer": "", "predicted_evidence": ["Ablation studies are conducted to show the contribution of different components of Bertsum. The results are shown in in Table 2. Interval segments increase the performance of base model. Trigram blocking is able to greatly improve the summarization results. This is consistent to previous conclusions that a sequential extractive decoder is helpful to generate more informative summaries. However, here we use the trigram blocking as a simple but robust alternative.", "As illustrated in the table, all BERT-based models outperformed previous state-of-the-art models by a large margin. Bertsum with Transformer achieved the best performance on all three metrics. The Bertsum with LSTM model does not have an obvious influence on the summarization performance compared to the Classifier model.", "The NYT dataset contains 110,540 articles with abstractive summaries. Following BIBREF15 , we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We took 4,000 examples from the training set as the validation set. We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The filtered test set (NYT50) includes 3,452 test examples. We first split sentences by CoreNLP and pre-process the dataset following methods in BIBREF15 .", "To stabilize the training, pergate layer normalization BIBREF8 is applied within each LSTM cell. At time step $i$ , the input to the LSTM layer is the BERT output $T_i$ , and the output is calculated as:", "The vector $T_i$ which is the vector of the $i$ -th [CLS] symbol from the top BERT layer will be used as the representation for $sent_i$ ."]}
{"question_id": "027814f3a879a6c7852e033f9d99519b8729e444", "predicted_answer": "", "predicted_evidence": ["In this paper, we focus on designing different variants of using BERT on the extractive summarization task and showing their results on CNN/Dailymail and NYT datasets. We found that a flat architecture with inter-sentence Transformer layers performs the best, achieving the state-of-the-art results on this task.", "Let $d$ denote a document containing several sentences $[sent_1, sent_2, \\cdots , sent_m]$ , where $sent_i$ is the $i$ -th sentence in the document. Extractive summarization can be defined as the task of assigning a label $y_i \\in \\lbrace 0, 1\\rbrace $ to each $sent_i$ , indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document.", "$$\\hat{Y}_i = \\sigma (W_oh_i^L+b_o)$$   (Eq. 10)", "Instead of a simple sigmoid classifier, Inter-sentence Transformer applies more Transformer layers only on sentence representations, extracting document-level features focusing on summarization tasks from the BERT outputs:", "The vector $T_i$ which is the vector of the $i$ -th [CLS] symbol from the top BERT layer will be used as the representation for $sent_i$ ."]}
{"question_id": "00df1ff914956d4d23299d02fd44e4c985bb61fa", "predicted_answer": "", "predicted_evidence": ["In this paper, we focus on designing different variants of using BERT on the extractive summarization task and showing their results on CNN/Dailymail and NYT datasets. We found that a flat architecture with inter-sentence Transformer layers performs the best, achieving the state-of-the-art results on this task.", "In this section we present our implementation, describe the summarization datasets and our evaluation protocol, and analyze our results.", "Instead of a simple sigmoid classifier, Inter-sentence Transformer applies more Transformer layers only on sentence representations, extracting document-level features focusing on summarization tasks from the BERT outputs:", "As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol.", "The vector $T_i$ which is the vector of the $i$ -th [CLS] symbol from the top BERT layer will be used as the representation for $sent_i$ ."]}
{"question_id": "b57ad10468e1ba2a7a34396688dbb10a575d89f5", "predicted_answer": "", "predicted_evidence": ["We formulate the task of table retrieval in this section. Given a query $q$ and a collection of tables $T=\\lbrace t_1, ..., t_N\\rbrace $ , the goal of table search is to find a table $t_i$ that is most relevant to $q$ .", "Table 4 shows the results of table ranking on the WikiTableQuestions dataset.", "Similar techniques have been successfully applied in table-based question answering BIBREF13 , BIBREF14 . Afterwards, we feed the concatenation of $v_q$ and $v_{header}$ to a linear layer followed by a $softmax$ function whose output length is 2. We regard the output of the first category as the relevance between query and header. We use $NN_1()$ to denote this model.", "We compare between different features for table ranking. An intuitive baseline is to represent a table as bag-of-words, represent a query with bag-of-words, and calculate their similarity with cosine similarity. Therefore, we use the BM25 score which is calculated in the candidate table retrieval step. This baseline is abbreviated as BM25. We also report the results of using designed features (Feature) described in Section \"Matching with Designed Features\" and neural networks (NeuralNet) described in Section \"Matching with Neural Networks\" . Results from Table 2 show that the neural networks perform comparably with the designed features, and obtain better performance than the BM25 baseline. This results reflect the necessary of taking into account the table structure for table retrieval. Furthermore, we can find that combining designed features and neural networks could achieve further improvement, which indicates the complementation between them.", "where $w_i$ is the weight associated with the $i$ -th regression tree, and $tr_i( \\cdot )$ is the value of a leaf node obtained by evaluating $i$ -th tree with features $\\left[ f_1(q,t), ... ,f_K(q,t) \\right]$ . The values of $w_i$ and the parameters in $tr_i(\\cdot )$ are learned with gradient descent during training."]}
{"question_id": "9d6d17120c42a834b2b5d96f2120d646218ed4bb", "predicted_answer": "", "predicted_evidence": ["We illustrate two examples generated by our NeuralNet approach in Figure 3 . The example in Figure 3 (a) is a satisfied case that the top ranked result is the correct answer. We can find that the model uses evidences from different aspects to match between a query and a table. In this example, the supporting evidences come from caption (\u201cramadan\" and \u201cmalaysia\"), headers (\u201cdates\") and cells (\u201c2016\"). The example in Figure 3 (b) is a dissatisfied case. We can find that the top ranked result contains \u201clife expectancy\" in both caption and header, however, it is talking about the people in U.S. rather than \u201cgerman shepherd\". Despite the correct table contains a cell whose content is \u201cgerman shepherd\", it still does not obtain a higher rank than the left table. The reason might be that the weight for header is larger than the weight for cells.", "Since a table caption is typically a descriptive word sequence. We model it with bi-directional GRU-RNN, the same strategy we have used for modeling the query. We concatenate the caption vector $v_{cap}$ with $v_{q}$ , and feed the results to a linear layer followed by $softmax$ .", "$$f_{s1}(t_a, q)=cosine(cdssm(t_a), cdssm(q)) \\nonumber $$   (Eq. 11)", "We formulate the task of table retrieval in this section. Given a query $q$ and a collection of tables $T=\\lbrace t_1, ..., t_N\\rbrace $ , the goal of table search is to find a table $t_i$ that is most relevant to $q$ .", "We separately train the parameters for each aspect with back-propagation. We use negative log-likelihood as the loss function."]}
{"question_id": "965e0ce975a0b8612a30cfc31bbfd4b8a57aa138", "predicted_answer": "", "predicted_evidence": ["Our work relates to a line of research works that learn continuous representation of structured knowledge with neural network for natural language processing tasks. For example, neelakantan2015neural,pengcheng2015 develop neural operator on the basis of table representation and apply the model to question answering. yin2015NGQA introduce a KB-enhanced sequence-to-sequence approach that generates natural language answers to simple factoid questions based on facts from KB. mei-bansal-walter:2016:N16-1 develop a LSTM based recurrent neural network to generate natural language weather forecast and sportscasting commentary from database records. serban-EtAl:2016:P16-1 introduce a recurrent neural network approach, which takes fact representation as input and generates factoid question from a fact from Freebase. table2textEMNLP2016 presented an neural language model that generates biographical sentences from Wikipedia infobox.", "Afterwards, we use $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $1 to calculate the relevance between a query and a table in paraphrase level. The intuition is that, two source phrases that are aligned to the same target phrase tend to be paraphrased. The phrase level score is calculated as follows, where $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $2 is the maximum n-gram order, which is set as 3, and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $3 and $PT = \\lbrace  \\langle src_i,trg_i,", "We formulate the task of table retrieval in this section. Given a query $q$ and a collection of tables $T=\\lbrace t_1, ..., t_N\\rbrace $ , the goal of table search is to find a table $t_i$ that is most relevant to $q$ .", "Since headers and cells have similar characteristics, we use a similar way to measure the relevance between a query and table cells. Specifically, we derive three memories $M_{cel}$ , $M_{row}$ and $M_{col}$ from table cells in order to match from cell level, row level and column level. Each memory cell in $M_{cel}$ represents the embedding of a table cell. Each cell in $M_{row}$ represent the vector a row, which is computed with weighted average over the embeddings of cells in the same row. We derive the column memory $M_{col}$ in an analogous way. We use the same module $NN_1()$ to calculate the relevance scores for these three memories.", "Measuring the relevance between a query and a table is of great importance for table retrieval. In this section, we present carefully designed features and neural network architectures for matching between a query and a table."]}
{"question_id": "8dfdd1ed805bb23c774fbb032ef1d97c6802e07c", "predicted_answer": "", "predicted_evidence": ["We conduct case study on our NeuralNet approach and find that the performance is sensitive to the length of queries. Therefore, we split the test set to several groups according to the length of queries. Results are given in Figure 4 . We can find that the performance of the approach decreases with the increase of query length. When the query length changes from 6 to 7, the performance of P@1 decreases rapidly from 58.12% to 50.23%. Through doing case study, we find that long queries contain more word dependencies. Therefore, having a good understanding about the intention of a query requires deep query understanding. Leveraging external knowledge to connect query and table is a potential solution to deal with long queries.", "We further investigate the effects of headers, cells and caption for table retrieval on WebQueryTable. We first use each aspect separately and then increasingly combine different aspects. Results are given in Table 3 . We can find that in general the performance of an aspect in designed features is consistent with its performance in neural networks. Caption is the most effective aspect on WebQueryTable. This is reasonable as we find that majority of the queries are asking about a list of objects, such as \u201cpolish rivers\", \u201cworld top 5 mountains\" and \u201clist of american cruise lines\". These intentions are more likely to be matched in the caption of a table. Combining more aspects could get better results. Using cells, headers and caption simultaneously gets the best results.", "To the best of our knowledge, there is no publicly available dataset for table retrieval. We introduce WebQueryTable, an open-domain dataset consisting of query-table pairs. We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not. In this way, we get 21,113 query-table pairs. In the real scenario of table retrieval, a system is required to find a table from a huge collection of tables. Therefore, in order to enlarge the search space of our dataset, we extract 252,703 web tables from Wikipedia and regard them as searchable tables as well. Data statistics are given in Table 1 .", "Our neural network approach relates to the recent advances of attention mechanism and reasoning over external memory in artificial intelligence BIBREF11 , BIBREF12 , BIBREF19 . Researchers typically represent a memory as a continuous vector or matrix, and develop neural network based controller, reader and writer to reason over the memory. The memory could be addressed by a \u201csoft\u201d attention mechanism trainable by standard back-propagation methods or a \u201chard\u201d attention mechanism trainable by REINFORCE BIBREF20 . In this work, we use the soft attention mechanism, which could be easily optimized and has been successfully applied in nlp tasks BIBREF11 , BIBREF12 .", "The intuition is that, two source phrases that are aligned to the same target phrase tend to be paraphrased. The phrase level score is calculated as follows, where $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $2 is the maximum n-gram order, which is set as 3, and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $3 and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $4 are the phrase in $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i),"]}
{"question_id": "c21675d8a90bda624d27e5535d1c10f08fcbc16b", "predicted_answer": "", "predicted_evidence": ["Moreover, we investigate whether using a higher threshold could obtain a better precision. Therefore, we increasingly use a set of thresholds, and calculate the corresponding precision and recall in different conditions. An instance is considered to be correct if the top ranked table is correct and its ranking score is greater than the threshold. Results of our NeuralNet approach on WebQueryTable are given in 2 . We can see that using larger threshold results in lower recall and higher precision. The results are consistent with our intuition.", "We separately train the parameters for each aspect with back-propagation. We use negative log-likelihood as the loss function.", "There exists several works in database community that aims at finding related tables from keyword queries. A representative work is given by VLDB2008GG, which considers table search as a special case of document search task and represent a table with its surrounding text and page title. VLDB2010india use YAGO ontology to annotate tables with column and relationship labels. VLDB2011GG go one step further and use labels and relationships extracted from the web. VLDB2012IBM focus on the queries that describe table columns, and retrieve tables based on column mapping. There also exists table-related studies such as searching related tables from a table BIBREF16 , assembling a table from list in web page BIBREF17 and extracting tables using tabular structure from web page BIBREF18 . Our work differs from this line of research in that we focus on exploring the content of table to find relevant tables from web queries.", "which is set as 3, and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $3 and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $4 are the phrase in $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $5 and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $6 starts from the $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $7 -th and $PT = \\lbrace  \\langle src_i,", "p(src_i|trg_i) \\rangle \\rbrace $3 and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $4 are the phrase in $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $5 and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $6 starts from the $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $7 -th and $PT = \\lbrace  \\langle src_i,trg_i, p(trg_i|src_i), p(src_i|trg_i) \\rangle \\rbrace $8 -th word with the length of $PT = \\lbrace  \\langle src_i,"]}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "predicted_answer": "", "predicted_evidence": ["ri = [h1; h2i; h1-h2i; h1 h2i;hmi]", "AliMe For the AliMe service in E-commerce, we collect 8,004 query-bag pairs to form our dataset. Negative sampling is also an important part of the matching model. For each query, we use the Lucene to retrieval the top-20 most similar questions from the whole question candidates. Then we filter the questions which are in the corresponding right bag. After that, we randomly sample one in the retrieved candidate and use the bag that the retrieved candidate belongs to as the negative case. In the bag construction stage, the annotators have already merged all the questions of the same meaning, so we can ensure that the after filtering retrieved cases are negative in our setting. We also restrict the number of questions in a bag not more than 5 and discard the redundant questions. Finally, we get 12,008 training cases, 2,000 valid cases, and 10,000 test cases. Please notice, for the testing, we sampled 9 negative bags instead of 1, and thus formed 10 candidates for ranking.", "After getting the Q-Q matching representation $r_i$, we combine the $\\lbrace r_1, \\dots , r_n\\rbrace $ by element-wise max and mean pooling in order to get $r_p$ to represent the query-bag matching representation: rp = [ max_pooling { r1, ..., rn }; mean_pooling { r1, ..., rn } ] where [;] denotes concatenation. After that, an MLP with softmax is applied to predict whether the query and the bag is asking the same question. Finally, the loss function minimizes the cross entropy of the training data. Due to the out-of-order of the bag, we do not model the bag representation by CNN or LSTM, and experiments show the pooling-based method works well.", "AliMe Bot is a kind of retrieval-based online service of E-commerce which collects a lot of predefined question-answering pairs. Through data analysis, we find that many variants of a question exist which means a sort of questions can correspond to a same answer. Based on the observation, naturally, we can view these questions with the same answer as a bag. Obviously, the bag contains diverse expressions of a question, which may provide more matching evidence than only one question due to the rich information contained in the bag. Motivated by the fact, different from existing query-question (Q-Q) matching method, we propose a new query-bag matching approach for retrieval-based chatbots. Concretely, when a user raises a query, the query-bag matching model provides the most suitable bag and returns the corresponding answer of the bag. To our knowledge, there is no query-bag matching study exists, and we focus on the new approach in this paper.", "AliMe For the AliMe service in E-commerce, we collect 8,004 query-bag pairs to form our dataset. Negative sampling is also an important part of the matching model. For each query, we use the Lucene to retrieval the top-20 most similar questions from the whole question candidates. Then we filter the questions which are in the corresponding right bag. After that, we randomly sample one in the retrieved candidate and use the bag that the retrieved candidate belongs to as the negative case. In the bag construction stage, the annotators have already merged all the questions of the same meaning, so we can ensure that the after filtering retrieved cases are negative in our setting. We also restrict the number of questions in a bag not more than 5 and discard the redundant questions. Finally, we get 12,008 training cases, 2,000 valid cases, and 10,000 test cases."]}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "predicted_answer": "", "predicted_evidence": ["The query-to-bag coverage, and bag-to-query coverage representation, and their summation are concatenated to the matching representation $r_p$ to predict the final score: [ rp ; cq ; cb ; sum(cq) ; sum(cb)]", "After getting the Q-Q matching representation $r_i$, we combine the $\\lbrace r_1, \\dots , r_n\\rbrace $ by element-wise max and mean pooling in order to get $r_p$ to represent the query-bag matching representation: rp = [ max_pooling { r1, ..., rn }; mean_pooling { r1, ..., rn } ] where [;] denotes concatenation. After that, an MLP with softmax is applied to predict whether the query and the bag is asking the same question. Finally, the loss function minimizes the cross entropy of the training data. Due to the out-of-order of the bag, we do not model the bag representation by CNN or LSTM, and experiments show the pooling-based method works well.", "Each element $c_{i,j}$ represents how many information of the $j$-th word in $q$ is mentioned by the $i$-th question in the bag. To get the coverage from a bag instead of the $i$-th question in a bag, a new element-wise max pooling is applied on the generated $\\lbrace c_1, \\dots , c_n \\rbrace $ to get bag-to-query coverage $c_q$. The process of the bag-to-query coverage is shown in Figure FIGREF2.", "Bag-to-query Considering the $i$-th question $b_i$ in the bag, the element-wise max pooling is performed on $\\lbrace M^i_0, \\cdots M^i_n \\rbrace $ to get the $b_i$ to $q$ coverage $c_i=\\text{max\\_pooling}\\lbrace M^i_0, \\cdots M^i_n \\rbrace $ where $M^i$ is the cross-attention matrix between $b_i$ and $q$ as in the background section, and $M^i_j$ is its $j$-th row. Each element $c_{i,j}$ represents how many information of the $j$-th word in $q$ is mentioned by the $i$-th question in the bag. To get the coverage from a bag instead of the $i$-th question in a bag, a new element-wise max pooling is applied on the generated $\\lbrace c_1, \\dots , c_n \\rbrace $ to get bag-to-query coverage $c_q$.", "AliMe Bot is a kind of retrieval-based online service of E-commerce which collects a lot of predefined question-answering pairs. Through data analysis, we find that many variants of a question exist which means a sort of questions can correspond to a same answer. Based on the observation, naturally, we can view these questions with the same answer as a bag. Obviously, the bag contains diverse expressions of a question, which may provide more matching evidence than only one question due to the rich information contained in the bag. Motivated by the fact, different from existing query-question (Q-Q) matching method, we propose a new query-bag matching approach for retrieval-based chatbots. Concretely, when a user raises a query, the query-bag matching model provides the most suitable bag and returns the corresponding answer of the bag. To our knowledge, there is no query-bag matching study exists, and we focus on the new approach in this paper."]}
{"question_id": "de4cc9e7fa5d700f5046d60789770f47911b3dd7", "predicted_answer": "", "predicted_evidence": ["We adopt the hCNN model, which measures the relationship between query-question pairs, to obtain the Q-Q matching representation. The model can be easily adapted to other query-question matching models. hCNN is a CNN based matching model which is fast enough to work on the industry application. The input of hCNN is a query $q$ and the $i$-th question $b_i$ in the bag. $q$ and $b_i$ are fed into a CNN respectively. A cross-attention matrix $M^i$ is fed into another CNN to get the interaction representation between them. Each element of $M^i$ is defined as $M^i_{a,b}=q_a^\\top \\cdot b_{i,b}$ where $q_a$ is the word embedding of the $a$-th word in query $q$ and $b_{i,b}$ is the embedding of the $b$-th word in $b_i$.", "Analysis of the Bag Representation Coverage is also applied in the bag representation layer. The results of the bag representation without coverage component (Base+(BR w/o Cov)) is shown in Table TABREF6. Compared with the Base+BR and BR without coverage, it shows that the coverage component contributes a lot on both the two datasets. The bag representation with coverage (Base+BR) gains improvement over Base model, especially in Quora dataset.", "The query-to-bag coverage, and bag-to-query coverage representation, and their summation are concatenated to the matching representation $r_p$ to predict the final score: [ rp ; cq ; cb ; sum(cq) ; sum(cb)]", "To prove the effectiveness of our models, we propose two baselines from different aspects: the Q-Q matching based baseline and the query-bag matching based baseline.", "Effectiveness of the Mutual Coverage To intuitively learn the coverage weight, we sample some words with their weights in Table TABREF17. It shows that the words like \u201cThe\u201d have low weight, which confirms that they contribute little to the matching. \u201cRefund\u201d in E-commerce is a very important element in a user query sentence. And \u201cAmerica\u201d is important in Quora, because question like \u201cwhat is the capital in <location>?\u201d is highly related to location \u201c<location>\u201d."]}
{"question_id": "8ad5ebca2f69023b60ccfa3aac0ed426234437ac", "predicted_answer": "", "predicted_evidence": ["Bag-to-query Considering the $i$-th question $b_i$ in the bag, the element-wise max pooling is performed on $\\lbrace M^i_0, \\cdots M^i_n \\rbrace $ to get the $b_i$ to $q$ coverage $c_i=\\text{max\\_pooling}\\lbrace M^i_0, \\cdots M^i_n \\rbrace $ where $M^i$ is the cross-attention matrix between $b_i$ and $q$ as in the background section, and $M^i_j$ is its $j$-th row. Each element $c_{i,j}$ represents how many information of the $j$-th word in $q$ is mentioned by the $i$-th question in the bag.", "When a user poses a query to the bot, the bot searches the most similar bag and uses the corresponding answer to reply to the user. The more information in the query covered by the bag, the more likely the bag's corresponding answer answers the query. What's more, the bag should not have too much information exceeding the query. Thus modelling the bag-to-query and query-to-bag coverage is essential in this task.", "The input of hCNN is a query $q$ and the $i$-th question $b_i$ in the bag. $q$ and $b_i$ are fed into a CNN respectively. A cross-attention matrix $M^i$ is fed into another CNN to get the interaction representation between them. Each element of $M^i$ is defined as $M^i_{a,b}=q_a^\\top \\cdot b_{i,b}$ where $q_a$ is the word embedding of the $a$-th word in query $q$ and $b_{i,b}$ is the embedding of the $b$-th word in $b_i$. Finally, the outputs of CNNs are combined via Equation SECREF3 to get the representation $r_i$, which indicates the matching representation of the query $q$ and the $i$-th question $b_i$ in the bag. For the Q-Q matching task, the $r_i$ is fed into an MLP (Multi-Layer Perceptron) to predict the matching score.", "Each element $c_{i,j}$ represents how many information of the $j$-th word in $q$ is mentioned by the $i$-th question in the bag. To get the coverage from a bag instead of the $i$-th question in a bag, a new element-wise max pooling is applied on the generated $\\lbrace c_1, \\dots , c_n \\rbrace $ to get bag-to-query coverage $c_q$. The process of the bag-to-query coverage is shown in Figure FIGREF2.", "ri = [h1; h2i; h1-h2i; h1 h2i;hmi]"]}
{"question_id": "4afd4cfcb30433714b135b977baff346323af1e3", "predicted_answer": "", "predicted_evidence": ["The input of hCNN is a query $q$ and the $i$-th question $b_i$ in the bag. $q$ and $b_i$ are fed into a CNN respectively. A cross-attention matrix $M^i$ is fed into another CNN to get the interaction representation between them. Each element of $M^i$ is defined as $M^i_{a,b}=q_a^\\top \\cdot b_{i,b}$ where $q_a$ is the word embedding of the $a$-th word in query $q$ and $b_{i,b}$ is the embedding of the $b$-th word in $b_i$. Finally, the outputs of CNNs are combined via Equation SECREF3 to get the representation $r_i$, which indicates the matching representation of the query $q$ and the $i$-th question $b_i$ in the bag. For the Q-Q matching task, the $r_i$ is fed into an MLP (Multi-Layer Perceptron) to predict the matching score.", "\u201cHow many parts of a query are covered by the bag?\u201d and \u201cIs all the information in the bag mentioned by the query?\u201d are two important problems in the query-bag matching task. We propose a novel mutual coverage module to model the above-mentioned inter-projection problems.", "Recalling the text matching task BIBREF0, recently, researchers have adopted the deep neural network to model the matching relationship. ESIM BIBREF1 judges the inference relationship between two sentences by enhanced LSTM and interaction space. SMN BIBREF2 performs the context-response matching for the open-domain dialog system. BIBREF3 BIBREF3 explores the usefulness of noisy pre-training in the paraphrase identification task. BIBREF4 BIBREF4 surveys the methods in query-document matching in web search which focuses on the topic model, the dependency model, etc. However, none of them pays attention to the query-bag matching which concentrates on the matching for a query and a bag containing multiple questions.", "The query-to-bag coverage, and bag-to-query coverage representation, and their summation are concatenated to the matching representation $r_p$ to predict the final score: [ rp ; cq ; cb ; sum(cq) ; sum(cb)]", "Finally, the outputs of CNNs are combined via Equation SECREF3 to get the representation $r_i$, which indicates the matching representation of the query $q$ and the $i$-th question $b_i$ in the bag. For the Q-Q matching task, the $r_i$ is fed into an MLP (Multi-Layer Perceptron) to predict the matching score. In our query-bag matching setting, we will aggregate the $\\lbrace r_1, \\dots , r_n\\rbrace $ to predict the query-bag matching score. Due to the page limitation, please refer to BIBREF5 BIBREF5 for more details on hCNN. h1 = CNN1(q) h2i = CNN1(bi) hmi = CNN2(qbi)"]}
{"question_id": "b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "predicted_answer": "", "predicted_evidence": ["Can we train a semantic parser in a language where annotation is available?. In this paper we show that this is indeed possible and we propose a zero-shot cross-lingual semantic parsing method based on language-independent features, where a parser trained in English \u2013 where labelled data is available, is used to parse sentences in three languages, Italian, German and Dutch.", "We perform an error analysis to assess the quality of the prediction for operators (i.e. logic operators like \u201cNot\u201d as well as discourse relations \u201cContrast\u201d), non-lexical predicates, such as binary predicates (e.g. Agent(e,x)) as well as unary predicates (e.g. time(t), entity(x), etc.), as well as for lexical predicates (e.g. open(e)). Results in Table TABREF15 show that predicting operators and binary predicates across language is hard, compared to the other two categories. Prediction of lexical predicates is relatively good even though most tokens in the test set where never seen during training; this can be attributable to the copy mechanism that is able to transfer tokens from the input directly during predication.", "BiLSTM are still state-of-the-art for monolingual semantic parsing for English. Table TABREF14 shows the result for the models trained and tested in English. Dependency features in conjunction with word and PoS embeddings lead to the best performance; however, in all settings explored treeLSTMs do not outperform a BiLSTM.", "What would that require? We show that universal dependency features can dramatically improve the performance of a cross-lingual semantic parser but modelling the tree structure directly does not outperform sequential BiLSTM architectures, not even when the two are combined together.", "TreeLSTMs slightly improve performance only for German. TreeLSTMs do not outperform a baseline BiLSTM for Italian and Dutch and they show little improvement in performance for German. This might be due to different factors that deserve more analysis including the performance of the parsers and syntactic similarity between these languages. When only dependency features are available, we found treeLSTM to boost performance only for Dutch."]}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "predicted_answer": "", "predicted_evidence": ["Universal POS tags. We use the Universal POS tags BIBREF19 obtained with UDPipe parser. Universal POS tag embeddings are randomly initialized and updated during training.", "BiLSTM are still state-of-the-art for monolingual semantic parsing for English. Table TABREF14 shows the result for the models trained and tested in English. Dependency features in conjunction with word and PoS embeddings lead to the best performance; however, in all settings explored treeLSTMs do not outperform a BiLSTM.", "We use the BiLSTM model as baseline (Bi) and compare it to the child-sum tree-LSTM (tree) with positional information added (Po/tree), as well as to a treeLSTM initialized with the hidden states of the BiLSTM(Bi/tree). We also conduct an ablation study on the features used, where WE, PE and DE are the word-embedding, PoS embedding and dependency relation embedding respectively. For completeness, along with the results for the cross-lingual task, we also report results for monolingual English semantic parsing, where word embedding features are randomly initialized.", "In order to be used as input to the parser, liu2018discourse first convert the DRS into tree-based representations, which are subsequently linearized into PTB-style bracketed sequences. This transformation is lossless in that re-entrancies are duplicated to fit in the tree structure. We use the same conversion in this work; for further detail we refer the reader to the original paper.", "This work was done while Federico Fancellu was a post-doctoral researcher at the University of Edinburgh. The views expressed are his own and do not necessarily represent the views of Samsung Research."]}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "predicted_answer": "", "predicted_evidence": ["Bi/treeLSTM. Finally, similarly to chen2017improved, we combine tree-LSTM and Bi-LSTM, where a tree-LSTM come is initialized using the last layer of a Bi-LSTM, which encodes order information. Computation is shown in Equation (3).", "To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORATION, etc.", "We perform an error analysis to assess the quality of the prediction for operators (i.e. logic operators like \u201cNot\u201d as well as discourse relations \u201cContrast\u201d), non-lexical predicates, such as binary predicates (e.g. Agent(e,x)) as well as unary predicates (e.g. time(t), entity(x), etc.), as well as for lexical predicates (e.g. open(e)). Results in Table TABREF15 show that predicting operators and binary predicates across language is hard, compared to the other two categories. Prediction of lexical predicates is relatively good even though most tokens in the test set where never seen during training; this can be attributable to the copy mechanism that is able to transfer tokens from the input directly during predication.", "We go back to the questions in the introduction:", "We use the BiLSTM model as baseline (Bi) and compare it to the child-sum tree-LSTM (tree) with positional information added (Po/tree), as well as to a treeLSTM initialized with the hidden states of the BiLSTM(Bi/tree). We also conduct an ablation study on the features used, where WE, PE and DE are the word-embedding, PoS embedding and dependency relation embedding respectively. For completeness, along with the results for the cross-lingual task, we also report results for monolingual English semantic parsing, where word embedding features are randomly initialized."]}
{"question_id": "6911e8724dfdb178fa81bf58019947b71ef8fbe7", "predicted_answer": "", "predicted_evidence": ["In order to make the model directly transferable to the German, Italian and Dutch test data, we use the following language-independent features.", "We perform an error analysis to assess the quality of the prediction for operators (i.e. logic operators like \u201cNot\u201d as well as discourse relations \u201cContrast\u201d), non-lexical predicates, such as binary predicates (e.g. Agent(e,x)) as well as unary predicates (e.g. time(t), entity(x), etc.), as well as for lexical predicates (e.g. open(e)). Results in Table TABREF15 show that predicting operators and binary predicates across language is hard, compared to the other two categories. Prediction of lexical predicates is relatively good even though most tokens in the test set where never seen during training; this can be attributable to the copy mechanism that is able to transfer tokens from the input directly during predication.", "Dependency features are crucial for zero-shot cross-lingual semantic parsing. Adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word-embedding and universal PoS embeddings alone. We hypothesize that the quality of the multilingual word-embeddings is poor, given that models using embeddings for the dependency relations alone outperform those using the other two features.", "The decoder of liu2018discourse reconstructs the DRS in three steps, by first predicting the overall structure (the `boxes'), then the predicates and finally the referents, with each subsequent step being conditioned on the output of the previous. During predicate prediction, the decoder uses a copying mechanism to predict those unary predicates that are also lemmas in the input sentence (e.g. `eat'). For the those that are not, soft attention is used instead. No modifications were done to the decoder; for more detail, we refer the reader to the original paper.", "BiLSTM. We use BIBREF0's Bi-LSTM as baseline. However, whereas the original model represents each token in the input sentence as the concatenation of word ($e_{w_i}$) and lemma embeddings, we discard the latter and add a POS tag embedding ($e_{p_i}$) and dependency relation embedding ($e_{d_i}$) feature. These embeddings are concatenated to represent the input token. The final encoder representation is obtained by concatenating both final forward and backward hidden states."]}
{"question_id": "b012df09fa2a3d6b581032d68991768cf4bc9d7b", "predicted_answer": "", "predicted_evidence": ["What would that require? We show that universal dependency features can dramatically improve the performance of a cross-lingual semantic parser but modelling the tree structure directly does not outperform sequential BiLSTM architectures, not even when the two are combined together.", "We use the BiLSTM model as baseline (Bi) and compare it to the child-sum tree-LSTM (tree) with positional information added (Po/tree), as well as to a treeLSTM initialized with the hidden states of the BiLSTM(Bi/tree). We also conduct an ablation study on the features used, where WE, PE and DE are the word-embedding, PoS embedding and dependency relation embedding respectively. For completeness, along with the results for the cross-lingual task, we also report results for monolingual English semantic parsing, where word embedding features are randomly initialized.", "This work was done while Federico Fancellu was a post-doctoral researcher at the University of Edinburgh. The views expressed are his own and do not necessarily represent the views of Samsung Research.", "We are planning to extend this initial survey to other DRS parsers that does not exclude presupposition and sense as well as to other semantic formalisms (e.g. AMR, MRS) where data sets annotated in languages other than English are available. Finally, we want to understand whether adding a bidirectionality to the treeLSTM will help improving the performance on modelling the dependency structure directly.", "Dependency features are crucial for zero-shot cross-lingual semantic parsing. Adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word-embedding and universal PoS embeddings alone. We hypothesize that the quality of the multilingual word-embeddings is poor, given that models using embeddings for the dependency relations alone outperform those using the other two features."]}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "predicted_answer": "", "predicted_evidence": ["Finally, it is worth noting that lexical predicates in PMB are in English, even for non-English languages. Since this is not compatible with our copy mechanism, we revert predicates to their original language by substituting them with the lemmas of the tokens they are aligned to (since gold alignment information is included in the PMB).", "To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORATION, etc.", "We perform an error analysis to assess the quality of the prediction for operators (i.e. logic operators like \u201cNot\u201d as well as discourse relations \u201cContrast\u201d), non-lexical predicates, such as binary predicates (e.g. Agent(e,x)) as well as unary predicates (e.g. time(t), entity(x), etc.), as well as for lexical predicates (e.g. open(e)). Results in Table TABREF15 show that predicting operators and binary predicates across language is hard, compared to the other two categories. Prediction of lexical predicates is relatively good even though most tokens in the test set where never seen during training; this can be attributable to the copy mechanism that is able to transfer tokens from the input directly during predication.", "We use Counter BIBREF20 to evaluate the performance of our models. Counter looks for the best alignment between the predicted and gold DRS and computes precision, recall and F1. For further details about Counter, the reader is referred to van2018evaluating. It is worth reminding that unlike other work on the PMB BIBREF21, BIBREF0 does not deal with presupposition. In the PMB, presupposed variables are extracted from a main box and included in a separate one. In our work, we revert this process so to ignore presupposed boxes. Similarly, we also do not deal with sense tags which we aim to include in future work.", "We use the BiLSTM model as baseline (Bi) and compare it to the child-sum tree-LSTM (tree) with positional information added (Po/tree), as well as to a treeLSTM initialized with the hidden states of the BiLSTM(Bi/tree). We also conduct an ablation study on the features used, where WE, PE and DE are the word-embedding, PoS embedding and dependency relation embedding respectively. For completeness, along with the results for the cross-lingual task, we also report results for monolingual English semantic parsing, where word embedding features are randomly initialized."]}
{"question_id": "d5c393df758dec6ea6827ae5b887eb6c303a4f4d", "predicted_answer": "", "predicted_evidence": ["The idea is to translate words in another language in the goal to generate sentiment lexicon. In BIBREF7 , the authors propose to estimate a transformation matrix INLINEFORM0 such that INLINEFORM1 , where INLINEFORM2 is the embedding of a word in the source language and INLINEFORM3 is the embedding of its translation in the target language. In order to estimate the INLINEFORM4 matrix, suppose we are given a set of word pairs and their associated vector representations INLINEFORM5 where INLINEFORM6 is the embeddings of word INLINEFORM7 in the source language and INLINEFORM8 is the embedding of its translation. The matrix INLINEFORM9 can be learned by the following optimization problem: DISPLAYFORM0", "Words n-grams", "which we solve with the least square method.", "In a last experiment, we look into the gains that can be obtained by manually translating a small part of the lexicon and use it as bilingual dictionary when training the transformation matrix. Figure FIGREF21 shows average macro-fmeasure on the four languages when translating up to 2,000 words from the MPQA lexicon (out of 8k). It can be observed that from 600 words on, performance is better than that of the statistical translation system.", "The French (FR) corpus comes from the DEFT'15 evaluation campaign . It consists of 7,836 tweets for training and 3,381 tweets for testing. The Italian (IT) corpus was released as part of the SentiPOLC'14 evaluation campaign BIBREF24 . It consists of 4,513 tweets for training and 1,930 tweets for testing. For Spanish (ES), the TASS'15 corpus is used BIBREF25 . Since the evaluation campaign was still ongoing at the time of writing, we use 3-fold validation on the training corpus composed of 7,219 tweets. German (DE) tweets come from the Multilingual Sentiment Dataset BIBREF26 . It consists of 844 tweets for training and 844 tweets for testing."]}
{"question_id": "11a3af3f056e0fb5559fe5cbff1640e022732735", "predicted_answer": "", "predicted_evidence": ["The BWE (Bilingual Word Embeddings) system consists in translating the sentiment lexicons with our method. This approach obtains results comparable to the SMT approach. The main advantage of this approach is to be able to generalize on words unknown to the SMT system.", "Emoticons: presence or absence of positive and negative emoticons at any position in the tweet", "The idea is to translate words in another language in the goal to generate sentiment lexicon. In BIBREF7 , the authors propose to estimate a transformation matrix INLINEFORM0 such that INLINEFORM1 , where INLINEFORM2 is the embedding of a word in the source language and INLINEFORM3 is the embedding of its translation in the target language. In order to estimate the INLINEFORM4 matrix, suppose we are given a set of word pairs and their associated vector representations INLINEFORM5 where INLINEFORM6 is the embeddings of word INLINEFORM7 in the source language and INLINEFORM8 is the embedding of its translation. The matrix INLINEFORM9 can be learned by the following optimization problem: DISPLAYFORM0", "Last punctuation: whether the last token contains an exclamation or question mark", "In order to extract features on those corpora, polarity lexicons are translated from English using the method described in Section SECREF3 . The following lexicons are translated:"]}
{"question_id": "07a214748a69b31400585aef7aba6af3e3d9cce2", "predicted_answer": "", "predicted_evidence": ["Lexicons: number of words present in each lexicon", "Emoticons: presence or absence of positive and negative emoticons at any position in the tweet", "The BWE (Bilingual Word Embeddings) system consists in translating the sentiment lexicons with our method. This approach obtains results comparable to the SMT approach. The main advantage of this approach is to be able to generalize on words unknown to the SMT system.", "Sentiment analysis is a task that aims at recognizing in text the opinion of the writer. It is often modeled as a classification problem which relies on features extracted from the text in order to feed a classifier. Relevant features proposed in the literature span from microblogging artifacts including hashtags, emoticons BIBREF0 , BIBREF1 , intensifiers like all-caps words and character repetitions BIBREF2 , sentiment-topic features BIBREF3 , to the inclusion of polarity lexicons.", "Our second experiment consists in varying the size of the bilingual dictionary used to train INLINEFORM0 . Figure FIGREF20 shows the evolution of average macro f-measure (over the four languages) when the INLINEFORM1 most frequent words from Wikipedia are part of the bilingual dictionary. It can be observed that using the 50k most frequent words leads to the best performance (an average macro-fmeasure of 61.72) while only 1,000 words already brings nice improvements."]}
{"question_id": "44bf3047ff7e5c6b727b2aaa0805dd66c907dcd6", "predicted_answer": "", "predicted_evidence": ["This paper is a step towards abstractive summarization of dialogues by (1) introducing a new dataset, created for this task, (2) comparison with news summarization by the means of automated (ROUGE) and human evaluation.", "Fast Abs RL (trained on dialogues),", "We show that the most popular summarization metric ROUGE does not reflect the quality of a summary. Looking at the ROUGE scores, one concludes that the dialogue summarization models perform better than the ones for news summarization. In fact, this hypothesis is not true \u2013 we performed an independent, manual analysis of summaries and we demonstrated that high ROUGE results, obtained for automatically-generated dialogue summaries, correspond with lower evaluation marks given by human annotators. An interesting example of the misleading behavior of the ROUGE metrics is presented in Table TABREF35 for Dialogue 4, where a wrong summary \u2013 'paul and cindy don't like red roses.' \u2013 obtained all ROUGE values higher than a correct summary \u2013 'paul asks cindy what color flowers should buy.'. Despite lower ROUGE values, news summaries were scored higher by human evaluators.", "In general, the Transformer-based architectures benefit from training on the joint dataset: news+dialogues, even though the news and the dialogue documents have very different structures. Interestingly, this does not seem to be the case for the Pointer Generator or Fast Abs RL model.", "The inclusion of a separation token between dialogue utterances is advantageous for most models \u2013 presumably because it improves the discourse structure. The improvement is most visible when training is performed on the joint dataset."]}
{"question_id": "c6f2598b85dc74123fe879bf23aafc7213853f5b", "predicted_answer": "", "predicted_evidence": ["Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop).", "DynamicConv + GPT-2 embeddings with a separator (trained on news + dialogues),", "Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e.", "We noticed a few annotations (7 for news and 4 for dialogues) with opposite marks (i.e. one annotator judgement was $-1$, whereas the second one was 1) and decided to have them annotated once again by another annotator who had to resolve conflicts. For the rest, we calculated the linear weighted Cohen's kappa coefficient BIBREF22 between annotators' scores. For news examples, we obtained agreement on the level of $0.371$ and for dialogues \u2013 $0.506$. The annotators' agreement is higher on dialogues than on news, probably because of structures of those data \u2013 articles are often long and it is difficult to decide what the key-point of the text is; dialogues, on the contrary, are rather short and focused mainly on one topic.", "Process of building the dataset. Our dialogue summarization dataset contains natural messenger-like conversations created and written down by linguists fluent in English. The style and register of conversations are diversified \u2013 dialogues could be informal, semi-formal or formal, they may contain slang phrases, emoticons and typos. We asked linguists to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger conversations. It includes chit-chats, gossiping about friends, arranging meetings, discussing politics, consulting university assignments with colleagues, etc. Therefore, this dataset does not contain any sensitive data or fragments of other corpora."]}
{"question_id": "bdae851d4cf1d05506cf3e8359786031ac4f756f", "predicted_answer": "", "predicted_evidence": ["Major research efforts have focused so far on summarization of single-speaker documents like news (e.g., BIBREF7) or scientific publications (e.g., BIBREF8). One of the reasons is the availability of large, high-quality news datasets with annotated summaries, e.g., CNN/Daily Mail BIBREF9, BIBREF7. Such a comprehensive dataset for dialogues is lacking.", "The goal of the summarization task is condensing a piece of text into a shorter version that covers the main points succinctly. In the abstractive approach important pieces of information are presented using words and phrases not necessarily appearing in the source text. This requires natural language generation techniques with high level of semantic understanding BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6.", "The inclusion of a separation token between dialogue utterances is advantageous for most models \u2013 presumably because it improves the discourse structure. The improvement is most visible when training is performed on the joint dataset.", "The paper is structured as follows: in Section SECREF2 we present details about the new corpus and describe how it was created, validated and cleaned. Brief description of baselines used in the summarization task can be found in Section SECREF3. In Section SECREF4, we describe our experimental setup and parameters of models. Both evaluations of summarization models, the automatic with ROUGE metric and the linguistic one, are reported in Section SECREF5 and Section SECREF6, respectively. Examples of models' outputs and some errors they make are described in Section SECREF7. Finally, discussion, conclusions and ideas for further research are presented in sections SECREF8 and SECREF9.", "Cleaning data. After preparing the dataset, we conducted a process of cleaning it in a semi-automatic way. Beforehand, we specified a format for written dialogues with summaries: a colon should separate an author of utterance from its content, each utterance is expected to be in a separate line. Therefore, we could easily find all deviations from the agreed structure \u2013 some of them could be automatically fixed (e.g. when instead of a colon, someone used a semicolon right after the interlocutor's name at the beginning of an utterance), others were passed for verification to linguists. We also tried to correct typos in interlocutors' names (if one person has several utterances, it happens that, before one of them, there is a typo in his/her name) \u2013 we used the Levenshtein distance to find very similar names (possibly with typos e.g."]}
{"question_id": "894bbb1e42540894deb31c04cba0e6cfb10ea912", "predicted_answer": "", "predicted_evidence": ["In a structured text, such as a news article, the information flow is very clear. However, in a dialogue, which contains discussions (e.g. when people try to agree on a date of a meeting), questions (one person asks about something and the answer may appear a few utterances later) and greetings, most important pieces of information are scattered across the utterances of different speakers. What is more, articles are written in the third-person point of view, but in a chat everyone talks about themselves, using a variety of pronouns, which further complicates the structure. Additionally, people talking on messengers often are in a hurry, so they shorten words, use the slang phrases (e.g. 'u r gr8' means 'you are great') and make typos. These phenomena increase the difficulty of performing dialogue summarization.", "One can easily notice problematic issues. Firstly, the models frequently have difficulties in associating names with actions, often repeating the same name, e.g., for Dialogue 1 in Table TABREF34, Fast Abs RL generates the following summary: 'lilly and lilly are going to eat salmon'. To help the model deal with names, the utterances are enhanced by adding information about the other interlocutors \u2013 Fast Abs RL enhanced variant described in Section SECREF11. In this case, after enhancement, the model generates a summary containing both interlocutors' names: 'lily and gabriel are going to pasta...'. Sometimes models correctly choose speakers' names when generating a summary, but make a mistake in deciding who performs the action (the subject) and who receives the action (the object), e.g. for Dialogue 4 DynamicConv + GPT-2 emb. w/o sep. model generates the summary 'randolph will buy some earplugs for maya', while the correct form is 'maya will buy some earplugs for randolph'.", "Cleaning data. After preparing the dataset, we conducted a process of cleaning it in a semi-automatic way. Beforehand, we specified a format for written dialogues with summaries: a colon should separate an author of utterance from its content, each utterance is expected to be in a separate line. Therefore, we could easily find all deviations from the agreed structure \u2013 some of them could be automatically fixed (e.g. when instead of a colon, someone used a semicolon right after the interlocutor's name at the beginning of an utterance), others were passed for verification to linguists. We also tried to correct typos in interlocutors' names (if one person has several utterances, it happens that, before one of them, there is a typo in his/her name) \u2013 we used the Levenshtein distance to find very similar names (possibly with typos e.g. 'George' and 'Goerge') in a single conversation, and those cases with very similar names were passed to linguists for verification.", "Fast Abs RL Enhanced (trained on dialogues),", "To help the model deal with names, the utterances are enhanced by adding information about the other interlocutors \u2013 Fast Abs RL enhanced variant described in Section SECREF11. In this case, after enhancement, the model generates a summary containing both interlocutors' names: 'lily and gabriel are going to pasta...'. Sometimes models correctly choose speakers' names when generating a summary, but make a mistake in deciding who performs the action (the subject) and who receives the action (the object), e.g. for Dialogue 4 DynamicConv + GPT-2 emb. w/o sep. model generates the summary 'randolph will buy some earplugs for maya', while the correct form is 'maya will buy some earplugs for randolph'. A closely related problem is capturing the context and extracting information about the arrangements after the discussion."]}
{"question_id": "75b3e2d2caec56e5c8fbf6532070b98d70774b95", "predicted_answer": "", "predicted_evidence": ["We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):", "In this case, after enhancement, the model generates a summary containing both interlocutors' names: 'lily and gabriel are going to pasta...'. Sometimes models correctly choose speakers' names when generating a summary, but make a mistake in deciding who performs the action (the subject) and who receives the action (the object), e.g. for Dialogue 4 DynamicConv + GPT-2 emb. w/o sep. model generates the summary 'randolph will buy some earplugs for maya', while the correct form is 'maya will buy some earplugs for randolph'. A closely related problem is capturing the context and extracting information about the arrangements after the discussion. For instance, for Dialogue 4, the Fast Abs RL model draws a wrong conclusion from the agreed arrangement. This issue is quite frequently visible in summaries generated by Fast Abs RL, which may be the consequence of the way it is constructed; it first chooses important utterances, and then summarizes each of them separately.", "We test a few general-purpose summarization models. In terms of human evaluation, the results of dialogues summarization are worse than the results of news summarization. This is connected with the fact that the dialogue structure is more complex \u2013 information is spread in multiple utterances, discussions, questions, more typos and slang words appear there, posing new challenges for summarization. On the other hand, dialogues are divided into utterances, and for each utterance its author is assigned. We demonstrate in experiments that the models benefit from the introduction of separators, which mark utterances for each person. This suggests that dedicated models having some architectural changes, taking into account the assignation of a person to an utterance in a systematic manner, could improve the quality of dialogue summarization.", "We would like to express our sincere thanks to Tunia B\u0142achno, Oliwia Ebebenge, Monika J\u0119dras and Ma\u0142gorzata Krawentek for their huge contribution to the corpus collection \u2013 without their ideas, management of the linguistic task and verification of examples we would not be able to create this paper. We are also grateful for the reviewers' helpful comments and suggestions.", "Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e."]}
{"question_id": "573b8b1ad919d3fd0ef7df84e55e5bfd165b3e84", "predicted_answer": "", "predicted_evidence": ["When the coefficient associated only with adversarial reward goes to 1, it begins to slowly deviate though not completely. This is motivated by the initial pretraining step on paraphrases and perturbations.", "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al.", "We qualitatively analyze the results by visualizing the attention scores and the perturbations introduces by our model. We further evaluate the importance of hyperparameters $\\gamma _{(.)}$ in the reward function. We set only one of the hyperparameters closer to 1 and set the remaining closer to zero to see how it affects the text generation. The results can be seen in Figure FIGREF43. Based on a subjective qualitative evaluation, we make the following observations:", "Alzantot et al. BIBREF20 proposed a black-box targeted attack using a population-based optimization via genetic algorithm BIBREF21. The perturbation procedure consists of random selection of words, finding their nearest neighbours, ranking and substitution to maximize the probability of target category. In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al.", "NMT-BT: We generate paraphrases of the sentences of the text using a back-translation approach BIBREF23. We used pretrained English$\\leftrightarrow $German translation models to obtain back-translations of input examples."]}
{"question_id": "07d98dfa88944abd12acd45e98fb7d3719986aeb", "predicted_answer": "", "predicted_evidence": ["In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN.", "In this paper, we use paraphrase datasets like PARANMT-50M corpusBIBREF33, Quora Question Pair dataset and Twitter URL paraphrasing corpus BIBREF34. These paraphrase datasets together contains text from various sources: Common Crawl, CzEng1.6, Europarl, News Commentary, Quora questions, and Twitter trending topic tweets. We do not use all the data for our pretraining. We randomly sample 5 million parallel texts and augment them using simple character-transformations (eg. random insertion, deletion or replacement) to words in the text. The number of words that undergo transformation is capped at 10% of the total number of words in the text. We further include examples which contain only character-transformations without paraphrasing the original input.", "However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier. While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy.", "The perturbation procedure consists of random selection of words, finding their nearest neighbours, ranking and substitution to maximize the probability of target category. In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics.", "While there is limited literature for such approaches in NLP systems, there have been some studies that have exposed the vulnerabilities of neural networks in text-based tasks like machine translations and question answering. Belinkov and Bisk BIBREF16 investigated the sensitivity of neural machine translation (NMT) to synthetic and natural noise containing common misspellings. They demonstrate that state-of-the-art models are vulnerable to adversarial attacks even after a spell-checker is deployed. Jia et al. BIBREF17 showed that networks trained for more difficult tasks, such as question answering, can be easily fooled by introducing distracting sentences into text, but these results do not transfer obviously to simpler text classification tasks. Following such works, different methods with the primary purpose of crafting adversarial example have been explored. Recently, a work by Ebrahimi et al."]}
{"question_id": "3a40559e5a3c2a87c7b9031c89e762b828249c05", "predicted_answer": "", "predicted_evidence": ["The encoder maps the input text sequence into a sequence of representations using word and character-level information. Our encoder (Figure FIGREF10) is a slight variant of Chen et al.BIBREF31. This approach providing multiple levels of granularity can be useful in order to handle rare or noisy words in the text. Given character embeddings $E^{(c)}=[e_1^{(c)}, e_2^{(c)},...e_{n^{\\prime }}^{(c)}]$ and word embeddings $E^{(w)}=[e_1^{(w)}, e_2^{(w)},...e_{n}^{(w)}]$ of the input, starting ($p_t$) and ending ($q_t$) character positions at time step $t$, we define inside character embeddings as: $E_I^{(c)}=[e_{p_t}^{(c)},...., e_{q_t}^{(c)}]$ and outside embeddings as: $E_O^{(c)}=[e_{1}^{(c)},....,e_{p_t-1}^{(c)}; e_{q_t+1}^{(c)},...,e_{n^{\\prime }}^{(c)}]$.", "First, we obtain the character-enhanced word representation $\\overleftrightarrow{h_t}$ by combining the word information from $E^{(w)}$ with the character context vectors. Character context vectors are obtained by attending over inside and outside character embeddings. Next, we compute a summary vector $S$ over the hidden states $\\overleftrightarrow{h_t}$ using an attention layer expressed as $Attn(\\overleftrightarrow{H})$. To generate adversarial examples, it is important to identify the most relevant text units that contribute towards the target model's prediction and then use this information during the decoding step to introduce perturbation on those units. Hence, the summary vector is optimized using target model predictions without back propagating through the entire encoder. This acts as a substitute network that learns to mimic the predictions of the target classifier.", "Thus, $\\tilde{s}_j^{(c)}$ is initialized to the character-GRU only for the first hidden state. With this mechanism, both word and character level information can be used to introduce necessary perturbations.", "Given a target model $T$, it takes a text sequence $y$ and outputs prediction probabilities $P$ across various categories of the target model. Given an input sample $(x, l)$, we compute a perturbation using our AEG model and produce a sequence $y$. We compute the adversarial reward as $R_{A}=(1-P_l)$, where the ground truth $l$ is an index to the list of categories and $P_l$ is the probability that the perturbed generated sequence $y$ belongs to target ground truth $l$. Since we want the target classifier to make mistakes, we promote it by rewarding higher when the sequences produce low target probabilities.", "Based on Bahdanau et al. BIBREF29, we encode the input text sequence using bidirectional gated recurrent units (GRUs) to encode the input text sequence $x$. Formally, we obtain an encoded representation given by: $\\overleftrightarrow{h_t}= \\overleftarrow{h_t} + \\overrightarrow{h_t}$."]}
{"question_id": "5db47bbb97282983e10414240db78154ea7ac75f", "predicted_answer": "", "predicted_evidence": ["No-RL: We use our pretrained model without the reinforcement learning objective.", "In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied.", "Inspired by the work of Li et al. BIBREF37, we train a deep matching model that can represent the degree of match between two texts. We use character based biLSTM models with attention BIBREF38 to handle word and character level perturbations. The matching model will help us compute the the semantic similarity $R_S$ between the text generated and the original input text.", "In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data. This can significantly improve the performance of our model.", "where $\\gamma _A, \\gamma _S, \\gamma _L$ are hyperparameters that can be modified depending upon the kind of textual generations expected from the model. The changes inflicted by different reward coefficients can be seen in Section SECREF44."]}
{"question_id": "c589d83565f528b87e355b9280c1e7143a42401d", "predicted_answer": "", "predicted_evidence": ["The performance of these methods are measured by the percentage fall in accuracy of these models on the generated adversarial texts. Higher the percentage dip in the accuracy of the target classifier, more effective is our model.", "Sentiment classification: We trained a word-based convolutional model (CNN-Word) BIBREF11 on IMDB sentiment dataset . The dataset contains 50k movie reviews in total which are labeled as positive or negative. The trained model achieves a test accuracy of 89.95% which is relatively close to the state-of-the-art results on this dataset.", "By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data. This can significantly improve the performance of our model. We hope that our method motivates a more nuanced exploration into generating adversarial examples and adversarial training for building robust classification models.", "where $f_d$ is used to compute a new attentional hidden state $\\tilde{s_j}$. Given the encoded input representations $\\overleftrightarrow{H}=\\lbrace \\overleftrightarrow{h_1}, ...,\\overleftrightarrow{h_n}\\rbrace $ and the previous decoder GRU state $s_{j-1}$, the context vector at time step $j$ is computed as: $c_j= Attn(\\overleftrightarrow{H}, s_{j-1})$. $Attn(\\cdot ,\\cdot )$ computes a weight $\\alpha _{jt}$ indicating the degree of relevance of an input text unit $x_t$ for predicting the target unit $y_j$ using a feed-forward network $f_{attn}$. Given a parallel corpus $D$, we train our model by minimizing the cross-entropy loss: $J=\\sum _{(x,y)\\in D}{-log p(y|x)}$.", "However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier."]}
{"question_id": "7f90e9390ad58b22b362a57330fff1c7c2da7985", "predicted_answer": "", "predicted_evidence": ["Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text.", "Thus, $\\tilde{s}_j^{(c)}$ is initialized to the character-GRU only for the first hidden state. With this mechanism, both word and character level information can be used to introduce necessary perturbations.", "Most of the sequence generation models follow an encoder-decoder framework BIBREF26, BIBREF27, BIBREF28 where encoder and decoder are modelled by separate recurrent neural networks. Usually these models are trained using a pair of text $(x,y)$ where $x=[x_1, x_2..,x_n]$ is the input text and the $y=[y_1, y_2..,y_m]$ is the target text to be generated. The encoder transforms an input text sequence into an abstract representation $h$. While the decoder is employed to generate the target sequence using the encoded representation $h$. However, there are several studies that have incorporated several modifications to the standard encoder-decoder framework BIBREF29, BIBREF25, BIBREF30.", "We evaluate our models on two different datasets associated with two different tasks: IMDB sentiment classification and AG's news categorization task. We run ablation studies on various components of the model and provide insights into decisions of our model.", "For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples."]}
{"question_id": "3e3e45094f952704f1f679701470c3dbd845999e", "predicted_answer": "", "predicted_evidence": ["When the coefficient associated only with adversarial reward goes to 1, it begins to slowly deviate though not completely. This is motivated by the initial pretraining step on paraphrases and perturbations.", "NMT-BT: We generate paraphrases of the sentences of the text using a back-translation approach BIBREF23. We used pretrained English$\\leftrightarrow $German translation models to obtain back-translations of input examples.", "The decoder is a forward GRU implementing an attention mechanism to recognize the units of input text sequence relevant for the generation of the next target work. The decoder GRU generates the next text unit at time step $j$ by conditioning on the current decoder state $s_j$, context vector $c_j$ computed using attention mechanism and previously generated text units. The probability of decoding each target unit is given by:", "In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence.", "In this section, we describe the evaluation setup used to measure the effectiveness of our model in generating adversarial examples. The success of our model lies in its ability to fool the target classifier. We pretrain our models with dataset that generates a number of character and word perturbations. We elaborate on the experimental setup and the results below."]}
{"question_id": "475ef4ad32a8589dae9d97048166d732ae5d7beb", "predicted_answer": "", "predicted_evidence": ["We test M-Bert on the CS Hindi/English UD corpus from BIBREF12 , which provides texts in two formats: transliterated, where Hindi words are written in Latin script, and corrected, where annotators have converted them back to Devanagari script. Table TABREF22 shows the results for models fine-tuned using a combination of monolingual Hindi and English, and using the CS training set (both fine-tuning on the script-corrected version of the corpus as well as the transliterated version).", "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.", "For script-corrected inputs, i.e., when Hindi is written in Devanagari, M-Bert's performance when trained only on monolingual corpora is comparable to performance when training on code-switched data, and it is likely that some of the remaining difference is due to domain mismatch. This provides further evidence that M-Bert uses a representation that is able to incorporate information from multiple languages.", "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "To further verify that En-Bert's inability to generalize is due to its lack of a multilingual representation and not an inability of its English-specific word piece vocabulary to represent data in other languages, we evaluate on non-cross-lingual ner and see that it performs comparably to a previous state of the art model (see Table TABREF12 )."]}
{"question_id": "3fd8eab282569b1c18b82f20d579b335ae70e79f", "predicted_answer": "", "predicted_evidence": ["In Figure FIGREF27 , we plot the nearest neighbor accuracy for en-de (solid line). It achieves over INLINEFORM0 accuracy for all but the bottom layers, which seems to imply that the hidden representations, although separated in space, share a common subspace that represents useful linguistic information, in a language-agnostic way. Similar curves are obtained for en-ru, and ur-hi (in-house dataset), showing this works for multiple languages.", "As to why M-Bert generalizes across languages, we hypothesize that having word pieces used in all languages (numbers, URLs, etc) which have to be mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close to a shared space.", "Because M-Bert uses a single, multilingual vocabulary, one form of cross-lingual transfer occurs when word pieces present during fine-tuning also appear in the evaluation languages. In this section, we present experiments probing M-Bert's dependence on this superficial form of generalization: How much does transferability depend on lexical overlap? And is transfer possible to languages written in different scripts (no overlap)?", "In the previous section, we showed that M-Bert's ability to generalize cannot be attributed solely to vocabulary memorization, and that it must be learning a deeper multilingual representation. In this section, we present probing experiments that investigate the nature of that representation: How does typological similarity affect M-Bert's ability to generalize? Can M-Bert generalize from monolingual inputs to code-switching text? Can the model generalize to transliterated text without transliterated language model pretraining?", "However, M-Bert is not able to effectively transfer to a transliterated target, suggesting that it is the language model pre-training on a particular language that allows transfer to that language. M-Bert is outperformed by previous work in both the monolingual-only and code-switched supervision scenarios. Neither BIBREF13 nor BIBREF12 use contextualized word embeddings, but both incorporate explicit transliteration signals into their approaches."]}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "predicted_answer": "", "predicted_evidence": ["Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.", "Because M-Bert uses a single, multilingual vocabulary, one form of cross-lingual transfer occurs when word pieces present during fine-tuning also appear in the evaluation languages. In this section, we present experiments probing M-Bert's dependence on this superficial form of generalization: How much does transferability depend on lexical overlap? And is transfer possible to languages written in different scripts (no overlap)?", "Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "It is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.", "As to why M-Bert generalizes across languages, we hypothesize that having word pieces used in all languages (numbers, URLs, etc) which have to be mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close to a shared space."]}
{"question_id": "50c1bf8b928069f3ffc7f0cb00aa056a163ef336", "predicted_answer": "", "predicted_evidence": ["In this paper, we empirically investigate the degree to which these representations generalize across languages. We explore this question using Multilingual BERT (henceforth, M-Bert), released by BIBREF0 as a single language model pre-trained on the concatenation of monolingual Wikipedia corpora from 104 languages. M-Bert is particularly well suited to this probing study because it enables a very straightforward approach to zero-shot cross-lingual model transfer: we fine-tune the model using task-specific supervised training data from one language, and evaluate that task in a different language, thus allowing us to observe the ways in which the model generalizes information across languages.", "Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "All models were fine-tuned with a batch size of 32, and a maximum sequence length of 128 for 3 epochs. We used a learning rate of INLINEFORM0 with learning rate warmup during the first INLINEFORM1 of steps, and linear decay afterwards. We also applied INLINEFORM2 dropout on the last layer. No parameter tuning was performed. We used the BERT-Base, Multilingual Cased checkpoint from https://github.com/google-research/bert.", "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "We would like to thank Mark Omernick, Livio Baldini Soares, Emily Pitler, Jason Riesa, and Slav Petrov for the valuable discussions and feedback."]}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "predicted_answer": "", "predicted_evidence": ["However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.", "All models were fine-tuned with a batch size of 32, and a maximum sequence length of 128 for 3 epochs. We used a learning rate of INLINEFORM0 with learning rate warmup during the first INLINEFORM1 of steps, and linear decay afterwards. We also applied INLINEFORM2 dropout on the last layer. No parameter tuning was performed. We used the BERT-Base, Multilingual Cased checkpoint from https://github.com/google-research/bert.", "Following BIBREF10 , we compare languages on a subset of the WALS features BIBREF11 relevant to grammatical ordering. Figure FIGREF17 plots pos zero-shot accuracy against the number of common WALS features. As expected, performance improves with similarity, showing that it is easier for M-Bert to map linguistic structures when they are more similar, although it still does a decent job for low similarity languages when compared to En-Bert.", "It is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.", "In this work, we showed that M-Bert's robust, often surprising, ability to generalize cross-lingually is underpinned by a multilingual representation, without being explicitly trained for it. The model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by BIBREF15 or BIBREF16 ."]}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 and n equals the number of candidates. Po and Pre are the poll and prediction ranks associated with RealClearPolitics.com and the model respectively.", "Although it has 95.8% accuracy during the model test, the model's prediction does not represent the poll. Table III shows that model's accuracy is not dependent of its number of training data. Model with less training data (e.g. Mike Huckabee's) can perform perfectly during the model test and only misses a rank on the prediction, whereas model with more training data (e.g. Donald Trump's) can have worse performance.", "Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates).", "Using simple preprocessed data, our Naive Bayesian model successfully achieves 95.8% accuracy on 10-fold cross validation and gets 54.8% accuracy on predicting the poll result. The model predicts Ted Cruz and Bernie Sanders as the nominee of Republican and Democratic Party respectively. Based on the positive predictions, it predicts that Bernie Sanders will be elected as the 2016 U.S. President.", "We built Naive Bayesian predictive models for 2016 U.S. Presidential Election. We use the official hashtag and simple preprocessing method to prepare the data without modifying its meaning. Our model achieves 95.8% accuracy during the model test and predicts the poll with 54.8% accuracy. The model predicts that Bernie Sanders and Ted Cruz will become the nominees of Democratic and Republican Party respectively, and the election will be won by Bernie Sanders."]}
{"question_id": "a2baa8e266318f23f43321c4b2b9cf467718c94a", "predicted_answer": "", "predicted_evidence": ["Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates).", "The preprocessed tweets are labeled manually by 11 annotators who understand English. All annotators are given either grade as part of their coursework or souvenirs for their work. The given label consists of the intended candidate and the sentiment. The annotators interpret the tweet and decide whom the tweet relates to. If they think the tweets does not relate to particular candidate nor understand the content, they can choose \"not clear\" as the label. Otherwise, they can relate it to one candidate and label it as positive or negative. We divide the tweets and annotators into three groups (Table II). They label as many tweets as they can since January 24 until April 16, 2016.", "We built Naive Bayesian predictive models for 2016 U.S. Presidential Election. We use the official hashtag and simple preprocessing method to prepare the data without modifying its meaning. Our model achieves 95.8% accuracy during the model test and predicts the poll with 54.8% accuracy. The model predicts that Bernie Sanders and Ted Cruz will become the nominees of Democratic and Republican Party respectively, and the election will be won by Bernie Sanders.", "The models use tweets gathered from February 3 until 9, 2016 as the prediction input. The prediction follows two steps: (1) we calculate the positive sentiment from tweets and consider the number of positive sentiment as the likelihood of a candidate to be the nominee, and (2) we sort the candidates by number of their positive sentiment. The ranks are compared to the poll results on RealClearPolitics.com. We calculate the error rate (E) by dividing the difference of the poll rank with our predicted rank with number of candidates ( INLINEFORM0 ). DISPLAYFORM0 DISPLAYFORM1", "We preprocess the data by: (1) removing URLs and pictures, also (2) by filtering tweets which have candidates' name. Hashtags, mentions, and retweets are not removed in order to maintain the original meaning of a tweet. We only save tweets which have passed the two requirements such as in Table 1. The first example shows no change in the tweet's content, since there isn't any URLs or pictures, and it contains a candidate's name: Bernie Sanders. The second example shows a removed tweet, which doesn't contain any candidates' name. The preprocessing stage changes the third tweet's contents. It removes the URLs and still keeps the tweet because it contains \"Hillary Clinton\" and \"Donald Trump\". The preprocessing stage removes 41% of the data (Figure 2)."]}
{"question_id": "97ff88c31dac9a3e8041a77fa7e34ce54eef5a76", "predicted_answer": "", "predicted_evidence": ["In recent days, a lot of effort is devoted to create benchmarks for artificial agents to assess their ability to comprehend natural language and to reason over facts. One of the first attempt is the bAbI BIBREF17 dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks. Another relevant benchmark is the one described in BIBREF18 , which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by reading and comprehending the given document. MCTest BIBREF19 requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Finally, SQuAD BIBREF20 consists in a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.", "Given the question \u201cwhat does Larenz Tate act in?\u201d shown in the above-mentioned figure, the model is able to understand that \u201cLarenz Tate\u201d is the subject of the question and \u201cact in\u201d represents the intent of the question. Reading the related documents, the model associates higher attention weights to the most relevant tokens needed to answer the question, such as \u201cThe Postman\u201d, \u201cA Man Apart\u201d and so on.", "With this work we try to show the effectiveness of our architecture for tasks which go from pure question answering to top-n recommendation through an experimental evaluation without any assumption on the task to be solved. To do that, we do not use any hand-crafted linguistic features but we let the system learn and leverage them in the inference process which leads to the answers through multiple reasoning steps. During these steps, the system understands relevant relationships between question and documents without relying on canonical matching, but repeating an attention mechanism able to unconver related aspects in distributed representations, conditioned on an encoding of the inference process given by another neural network. Equipping agents with a reasoning mechanism like the one described in this work and exploiting the ability of neural network models to learn from data, we may be able to create truly intelligent agents.", "For the task related to QA, a lot of datasets have been released in order to assess the machine reading and comprehension capabilities and a lot of neural network-based models have been proposed. Our model takes inspiration from BIBREF3 , which is able to answer Cloze-style BIBREF4 questions repeating an attention mechanism over the query and the documents multiple times. Despite the effectiveness on the Cloze-style task, the original model does not consider multiple documents as a source of information to answer questions, which is fundamental in order to extract the answer from different relevant facts. The restricted assumption that the answer is contained in the given document does not allow the model to provide an answer which does not belong to the document. Moreover, this kind of task does not expect multiple answers for a given question, which is important for the complex information needs required for a conversational recommender system.", "In recent days, a lot of effort is devoted to create benchmarks for artificial agents to assess their ability to comprehend natural language and to reason over facts. One of the first attempt is the bAbI BIBREF17 dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks. Another relevant benchmark is the one described in BIBREF18 , which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by reading and comprehending the given document. MCTest BIBREF19 requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension."]}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 returns 0 if INLINEFORM1 , INLINEFORM2 otherwise; INLINEFORM3 returns the word in position INLINEFORM4 of the stacked documents matrix INLINEFORM5 and INLINEFORM6 returns the frequency of the word INLINEFORM7 in the documents INLINEFORM8 related to the query INLINEFORM9 . The relevance score takes into account the importance of token occurrences in the considered documents given by the computed attention weights. Moreover, the normalization term INLINEFORM10 is applied to the relevance score in order to mitigate the weight associated to highly frequent tokens.", "The optimization method and tricks are adopted from BIBREF3 . The model is trained using ADAM BIBREF7 optimizer (learning rate= INLINEFORM0 ) with a batch size of 128 for at most 100 epochs considering the best model until the HITS@k on the validation set decreases for 5 consecutive times. Dropout BIBREF8 is applied on INLINEFORM1 and on INLINEFORM2 with a rate of INLINEFORM3 and on the prediction neural network hidden layer with a rate of INLINEFORM4 . L2 regularization is applied to the embedding matrix INLINEFORM5 with a coefficient equal to INLINEFORM6 . We clipped the gradients if their norm is greater than 5 to stabilize learning BIBREF9 . Embedding size INLINEFORM7 is fixed to 50. All GRU output sizes are fixed to 128. The number of inference steps INLINEFORM8 is set to 3.", "The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 . From now on, we denote the contextual representation for the word INLINEFORM5 by INLINEFORM6 and the contextual representation for the word INLINEFORM7 in the document INLINEFORM8 by INLINEFORM9 . Differently from BIBREF3 , we build a unique representation for the whole set of documents INLINEFORM10 related to the query INLINEFORM11 by stacking each contextual representation INLINEFORM12 obtaining a matrix INLINEFORM13 , where INLINEFORM14 .", "Dropout BIBREF8 is applied on INLINEFORM1 and on INLINEFORM2 with a rate of INLINEFORM3 and on the prediction neural network hidden layer with a rate of INLINEFORM4 . L2 regularization is applied to the embedding matrix INLINEFORM5 with a coefficient equal to INLINEFORM6 . We clipped the gradients if their norm is greater than 5 to stabilize learning BIBREF9 . Embedding size INLINEFORM7 is fixed to 50. All GRU output sizes are fixed to 128. The number of inference steps INLINEFORM8 is set to 3. The size of the prediction neural network hidden layer INLINEFORM9 is fixed to 4096. Biases INLINEFORM10 and INLINEFORM11 are initialized to zero vectors. All weight matrices are initialized sampling from the normal distribution INLINEFORM12 . The ReLU activation function in the prediction neural network has been experimentally chosen comparing different activation functions such as sigmoid and tanh and taking the one which leads to the best performance.", "Figure FIGREF11 shows the attention weights computed in the last inference step of the iterative attention mechanism used by the model to answer to a given question. Attention weights, represented as red boxes with variable color shades around the tokens, can be used to interpret the reasoning mechanism applied by the model because higher shades of red are associated to more relevant tokens on which the model focus its attention. It is worth to notice that the attention weights associated to each token are the result of the inference mechanism uncovered by the model which progressively tries to focus on the relevant aspects of the query and the documents which are exploited to generate the answers."]}
{"question_id": "860257956b83099cccf1359e5d960289d7d50265", "predicted_answer": "", "predicted_evidence": ["Given the contextual representations for the query words INLINEFORM0 and the inference GRU state INLINEFORM1 , we obtain a refined query representation INLINEFORM2 (query glimpse) by performing an attention mechanism over the query at inference step INLINEFORM3 : INLINEFORM4", "The neural network weights are supposed to learn latent features which encode relationships between the most relevant words for the given query to predict the correct answers. The outer sigmoid activation function is used to treat the problem as a multi-label classification problem, so that each candidate answer is independent and not mutually exclusive. In this way the neural network generates a score which represents the probability that the candidate answer is correct. Moreover, differently from BIBREF3 , the candidate answer INLINEFORM0 can be any word, even those which not belong to the documents related to the query.", "The paper is organized as follows: Section SECREF2 describes our model, while Section SECREF3 summarizes the evaluation of the model on the two above-mentioned tasks and the comparison with respect to state-of-the-art approaches. Section SECREF4 gives an overview of the literature of both QA and recommender systems, while final remarks and our long-term vision are reported in Section SECREF5 .", "In recent days, a lot of effort is devoted to create benchmarks for artificial agents to assess their ability to comprehend natural language and to reason over facts. One of the first attempt is the bAbI BIBREF17 dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks. Another relevant benchmark is the one described in BIBREF18 , which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by reading and comprehending the given document. MCTest BIBREF19 requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.", "For the task related to QA, a lot of datasets have been released in order to assess the machine reading and comprehension capabilities and a lot of neural network-based models have been proposed. Our model takes inspiration from BIBREF3 , which is able to answer Cloze-style BIBREF4 questions repeating an attention mechanism over the query and the documents multiple times. Despite the effectiveness on the Cloze-style task, the original model does not consider multiple documents as a source of information to answer questions, which is fundamental in order to extract the answer from different relevant facts. The restricted assumption that the answer is contained in the given document does not allow the model to provide an answer which does not belong to the document. Moreover, this kind of task does not expect multiple answers for a given question, which is important for the complex information needs required for a conversational recommender system."]}
{"question_id": "1b1849ad0bdd79c6645572849fe7873ec7bd7e6d", "predicted_answer": "", "predicted_evidence": ["I here propose to see a domain as variety in a high-dimensional variety space. Points in the space are the data instances, and regions form domains. A dataset $\\mathcal {D}$ is a sample from the variety space, conditioned on latent factors $V$ : $\\mathcal {D} \\sim P(X,Y|V)$", "What we need are methods that can adapt quickly to unknown domains and languages, without much assumptions on what to expect, and use multiple sources, rather than just one. In addition, our models need to detect when to trigger domain adaptation approaches.", "An extreme case of adaptation is cross-lingual learning, whose goal is similar: adapt models trained on some source languages to languages in which few or no resources exist. Also here a large body of work assumes knowledge of the target language and requires some in-domain, typically parallel data. However, most work has focused on a restricted set of languages, only recently approaches emerged that aim to transfer from multiple sources to many target languages BIBREF11 .", "While newswire has advanced the field in so many ways, it has also introduced almost imperceptible biases. What we need is to be aware of such biases, collect enough biased data, and model variety. I argue that if we embrace the variety of this heterogeneous data by combining it with proper algorithms, in addition to including text covariates/latent factors, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation.", "I would like to thank the organizers for the invitation to the keynote at KONVENS 2016. I am also grateful to H\u00e9ctor Mart\u00ednez Alonso, Dirk Hovy, Anders Johannsen, Zeljko Agi\u0107 and Gertjan van Noord for valuable discussions and feedback on earlier drafts of this paper."]}
{"question_id": "deb0c3524a3b3707e8b20abd27f54ad6188d6e4e", "predicted_answer": "", "predicted_evidence": ["In this paper, we presented Tweet2Vec, a novel method for generating general-purpose vector representation of tweets, using a character-level CNN-LSTM encoder-decoder architecture. To the best of our knowledge, ours is the first attempt at learning and applying character-level tweet embeddings. Our character-level model can deal with the noisy and peculiar nature of tweets better than methods that generate embeddings at the word level. Our model is also robust to synonyms with the help of our data augmentation technique using WordNet.", "For future work, we plan to extend the method to include: 1) Augmentation of data through reordering the words in the tweets to make the model robust to word-order, 2) Exploiting attention mechanism BIBREF13 in our model to improve alignment of words in tweets during decoding, which could improve the overall performance.", "The vector representations generated by our model are generic, and thus can be applied to tasks of different nature. We evaluated our model using two different SemEval 2015 tasks: Twitter semantic relatedness, and sentiment classification. Simple, off-the-shelf logistic regression classifiers trained using the vector representations generated by our model outperformed the top-performing methods for both tasks, without the need for any extensive feature engineering. This was despite the fact that due to resource limitations, our Tweet2Vec model was trained on a relatively small set (3 million tweets). Also, our method outperformed ParagraphVec, which is an extension of Word2Vec to handle sentences. This is a small but noteworthy illustration of why our tweet embeddings are best-suited to deal with the noise and idiosyncrasies of tweets.", "As with the last task, we first extract the vector representation of all the tweets in the dataset using Tweet2Vec and use that to train a logistic regression classifier using the vector representations. Even though there are three classes, the SemEval task is a binary task. The performance is measured as the average F1-score of the positive and the negative class. Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec. Our model outperforms all these models, again without resorting to any feature engineering.", "In recent years, the micro-blogging site Twitter has become a major social media platform with hundreds of millions of users. The short (140 character limit), noisy and idiosyncratic nature of tweets make standard information retrieval and data mining methods ill-suited to Twitter. Consequently, there has been an ever growing body of IR and data mining literature focusing on Twitter. However, most of these works employ extensive feature engineering to create task-specific, hand-crafted features. This is time consuming and inefficient as new features need to be engineered for every task."]}
{"question_id": "d7e43a3db8616a106304ac04ba729c1fee78761d", "predicted_answer": "", "predicted_evidence": ["", "abcdefghijklmnopqrstuvwxyz0123456789", "where INLINEFORM0 is an offset constant.", "We trained the CNN-LSTM encoder-decoder model on 3 million randomly selected English-language tweets populated using data augmentation techniques, which are useful for controlling generalization error for deep learning models. Data augmentation, in our context, refers to replicating tweet and replacing some of the words in the replicated tweets with their synonyms. These synonyms are obtained from WordNet BIBREF8 which contains words grouped together on the basis of their meanings. This involves selection of replaceable words (example of non-replaceable words are stopwords, user names, hash tags, etc) from the tweet and the number of words INLINEFORM0 to be replaced. The probability of the number, INLINEFORM1 , is given by a geometric distribution with parameter INLINEFORM2 in which INLINEFORM3 . Words generally have several synonyms, thus the synonym index INLINEFORM4 , of a given word is also determined by another geometric distribution in which INLINEFORM5 . In our encoder-decoder model, we decode the encoded representation to the actual tweet or a synonym-replaced version of the tweet from the augmented data.", "-,;.!?:'\"/\\|_#$%&^*~`+-=<>()[]{}"]}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "predicted_answer": "", "predicted_evidence": ["We evaluated our model using two classification tasks: Tweet semantic relatedness and Tweet sentiment classification.", "We adapted this model, which employs temporal convolution and pooling operations, for tweets. The character set includes the English alphabets, numbers, special characters and unknown character. There are 70 characters in total, given below:", "As with the last task, we first extract the vector representation of all the tweets in the dataset using Tweet2Vec and use that to train a logistic regression classifier using the vector representations. Even though there are three classes, the SemEval task is a binary task. The performance is measured as the average F1-score of the positive and the negative class. Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec. Our model outperforms all these models, again without resorting to any feature engineering.", "", "where INLINEFORM0 refers to the character at time-step INLINEFORM1 , INLINEFORM2 represents the one-hot vector of the character at time-step INLINEFORM3 . The result from the softmax is a decoded tweet matrix INLINEFORM4 , which is eventually compared with the actual tweet or a synonym-replaced version of the tweet (explained in Section SECREF3 ) for learning the parameters of the model."]}
{"question_id": "b7d02f12baab5db46ea9403d8932e1cd1b022f79", "predicted_answer": "", "predicted_evidence": ["This involves selection of replaceable words (example of non-replaceable words are stopwords, user names, hash tags, etc) from the tweet and the number of words INLINEFORM0 to be replaced. The probability of the number, INLINEFORM1 , is given by a geometric distribution with parameter INLINEFORM2 in which INLINEFORM3 . Words generally have several synonyms, thus the synonym index INLINEFORM4 , of a given word is also determined by another geometric distribution in which INLINEFORM5 . In our encoder-decoder model, we decode the encoded representation to the actual tweet or a synonym-replaced version of the tweet from the augmented data. We used INLINEFORM6 , INLINEFORM7 for our training. We also make sure that the POS tags of the replaced words are not completely different from the actual words. For regularization, we apply a dropout mechanism after the penultimate layer. This prevents co-adaptation of hidden units by randomly setting a proportion INLINEFORM8 of the hidden units to zero (for our case, we set INLINEFORM9 ).", "where INLINEFORM0 refers to the character at time-step INLINEFORM1 , INLINEFORM2 represents the one-hot vector of the character at time-step INLINEFORM3 . The result from the softmax is a decoded tweet matrix INLINEFORM4 , which is eventually compared with the actual tweet or a synonym-replaced version of the tweet (explained in Section SECREF3 ) for learning the parameters of the model.", "In this section, we describe the CNN-LSTM encoder-decoder model that operates at the character level and generates vector representation of tweets. The encoder consists of convolutional layers to extract features from the characters and an LSTM layer to encode the sequence of features to a vector representation, while the decoder consists of two LSTM layers which predict the character at each time step from the output of encoder.", "Data augmentation, in our context, refers to replicating tweet and replacing some of the words in the replicated tweets with their synonyms. These synonyms are obtained from WordNet BIBREF8 which contains words grouped together on the basis of their meanings. This involves selection of replaceable words (example of non-replaceable words are stopwords, user names, hash tags, etc) from the tweet and the number of words INLINEFORM0 to be replaced. The probability of the number, INLINEFORM1 , is given by a geometric distribution with parameter INLINEFORM2 in which INLINEFORM3 . Words generally have several synonyms, thus the synonym index INLINEFORM4 , of a given word is also determined by another geometric distribution in which INLINEFORM5 . In our encoder-decoder model, we decode the encoded representation to the actual tweet or a synonym-replaced version of the tweet from the augmented data. We used INLINEFORM6 , INLINEFORM7 for our training. We also make sure that the POS tags of the replaced words are not completely different from the actual words.", "We trained the CNN-LSTM encoder-decoder model on 3 million randomly selected English-language tweets populated using data augmentation techniques, which are useful for controlling generalization error for deep learning models. Data augmentation, in our context, refers to replicating tweet and replacing some of the words in the replicated tweets with their synonyms. These synonyms are obtained from WordNet BIBREF8 which contains words grouped together on the basis of their meanings. This involves selection of replaceable words (example of non-replaceable words are stopwords, user names, hash tags, etc) from the tweet and the number of words INLINEFORM0 to be replaced. The probability of the number, INLINEFORM1 , is given by a geometric distribution with parameter INLINEFORM2 in which INLINEFORM3 . Words generally have several synonyms, thus the synonym index INLINEFORM4 , of a given word is also determined by another geometric distribution in which INLINEFORM5 . In our encoder-decoder model, we decode the encoded representation to the actual tweet or a synonym-replaced version of the tweet from the augmented data."]}
{"question_id": "ff2b58c90784eda6dddd8a92028e6432442c1093", "predicted_answer": "", "predicted_evidence": ["Formally, an HMM is a 4-tuple INLINEFORM1 , where INLINEFORM2 is a set of states, INLINEFORM3 is the probability of transition from INLINEFORM4 to INLINEFORM5 , INLINEFORM6 is a set of possible non-null observations, and INLINEFORM7 is the probability of observing INLINEFORM8 when in state INLINEFORM9 , where INLINEFORM11 , and INLINEFORM12 is the terminal state. An HMM is Left-to-Right if the states of the HMM can be ordered from INLINEFORM13 thru INLINEFORM14 such that INLINEFORM15 is non-zero only if INLINEFORM16 . We assume that our target HMM is Left-to-Right. We index its states according to a topological ordering of the transition graph. An HMM is a generative model of a distribution over sequences of observations. For convenience w.l.o.g. we assume that each time it is \u201crun\u201d to generate a sample, the HMM starts in the same initial state INLINEFORM17 , and goes through a sequence of transitions according to INLINEFORM18 until it reaches the same final state INLINEFORM19 , while emitting an observation in INLINEFORM20 in each state according to INLINEFORM21 .", "The backward part of the standard Forward-Backward algorithm starts from the last time step INLINEFORM0 and reasons backwards. Unfortunately in our setting, we do not know INLINEFORM1 \u2014the true number of state transitions\u2014as some of the observations are missing. Hence, we define INLINEFORM2 as the conditional probability of observing INLINEFORM3 in the remaining INLINEFORM4 steps given that the current state is INLINEFORM5 . This allows us to increment INLINEFORM6 starting from 0 as recursion proceeds, rather than decrementing it from INLINEFORM7 .", "Given the above assumptions, following BIBREF4 , we apply a simple agglomerative clustering algorithm that uses a semantic similarity function over sentence pairs INLINEFORM0 given by INLINEFORM1 , where INLINEFORM2 is the verb and INLINEFORM3 is the object in the sentence INLINEFORM4 . Here INLINEFORM5 is the path similarity metric from Wordnet BIBREF13 . It is applied to the first verb (preferring verbs that are not stop words) and to the objects from each pair of sentences. The constants INLINEFORM6 and INLINEFORM7 are tuning parameters that adjust the relative importance of each component. Like BIBREF4 , we found that a high weight on the verb similarity was important to finding meaningful clusters of events. The most frequent verb in each cluster is extracted to name the event type that corresponds to that cluster.", "For convenience w.l.o.g. we assume that each time it is \u201crun\u201d to generate a sample, the HMM starts in the same initial state INLINEFORM17 , and goes through a sequence of transitions according to INLINEFORM18 until it reaches the same final state INLINEFORM19 , while emitting an observation in INLINEFORM20 in each state according to INLINEFORM21 . The initial state INLINEFORM22 and the final state INLINEFORM23 respectively emit the distinguished observation symbols, \u201c INLINEFORM24 \u201d and \u201c INLINEFORM25 \u201d in INLINEFORM26 , which are emitted by no other state. The concatenation of observations in successive states consitutes a sample of the distribution represented by the HMM. Because the null observations are removed from the generated observations, the length of the output string may be smaller than the number of state transitions. It could also be larger than the number of distinct state transitions, since we allow observations to be generated on the self transitions. Thus spurious and missing observations model insertions and deletions in the outputs of HMMs without introducing special states as in profile HMMs BIBREF8 .", "The main difficulty in HMM parameter estimation is that the states of the HMM are not observed. The Expectation-Maximization (EM) procedure (also called the Baum-Welch algorithm in HMMs) alternates between estimating the hidden states in the event sequences by running the Forward-Backward algorithm (the Expectation step) and finding the maximum likelihood estimates (the Maximization step) of the transition and observation parameters of the HMM BIBREF16 . Unfortunately, because of the INLINEFORM0 -transitions the state transitions of our HMM are not necessarily aligned with the observations. Hence we explicitly maintain two indices, the time index INLINEFORM1 and the observation index INLINEFORM2 . We define INLINEFORM3 to be the joint probability that the HMM is in state INLINEFORM4 at time INLINEFORM5 and has made the observations INLINEFORM6 . This is computed by the forward pass of the algorithm using the following recursion. Equations EQREF5 and represent the base case of the recursion, while Equation represents the case for null observations."]}
{"question_id": "5e4eac0b0a73d465d74568c21819acaec557b700", "predicted_answer": "", "predicted_evidence": ["DISPLAYFORM0", "In principle, one could continue making incremental structural changes and parameter updates and never run EM again. This is exactly what is done in Bayesian Model Merging (BMM) BIBREF9 . However, a series of structural changes followed by approximate incremental parameter updates could lead to bad local optima. Hence, after merging each batch of INLINEFORM0 sequences into the HMM, we re-run EM for parameter estimation on all sequences seen thus far.", "INLINEFORM0 INLINEFORM1 INLINEFORM2", "Equation , the joint distribution of the state and observation index INLINEFORM0 at time INLINEFORM1 is computed by convolution, i.e., multiplying the INLINEFORM2 and INLINEFORM3 that correspond to the same time step and the same state and marginalizing out the length of the state-sequence INLINEFORM4 . Convolution is necessary, as the length of the state-sequence INLINEFORM5 is a random variable equal to the sum of the corresponding time indices of INLINEFORM6 and INLINEFORM7 .", "The 84 domains with at least 50 narratives and 3 event types were used for evaluation. For each domain, forty percent of the narratives were withheld for testing, each with one randomly-chosen event omitted. The model was evaluated on the proportion of correctly predicted events given the remaining sequence. On average each domain has 21.7 event types with a standard deviation of 4.6. Further, the average narrative length across domains is 3.8 with standard deviation of 1.7. This implies that only a frcation of the event types are present in any given narrative. There is a high degree of omission of events and many different ways of accomplishing each task. Hence, the prediction task is reasonably difficult, as evidenced by the simple baselines. Neither the frequency of events nor simple temporal structure is enough to accurately fill in the gaps which indicates that most sophisticated modeling such as SEM-HMM is needed."]}
{"question_id": "bc6ad5964f444cf414b661a4b942dafb7640c564", "predicted_answer": "", "predicted_evidence": ["Forward-Backward algorithm to delete an edge and re-distribute the expected counts.", "", "More recently, some new graph-based algorithms for inducing script-like structures from text have emerged. \u201cNarrative Chains\u201d is a narrative model similar to Scripts BIBREF3 . Each Narrative Chain is a directed graph indicating the most frequent temporal relationship between the events in the chain. Narrative Chains are learned by a novel application of pairwise mutual information and temporal relation learning. Another graph learning approach employs Multiple Sequence Alignment in conjunction with a semantic similarity function to cluster sequences of event descriptions into a directed graph BIBREF4 . More recently still, graphical models have been proposed for representing script-like knowledge, but these lack the temporal component that is central to this paper and to the early script work. These models instead focus on learning bags of related events BIBREF5 , BIBREF6 .", "In this paper, we have given the first formal treatment of scripts as HMMs with missing observations. We adapted the HMM inference and parameter estimation procedures to scripts and developed a new structure learning algorithm, SEM-HMM, based on the EM procedure. It improves upon BMM by allowing for INLINEFORM0 transitions and by incorporating maximum likelihood parameter estimation via EM. We showed that our algorithm is effective in learning scripts from documents and performs better than other baselines on sequence prediction tasks. Thanks to the assumption of missing observations, the graphical structure of the scripts is usually sparse and intuitive. Future work includes learning from more natural text such as newspaper articles, enriching the representations to include objects and relations, and integrating HMM inference into text understanding.", "The backward part of the standard Forward-Backward algorithm starts from the last time step INLINEFORM0 and reasons backwards. Unfortunately in our setting, we do not know INLINEFORM1 \u2014the true number of state transitions\u2014as some of the observations are missing. Hence, we define INLINEFORM2 as the conditional probability of observing INLINEFORM3 in the remaining INLINEFORM4 steps given that the current state is INLINEFORM5 . This allows us to increment INLINEFORM6 starting from 0 as recursion proceeds, rather than decrementing it from INLINEFORM7 ."]}
{"question_id": "380e71848d4b0d1e983d504b1249119612f00bcb", "predicted_answer": "", "predicted_evidence": ["Proposed Methods: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.", "We experimented with a dataset of 16K annotated tweets made available by the authors of BIBREF0 . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist. For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings. GloVe embeddings have been trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased). We experimented with multiple word embedding sizes for our task. We observed similar results with different sizes, and hence due to lack of space we report results using embedding size=200. We performed 10-Fold Cross Validation and calculated weighted macro precision, recall and F1-scores.", "In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).", "Main contributions of our paper are as follows: (1) We investigate the application of deep learning methods for the task of hate speech detection. (2) We explore various tweet semantic embeddings like char n-grams, word Term Frequency-Inverse Document Frequency (TF-IDF) values, Bag of Words Vectors (BoWV) over Global Vectors for Word Representation (GloVe), and task-specific embeddings learned using FastText, CNNs and LSTMs. (3) Our methods beat state-of-the-art methods by a large margin ( INLINEFORM0 18 F1 points better).", "With the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure. On Twitter, hateful tweets are those that contain abusive speech targeting individuals (cyber-bullying, a politician, a celebrity, a product) or particular groups (a country, LGBT, a religion, gender, an organization, etc.). Detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities. It is also useful to filter tweets before content recommendation, or learning AI chatterbots from tweets."]}
{"question_id": "21c89ee0281f093b209533453196306b9699b552", "predicted_answer": "", "predicted_evidence": ["Proposed Methods: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection.", "As the table shows, our proposed methods in part B are significantly better than the baseline methods in part A. Among the baseline methods, the word TF-IDF method is better than the character n-gram method. Among part B methods, CNN performed better than LSTM which was better than FastText. Surprisingly, initialization with random embeddings is slightly better than initialization with GloVe embeddings when used along with GBDT. Finally, part C methods are better than part B methods. The best method is \u201cLSTM + Random Embedding + GBDT\u201d where tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier. Combinations of CNN, LSTM, FastText embeddings as features for GBDTs did not lead to better results. Also note that the standard deviation for all these methods varies from 0.01 to 0.025.", "Baseline Methods: As baselines, we experiment with three broad representations. (1) Char n-grams: It is the state-of-the-art method BIBREF0 which uses character n-grams for hate speech detection. (2) TF-IDF: TF-IDF are typical features used for text classification. (3) BoWV: Bag of Words Vector approach uses the average of the word (GloVe) embeddings to represent a sentence. We experiment with multiple classifiers for both the TF-IDF and the BoWV approaches.", "In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).", "In this paper, we investigated the application of deep neural network architectures for the task of hate speech detection. We found them to significantly outperform the existing methods. Embeddings learned from deep neural network models when combined with gradient boosted decision trees led to best accuracy values. In the future, we plan to explore the importance of the user network features for the task."]}
{"question_id": "5096aaea2d0f4bea4c12e14f4f7735e1aea1bfa6", "predicted_answer": "", "predicted_evidence": ["As the table shows, our proposed methods in part B are significantly better than the baseline methods in part A. Among the baseline methods, the word TF-IDF method is better than the character n-gram method. Among part B methods, CNN performed better than LSTM which was better than FastText. Surprisingly, initialization with random embeddings is slightly better than initialization with GloVe embeddings when used along with GBDT. Finally, part C methods are better than part B methods. The best method is \u201cLSTM + Random Embedding + GBDT\u201d where tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier. Combinations of CNN, LSTM, FastText embeddings as features for GBDTs did not lead to better results. Also note that the standard deviation for all these methods varies from 0.01 to 0.025.", "We first discuss a few baseline methods and then discuss the proposed approach. In all these methods, an embedding is generated for a tweet and is used as its feature representation with a classifier.", "To verify the task-specific nature of the embeddings, we show top few similar words for a few chosen words in Table TABREF7 using the original GloVe embeddings and also embeddings learned using DNNs. The similar words obtained using deep neural network learned embeddings clearly show the \u201chatred\u201d towards the target words, which is in general not visible at all in similar words obtained using GloVe.", "Proposed Methods: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection.", "With the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure. On Twitter, hateful tweets are those that contain abusive speech targeting individuals (cyber-bullying, a politician, a celebrity, a product) or particular groups (a country, LGBT, a religion, gender, an organization, etc.). Detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities. It is also useful to filter tweets before content recommendation, or learning AI chatterbots from tweets."]}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "predicted_answer": "", "predicted_evidence": ["As the table shows, our proposed methods in part B are significantly better than the baseline methods in part A. Among the baseline methods, the word TF-IDF method is better than the character n-gram method. Among part B methods, CNN performed better than LSTM which was better than FastText. Surprisingly, initialization with random embeddings is slightly better than initialization with GloVe embeddings when used along with GBDT. Finally, part C methods are better than part B methods. The best method is \u201cLSTM + Random Embedding + GBDT\u201d where tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier. Combinations of CNN, LSTM, FastText embeddings as features for GBDTs did not lead to better results. Also note that the standard deviation for all these methods varies from 0.01 to 0.025.", "The manual way of filtering out hateful tweets is not scalable, motivating researchers to identify automated ways. In this work, we focus on the problem of classifying a tweet as racist, sexist or neither. The task is quite challenging due to the inherent complexity of the natural language constructs \u2013 different forms of hatred, different kinds of targets, different ways of representing the same meaning. Most of the earlier work revolves either around manual feature extraction BIBREF0 or use representation learning methods followed by a linear classifier BIBREF1 , BIBREF2 . However, recently deep learning methods have shown accuracy improvements across a large number of complex problems in speech, vision and text applications. To the best of our knowledge, we are the first to experiment with deep learning architectures for the hate speech detection task.", "We first discuss a few baseline methods and then discuss the proposed approach. In all these methods, an embedding is generated for a tweet and is used as its feature representation with a classifier.", "We use `adam' for CNN and LSTM, and `RMS-Prop' for FastText as our optimizer. We perform training in batches of size 128 for CNN & LSTM and 64 for FastText. More details on the experimental setup can be found from our publicly available source code.", "All of these networks are trained (fine-tuned) using labeled data with back-propagation. Once the network is learned, a new tweet is tested against the network which classifies it as racist, sexist or neither. Besides learning the network weights, these methods also learn task-specific word embeddings tuned towards the hate speech labels. Therefore, for each of the networks, we also experiment by using these embeddings as features and various other classifiers like SVMs and GBDTs as the learning method."]}
{"question_id": "cdb211be0340bb18ba5a9ee988e9df0e2ba8b793", "predicted_answer": "", "predicted_evidence": ["Extending our study beyond North America is a task for future work. Social networks vary dramatically across cultures, with traditional societies tending toward networks with fewer but stronger ties BIBREF3 . The social properties of language variation in these societies may differ as well. Another important direction for future work is to determine the impact of exogenous events, such as the appearance of new linguistic forms in mass media. Exogeneous events pose potential problems for estimating both infection risks and social influence. However, it may be possible to account for these events by incorporating additional data sources, such as search trends. Finally, we plan to use our framework to study the spread of terminology and ideas through networks of scientific research articles. Here too, authors may make socially motivated decisions to adopt specific terms and ideas BIBREF50 . The principles behind these decisions might therefore be revealed by an analysis of linguistic events propagating over a social network.", "In sec:results, we describe how we used these features to construct a set of nested models that enabled us to test our hypotheses. In the remainder of this section, we provide the mathematical details of our parameter estimation method.", "Instead of directly estimating all $O(M^2)$ pairwise influence parameters, we used Li and Zha's parametric Hawkes process BIBREF12 . This model defines each pairwise influence parameter in terms of a linear combination of pairwise features:", "Features F3 and F4 did not improve the goodness of fit for less frequent words, such as ain, graffiti, and yeen, which occur fewer than $10^4$ times. Below this count threshold, there is not enough data to statistically distinguish between different types of social network connections. However, above this count threshold, adding in F3 (tie strength) yielded a statistically significant increase in goodness of fit for ard, asl, cookout, hella, jawn, mfs, and tfti. This finding provides evidence in favor of hypothesis H1\u2014that the linguistic influence exerted across densely embedded ties is greater than the linguistic influence exerted across other ties.", "This feature fires if the dyad $(m, m^{\\prime })$ is in the Twitter mutual-reply network described in sec:data-social. We also used this feature to define the remaining two features. By doing this, we ensured that features F2, F3, and F4 were (at least) as sparse as the mutual-reply network."]}
{"question_id": "4cb2e80da73ae36de372190b4c1c490b72977ef8", "predicted_answer": "", "predicted_evidence": ["The explosive rise in popularity of social media has led to an increase in linguistic diversity and creativity BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF1 , BIBREF18 , affecting written language at all levels, from spelling BIBREF19 all the way up to grammatical structure BIBREF20 and semantic meaning across the lexicon BIBREF21 , BIBREF22 . Here, we focus on the most easily observable and measurable level: variation and change in the use of individual words.", "We chose a set of four binary features that would enable us to test our hypotheses about the roles of different types of social network connections:", "This feature fires if the dyad $(m, m^{\\prime })$ is in the Twitter mutual-reply network described in sec:data-social. We also used this feature to define the remaining two features. By doing this, we ensured that features F2, F3, and F4 were (at least) as sparse as the mutual-reply network.", "Because the words described in sec:data-language are relatively rare, most of the users in our data set never used them. However, it is important to include these users in the model. Because they did not adopt these words, despite being exposed to them by users who did, their presence exerts a negative gradient on the feature weights. Moreover, such users impose a minimal cost on parameter estimation because they need to be considered only when pre-computing feature counts.", "The first condition implies that language change is related to the structure of social networks. If a significant number of speakers are isolated from a potential change, then they are unlikely to adopt it BIBREF3 . But mere exposure is not sufficient\u2014we are all exposed to language varieties that are different from our own, yet we nonetheless do not adopt them in our own speech and writing. For example, in the United States, many African American speakers maintain a distinct dialect, despite being immersed in a linguistic environment that differs in many important respects BIBREF4 , BIBREF5 . Researchers have made a similar argument for socioeconomic language differences in Britain BIBREF6 . In at least some cases, these differences reflect questions of identity: because language is a key constituent in the social construction of group identity, individuals must make strategic choices when deciding whether to adopt new linguistic forms BIBREF7 , BIBREF8 , BIBREF9 . By analyzing patterns of language change, we can learn more about the latent structure of social organization: to whom people talk, and how they see themselves."]}
{"question_id": "a064337bafca8cf01e222950ea97ebc184c47bc0", "predicted_answer": "", "predicted_evidence": ["where the second term includes a sum over all events $n = \\lbrace 1, \\ldots ,\nN\\rbrace $ that contibute to the final intensity $\\lambda ^{(m^{\\prime })}(T).$ To ease computation, however, we can rearrange the second term around the source $m$ rather than the recipient $m^{\\prime }$ :", "We tested hypothesis H1 (strong ties are more influential) by comparing the goodness of fit for feature set F1+F2+F3 to that of feature set F1+F2. Similarly, we tested H2 (geographically local ties are more influential) by comparing the goodness of fit for feature set F1+F2+F4 to that of feature set F1+F2.", "A Hawkes process defined in terms of eq:intensity has a log likelihood that is convex in the pairwise influence parameters and the base intensities. For a parametric Hawkes process, $\\alpha _{m \\rightarrow m^{\\prime }}$ is an affine function of $\\theta $ , so, by composition, the log likelihood is convex in $\\theta $ and remains convex in the base intensities.", "$$\\mathcal {L} = \\sum _{n=1}^N \\log \\lambda ^{(m_n)}(t_n) - \\sum _{m = 1}^M \\int _0^T \\lambda ^{(m)}(t)\\ \\textrm {d}t,$$   (Eq. 42)", "This feature fires if the dyad $(m, m^{\\prime })$ is in the Twitter mutual-reply network described in sec:data-social. We also used this feature to define the remaining two features. By doing this, we ensured that features F2, F3, and F4 were (at least) as sparse as the mutual-reply network."]}
{"question_id": "993d5bef2bf1c0cd537342ef76d4b952f0588b83", "predicted_answer": "", "predicted_evidence": ["For a parametric Hawkes process, it is not necessary to compute a set of aggregate messages for each dyad. It is sufficient to compute a set of aggregate messages for each possible configuration of the features. In our setting, there are only four binary features, and some combinations of features are impossible.", "The second term is linear in the number of social network connections and linear in the number of events. Again, we can use the exponential decay of the kernel $\\kappa (\\cdot )$ to approximate $\\kappa (T - t_n)\n\\approx 0$ for $T - t_n \\ge \\tau ^{\\star }$ , where $\\tau ^{\\star } = 24\n\\textrm { hours}$ . This approximation means that we only need to consider a small number of tweets near temporal endpoint of the cascade. For each user, we also pre-computed $\\sum _{\\lbrace n : m_n = m^{\\prime }\\rbrace }\nf(m^{\\prime } \\rightarrow \\star )\\,\\kappa (T - t_n)$ . Finally, both terms in the log likelihood and its gradient can also be trivially parallelized over users $m = \\lbrace 1, \\ldots , M\\rbrace $ .", "In fig:ll-diffs, we show the improvement in goodness of fit from adding in features F3 and F4. Under the null hypothesis, the log of the likelihood ratio follows a $\\chi ^2$ distribution with one degree of freedom, because the models differ by one parameter. Because we performed thirty-two hypothesis tests (sixteen words, two features), we needed to adjust the significance thresholds to correct for multiple comparisons. We did this using the Benjamini-Hochberg procedure BIBREF46 .", "Features F3 and F4 did not improve the goodness of fit for less frequent words, such as ain, graffiti, and yeen, which occur fewer than $10^4$ times. Below this count threshold, there is not enough data to statistically distinguish between different types of social network connections. However, above this count threshold, adding in F3 (tie strength) yielded a statistically significant increase in goodness of fit for ard, asl, cookout, hella, jawn, mfs, and tfti. This finding provides evidence in favor of hypothesis H1\u2014that the linguistic influence exerted across densely embedded ties is greater than the linguistic influence exerted across other ties.", "Extending our study beyond North America is a task for future work. Social networks vary dramatically across cultures, with traditional societies tending toward networks with fewer but stronger ties BIBREF3 . The social properties of language variation in these societies may differ as well. Another important direction for future work is to determine the impact of exogenous events, such as the appearance of new linguistic forms in mass media. Exogeneous events pose potential problems for estimating both infection risks and social influence. However, it may be possible to account for these events by incorporating additional data sources, such as search trends. Finally, we plan to use our framework to study the spread of terminology and ideas through networks of scientific research articles. Here too, authors may make socially motivated decisions to adopt specific terms and ideas BIBREF50 . The principles behind these decisions might therefore be revealed by an analysis of linguistic events propagating over a social network."]}
{"question_id": "a8e5e10d13b3f21dd11e8eb58e30cc25efc56e93", "predicted_answer": "", "predicted_evidence": ["We have also compared hrLDA to several unsupervised ontology learning models and shown that hrLDA can learn applicable terminological ontologies from real world data. Although hrLDA cannot be applied directly in formal reasoning, it is efficient for building knowledge bases for information retrieval and simple question answering. Also, hrLDA is sensitive to the quality of extracted relation triplets. In order to give optimal answers, hrLDA should be embedded in more complex probabilistic modules to identify true facts from extracted ontology rules. Finally, one issue we have not addressed in our current study is capturing pre-knowledge. Although a direct solution would be adding the missing information to the data set, a more advanced approach would be to train topic embeddings to extract hidden semantics.", "where INLINEFORM0 is a vector containing the INLINEFORM1 relation triplets in document INLINEFORM2 , and INLINEFORM3 is the topic assignment for INLINEFORM4 .", "The comparison results on our Wiki corpus are shown in Figure FIGREF42 . hrLDA yields the lowest perplexity and reasonable running time. As the running time spent on parameter optimization is extremely long (the optimized_LDA requires 19.90 hours to complete one run), for efficiency, we adhere to the fixed parameter settings for hrLDA.", "We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird.", "hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than phrase_hLDA and LDA+GSHL, and phrase_hLDA performs similarly to LDA+GSHL. In general, hrLDA works well especially when the pre-knowledge already exists inside the corpora. Consider the following two statements taken from the corpus on Birds of the United States as an example. In order to use two short documents \u201cThe Acadian flycatcher is a small insect-eating bird.\" and \u201cThe Pacific loon is a medium-sized member of the loon.\" to infer that the Acadian flycatcher and the Pacific loon are both related to topic bird, the pre-knowledge that \u201cthe loon is a species of bird\" is required for hrLDA. This example explains why the accuracy of extracting ontologies from this kind of corpus is low."]}
{"question_id": "949a2bc34176e47a4d895bcc3223f2a960f15a81", "predicted_answer": "", "predicted_evidence": ["We have compared hrLDA with popular topic models to interpret how our algorithm learns meaningful hierarchies. By taking syntax and document structures into consideration, hrLDA is able to extract more descriptive topics. In addition, hrLDA eliminates the restrictions on the fixed topic tree depth and the limited number of topic paths. Furthermore, ACRP allows hrLDA to create more reasonable topics and to converge faster in Gibbs sampling.", "In this paper, we have proposed a completely unsupervised model, hrLDA, for terminological ontology learning. hrLDA is a domain-independent and self-learning model, which means it is very promising for learning ontologies in new domains and thus can save significant time and effort in ontology acquisition.", "This work was supported in part by Intel Corporation, Semiconductor Research Corporation (SRC). We are obliged to Professor Goce Trajcevski from Northwestern University for his insightful suggestions and discussions. This work was partly conducted using the Protege resource, which is supported by grant GM10331601 from the National Institute of General Medical Sciences of the United States National Institutes of Health.", "We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia.", "A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase_hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases."]}
{"question_id": "70abb108c3170e81f8725ddc1a3f2357be5a4959", "predicted_answer": "", "predicted_evidence": ["Superiority", "Based on the probabilities EQREF18 to EQREF25 , we sample a topic index INLINEFORM0 from INLINEFORM1 for every noun phrase, and we count the number of unique topics INLINEFORM2 in the end. We shuffle the order of documents and iterate ACRP until INLINEFORM3 is unchanged.", "For every atomic sentence whose object is also a noun phrase, there are at least two relation triplets (e.g., \u201cThe tiger that gave the excellent speech is handsome\" has relation triplets: (tiger, give, speech), (speech, be given by, tiger), and (tiger, be, handsome)). By contrast, a complex sentence can be subdivided into multiple atomic sentences. Given that the syntactic verb in a relation triplet is determined by the subject and the object, a document INLINEFORM4 in a corpus INLINEFORM5 can be ultimately reduced to INLINEFORM6 subject phrases (we convert objects to subjects using passive voice) associated with INLINEFORM7 relation triplets INLINEFORM8 . Number INLINEFORM9 is usually larger than the actual number of noun phrases in document INLINEFORM10 . By replacing the unigrams in LDA with relation triplets, we retain definitive information and assign salient noun phrases high weights.", "The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase_hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases. hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than phrase_hLDA and LDA+GSHL, and phrase_hLDA performs similarly to LDA+GSHL. In general, hrLDA works well especially when the pre-knowledge already exists inside the corpora. Consider the following two statements taken from the corpus on Birds of the United States as an example. In order to use two short documents \u201cThe Acadian flycatcher is a small insect-eating bird.\"", "A special type of relation triplets can be extracted from presentation documents such as those written in PowerPoint using document structures. Normally lines in a slide are not complete sentences, which means language parsing does not work. However, indentations and bullet types usually express inclusion relationships between adjacent lines. Starting with the first line in an itemized section, our algorithm scans the content in a slide line by line, and creates relations based on the current item and the item that is one level higher."]}
{"question_id": "ce504a7ee2c1f068ef4dde8d435245b4e77bb0b5", "predicted_answer": "", "predicted_evidence": ["Mark the number of topics (child nodes) of parent node INLINEFORM0 at level INLINEFORM1 as INLINEFORM2 . Build a INLINEFORM3 - dimensional topic proportion vector INLINEFORM4 based on INLINEFORM5 .", "In this paper, we have proposed a completely unsupervised model, hrLDA, for terminological ontology learning. hrLDA is a domain-independent and self-learning model, which means it is very promising for learning ontologies in new domains and thus can save significant time and effort in ontology acquisition.", "Robustness", "hLDA combines LDA with CRP by setting one topic path with fixed depth INLINEFORM0 for each document. The hierarchical relationships among nodes in the same path depend on an INLINEFORM1 dimensional Dirichlet distribution that actually arranges the probabilities of topics being on different topic levels. Despite the fact that the single path was changed to multiple paths in some extensions of hLDA - the nested Chinese restaurant franchise processes BIBREF22 and the nested hierarchical Dirichlet Processes BIBREF23 , - this topic path drawing strategy puts words from different domains into one topic when input data are mixed with topics from multiple domains. This means that if a corpus contains documents in four different domains, hLDA is likely to include words from the four domains in every topic (see Figure FIGREF55 ). In light of the various inadequacies discussed above, we propose a relation-based model, hrLDA.", "The ultimate purposes of ACRP are to estimate INLINEFORM0 , the number of topics for rLDA, and to set the initial topic distribution states for rLDA. Suppose a document is read from top to bottom and left to right. As each noun phrase belongs to one sentence and one text chunk (e.g., section, paragraph and slide), the locations of all noun phrases in a document can be mapped to a two-dimensional space where sentence location is the x axis and text chunk location is the y axis (the first noun phrase of a document holds value (0, 0)). More specifically, every noun phrase has four attributes: content, location, one-to-many relation triplets, and document ID. Noun phrases in the same text chunk are more likely to be \u201cacquaintances;\" they are even closer to each other if they are in the same sentence. In contrast to CRP, ACRP assigns probabilities based on closeness, which is specified in the following procedure."]}
{"question_id": "468eb961215a554ace8088fa9097a7ad239f2d71", "predicted_answer": "", "predicted_evidence": ["A possible future work is to use a weighted combination of multiple metrics for source domain selection. These similarity metrics may be used to extract suitable data or features for efficient CDSA. Similarity metrics may also be used as features to predict the CDSA performance in terms of accuracy degradation.", "Ranking Accuracy (RA): It is the number of predicted source domains that are ranked correctly by the metric.", "To compare metrics, we use two parameters: Precision and Ranking Accuracy.", "Precision: It is the intersection between the top-K source domains predicted by the metric and top-K source domains as per CDSA accuracy, for a particular target domain. In other words, it is the number of true positives.", "For labelled data, we observe that LM2 (Symmetric KL-Divergence) and LM3 (Chameleon Words Similarity) perform better than other metrics. Interestingly, they also perform identically for K = 3 and K = 5 in terms of both precision percentage and NRA. We accredit this observation to the fact that both determine the distance between probabilistic distributions of polar words in domain-pairs."]}
{"question_id": "57d07d2b509c5860880583efe2ed4c5620a96747", "predicted_answer": "", "predicted_evidence": ["We show the results of the classifier's CDSA performance followed by metrics evaluation on the top 10 domains. Finally, we present an overall comparison of metrics for all the domains.", "For every common polar word between two domains, $L_1 \\ Distance$ between two vectors $[P_1,N_1]$ and $[P_2,N_2]$ is calculated as;", "where $P$ is the probability of a word appearing in a review which is labelled positive and $N$ is the probability of a word appearing in a review which is labelled negative.", "The overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs.", "Table TABREF33 shows results for different values of K in terms of precision percentage and normalized RA (NRA) over all domains. Normalized RA is RA scaled between 0 to 1. For example, entries 45.00 and 0.200 indicate that there is 45% precision with NRA of 0.200 for the top 3 source domains."]}
{"question_id": "d126d5d6b7cfaacd58494f1879547be9e91d1364", "predicted_answer": "", "predicted_evidence": ["In this paper, we investigate how text similarity-based metrics facilitate the selection of a suitable source domain for CDSA. Based on a dataset of reviews in 20 domains, our recommendation chart that shows the best source and target domain pairs for CDSA would be useful for deployments of sentiment classifiers for these domains.", "Training models for prediction of sentiment can cost one both valuable time and resources. The availability of pre-trained models is cost-effective in terms of both time and resources. One can always train new models and test for each source domain since labels are present for the source domain data. However, it is feasible only when trained classification models are available for all source domains. If pre-trained models are unavailable, training for each source domain can be highly intensive both in terms of time and resources. This makes it important to devise easy-to-compute metrics that use labelled data in the source and target domains.", "The overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs.", "The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1. To ensure that the datasets are balanced across all domains, we randomly select 5000 positive and 5000 negative reviews from each domain. The length of the reviews ranges from 5 words to 1654 words across all domains, with an average length ranging from 71 words to 125 words per domain. We point the reader to the original paper for detailed dataset statistics.", "For labelled data, we observe that LM2 (Symmetric KL-Divergence) and LM3 (Chameleon Words Similarity) perform better than other metrics. Interestingly, they also perform identically for K = 3 and K = 5 in terms of both precision percentage and NRA. We accredit this observation to the fact that both determine the distance between probabilistic distributions of polar words in domain-pairs."]}
{"question_id": "7dca806426058d59f4a9a4873e9219d65aea0987", "predicted_answer": "", "predicted_evidence": ["Table TABREF33 shows results for different values of K in terms of precision percentage and normalized RA (NRA) over all domains. Normalized RA is RA scaled between 0 to 1. For example, entries 45.00 and 0.200 indicate that there is 45% precision with NRA of 0.200 for the top 3 source domains.", "We show the results of the classifier's CDSA performance followed by metrics evaluation on the top 10 domains. Finally, we present an overall comparison of metrics for all the domains.", "Doc2Vec represents each sentence by a dense vector which is trained to predict words in the sentence, given the model. It tries to overcome the weaknesses of the bag-of-words model. Similar to Word2Vec, we train Doc2Vec models on each domain to extract sentence vectors. We train the models over 100 epochs for 100 dimensions, where the learning rate is 0.025. Since we can no longer leverage adjectives for sentiment, we use SentiWordnet for assigning sentiment scores (ranging from -1 to +1 where -1 denotes a negative sentiment, and +1 denotes a positive sentiment) to reviews (as detailed above) and select reviews which have a score above a certain threshold. We have empirically arrived at $\\pm 0.01$ as the threshold value. Any review with a score outside this window is selected. We also restrict the length of reviews to a maximum of 100 words to reduce sparsity.", "where $D_2+D_1$ indicates combined data obtained by mixing $D_1$ in $D_2$ and $\\Delta E$ indicates percentage change in entropy before and after mixing of source and target domains.", "We train monolingual word embeddings-based models for each domain using the FastText library. We train these models with 100 dimensions and 0.1 as the learning rate. The size of the context window is limited to 5 since FastText also uses sub-word information. Our model takes into account character n-grams from 3 to 6 characters, and we train our model over 5 epochs. We use the default loss function (softmax) for training."]}
{"question_id": "800fcd8b08d36c5276f9e5e1013208d41b46de59", "predicted_answer": "", "predicted_evidence": ["The difference in how people can express medication intake vs. how they express that they have not taken a medication can be rather subtle. For example, the expression I need Tylenol indicates that the person has not taken the medication yet (class 3), whereas the expression I need more Tylenol indicates that the person has taken the medication (class 1). In still other instances, the word more might not be the deciding factor in whether a medication was taken or not (e.g., more Tylenol didn't help). A useful avenue of future work is to explore the role function words play in determining the semantics of a sentence, specifically, when they imply medication intake, when they imply the lack of medication intake, and when they are not relevant to determining medication intake.", "Table TABREF42 shows the performance of our best system (submission 1) when one of the feature groups is removed. In this task, the general textual features (row b) played a bigger role in the overall performance than the domain-specific (row c) or sentiment lexicon (row d) features. Removing this group of features results in more than 2.5 percentage points drop in the F-measure affecting both precision and recall (row b). However, removing any one feature subgroup in this group (e.g., general INLINEFORM0 -grams, general clusters, general embeddings, etc.) results only in slight drop or even increase in the performance (rows b.1\u2013b.4). This indicates that the features in this group capture similar information. Among the domain-specific features, the INLINEFORM1 -grams generalized over domain terms are the most useful.", "4. took", "8. me <ADR>", ".2"]}
{"question_id": "cdbbba22e62bc9402aea74ac5960503f59e984ff", "predicted_answer": "", "predicted_evidence": ["17. <ADR> INLINEFORM0 and", "25. need <MED>", "23. this <MED>", "To investigate the impact of each feature group on the overall performance, we conduct ablation experiments where we repeat the same classification process but remove one feature group at a time. Table TABREF40 shows the results of these ablation experiments for our best system (submission 1). Comparing the two major groups of features, general textual features (row b) and domain-specific features (row c), we observe that they both have a substantial impact on the performance. Removing one of these groups leads to a two percentage points drop in INLINEFORM0 . The general textual features mostly affect recall of the ADR class (row b) while the domain-specific features impact precision (row c). Among the general textual features, the most influential feature is general-domain word embeddings (row b.2). Among the domain-specific features, INLINEFORM1 -grams generalized over domain terms (row c.1) and domain word embeddings (row c.3) provide noticeable contribution to the overall performance.", "3. i"]}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "predicted_answer": "", "predicted_evidence": ["6. makes me", "25. need <MED>", "16. weight", "24. need a <MED>", ""]}
{"question_id": "f3673f6375f065014e8e4bb8c7adf54c1c7d7862", "predicted_answer": "", "predicted_evidence": ["2. withdraw", "18. kick", "The results for our three official submissions are presented in Table TABREF39 (rows c.1\u2013c.3). The best results in INLINEFORM0 were obtained with submission 1 (row c.1). The results for submission 2 are the lowest, with F-measure being 3.5 percentage points lower than the result for submission 1 (row c.2). The ensemble classifier (submission 3) shows a slightly worse performance than the best result. However, in the post-competition experiments, we found that larger ensembles (with 7\u201311 classifiers, each trained on a random sub-sample of the majority class to reduce class imbalance to 1:2) outperform our best single-classifier model by over one percentage point with INLINEFORM1 reaching up to INLINEFORM2 (row d). Our best submission is ranked first among the nine teams participated in this task (rows b.1\u2013b.3).", "21. rivaroxaban diary", "5. on steroids"]}
{"question_id": "0bd3bea892c34a3820e98c4a42cdeda03753146b", "predicted_answer": "", "predicted_evidence": ["14. <MED> INLINEFORM0 makes me", "Our strategy of handling class imbalance through class weights did not prove successful on the test set (even though it resulted in increase of one point in F-measure in the cross-validation experiments). The model trained with the default class weights of 1 for all classes performs 0.7 percentage points better than the model trained with the class weights selected in cross-validation (row e).", "The results for our three official submissions on Task 2 are presented in Table TABREF41 (rows c.1\u2013c.3). The best results in INLINEFORM0 are achieved with submission 1 (row c.1). The results for the other two submissions, submission 2 and submission 3, are quite similar to the results of submission 1 in both precision and recall (rows c.2\u2013c.3). Adding the features from the ADR lexicon and the Pronoun lexicon did not result in performance improvement on the test set. Our best system is ranked third among the nine teams participated in this task (rows b.1\u2013b.3).", "19. made", "18. headache"]}
{"question_id": "8cf5abf0126f19253930478b02f0839af28e4093", "predicted_answer": "", "predicted_evidence": ["Our submissions to the 2017 SMM4H Shared Tasks Workshop obtained the first and third ranks in Task1 and Task 2, respectively. In Task 1, the systems had to determine whether a given tweet mentions an adverse drug reaction. In Task 2, the goal was to label a given tweet with one of the three classes: personal medication intake, possible medication intake, or non-intake. For both tasks, we trained an SVM classifier leveraging a number of textual, sentiment, and domain-specific features. Our post-competition experiments demonstrate that the most influential features in our system for Task 1 were general-domain word embeddings, domain-specific word embeddings, and INLINEFORM0 -grams generalized over domain terms. Moreover, under-sampling the majority class (non-ADR) to reduce class imbalance to 1:2 proved crucial to the success of our submission.", "INLINEFORM0", "9. <MED> INLINEFORM0 <ADR>", "18. headache", "17. <ADR> INLINEFORM0 and"]}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "predicted_answer": "", "predicted_evidence": ["INLINEFORM0", ".2", "22. kick in", "Error analysis on our best submission showed that there were 395 false negative errors (tweets that report ADRs, but classified as non-ADR) and 582 false positives (non-ADR tweets classified as ADR). Most of the false negatives were due to the creative ways in which people express themselves (e.g., i have metformin tummy today :-( ). Large amounts of labeled training data or the use of semi-supervised techniques to take advantage of large unlabeled domain corpora may help improve the detection of ADRs in such tweets. False positives were caused mostly due to the confusion between ADRs and other relations between a medication and a symptom. Tweets may mention both a medication and a symptom, but the symptom may not be an ADR. The medication may have an unexpected positive effect (e.g., reversal of hair loss), or may alleviate an existing health condition. Sometimes, the relation between the medication and the symptom is not explicitly mentioned in a tweet, yet an ADR can be inferred by humans.", "the total score = INLINEFORM0 ;"]}
{"question_id": "c3ce95658eea1e62193570955f105839de3d7e2d", "predicted_answer": "", "predicted_evidence": ["To better understand the improvement brought by augmentation data, we conduct a human evaluation of the output summaries before and after data augmentation. We sample 30 output summaries of the DUC 2006 dataset for analysis. And we find that the model augmented by the WikiRef dataset produces more query-related summaries on 23 examples. Meanwhile,the extracted sentences are usually less redundant. We attribute these benefits to the improved coverage and query-focused extraction brought by the large-scale augmentation data.", "To maintain the highest standards possible, most statements in Wikipedia are attributed to reliable, published sources that can be accessed through hyperlinks. In the first step, we parse the English Wikipedia database dump into plain text and save statements with citations. If a statement is attributed multiple citations, only the first citation is used. We also limit the sources of the citations to four types, namely web pages, newspaper articles, press and press release. A statement may contain more than one sentence.", "Include evidence, theories and speculation.", "We show an example in Figure FIGREF8 to illustrate the raw data collection process. The associated query, summary and the document are highlighted in colors in the diagram. At last, we have collected more than 2,000,000 English examples in total after the raw data collection step.", "The first narrative is usually a title and followed by several natural language questions or narratives."]}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "predicted_answer": "", "predicted_evidence": ["The improvement introduced by using the WikiRef dataset as augmentation data is traceable. At first, the document in the DUC datasets are news articles and we crawl newspaper webpages as one source of the WikiRef documents. Secondly, queries in the WikiRef dataset are hierarchical that specify the aspects it focuses on gradually. This is similar to the DUC datasets that queries are composed of several narratives to specify the desired information. The key difference is that queries in the WikiRef dataset are composed of key words, while the ones in the DUC datasets are mostly natural language. At last, we construct the WikiRef dataset to be a large-scale query-focused summarization dataset that contains more than 280,000 examples. In comparison, the DUC datasets contain only 145 clusters with around 10,000 documents. Therefore, query relevance and sentence context can be better modeled using data-driven neural methods with WikiRef. And it provides a better starting point for fine-tuning on the DUC datasets.", "EgyptAir Flight 990?", "In sentence regression, extractive summarization is achieved via sentence scoring and sentence selection. The former scores $\\textrm {r}(s_i\\vert \\mathcal {Q},\\mathcal {D})$ a sentence $s_i$ by considering its relevance to the query $\\mathcal {Q}$ and its salience to the document $\\mathcal {D}$. The latter generates a summary by ranking sentences under certain constraints, e.g., the number of sentences and the length of the summary.", "The output layer is used to score sentences for extractive query-focused summarization. Given $\\mathbf {h}_i^L\\in \\mathbb {R}^d$ is the vector representation for the i-th sentence. When the extracive summarization is carried out through sentence classification , the output layer is a linear layer followed by a sigmoid function: P(siQ,D)=sigmoid(WchiL+bc) where $\\mathbf {W}_c$ and $\\mathbf {b}_c$ are trainable parameters. The output is the probability of including the i-th sentence in the summary.", "The training objective of sentence regression is to minimize the mean square error between the estimated score and the oracle score: L=1nin (r(siQ,D) - f(siS*))2 where $\\mathcal {S}^*$ is the oracle summary and $\\textrm {f}(s_i\\vert \\mathcal {S}^*)$ is the oracle score of the i-th sentence."]}
{"question_id": "2c4db4398ecff7e4c1c335a2cb3864bfdc31df1a", "predicted_answer": "", "predicted_evidence": ["By contrast, abstractive approaches generate human-readable summaries that primarily capture the semantics of input documents and contain rephrased key content. The former task falls under the classification paradigm, and the latter belongs to the generative modeling paradigm, and therefore, it is a much harder problem to solve. The backbone of state-of-the-art summarization models is a typical encoder-decoder BIBREF10 architecture that has proved to be effective for various sequential modeling tasks such as machine translation, sentiment analysis, and natural language generation. It contains an encoder that maps the raw input word vector representations to a latent vector. Then, the decoder usually equipped with a variant of the attention mechanism BIBREF11 uses the latent vectors to generate the output sequence, which is the summary in our case. These models are trained in a supervised learning setting where we minimize the cross-entropy loss between the predicted and the target summary. Encoder-decoder models have proved effective for short sequence tasks such as machine translation where the length of a sequence is less than 120 tokens.", "Though encoder-decoder models gave a state-of-the-art performance for Neural Machine Translation (NMT), the maximum sequence length used in NMT is just 100 tokens. Typical document lengths in text summarization vary from 400 to 800 tokens, and LSTM is not effective due to the loss in memory over time for very long sequences. BIBREF7 used hierarchical attentionBIBREF16 to mitigate this effect where, a word LSTM is used to encode (decode) words, and a sentence LSTM is used to encode (decode) sentences. The use of two LSTMs separately for words and sentences improves the ability of the model to retain its memory for longer sequences. Additionally, BIBREF7 explored using a hierarchical model consisting of a feature-rich encoder incorporating position, Named Entity Recognition (NER) tag, Term Frequency (TF) and Inverse Document Frequency (IDF) scores. Since an RNN is a sequential model, computing at one time-step needs all of the previous time-steps to have computed before and is slow because the computation at all the time steps cannot be performed in parallel.", "Where, $x_{t} \\in \\mathbb {R}^D$ is the raw embedding vector at the current time-step. $f_{r}^{LSTM}$ , $f_{c}^{MLP}$ (Multi-Layer Perceptron), $f_{w}^{LSTM}$ be the read, compose and write operations respectively. $e_{l} \\in R^{l}$ , $e_{k} \\in R^{k}$ are vectors of ones, $\\mathbf {1}$ is a matrix of ones and $\\otimes $ is the outer product.", "When there are a very large number of documents that need to be read in limited time, we often resort to reading summaries instead of the whole document. Automatically generating (abstractive) summaries is a problem with various applications, e.g., automatic authoring BIBREF0. We have developed automatic text summarization systems that condense large documents into short and readable summaries. It can be used for both single (e.g., BIBREF1, BIBREF2 and BIBREF3) and multi-document summarization (e.g.,BIBREF4, BIBREF3, BIBREF5).", "Let $D = \\lbrace (w_{ij})_{j=1}^{T_{in}}\\rbrace _{i=1}^{S_{in}}$ be the input document sequence, where $S_{in}$ is the number of sentences in a document and $T_{in}$ is the number of words per sentence. Let $\\lbrace M_{i}\\rbrace _{i=1}^{S_{in}}, M_{i} \\in R^{T_{in} \\times D}$ be the sentence memories that encode all the words in a sentence and $M^{d}, M^{d} \\in R^{S_{in} \\times D}$ be the document memory that encodes all the sentences present in the document. At each time-step, an input token $x_{t}$ is read and is used to retrieve aligned content from both corresponding sentence memory $M_{t}^{i, s}$ and document memory $M_{t}^{d}$."]}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "predicted_answer": "", "predicted_evidence": ["Humans first form an abstractive representation of what they want to say and then try to put it into words while communicating. Though it seems intuitive that there is a hierarchy from sentence representation to words, as observed by both BIBREF7 and BIBREF15, these hierarchical attention models failed to outperform a simple attention model BIBREF1. Unlike feedforward networks, RNNs are expected to capture the input sequence order. But strangely, positional embeddings are found to be effective (BIBREF7, BIBREF8, BIBREF15 and BIBREF3). We explored a few approaches to solve these issues and improve the performance of neural models for abstractive summarization.", "Although the vanilla NSE described above performed well for machine translation, just a dot-product attention mechanism is too simplistic for text summarization. In machine translation, it is sufficient to compute the correlation between word-vectors from the semantic spaces of different languages. In contrast, text summarization also needs a word-sentence and sentence-sentence correlation along with the word-word correlation. So, in search of an attention mechanism with a better capacity to model the complex semantic relationships inherent in text summarization, we found that the additive attention mechanism BIBREF11 given by the equation below performs well.", "As discussed earlier, training in a supervised learning setting creates a mismatch between training and testing objectives. Also, feeding the ground-truth labels in training time-step creates an exposure bias while testing in which we feed the predictions from the previous time-step. Policy gradient methods overcome this by directly optimizing the non-differentiable metrics such as ROUGE BIBREF12 and METEOR BIBREF19. It can be posed as a Markov Decision Process in which the set of actions $\\mathcal {A}$ is the vocabulary and reward $\\mathcal {R}$ is the ROUGE score itself. So, we should find a policy $\\pi (\\theta )$ such that the set of sampled words $\\tilde{y} = \\lbrace \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{T}\\rbrace $ achieves highest ROUGE score among all possible summaries.", "But both works BIBREF7 and BIBREF8 found positional embeddings to be quite useful for reasons unknown. BIBREF3 proposed an extractive summarization model that classifies sentences based on content, saliency, novelty, and position. To deal with out-of-vocabulary (OOV) words and to facilitate copying salient information from input sequence to the output, BIBREF2 proposed a pointer-generator network that combines pointing BIBREF17 with generation from vocabulary using a soft-switch. Attention models for longer sequences tend to be repetitive due to the decoder repeatedly attending to the same position from the encoder. To mitigate this issue, BIBREF2 used a coverage mechanism to penalize a decoder from attending to same locations of an encoder. However, the pointer generator and the coverage model BIBREF2 are still highly extractive; copying the whole article sentences 35% of the time. BIBREF18 introduced an intra-attention model in which attention also depends on the predictions from previous time steps.", "Later, BIBREF15 proposed a coarse-to-fine hierarchical attention model to select a salient sentence using sentence attention using REINFORCE BIBREF20 and feed it to the decoder. BIBREF6 used REINFORCE to rank sentences for extractive summarization. BIBREF4 proposed deep communicating agents that operate over small chunks of a document, which is learned using a self-critical BIBREF13 training approach consisting of intermediate rewards. BIBREF9 used a advantage actor-critic (A2C) method to extract sentences followed by a decoder to form abstractive summaries. Our model does not suffer from their limiting assumption that a summary sentence is an abstracted version of a single source sentence. BIBREF18 trained their intra-attention model using a self-critical policy gradient algorithm BIBREF13. Though an RL objective gives a high ROUGE score, the output summaries are not readable by humans. To mitigate this problem, BIBREF18 used a weighted sum of supervised learning loss and RL loss."]}
{"question_id": "4dadde7c61230553ef14065edd8c1c7e41b9c329", "predicted_answer": "", "predicted_evidence": ["Where, $v, W, U, b_{attn}$ are learnable parameters. One other important difference is the compose function: a Multi-layer Perceptron (MLP) is enough for machine translation as the sequences are short in length. However, text summarization consists of longer sequences that have sentence-to-sentence dependencies, and a history of previously composed words is necessary for overcoming repetition BIBREF1 and thereby maintaining novelty. A powerful function already at our disposal is the LSTM; we replaced the MLP with an LSTM, as shown below:", "Where, $p(\\tilde{y}_{t})=p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\\alpha $ corresponds to more exploration, and a lower $\\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation.", "Despite the metric's known drawbacks, text summarization models are evaluated using ROUGE BIBREF12, a discrete similarity score between predicted and target summaries based on 1-gram, 2-gram, and n-gram overlap. Cross-entropy loss would be a convenient objective on which to train the model since ROUGE is not differentiable, but doing so would create a mismatch between metrics used for training and evaluation. Though a particular summary scores well on ROUGE evaluation comparable to the target summary, it will be assigned lower probability by a supervised model. To tackle this problem, we have used a self-critic policy gradient method BIBREF13 to train the models directly using the ROUGE score as a reward. In this paper, we propose an architecture that addresses the issues discussed above.", "As discussed earlier, training in a supervised learning setting creates a mismatch between training and testing objectives. Also, feeding the ground-truth labels in training time-step creates an exposure bias while testing in which we feed the predictions from the previous time-step. Policy gradient methods overcome this by directly optimizing the non-differentiable metrics such as ROUGE BIBREF12 and METEOR BIBREF19. It can be posed as a Markov Decision Process in which the set of actions $\\mathcal {A}$ is the vocabulary and reward $\\mathcal {R}$ is the ROUGE score itself. So, we should find a policy $\\pi (\\theta )$ such that the set of sampled words $\\tilde{y} = \\lbrace \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{T}\\rbrace $ achieves highest ROUGE score among all possible summaries.", "Now, the alignment scores $z_{t}$ of the past memory $M_{t-1}$ are calculated using $o_{t}$ as the key with a simple dot-product attention mechanism shown in equation DISPLAY_FORM6. A weighted sum gives the retrieved input memory that is used in equation DISPLAY_FORM8 by a Multi-Layer Perceptron in composing new information. Equation DISPLAY_FORM9 uses an LSTM and projects the composed states into the internal space of memory $M_{t-1}$ to obtain the write states $h_{t}$. Finally, in equation DISPLAY_FORM10, the memory is updated by erasing the retrieved memory as per $z_{t}$ and writing as per the write vector $h_{t}$. This process is performed at each time-step throughout the input sequence. The encoded memories $\\lbrace M\\rbrace _{t=1}^{T}$ are similarly used by the decoder to obtain the write vectors $\\lbrace h\\rbrace _{t=1}^{T}$ that are eventually fed to projection and softmax layers to get the vocabulary distribution."]}
{"question_id": "014830892d93e3c01cb659ad31c90de4518d48f3", "predicted_answer": "", "predicted_evidence": ["where $\\alpha $ is a hyper-parameter, $t$ is the current iteration, $h$ is the history determines, the number of previous rewards are used to estimate. The denominator of the history factor is used to normalize the current greedy reward $ r(y^{g},y^*)$ with the mean greedy reward of previous $h$ iterations. The numerator of the history factor ensures the greedy reward has a similar magnitude with the mean sample reward of previous $h$ iterations.", "In multi-hop question generation, we consider a document list $L$ with $n_L$ documents, and an $m$-word answer $A$. Let the total number of words in all the documents $D_i \\in L$ combined be $N$. Let a document list $L$ contains a total of $K$ candidate sentences $CS=\\lbrace S_1, S_2, \\ldots , S_K\\rbrace $ and a set of supporting facts $SF$ such that $SF \\in CS$. The answer $A=\\lbrace w_{D_k^{a_1}} , w_{D_k^{a_2}}, \\ldots , w_{D_k^{a_m}} \\rbrace $ is an $m$-length text span in one of the documents $D_k \\in L$. Our task is to generate an $n_Q$-word question sequence $\\hat{Q}= \\lbrace y_1, y_2, \\ldots , y_{n_Q} \\rbrace $ whose answer is based on the supporting facts $SF$ in document list $L$.", "Multi-hop QG has real-world applications in several domains, such as education, chatbots, etc. The questions generated from the multi-hop approach will inspire critical thinking in students by encouraging them to reason over the relationship between multiple sentences to answer correctly. Specifically, solving these questions requires higher-order cognitive-skills (e.g., applying, analyzing). Therefore, forming challenging questions is crucial for evaluating a student\u2019s knowledge and stimulating self-learning. Similarly, in goal-oriented chatbots, multi-hop QG is an important skill for chatbots, e.g., in initiating conversations, asking and providing detailed information to the user by considering multiple sources of information. In contrast, in a single-hop QG, only single source of information is considered while generation.", "In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search. We initialize the model parameters randomly using a Gaussian distribution with Xavier scheme BIBREF36. We first pre-train the network by minimizing only the maximum likelihood (ML) loss. Next, we initialize our model with the pre-trained ML weights and train the network with the mixed-objective learning function. The following values of hyperparameters are found to be optimal: (i) $\\gamma _1=0.99$, $\\gamma _2=0.01$, $\\gamma _3=0.1$, (ii) $d_1=300$, $d_2=d_3=3$, (iii) $\\alpha =0.9, \\beta = 10$, $h=5000$.", "In this paper, we propose to tackle Multi-hop QG problem in two stages. In the first stage, we learn supporting facts aware encoder representation to predict the supporting facts from the documents by jointly training with question generation and subsequently enforcing the utilization of these supporting facts. The former is achieved by sharing the encoder weights with an answer-aware supporting facts prediction network, trained jointly in a multi-task learning framework. The latter objective is formulated as a question-aware supporting facts prediction reward, which is optimized alongside supervised sequence loss. Additionally, we observe that multi-task framework offers substantial improvements in the performance of question generation and also avoid the inclusion of noisy sentences information in generated question, and reinforcement learning (RL) brings the complete and complex question to otherwise maximum likelihood estimation (MLE) optimized QG model."]}
{"question_id": "ae7c5cf9c2c121097eb00d389cfd7cc2a5a7d577", "predicted_answer": "", "predicted_evidence": ["We also measure the multi-hopping in terms of SF coverage and reported the results in Table TABREF26 and Table TABREF27. We achieve skyline performance of $80.41$ F1 value on the ground-truth questions of the test dataset of HotPotQA.", "For each generated question, we compute the F1 score (as a reward) between the ground truth supporting facts and the predicted supporting facts. This reward is supposed to be carefully used because the QG model can cheat by greedily copying words from the supporting facts to the generated question. In this case, even though high MER is achieved, the model loses the question generation ability. To handle this situation, we regularize this reward function with additional Rouge-L reward, which avoids the process of greedily copying words from the supporting facts by ensuring the content matching between the ground truth and generated question. We also experiment with BLEU as an additional reward, but Rouge-L as a reward has shown to outperform the BLEU reward function.", "To analyze the contribution of each component of the proposed model, we perform an ablation study reported in Table TABREF27. Our results suggest that providing multitask learning with shared encoder helps the model to improve the QG performance from $19.55$ to $20.64$ BLEU-4. Introducing the supporting facts information obtained from the answer-aware supporting fact prediction task further improves the QG performance from $20.64$ to $21.28$ BLEU-4. Joint training of QG with the supporting facts prediction provides stronger supervision for identifying and utilizing the supporting facts information. In other words, by sharing the document encoder between both the tasks, the network encodes better representation (supporting facts aware) of the input document. Such presentation is capable of efficiently filtering out the irrelevant information when processing multiple documents and performing multi-hop reasoning for question generation.", "The attention distribution $\\alpha _t$ and context vector $c_t$ are obtained using the following equations:", "In multi-hop question answering, one has to reason over multiple relevant sentences from different paragraphs to answer a given question. We refer to these relevant sentences as supporting facts in the context. Hence, we frame Multi-hop question generation as the task of generating the question conditioned on the information gathered from reasoning over all the supporting facts across multiple paragraphs/documents. Since this task requires assembling and summarizing information from multiple relevant documents in contrast to a single sentence/paragraph, therefore, it is more challenging than the existing single-hop QG task. Further, the presence of irrelevant information makes it difficult to capture the supporting facts required for question generation. The explicit information about the supporting facts in the document is not often readily available, which makes the task more complex. In this work, we provide an alternative to get the supporting facts information from the document with the help of multi-task learning. Table TABREF1 gives sample examples from SQuAD and HotPotQA dataset. It is cleared from the example that the single-hop question is formed by focusing on a single sentence/document and answer, while in multi-hop question, multiple supporting facts from different documents and answer are accumulated to form the question."]}
{"question_id": "af948ea91136c700957b438d927f58d9b051c97c", "predicted_answer": "", "predicted_evidence": ["Human Evaluation: For human evaluation, we directly compare the performance of the proposed approach with NQG model. We randomly sample 100 document-question-answer triplets from the test set and ask four professional English speakers to evaluate them. We consider three modalities: naturalness, which indicates the grammar and fluency; difficulty, which measures the document-question syntactic divergence and the reasoning needed to answer the question, and SF coverage similar to the metric discussed in Section SECREF4 except we replace the supporting facts prediction network with a human evaluator and we measure the relative supporting facts coverage compared to the ground-truth supporting facts. measure the relative coverage of supporting facts in the questions with respect to the ground-truth supporting facts. SF coverage provides a measure of the extent of supporting facts used for question generation. For the first two modalities, evaluators are asked to rate the performance of the question generator on a 1\u20135 scale (5 for the best).", "In multi-hop question generation, we consider a document list $L$ with $n_L$ documents, and an $m$-word answer $A$. Let the total number of words in all the documents $D_i \\in L$ combined be $N$. Let a document list $L$ contains a total of $K$ candidate sentences $CS=\\lbrace S_1, S_2, \\ldots , S_K\\rbrace $ and a set of supporting facts $SF$ such that $SF \\in CS$. The answer $A=\\lbrace w_{D_k^{a_1}} , w_{D_k^{a_2}}, \\ldots , w_{D_k^{a_m}} \\rbrace $ is an $m$-length text span in one of the documents $D_k \\in L$.", "We use a mixed-objective learning function BIBREF32, BIBREF33 to train the final network:", "where $\\alpha $ is a hyper-parameter, $t$ is the current iteration, $h$ is the history determines, the number of previous rewards are used to estimate. The denominator of the history factor is used to normalize the current greedy reward $ r(y^{g},y^*)$ with the mean greedy reward of previous $h$ iterations. The numerator of the history factor ensures the greedy reward has a similar magnitude with the mean sample reward of previous $h$ iterations.", "where, $\\mathbf {W_a}$ and $\\mathbf {W_b}$ are the weight matrices and $\\sigma $ represents the Sigmoid function. The probability distribution over the words in the document is computed by summing over all the attention scores of the corresponding words:"]}
{"question_id": "a913aa14d4e05cc9d658bf6697fe5b2652589b1b", "predicted_answer": "", "predicted_evidence": ["mtl models that use auxiliary tasks (d-mtl-aux) consistently outperform the single-task models (s-s) in all datasets, both for constituency parsing and for dependency parsing in terms of uas. However, this does not extend to las. This different behavior between uas and las seems to be originated by the fact that 2-task dependency parsing models, which are the basis for the corresponding auxiliary task and mtl models, improve uas but not las with respect to single-task dependency parsing models. The reason might be that the single-task setup excludes unlikely combinations of dependency labels with PoS tags or dependency directions that are not found in the training set, while in the 2-task setup, both components are treated separately, which may be having a negative influence on dependency labeling accuracy.", "Constituency BIBREF0 and dependency grammars BIBREF1 , BIBREF2 are the two main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities BIBREF3 . While in constituency parsing the structure of sentences is abstracted as a phrase-structure tree (see Figure FIGREF6 ), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure FIGREF6 ).", "Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. klein2003fast considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal parses. With a similar aim, ren2013combine first compute the n best constituency trees using a probabilistic context-free grammar, convert those into dependency trees using a dependency model, compute a probability score for each of them, and finally rerank the most plausible trees based on both scores. However, these methods are complex and intended for statistical parsers. Instead, we propose a extremely simple framework to learn across constituency and dependency representations.", "We have described a framework to leverage the complementary nature of constituency and dependency parsing. It combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations. We have shown that mtl models with auxiliary losses outperform single-task models, and mtl models that treat both constituency and dependency parsing as main tasks obtain strong results, coming almost at no cost in terms of speed. Source code will be released upon acceptance.", "Table TABREF18 compares single-paradigm models against their double-paradigm mtl versions. On average, mtl models with auxiliary losses achieve the best performance for both parsing abstractions. They gain INLINEFORM0 F1 points on average in comparison with the single model for constituency parsing, and INLINEFORM1 uas and INLINEFORM2 las points for dependency parsing. In comparison to the single-paradigm MTL models, the average gain is smaller: 0.05 f1 points for constituency parsing, and 0.09 uas and 0.21 las points for dependency parsing."]}
{"question_id": "b065a3f598560fdeba447f0a100dd6c963586268", "predicted_answer": "", "predicted_evidence": ["For constituency parsing, instead of predicting a single label output of the form INLINEFORM0 , we generate three partial and separate labels INLINEFORM1 , INLINEFORM2 and INLINEFORM3 through three task-dependent feed-forward networks on the top of the stacked bilstms. This is similar to BIBREF28 . For dependency parsing, we propose in this work a mtl version too. We observed in preliminary experiments, as shown in Table TABREF14 , that casting the problem as 3-task learning led to worse results. Instead, we cast it as a 2-task learning problem, where the first task consists in predicting the head of a word INLINEFORM4 , i.e. predicting the tuple INLINEFORM5 , and the second task predicts the type of the relation INLINEFORM6 . The loss is here computed as INLINEFORM7 = INLINEFORM8 , where INLINEFORM9 is the partial loss coming from the subtask INLINEFORM10 .", "The models were trained up to 150 iterations and optimized with Stochastic Gradient Descent (SGD) with a batch size of 8. The best model for constituency parsing was chosen with the highest achieved F1 score on the development set during the training and for dependency parsing with the highest las score. The best double paradigm, multi-task model was chosen based on the highest harmonic mean among las and F1 scores.", "All tasks are learned as main tasks instead.", "Table TABREF30 shows model hyperparameters.", "We have described a framework to leverage the complementary nature of constituency and dependency parsing. It combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations. We have shown that mtl models with auxiliary losses outperform single-task models, and mtl models that treat both constituency and dependency parsing as main tasks obtain strong results, coming almost at no cost in terms of speed. Source code will be released upon acceptance."]}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "predicted_answer": "", "predicted_evidence": ["When it comes to developing natural language processing (nlp) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .", "Constituency BIBREF0 and dependency grammars BIBREF1 , BIBREF2 are the two main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities BIBREF3 . While in constituency parsing the structure of sentences is abstracted as a phrase-structure tree (see Figure FIGREF6 ), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure FIGREF6 ).", "To learn across representations we cast the problem as multi-task learning. mtl enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation BIBREF12 , BIBREF22 . In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked bilstms shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task's outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks BIBREF23 , BIBREF24 , BIBREF25 , where tasks are learned together with the main task in the mtl setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to better generalization of the model. For instance, BIBREF26 have shown that semantic parsing benefits from that approach.", "Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. klein2003fast considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal parses. With a similar aim, ren2013combine first compute the n best constituency trees using a probabilistic context-free grammar, convert those into dependency trees using a dependency model, compute a probability score for each of them, and finally rerank the most plausible trees based on both scores. However, these methods are complex and intended for statistical parsers. Instead, we propose a extremely simple framework to learn across constituency and dependency representations.", "We have described a framework to leverage the complementary nature of constituency and dependency parsing. It combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations. We have shown that mtl models with auxiliary losses outperform single-task models, and mtl models that treat both constituency and dependency parsing as main tasks obtain strong results, coming almost at no cost in terms of speed. Source code will be released upon acceptance."]}
{"question_id": "179bc57b7b5231ea6ad3e93993a6935dda679fa2", "predicted_answer": "", "predicted_evidence": ["Proposition 2 INLINEFORM0 for all INLINEFORM1 .", "Because INLINEFORM0 for all INLINEFORM1 , this expression is equal to INLINEFORM2", "We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).", "Assume that INLINEFORM0 for all INLINEFORM1 . Then, INLINEFORM2", "(according to Equation EQREF5 )."]}
{"question_id": "a59e86a15405c8a11890db072b99fda3173e5ab2", "predicted_answer": "", "predicted_evidence": ["We use five most popular metrics,", "LEA BIBREF13 .", "In Figure FIGREF29 (a) the baseline, but not INLINEFORM0 nor INLINEFORM1 , mistakenly links INLINEFORM2 [it] with INLINEFORM3 [the virus]. Under-clustering, on the other hand, is a problem for our resolvers with INLINEFORM4 : in example (b), INLINEFORM5 missed INLINEFORM6 [We]. This behaviour results in a reduced recall but the recall is not damaged severely, as we still obtain a better INLINEFORM7 score. We conjecture that this behaviour is a consequence of using the INLINEFORM8 score in the objective, and, if undesirable, F INLINEFORM9 with INLINEFORM10 can be used instead. For instance, also in Figure FIGREF29 , INLINEFORM11 correctly detects INLINEFORM12 [it] as non-anaphoric and links INLINEFORM13 [We] with INLINEFORM14 [our].", "We firstly compare our resolvers against P15-1137 and N16-1114. Results are shown in the first half of Table TABREF25 . Our baseline surpasses P15-1137. It is likely due to using features from N16-1114. Using the entity-centric heuristic cross entropy loss and the relaxations are clearly beneficial: INLINEFORM0 is slightly better than our baseline and on par with the global model of N16-1114. INLINEFORM1 outperform the baseline, the global model of N16-1114, and INLINEFORM2 . However, the best values of INLINEFORM3 are INLINEFORM4 , INLINEFORM5 respectively for INLINEFORM6 , and INLINEFORM7 . Among these resolvers, INLINEFORM8 achieves the highest F INLINEFORM9 scores across all the metrics except BLANC.", "where INLINEFORM0 are model parameters, INLINEFORM1 is the set of the indices of correct antecedents of INLINEFORM2 , and INLINEFORM3 . INLINEFORM4 is a cost function used to manipulate the contribution of different error types to the loss function: INLINEFORM5"]}
{"question_id": "9489b0ecb643c1fc95c001c65d4e9771315989aa", "predicted_answer": "", "predicted_evidence": ["baseline: the resolver presented in Section SECREF2 . We use the identical configuration as in N16-1114: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 (where INLINEFORM3 are respectively the numbers of mention features and pair-wise features). We also employ their pretraining methodology.", "We prove this proposition by induction.", "Proposition 2 INLINEFORM0 for all INLINEFORM1 .", "We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).", "Using differentiable relaxations of evaluation metrics as in our work is related to a line of research in reinforcement learning where a non-differentiable action-value function is replaced by a differentiable critic BIBREF26 , BIBREF27 . The critic is trained so that it is as close to the true action-value function as possible. This technique is applied to machine translation BIBREF28 where evaluation metrics (e.g., BLUE) are non-differentiable. A disadvantage of using critics is that there is no guarantee that the critic converges to the true evaluation metric given finite training data. In contrast, our differentiable relaxations do not need to train, and the convergence is guaranteed as INLINEFORM0 ."]}
{"question_id": "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654", "predicted_answer": "", "predicted_evidence": ["Even though we use this setting for our main experiments, the optimum setting is likely to be language- and corpus-dependent. For Turkish, experiments show that freezing target attention parameters as well gives slightly better results. For parent-child models with closely related languages we expect freezing, or strongly regularizing, more components of the model to give better results.", "We also use the NMT model with transfer learning to re-score output $n$ -best lists ( $n = 1000$ ) from the SBMT system. Table 3 shows the results of re-scoring. Transfer NMT models give the highest gains over using re-scoring with a neural language model or an NMT system that does not use transfer. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 . Additionally, it is trained using dropout with a dropout probability of 0.2 as in BIBREF19 . From re-scoring with the transfer model, we get an improvement of 1.1\u20131.6 Bleu points above the strong SBMT system.", "We analyze the effects of using different parent models, regularizing different parts of the child model and trying different regularization techniques.", "We ran a number of additional experiments to understand what components of our final transfer model significantly contribute to the overall result. Section \"Analysis\" details these experiments.", "Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section \"Conclusion\" ."]}
{"question_id": "00341a46a67d31d36e6dc54d5297626319584891", "predicted_answer": "", "predicted_evidence": ["Overall, we can see that the choice of parent language can make a difference in the Bleu score, so in our Hausa, Turkish, Uzbek, and Urdu experiments, a parent language more optimal than French might improve results.", "This work was carried out with funding from DARPA (HR0011-15-C-0115) and ARL/ARO (W911NF-10-1-0533).", "Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section \"Conclusion\" .", "In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to see what yields optimal performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to allow more components of the child NMT model to be trained and see the effect on performance in the model. We see that the optimal setting for transferring from French-English to Uzbek-English in terms of Bleu performance is to allow all of the components of the child model to be trained except for the input and output target embeddings.", "Transfer learning uses knowledge from a learned task to improve the performance on a related task, typically reducing the amount of required training data BIBREF9 , BIBREF10 . In natural language processing, transfer learning methods have been successfully applied to speech recognition, document classification and sentiment analysis BIBREF11 . Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning BIBREF12 . For example, cirecsan2012transfer use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters. Ours is the first study to apply transfer learning to neural machine translation."]}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "predicted_answer": "", "predicted_evidence": ["Even though we use this setting for our main experiments, the optimum setting is likely to be language- and corpus-dependent. For Turkish, experiments show that freezing target attention parameters as well gives slightly better results. For parent-child models with closely related languages we expect freezing, or strongly regularizing, more components of the model to give better results.", "In Figure 3 we plot learning curves for both a transfer and a non-transfer model on training and development sets. We see that the final training set perplexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better.", "We start by a brief description of our NMT model in Section \"NMT Background\" . Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem.", "We analyze the effects of using different parent models, regularizing different parts of the child model and trying different regularization techniques.", "The results of this experiment are shown in Table 6 . We get a 4.3 Bleu improvement with an unrelated parent (i.e. French-parent and Uzbek-child), but we get a 6.7 Bleu improvement with a `closely related' parent (i.e. French-parent and French'-child). We conclude that the choice of parent model can have a strong impact on transfer models, and choosing better parents for our low-resource languages (if data for such parents can be obtained) could improve the final results."]}
{"question_id": "17fd6deb9e10707f9d1b70165dedb045e1889aac", "predicted_answer": "", "predicted_evidence": ["2. Rank existing query structures. To find an appropriate query structure for the input question, we rank existing query structures ( INLINEFORM0 ) by using a scoring function, see Section SECREF20 .", "1. Predict query substructures. We first predict the probability that INLINEFORM0 for each INLINEFORM1 , using the query substructure predictors trained in the offline step. An example question and four query substructures with highest prediction probabilities are shown in the top of Figure FIGREF12 .", "Merge query substructures This setting ignored existing query structures in the training data, and only considered the merged results of query substructures.", "3. Train query substructure predictors. We train a neural network for each query substructure INLINEFORM0 , to predict the probability that INLINEFORM1 has INLINEFORM2 (i.e., INLINEFORM3 ) for input question INLINEFORM4 , where INLINEFORM5 denotes the formal query for INLINEFORM6 . Details for this step are described in Section SECREF13 .", "An example for merging two query substructures is shown in Figure FIGREF26 ."]}
{"question_id": "c4a3f270e942803dab9b40e5e871a2e8886ce444", "predicted_answer": "", "predicted_evidence": ["Template-based approaches transform the input question into a formal query by employing pre-collected query templates. BIBREF1 ( BIBREF1 ) collect different natural language expressions for the same query intention from question-answer pairs. BIBREF3 ( BIBREF3 ) re-implement and evaluate the query generation module in NLIWOD, which selects an existing template by some simple features such as the number of entities and relations in the input question. Recently, several query decomposition methods are studied to enlarge the coverage of the templates. BIBREF5 ( BIBREF5 ) present a KBQA system named QUINT, which collects query templates for specific dependency structures from question-answer pairs. Furthermore, it rewrites the dependency parsing results for questions with conjunctions, and then performs sub-question answering and answer stitching. BIBREF15 ( BIBREF15 ) decompose questions by using a huge number of triple-level templates extracted by distant supervision.", "gAnswer BIBREF10 , BIBREF16 builds up semantic query graph for question analysis and utilize subgraph matching for disambiguation. Recent studies combine parsing based approaches with neural networks, to enhance the ability for structure disambiguation. BIBREF0 ( BIBREF0 ), BIBREF2 ( BIBREF2 ) and BIBREF4 ( BIBREF4 ) build query graphs by staged query generation, and follow an encode-and-compare framework to rank candidate queries with neural networks. These approaches try to learn entire representations for questions with different query structures by using a single network. Thus, they may suffer from the lack of training data, especially for questions with rarely appeared structures. By contrast, our approach utilizes multiple networks to learn predictors for different query substructures, which can gain a stable performance with limited training data. Also, our approach does not require manually-written rules, and performs stably with noisy linking results.", "Query structure errors (71%) occurred due to multiple reasons. Firstly, 21% of error cases have entity mentions that are not correctly detected before query substructure prediction, which highly influenced the prediction result. Secondly, in 39% of the cases a part of substructure predictors provided wrong prediction, which led to wrong structure ranking results. Finally, in the remaining 11% of the cases the correct query structure did not appear in the training data, and they cannot be generated by merging substructures.", "Semantic parsing-based approaches translate questions into formal queries using bottom up parsing BIBREF11 or staged query graph generation BIBREF14 . gAnswer BIBREF10 , BIBREF16 builds up semantic query graph for question analysis and utilize subgraph matching for disambiguation. Recent studies combine parsing based approaches with neural networks, to enhance the ability for structure disambiguation. BIBREF0 ( BIBREF0 ), BIBREF2 ( BIBREF2 ) and BIBREF4 ( BIBREF4 ) build query graphs by staged query generation, and follow an encode-and-compare framework to rank candidate queries with neural networks. These approaches try to learn entire representations for questions with different query structures by using a single network. Thus, they may suffer from the lack of training data, especially for questions with rarely appeared structures. By contrast, our approach utilizes multiple networks to learn predictors for different query substructures, which can gain a stable performance with limited training data.", "Assume that INLINEFORM0 , INLINEFORM1 , we have INLINEFORM2 . Thus, INLINEFORM3 should be 1 in the ideal condition. On the other hand, INLINEFORM4 , INLINEFORM5 should be 0. Thus, we have INLINEFORM6 , and INLINEFORM7 , we have INLINEFORM8 ."]}
{"question_id": "1faccdc78bbd99320c160ac386012720a0552119", "predicted_answer": "", "predicted_evidence": ["In future work, we plan to add support for other complex questions whose queries require Union, Group By, or numerical comparison. Also, we are interested in mining natural language expressions for each query substructures, which may help current parsing approaches.", "This work is supported by the National Natural Science Foundation of China (Nos. 61772264 and 61872172). We would like to thank Yao Zhao for his help in preparing evaluation.", "Both SINA and NLIWOD did not employ a query ranking mechanism, i.e., their accuracy and coverage are limited by the rules and templates. Although both CompQA and SQG have a strong ability of generating candidate queries, they perform not quite well in query ranking. According to our observation, the main reason is that these approaches tried to learn entire representations for questions with different query structures (from simple to complex) using a single network, thus, they may suffer from the lack of training data, especially for the questions with rarely appeared structures. As a contrast, our approach leveraged multiple networks to learn predictors for different query substructures, and ranked query structures using combinational function, which gained a better performance.", "We compared SubQG with several existing approaches. SINA BIBREF13 and NLIWOD conduct query generation by predefined rules and existing templates. SQG BIBREF4 firstly generates candidate queries by finding valid walks containing all of entities and properties mentioned in questions, and then ranks them based on Tree-LSTM similarity. CompQA BIBREF2 is a KBQA system which achieved state-of-the-art performance on WebQuesions and ComplexQuestions over Freebase. We re-implemented its query generation component for DBpedia, which generates candidate queries by staged query generation, and ranks them using an encode-and-compare network.", "We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple."]}
{"question_id": "804466848f4fa1c552f0d971dce226cd18b9edda", "predicted_answer": "", "predicted_evidence": ["Alongside with entity and relation linking, existing KBQA systems often leverage formal query generation for complex question answering BIBREF0 , BIBREF8 . Based on our investigation, the query generation approaches can be roughly divided into two kinds: template-based and semantic parsing-based.", "We tested the performance of SubQG with different sizes of training data. The results on LC-QuAD dataset are shown in Figure FIGREF44 . With more training data, our query substructure based approaches obtained stable improvements on both precision and recall. Although the merging module impaired the overall precision a little bit, it shows a bigger improvement on recall, especially when there is very few training data. Generally speaking, equipped with the merging module, our substructure based query generation approach showed the best performance.", "Template-based approaches transform the input question into a formal query by employing pre-collected query templates. BIBREF1 ( BIBREF1 ) collect different natural language expressions for the same query intention from question-answer pairs. BIBREF3 ( BIBREF3 ) re-implement and evaluate the query generation module in NLIWOD, which selects an existing template by some simple features such as the number of entities and relations in the input question. Recently, several query decomposition methods are studied to enlarge the coverage of the templates. BIBREF5 ( BIBREF5 ) present a KBQA system named QUINT, which collects query templates for specific dependency structures from question-answer pairs. Furthermore, it rewrites the dependency parsing results for questions with conjunctions, and then performs sub-question answering and answer stitching. BIBREF15 ( BIBREF15 ) decompose questions by using a huge number of triple-level templates extracted by distant supervision. Compared with these approaches, our approach predicts all kinds of query substructures (usually 1 to 4 triples) contained in the question, making full use of the training data.", "Merge query substructures This setting ignored existing query structures in the training data, and only considered the merged results of query substructures.", "Assume that INLINEFORM0 , INLINEFORM1 , we have INLINEFORM2 . Thus, INLINEFORM3 should be 1 in the ideal condition. On the other hand, INLINEFORM4 , INLINEFORM5 should be 0. Thus, we have INLINEFORM6 , and INLINEFORM7 , we have INLINEFORM8 ."]}
{"question_id": "8d683d2e1f46626ceab60ee4ab833b50b346c29e", "predicted_answer": "", "predicted_evidence": ["For example, although the query structures for the two questions in Figures FIGREF1 and FIGREF9 are different, they share the same query substructure INLINEFORM0 INLINEFORM1 INLINEFORM2 , which corresponds to the phrase \u201chow many movies\". Note that, a query substructure can be the query structure of another question.", "A formal query (or simply called query) is the structured representation of a natural language question executable on a given KB. Formally, a query is a pair INLINEFORM0 , where INLINEFORM1 denotes the set of vertices, and INLINEFORM2 denotes the set of labeled edges. A vertex can be either a variable, an entity or a literal, and the label of an edge can be either a built-in property or a user-defined one. For simplicity, the set of all edge labels are denoted by INLINEFORM3 . In this paper, the built-in properties include Count, Avg, Max, Min, MaxAtN, MinAtN and IsA (rdf:type), where the former four are used to connect two variables. For example, INLINEFORM4 represents that INLINEFORM5 is the counting result of INLINEFORM6 . MaxAtN and MinAtN take the meaning of Order By in SPARQL BIBREF0 . For instance, INLINEFORM7 means Order By Desc INLINEFORM8 Limit 1 Offset 1.", "The output of the network is a probability DISPLAYFORM0", "Query structure errors (71%) occurred due to multiple reasons. Firstly, 21% of error cases have entity mentions that are not correctly detected before query substructure prediction, which highly influenced the prediction result. Secondly, in 39% of the cases a part of substructure predictors provided wrong prediction, which led to wrong structure ranking results. Finally, in the remaining 11% of the cases the correct query structure did not appear in the training data, and they cannot be generated by merging substructures.", "Online. The online query generation process takes as input a natural language question INLINEFORM0 , and mainly contains four steps:"]}
{"question_id": "5ae005917efc17a505ba1ba5e996c4266d6c74b6", "predicted_answer": "", "predicted_evidence": ["We are working on a better test set for word embeddings which would include many more relations over a larger vocabulary especially semantics relations. We want to extend the test set with Czech and perhaps other languages, to see what word embeddings can bring to languages morphologically richer than English.", "comparative: If the adjective ends with y, replace it with ier. If it ends with e, add r. Otherwise add er at the end.", "We test our model on the original test set BIBREF7 . The test set consists of 19544 \u201cquestions\u201d, of which 8869 are called \u201csemantic\u201d and 10675 are called \u201csyntactic\u201d and further divided into 14 types, see Table TABREF4 . Each question contains two pairs of words ( INLINEFORM0 ) and captures relations like \u201cWhat is to `woman' ( INLINEFORM1 ) as `king' ( INLINEFORM2 ) is to `man' ( INLINEFORM3 )?\u201d, together with the expected answer `queen' ( INLINEFORM4 ). The model is evaluated by finding the word whose representation is the nearest (cosine similarity) to the vector INLINEFORM5 . If the nearest neighbor is INLINEFORM6 , we consider the question answered correctly.", "past-tense: Remove ing and add ed at the end of the verb.", "An useful feature of our model is the ability to generate vector embeddings even for unseen words. This could be exploited by NNs also in different tasks."]}
{"question_id": "72c04eb3fc323c720f7f8da75c70f09a35abf3e6", "predicted_answer": "", "predicted_evidence": ["Traditionally, every input word of an NN is stored in the \u201cone-hot\u201d representation, where the vector has only one element set to one and the rest of the vector are zeros. The size of the vector equals to the size of the vocabulary. The NN is trained to perform some prediction, e.g. to predict surrounding words given a word of interest. Instead of using this prediction capacity in some task, the practice is to extract the output of NN's hidden layer of each word (called distributed representation) and directly use this deterministic mapping INLINEFORM0 of word forms to the vectors of real numbers as the word representation.", "The Skip-gram model defined in BIBREF7 is trained to predict context words of the input word. Given a corpus INLINEFORM0 of words INLINEFORM1 and their context words INLINEFORM2 (i.e. individual words INLINEFORM3 appearing close the original word INLINEFORM4 ), it considers the conditional probabilities INLINEFORM5 . The training finds the parameters INLINEFORM6 of INLINEFORM7 to maximize the corpus probability: DISPLAYFORM0", "Vector representations of words learned using neural networks (NN) have proven helpful in many algorithms for image annotation BIBREF0 or BIBREF1 , language modeling BIBREF2 , BIBREF3 and BIBREF4 or other natural language processing (NLP) tasks BIBREF5 or BIBREF6 .", "The learned distributed representation of words relies on much shorter vectors (e.g. vocabularies containing millions words are represented in vectors of a few hundred elements) and semantic or syntactic information is often found to be implicitly present (\u201cembedded\u201d) in the vector space. For example, the Euclidean distance between two words in the vector space may be related to semantic or syntactic similarity between them.", "We limit the vocabulary, requiring each word form to appear at least 10 times in the corpus and each substring to appear at least 500 times in the corpus. This way, we get the 141K unique words mentioned above and 170K unique substrings (+141K words, as we downsample words separately)."]}
{"question_id": "0715d510359eb4c851cf063c8b3a0c61b8a8edc0", "predicted_answer": "", "predicted_evidence": ["In this paper, we propose the task of summarizing legal documents in plain English and present an initial evaluation dataset for this task. We gather our dataset from online sources dedicated to explaining sections of contracts in plain English and manually verify the quality of the summaries. We show that our dataset is highly abstractive and that the summaries are much simpler to read. This task is challenging, as popular unsupervised extractive summarization methods do not perform well on this dataset and, as discussed in section SECREF6 , current methods that address the change in register are mostly supervised as well. We call for the development of resources for unsupervised simplification and style transfer in this domain.", "TL;DRLegal focuses mostly on software licenses, however, we only scraped documents related to specific companies rather than generic licenses (i.e. Creative Commons, etc). The scraped data consists of 84 sets sourced from 9 documents: Pokemon GO Terms of Service, TLDRLegal Terms of Service, Minecraft End User Licence Agreement, YouTube Terms of Service, Android SDK License Agreement (June 2014), Google Play Game Services (May 15th, 2013), Facebook Terms of Service (Statement of Rights and Responsibilities), Dropbox Terms of Service, and Apple Website Terms of Service.", "To verify that the summaries more accessible to a wider audience, we also compare the readability of the reference summaries and the original texts.", "Hence, there is a INLINEFORM0 6-year reading level distinction between the two sets of words, an indication that lexical difficulty is paramount in legal text.", "We ran each measurement on the reference summaries and original texts. As shown in Table TABREF23 , the reference summaries scored lower than the original texts for each test by an average of 6 years."]}
{"question_id": "4e106b03cc2f54373e73d5922e97f7e5e9bf03e4", "predicted_answer": "", "predicted_evidence": ["Of the 361 accepted summaries, more than two-thirds of them (152) are `templatic' summaries. A summary deemed templatic if it could be found in more than one summary set, either word-for-word or with just the service name changed. However, of the 152 templatic summaries which were selected as the best of their set, there were 111 unique summaries. This indicates that the templatic summaries which were selected for the final dataset are relatively unique.", "Automated readability index (ARI): the weighted sum of the number of characters per word and number of words per sentence BIBREF31 .", "The list of words with the highest log odds ratios for the reference summaries ( INLINEFORM0 ) and original texts ( INLINEFORM1 ) can be found in Table TABREF25 .", "The dataset we present summarizes contracts in plain English. While there is no precise definition of plain English, the general philosophy is to make a text readily accessible for as many English speakers as possible. BIBREF19 , BIBREF20 . Guidelines for plain English often suggest a preference for words with Saxon etymologies rather than a Latin/Romance etymologies, the use of short words, sentences, and paragraphs, etc. BIBREF20 , BIBREF21 . In this respect, the proposed task involves some level of text simplification, as we will discuss in Section SECREF16 . However, existing resources for text simplification target literacy/reading levels BIBREF22 or learners of English as a second language BIBREF23 . Additionally, these models are trained using Wikipedia or news articles, which are quite different from legal documents.", "Though these metrics were originally formulated based on US grade levels, we have adjusted the numbers to provide the equivalent age correlated with the respective US grade level."]}
{"question_id": "f8edc911f9e16559506f3f4a6bda74cde5301a9a", "predicted_answer": "", "predicted_evidence": ["In this work, we have used two variants of this original model. The first one (called HMM model in the remainder of this paper), following the analysis led in BIBREF8 , approximates the Dirichlet Process prior by a mere symmetric Dirichlet prior. This approximation, while retaining the sparsity constraint, avoids the complication of dealing with the variational treatment of the stick breaking process frequent in Bayesian non-parametric models. The second variant, which we shall denote Structured Variational AutoEncoder (SVAE) AUD, is based upon the work of BIBREF4 and embeds the HMM model into the Variational AutoEncoder framework BIBREF9 . A very similar version of the SVAE for AUD was developed independently and presented in BIBREF5 . The main noteworthy difference between BIBREF5 and our model is that we consider a fully Bayesian version of the HMM embedded in the VAE; and the posterior distribution and the VAE parameters are trained jointly using the Stochastic Variational Bayes BIBREF4 , BIBREF10 .", "Broadening ASR technologies to ideally all possible languages is a challenge with very high stakes in many areas and is at the heart of several fundamental research problems ranging from psycholinguistic (how humans learn to recognize speech) to pure machine learning (how to extract knowledge from unlabeled data). The present work focuses on the narrow but important problem of unsupervised Acoustic Unit Discovery (AUD). It takes place as the continuation of an ongoing effort to develop a Bayesian model suitable for this task, which stems from the seminal work of BIBREF0 later refined and made scalable in BIBREF1 . This model, while rather crude, has shown that it can provide a clustering accurate enough to be used in topic identification of spoken document in unknown languages BIBREF2 . It was also shown that this model can be further improved by incorporating a Bayesian \"phonotactic\" language model learned jointly with the acoustic units BIBREF3 . Finally, following the work in BIBREF4 it has been combined successfully with variational auto-encoders leading to a model combining the potential of both deep neural networks and Bayesian models BIBREF5 .", "These results are contradictory to those reported in BIBREF3 . Two factors may explain this discrepancy: the Mboshi5k data being different from the training data of the MBN neural network, the neural network may not generalize well. Another possibility may be that the initialization scheme of the model is not suitable for this type of features. Indeed, Variational Bayesian Inference algorithm converges only to a local optimum of the objective function and is therefore dependent of the initialization. We believe the second explanation is the more likely since, as we shall see shortly, the best results in term of word segmentation and NMI are eventually obtained with the MBN features when the inference is done with the informative prior. Next, we compared the HMM and the SVAE models when trained with an uninformative prior (lines with \"Inf. Prior\" set to \"no\" in Table TABREF23 ). The SVAE significantly improves the NMI and the precision showing that it extracts more consistent units than the HMM model.", "We have conducted an analysis of the state-of-the-art Bayesian approach for acoustic unit discovery on a real case of low-resource language. This analysis was focused on the quality of the discovered units compared to the gold standard phone alignments. Outcomes of the analysis are i) the combination of neural network and Bayesian model (SVAE) yields a significant improvement in the AUD in term of consistency ii) Bayesian models can naturally embed information from a resourceful language and consequently improve the consistency of the discovered units. Finally, we hope this work can serve as a baseline for future research on unsupervised acoustic unit discovery in very low resource scenarios.", "We then evaluated the effect of the informative prior on the acoustic unit discovery (Table TABREF23 ). On all 4 combinations (2 features sets INLINEFORM0 2 models) we observe an improvement in terms of precision and NMI but a degradation of the recall. This result is encouraging since the informative prior was trained on English data (TIMIT) which is very different from Mboshi. Indeed, this suggests that even speech from an unrelated language can be of some help in the design of an ASR for a very low resource language. Finally, similarly to the SVAE/HMM case described above, we found that the degradation of the recall is due to longer units discovered for models with an informative prior (numbers omitted due to lack of space)."]}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "predicted_answer": "", "predicted_evidence": ["The AUD model described in BIBREF0 , BIBREF1 is a non-parametric Bayesian Hidden Markov Model (HMM). This model is topologically equivalent to a phone-loop model with two major differences:", "In this work, we have used two variants of this original model. The first one (called HMM model in the remainder of this paper), following the analysis led in BIBREF8 , approximates the Dirichlet Process prior by a mere symmetric Dirichlet prior. This approximation, while retaining the sparsity constraint, avoids the complication of dealing with the variational treatment of the stick breaking process frequent in Bayesian non-parametric models. The second variant, which we shall denote Structured Variational AutoEncoder (SVAE) AUD, is based upon the work of BIBREF4 and embeds the HMM model into the Variational AutoEncoder framework BIBREF9 . A very similar version of the SVAE for AUD was developed independently and presented in BIBREF5 .", "The present work focuses on the narrow but important problem of unsupervised Acoustic Unit Discovery (AUD). It takes place as the continuation of an ongoing effort to develop a Bayesian model suitable for this task, which stems from the seminal work of BIBREF0 later refined and made scalable in BIBREF1 . This model, while rather crude, has shown that it can provide a clustering accurate enough to be used in topic identification of spoken document in unknown languages BIBREF2 . It was also shown that this model can be further improved by incorporating a Bayesian \"phonotactic\" language model learned jointly with the acoustic units BIBREF3 . Finally, following the work in BIBREF4 it has been combined successfully with variational auto-encoders leading to a model combining the potential of both deep neural networks and Bayesian models BIBREF5 . The contribution of this work is threefold:", "This approximation, while retaining the sparsity constraint, avoids the complication of dealing with the variational treatment of the stick breaking process frequent in Bayesian non-parametric models. The second variant, which we shall denote Structured Variational AutoEncoder (SVAE) AUD, is based upon the work of BIBREF4 and embeds the HMM model into the Variational AutoEncoder framework BIBREF9 . A very similar version of the SVAE for AUD was developed independently and presented in BIBREF5 . The main noteworthy difference between BIBREF5 and our model is that we consider a fully Bayesian version of the HMM embedded in the VAE; and the posterior distribution and the VAE parameters are trained jointly using the Stochastic Variational Bayes BIBREF4 , BIBREF10 . For both variants, the prior over the HMM parameters were set to the conjugate of the likelihood density: Normal-Gamma prior for the mean and variance of the Gaussian components, symmetric Dirichlet prior over the HMM's state mixture's weights and symmetric Dirichlet prior over the acoustic units' weights.", "We have conducted an analysis of the state-of-the-art Bayesian approach for acoustic unit discovery on a real case of low-resource language. This analysis was focused on the quality of the discovered units compared to the gold standard phone alignments. Outcomes of the analysis are i) the combination of neural network and Bayesian model (SVAE) yields a significant improvement in the AUD in term of consistency ii) Bayesian models can naturally embed information from a resourceful language and consequently improve the consistency of the discovered units. Finally, we hope this work can serve as a baseline for future research on unsupervised acoustic unit discovery in very low resource scenarios."]}
{"question_id": "a464052fd11af1d2d99e407c11791269533d43d1", "predicted_answer": "", "predicted_evidence": ["The parameters are divided into two subgroups INLINEFORM0 where INLINEFORM1 are the global parameters of the model, and INLINEFORM2 are the latent variables which, in our case, correspond to the sequences of acoustic units. The global parameters are separated into two independent subsets : INLINEFORM3 , corresponding to the acoustic parameters ( INLINEFORM4 ) and the \"phonotactic\" language model parameters ( INLINEFORM5 ). Replacing INLINEFORM6 and following the conditional independence of the variable induced by the model (see BIBREF1 for details) leads to: DISPLAYFORM0", "Note that when the model is trained with an uninformative prior the loss function is the as in Eq. EQREF13 but with INLINEFORM0 instead of the INLINEFORM1 . For the case of the uninformative prior, the Variational Bayes Inference was initialized as described in BIBREF1 . In the informative prior case, we initialized the algorithm by setting INLINEFORM2 .", "Out of nearly 7000 languages spoken worldwide, current speech (ASR, TTS, voice search, etc.) technologies barely address 200 of them. Broadening ASR technologies to ideally all possible languages is a challenge with very high stakes in many areas and is at the heart of several fundamental research problems ranging from psycholinguistic (how humans learn to recognize speech) to pure machine learning (how to extract knowledge from unlabeled data). The present work focuses on the narrow but important problem of unsupervised Acoustic Unit Discovery (AUD). It takes place as the continuation of an ongoing effort to develop a Bayesian model suitable for this task, which stems from the seminal work of BIBREF0 later refined and made scalable in BIBREF1 . This model, while rather crude, has shown that it can provide a clustering accurate enough to be used in topic identification of spoken document in unknown languages BIBREF2 .", "To evaluate our work we measured how the discovered units compared to the forced aligned phones in term of segmentation and information. The accuracy of the segmentation was measured in term of Precision, Recall and F-score. If a unit boundary occurs at the same time (+/- 10ms) of an actual phone boundary it is considered as a true positive, otherwise it is considered to be a false positive. If no match is found with a true phone boundary, this is considered to be a false negative. The consistency of the units was evaluated in term of normalized mutual information (NMI - see BIBREF1 , BIBREF3 , BIBREF5 for details) which measures the statistical dependency between the units and the forced aligned phones. A NMI of 0 % means that the units are completely independent of the phones whereas a NMI of 100 % indicates that the actual phones could be retrieved without error given the sequence of discovered units.", "We also wanted to experiment using lattices as an input for the word segmentation task, instead of using single sequences of units, so as to better mitigate the uncertainty of the AUD task and provide a companion metric that would be more robust to noise. A model capable of performing word segmentation both on lattices and text sequences was introduced by BIBREF6 . Building on the work of BIBREF17 , BIBREF18 they combine a nested hierarchical Pitman-Yor language model with a Weighted Finite State Transducer approach. Both for lattices and acoustic units sequences, we use the implementation of the authors with a bigram language model and a unigram character model. Word discovery is evaluated using the Boundary metric from the Zero Resource Challenge 2017 BIBREF20 and BIBREF21 . This metric measures the quality of a word segmentation and the discovered boundaries with respect to a gold corpus (Precision, Recall and F-score are computed)."]}
{"question_id": "5f6c1513cbda9ae711bc38df08fe72e3d3028af2", "predicted_answer": "", "predicted_evidence": ["We also wanted to experiment using lattices as an input for the word segmentation task, instead of using single sequences of units, so as to better mitigate the uncertainty of the AUD task and provide a companion metric that would be more robust to noise. A model capable of performing word segmentation both on lattices and text sequences was introduced by BIBREF6 . Building on the work of BIBREF17 , BIBREF18 they combine a nested hierarchical Pitman-Yor language model with a Weighted Finite State Transducer approach. Both for lattices and acoustic units sequences, we use the implementation of the authors with a bigram language model and a unigram character model. Word discovery is evaluated using the Boundary metric from the Zero Resource Challenge 2017 BIBREF20 and BIBREF21 . This metric measures the quality of a word segmentation and the discovered boundaries with respect to a gold corpus (Precision, Recall and F-score are computed).", "Indeed, Variational Bayesian Inference algorithm converges only to a local optimum of the objective function and is therefore dependent of the initialization. We believe the second explanation is the more likely since, as we shall see shortly, the best results in term of word segmentation and NMI are eventually obtained with the MBN features when the inference is done with the informative prior. Next, we compared the HMM and the SVAE models when trained with an uninformative prior (lines with \"Inf. Prior\" set to \"no\" in Table TABREF23 ). The SVAE significantly improves the NMI and the precision showing that it extracts more consistent units than the HMM model. However, it also degrades the segmentation in terms of recall. We further investigated this behavior by looking at the duration of the units found by both models compared to the true phones (Table TABREF22 ). We observe that the SVAE model favors longer units than the HMM model hence leading to fewer boundaries and consequently smaller recall.", "Another possibility may be that the initialization scheme of the model is not suitable for this type of features. Indeed, Variational Bayesian Inference algorithm converges only to a local optimum of the objective function and is therefore dependent of the initialization. We believe the second explanation is the more likely since, as we shall see shortly, the best results in term of word segmentation and NMI are eventually obtained with the MBN features when the inference is done with the informative prior. Next, we compared the HMM and the SVAE models when trained with an uninformative prior (lines with \"Inf. Prior\" set to \"no\" in Table TABREF23 ). The SVAE significantly improves the NMI and the precision showing that it extracts more consistent units than the HMM model. However, it also degrades the segmentation in terms of recall. We further investigated this behavior by looking at the duration of the units found by both models compared to the true phones (Table TABREF22 ).", "The parameters are divided into two subgroups INLINEFORM0 where INLINEFORM1 are the global parameters of the model, and INLINEFORM2 are the latent variables which, in our case, correspond to the sequences of acoustic units. The global parameters are separated into two independent subsets : INLINEFORM3 , corresponding to the acoustic parameters ( INLINEFORM4 ) and the \"phonotactic\" language model parameters ( INLINEFORM5 ). Replacing INLINEFORM6 and following the conditional independence of the variable induced by the model (see BIBREF1 for details) leads to: DISPLAYFORM0", "If we further assume that we have at our disposal speech data in a different language than the target one, denoted INLINEFORM0 , along with its phonetic transcription INLINEFORM1 , it is then straightforward to show that: DISPLAYFORM0"]}
{"question_id": "130d73400698e2b3c6860b07f2e957e3ff022d48", "predicted_answer": "", "predicted_evidence": ["We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching. If only frequency were considered, a more gender-neutral name (e.g. Taylor) could be paired with a very gender-specific name (e.g. John), which would negate the gender intervention in many cases (namely whenever a male occurrence of Taylor is transformed into John, which would also result in incorrect pronouns, if present). If, on the other hand, only the degree of gender-specificity were considered, we would see frequent names (like James) being paired with far less frequent names (like Sybil), which would distort the overall frequency distribution of names.", "gCDS and nCDS are variants of the grammar and Names Intervention using CDS. WED40 is our reimplementation of BIBREF1's (BIBREF1) method, which (like the original) uses a single component to define the gender subspace, accounting for $>40\\%$ of variance. As this is much lower than in the original paper (where it was 60%, reproduced in Figure FIGREF18), we define a second space, WED70, which uses a 2D subspace accounting for $>70\\%$ of variance. To test whether WED profits from additional names, we use the 5000 paired names in the names gazetteer as additional equalise pairs (nWED70). As control, we also evaluate the unmitigated space (none).", "A fundamental limitation of all the methods compared is their reliance on predefined lists of gender words, in particular of pairs. BIBREF5's pairs of manager::manageress and murderer::murderess may be counterproductive, as their augmentation method perpetuates a male reading of manager, which has become gender-neutral over time. Other issues arise from differences in spelling (e.g. mum vs. mom) and morphology (e.g. his vs. her and hers). Biologically-rooted terms like breastfeed or uterus do not lend themselves to pairing either. The strict use of pairings also imposes a gender binary, and as a result non-binary identities are all but ignored in the bias mitigation literature. [author=rowan,color=green!40,size=,fancyline,caption=,]added this para back in and chopped it up a bit, look okay?", "The United States Social Security Administration (SSA) dataset contains a list of all first names from Social Security card applications for births in the United States after 1879, along with their gender. Figure FIGREF8 plots a few example names according to their male and female occurrences, and shows that names have varying degrees of gender-specificity.", "Suppose we embed our words into $\\mathbb {R}^d$. The fundamental assumption is that there exists a linear subspace $B \\subset \\mathbb {R}^d$ that contains (most of) the gender bias in the space of word embeddings. (Note that $B$ is a direction when it is a single vector.) We term this assumption the gender subspace hypothesis. Thus, by basic linear algebra, we may decompose any word vector $\\mathbf {v}\\in \\mathbb {R}^d$ as the sum of the projections onto the bias subspace and its complement: $\\mathbf {v}= \\mathbf {v}_{B} + \\mathbf {v}_{\\perp B}$. The (implicit) operationalisation of gender bias under this hypothesis is, then, the magnitiude of the bias vector $||\\mathbf {v}_{B}||_2$."]}
{"question_id": "7e9aec2bdf4256c6249cad9887c168d395b35270", "predicted_answer": "", "predicted_evidence": ["Proposition 1 The neutralise step of DBLP:conf/nips/BolukbasiCZSK16 yields a unit vector. Specifically, DBLP:conf/nips/BolukbasiCZSK16 define", "Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia). nWED70's V-Measure is significantly higher than either of the other Names variants (reduction of 11% on Gigaword, only 1% on Wikipedia), suggesting that the success of nCDS and nCDA is not merely due to their larger list of gender-words.", "Table TABREF35 reports the SimLex-999 Spearman rank-order correlation coefficients $r_s$ (all are significant, $p<0.01$). Surprisingly, the WED40 and 70 methods outperform the unmitigated embedding, although the difference in result is small (0.386 and 0.395 vs. 0.385 on Gigaword, 0.371 and 0.367 vs. 0.368 on Wikipedia). nWED70, on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on Gigaword, 0.367 vs. 0.368 on Wikipedia). CDA and CDS methods do not match the quality of the unmitigated space, but once again the difference is small. [author=simone,color=blue!40,size=,fancyline,caption=,]Second Part of Reaction to Reviewer 4.It should be noted that since SimLex-999 was produced by human raters, it will reflect the human biases these methods were designed to remove, so worse performance might result from successful bias mitigation.", "BIBREF1 find that this method results in a 68% reduction of stereotypical analogies as identified by human judges. However, bias is removed only insofar as the operationalisation allows. In a comprehensive analysis, hila show that the original structure of bias in the WED embedding space remains intact.", "The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset BIBREF17 provides a ground-truth measure of similarity produced by 500 native English speakers. Similarity scores in an embedding are computed as the cosine angle between word-vector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at $\\alpha = 0.01$."]}
{"question_id": "1acf06105f6c1930f869347ef88160f55cbf382b", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF13 shows a plot of various names' number of primary gender occurances against their secondary gender occurrences, with red dots for primary-male and blue crosses for primary-female names. The problem of finding name-pairs thus decomposes into a Euclidean-distance bipartite matching problem, which can be solved using the Hungarian method BIBREF7. We compute pairs for the most frequent 2500 names of each gender in the SSA dataset. There is also the problem that many names are also common nouns (e.g. Amber, Rose, or Mark), which we solve using Named Entity Recognition.", "The method uses three sets of words or word pairs: 10 definitional pairs (used to define the gender direction), 218 gender-specific seed words (expanded to a larger set using a linear classifier, the compliment of which is neutralised in the first step), and 52 equalise pairs (equalised in the second step). The relationships among these sets are illustrated in Figure FIGREF3; for instance, gender-neutral words are defined as all words in an embedding that are not gender-specific.", "BIBREF0 introduce the Word Embedding Association Test (WEAT), which provides results analogous to earlier psychological work by BIBREF12 by measuring the difference in relative similarity between two sets of target words $X$ and $Y$ and two sets of attribute words $A$ and $B$. We compute Cohen's $d$ (a measure of the difference in relative similarity of the word sets within each embedding; higher is more biased), and a one-sided $p$-value which indicates whether the bias detected by WEAT within each embedding is significant (the best outcome being that no such bias is detectable). We do this for three tests proposed by BIBREF13 which measure the strength of various gender stereotypes: art\u2013maths, arts\u2013sciences, and careers\u2013family.", "We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword. In our empirical comparison, we found that although both methods mitigate direct gender bias and maintain the interpretability of the space, WED failed to maintain a robust representation of gender (the best variants had an error rate of 23% average when drawing non-biased analogies, suggesting that too much gender information was removed). A new variant of CDA we propose (the Names Intervention) is the only to successfully mitigate indirect gender bias: following its application, previously biased words are significantly less clustered according to gender, with an average of 49% reduction in cluster purity when clustering the most biased words.", "The United States Social Security Administration (SSA) dataset contains a list of all first names from Social Security card applications for births in the United States after 1879, along with their gender. Figure FIGREF8 plots a few example names according to their male and female occurrences, and shows that names have varying degrees of gender-specificity."]}
{"question_id": "9ce90f4132b34a328fa49a63e897f376a3ad3ca8", "predicted_answer": "", "predicted_evidence": ["The grammar intervention, BIBREF5's improved intervention, uses coreference information to veto swapping gender words when they corefer to a proper noun. This avoids Elizabeth ...she ...queen being changed to, for instance, Elizabeth ...he ...king. It also uses POS information to avoid ungrammaticality related to the ambiguity of her between personal pronoun and possessive determiner. In the context, `her teacher was proud of her', this results in the correct sentence `his teacher was proud of him'.", "Table TABREF35 reports the SimLex-999 Spearman rank-order correlation coefficients $r_s$ (all are significant, $p<0.01$). Surprisingly, the WED40 and 70 methods outperform the unmitigated embedding, although the difference in result is small (0.386 and 0.395 vs. 0.385 on Gigaword, 0.371 and 0.367 vs. 0.368 on Wikipedia). nWED70, on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on Gigaword, 0.367 vs. 0.368 on Wikipedia). CDA and CDS methods do not match the quality of the unmitigated space, but once again the difference is small. [author=simone,color=blue!40,size=,fancyline,caption=,]Second Part of Reaction to Reviewer 4.It should be noted that since SimLex-999 was produced by human raters, it will reflect the human biases these methods were designed to remove, so worse performance might result from successful bias mitigation.", "On Wikipedia, nWED70 outperforms every other method ($p<0.01$), and even at $\\alpha =0.1$ bias was undetectable. In all CDA/S variants, the Names Intervention performs significantly better than other intervention strategies (average $d$ for nCDS across all tests 0.95 vs. 1.39 for the best non-names CDA/S variants). Excluding the Wikipedia careers\u2013family test (in which the CDA and CDS variants are indistinguishable at $\\alpha =0.01$), the CDS variants are numerically better than their CDA counterparts in 80% of the test cases, although many of these differences are not significant. Generally, we notice a trend of WED reducing direct gender bias slightly better than CDA/S. Impressively, WED even successfully reduces bias in the careers\u2013family test, where gender information is captured by names, which were not in WED's gender-equalise word-pair list for treatment.", "We note that nullifying indirect bias associations entirely is not necessarily the goal of debiasing, since some of these may result from causal links in the domain. For example, whilst associations between man and engineer and between man and car are each stereotypic (and thus could be considered examples of direct bias), an association between engineer and car might well have little to do with gender bias, and so should not be mitigated.", "We found the equations suggested in DBLP:conf/nips/BolukbasiCZSK16 on the opaque side of things. So we provide here proofs missing from the original work ourselves."]}
{"question_id": "3138f916e253abed643d3399aa8a4555b2bd8c0f", "predicted_answer": "", "predicted_evidence": ["Using this operalisation of gender bias, BIBREF1 go on to provide a linear-algebraic method (Word Embedding Debiasing, WED, originally \u201chard debiasing\u201d) to remove gender bias in two phases: first, for non-gendered words, the gender direction is removed (\u201cneutralised\u201d). Second, pairs of gendered words such as mother and father are made equidistant to all non-gendered words (\u201cequalised\u201d). Crucially, under the gender subspace hypothesis, it is only necessary to identify the subspace $B$ as it is possible to perfectly remove the bias under this operationalisation using tools from numerical linear algebra.", "The duplication of text which lies at the heart of CDA will produce debiased corpora with peculiar statistical properties unlike those of naturally occurring text. Almost all observed word frequencies will be even, with a notable jump from 2 directly to 0, and a type\u2013token ratio far lower than predicted by Heaps' Law for text of this length. The precise effect this will have on the resulting embedding space is hard to predict, but we assume that it is preferable not to violate the fundamental assumptions of the algorithms used to create embeddings. As such, we propose to apply substitutions probabilistically (with 0.5 probability), which results in a non-duplicated counterfactual training corpus, a method we call Counterfactual Data Substitution (CDS). Substitutions are performed on a per-document basis in order to maintain grammaticality and discourse coherence. This simple change should have advantages in terms of naturalness of text and processing efficiency, as well as theoretical foundation.", "Wikipedia is of particular interest, since though its Neutral Point of View (NPOV) policy predicates that all content should be presented without bias, women are nonetheless less likely to be deemed \u201cnotable\u201d than men of equal stature BIBREF9, and there are differences in the choice of language used to describe them BIBREF10, BIBREF11. We use the annotation native to the Annotated English Gigaword, and process Wikipedia with CoreNLP (statistical coreference; bidirectional tagger). Embeddings are created using Word2Vec. We use the original complex lexical input (gender-word pairs and the like) for each algorithm as we assume that this benefits each algorithm most. [author=simone,color=blue!40,size=,fancyline,caption=,]I am not 100% sure of which \"expansion\" you are talking about here. The classifier Bolucbasi use maybe?", "Second, we test whether a classifier can be trained to reclassify the gender of debiased words. If it succeeds, this would indicate that bias-information still remains in the embedding. We trained an RBF-kernel SVM classifier on a random sample of 1000 out of the 5000 most biased words from each corpus using $\\vec{b}_\\text{test}$ (500 from each gender), then report the classifier's accuracy when reclassifying the remaining 4000 words.", "Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia)."]}
{"question_id": "810e6d09813486a64e87ef6c1fb9b1e205871632", "predicted_answer": "", "predicted_evidence": ["We combine scores of E2E model $P_{s2s}$, CTC score $P_{ctc}$ and a RNN based language model $P_{rnn}$ in the decoding process, which is formulated as", "We also analyze the performance of different masking strategies, showing in Table TABREF20, where all models are shallow fused with the RNN language model. The SpecAugment provides 30$\\%$ relative gains on test-clean and other datasets. According to the comparison between the second line and the third line, we find that the word masking is more effective on test-other dataset. The last row indicates word mask is complementary to random mask on the time axis.", "We evaluate our model in different settings. The baseline Transformer represents the model with position embedding. The comparison between baseline Transformer and our architecture (Model with SpecAugment) indicates the improvements attributed to the architecture. Model with semantic mask is we use the semantic mask strategy on top of SpecAugment, which outperforms Model with SpecAugment with a large margin in a no external language model fusion setting, demonstrating that our masking strategy helps the E2E model to learn a better language model. The gap becomes smaller when equipped with a language model fusion component, which further confirms our motivation in Section SECREF1. Speed Perturbation does not help model performance on the clean dataset, but it is effective on the test-other dataset. Rescore is beneficial to both test-clean and test-other datasets.", "The RNN language model uses the released LSTM language model provided by ESPnet. The Transformer language model for rescoring is trained on LibriSpeech language model corpus with the GPT-2 base setting (308M parameters). We use the code of NVIDIA Megatron-LM to train the Transformer language model.", "Our masking approach requires the alignment information in order to perform the token-wise masking as shown in Figure FIGREF2. There are multiple speech recognition toolkits available to generate such kind of alignments. In this work, we used the Montreal Forced Alignertrained with the training data to perform forced-alignment between the acoustic signals and the transcriptions to obtain the word-level timing information. During model training, we randomly select a percentage of the tokens and mask the corresponding speech segments in each iteration. Following BIBREF4, in our work, we randomly sample 15% of the tokens and set the masked piece to the mean value of the whole utterance."]}
{"question_id": "ab8b0e6912a7ca22cf39afdac5531371cda66514", "predicted_answer": "", "predicted_evidence": ["We represent input signals as a sequence of 80-dim log-Mel filter bank with 3-dim pitch features BIBREF17. SentencePiece is employed as the tokenizer, and the vocabulary size is 5000. The hyper-parameters in Transformer and SpecAugment follow BIBREF6 for a fair comparison. We use Adam algorithm to update the model, and the warmup step is 25000. The learning rate decreases proportionally to the inverse square root of the step number after the 25000-th step. We train our model 100 epochs on 4 P40 GPUs, which approximately costs 5 days to coverage. We also apply speed perturbation by changing the audio speed to 0.9, 1.0 and 1.1. Following BIBREF6, we average the last 5 checkpoints as the final model. Unlike BIBREF14 and BIBREF15, we use the same checkpoint for test-clean and test-other dataset.", "Our masking approach requires the alignment information in order to perform the token-wise masking as shown in Figure FIGREF2. There are multiple speech recognition toolkits available to generate such kind of alignments. In this work, we used the Montreal Forced Alignertrained with the training data to perform forced-alignment between the acoustic signals and the transcriptions to obtain the word-level timing information. During model training, we randomly select a percentage of the tokens and mask the corresponding speech segments in each iteration. Following BIBREF4, in our work, we randomly sample 15% of the tokens and set the masked piece to the mean value of the whole utterance.", "As far as we know, our model is the best E2E ASR system on the Librispeech testset, which achieves a comparable result with wav2letter Transformer on test-clean dataset and a better result on test-other dataset, even though our model (75M parameters) is much smaller than the wav2letter Transformer (210M parameters). The reason might be that our semantic masking is more suitable on a noisy setting, because the input features are not reliable and the model has to predict the next token relying on previous ones and the whole context of the input. Our model is built upon the code base of ESPnet, and achieves relative $10\\%$ gains due to the better architecture and masking strategy. Comparing with hybrid methods, our model obtains a similar performance on the test-clean set, but is still worse than the best hybrid model on the test-other dataset.", "In this section, we describe our experiments on LibriSpeech BIBREF1 and TedLium2 BIBREF13. We compare our results with state-of-the-art hybrid and E2E systems. We implemented our approach based on ESPnet BIBREF6, and the specific settings on two datasets are the same with BIBREF6, except the decoding setting. We use the beam size 20, $\\beta _1 = 0.5$, and $\\beta _2=0.7$ in our experiment.", "This paper presents a semantic mask method for E2E speech recognition, which is able to train a model to better consider the whole audio context for the disambiguation. Moreover, we elaborate a new architecture for E2E model, achieving state-of-the-art performance on the Librispeech test set in the scope of E2E models."]}
{"question_id": "89373db8ced1fe420eae0093b2736f06b565616e", "predicted_answer": "", "predicted_evidence": ["On the Mall dataset, SVM is dominant in both accuracy and F $_1$ . It is followed by Logistic Regression, Na\u00efve Bayes and NuSVM. Maximum Entropy is near whereas MLP and Random Forest are again considerably weaker. Similar results are also reported in other works like BIBREF23 where again, SVM and Na\u00efve Bayes outrun Random Forest on text analysis tasks. From the top three algorithms, Na\u00efve Bayes was the fastest to train, followed by Logistic Regression. SVM was instead considerably slower. The 91.6 % of SVM in F $_1$ score on Mall dataset is considerably higher than the 78.1 % F $_1$ score reported in Table 7 of BIBREF7 . They used Na\u00efve Bayes with $ \\alpha = 0.005 $ and 5-fold cross-validation, same as we did here. Unfortunately, no other direct comparisons with similar studies are possible.", "We used the best performing vectorizer and classifier parameters to assess the classification performance of each algorithm in both datasets. The top grid-search accuracy, test accuracy and test macro F $_1$ scores are shown in Table 6 . For lower variance, the average of five measures is reported. The top scores on the two datasets differ a lot. That is because Facebook data classification is a multiclass discrimination problem (negative vs. neutral vs. positive), in contrast with Mall review analysis which is purely binary (negative vs. positive). As we can see, Logistic Regression and SVM are the top performers in Facebook data. NuSVM and Na\u00efve Bayes perform slightly worse. MLP and Random Forest, on the other hand, fall discretely behind. On the Mall dataset, SVM is dominant in both accuracy and F $_1$ . It is followed by Logistic Regression, Na\u00efve Bayes and NuSVM. Maximum Entropy is near whereas MLP and Random Forest are again considerably weaker.", "Random Forests (RF) were also invented in the 90s BIBREF21 , BIBREF22 . They average results of multiple decision trees (bagging) aiming for lower variance. Among the many parameters, we explored maxdepth which limits the depth of decision trees. We also grid-searched maxfeat, the maximal number of features to consider for best tree split. If sqrt is given, it will use the square root of total features. If None is given then it will use all features. Finally, nest dictates the number of trees (estimators) that will be used. Obviously, more trees may produce better results but they also increase the computation time. Logistic Regression is probably the most basic classifier that still provides reasonably good results for a wide variety of problems. It uses a logistic function to determine the probability of a value belonging to a class or not. C parameter represents the inverse of the regularization term and is important to prevent overfitting.", "The last parameter we tried is gamma that represents the kernel coefficient for rfb, poly and sigmoid (non linear) kernels. The other algorithm we tried is NuSVM which is very similar to SVM. The only difference is that a new parameter (nu) is utilized to control the number of support vectors. Random Forests (RF) were also invented in the 90s BIBREF21 , BIBREF22 . They average results of multiple decision trees (bagging) aiming for lower variance. Among the many parameters, we explored maxdepth which limits the depth of decision trees. We also grid-searched maxfeat, the maximal number of features to consider for best tree split. If sqrt is given, it will use the square root of total features. If None is given then it will use all features. Finally, nest dictates the number of trees (estimators) that will be used. Obviously, more trees may produce better results but they also increase the computation time.", "If None is given then it will use all features. Finally, nest dictates the number of trees (estimators) that will be used. Obviously, more trees may produce better results but they also increase the computation time. Logistic Regression is probably the most basic classifier that still provides reasonably good results for a wide variety of problems. It uses a logistic function to determine the probability of a value belonging to a class or not. C parameter represents the inverse of the regularization term and is important to prevent overfitting. We also explored the classweight parameter which sets weights to sample classes inversely proportional to class frequencies in the input data (for balanced). If None is given, all classes have the same weight. Finally, penalty parameter specifies the norm to use when computing the cost function. To have an idea about the performance of small and shallow neural networks on small datasets, we tried Multilayer Perceptron (MLP) classifier. It comes with a rich set of parameters such as alpha which is the regularization term, solver which is the weight optimization algorithm used during training or activation that is the function used in each neuron of hidden layers to determine its output value."]}
{"question_id": "74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "predicted_answer": "", "predicted_evidence": ["In the definition of the prefix and suffix, we also allow them to be an empty string. By doing so, we can add the first word of each phrase into the word recommendation set, since the suffix of INLINEFORM0 and the prefix of any target phrase always contain a match part INLINEFORM1 . The reason we add the first word of the phrase into recommendation set is that we hope our methods can still recommend some possible words when NMT has finished the translation of one phrase and begins to translate another new one, or predicts the first target word of the whole sentence.", "2) Our empirical experiments on Chinese-English translation and English-Japanese translation tasks show the efficacy of our methods. For Chinese-English translation, we can obtain an average improvement of 2.23 BLEU points. For English-Japanese translation, the improvement can reach 1.96 BLEU points. We further find that the phrase table is much more beneficial than bilingual lexicons to NMT.", "1) Which words are worthy to recommend at each decoding step?", "In Fig. 3, we show an illustrative example of CH-EN translation. In this example, our method is able to obtain a correct translation while the baseline is not. Specifically, baseline NMT system mistranslates \u201cjinkou dafu xiahua (the sharp decline in imports)\u201d into \u201cimport of imports\u201d, and incorrectly translates \u201cmaoyi shuncha (trade surplus)\u201d into \u201ctrade\u201d. But these two mistakes are fixed by our method, because there are two phrase translation pairs (\u201cjinkou dafu xiahua\u201d to \u201cthe sharp decline in imports\u201d and \u201cmaoyi shuncha\u201d to \u201ctrade surplus\u201d) in the phrase table, and the correct translations are obtained due to our recommendation method.", "Definition 1 (prefix of phrase): the prefix of a phrase is a word sequence which begins with the first word of the phrase and ends with any word of the phrase. Note that the prefix string can be empty. For a phrase INLINEFORM0 , this phrase contains four prefixes: INLINEFORM1 ."]}
{"question_id": "4b6745982aa64fbafe09f7c88c8d54d520b3f687", "predicted_answer": "", "predicted_evidence": ["Incorporating translation lexicons. BIBREF6 , BIBREF17 attempted to integrate NMT with the probabilistic translation lexicons. BIBREF16 moved forward further by incorporating a bilingual dictionaries in NMT.", "2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .", "The decoder generates one target word at a time by computing the probability of INLINEFORM0 as follows: DISPLAYFORM0", "Later, the researchers transfers to NMT framework. Specifically, coverage mechanism BIBREF9 , BIBREF10 , SMT features BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 and translation lexicons BIBREF6 , BIBREF16 , BIBREF17 have been fully explored. In contrast, phrase translation table, as the core of SMT, has not been fully studied. Recently, BIBREF18 and BIBREF19 explore the possibility of translating phrases in NMT. However, the \u201cphrase\u201d in their approaches are different from that used in phrase-based SMT. In BIBREF18 's models, the phrase pair must be a one-to-one mapping with a source phrase having a unique target phrase (named entity translation pairs). In BIBREF19 's models, the source side of a phrase pair must be a chunk. Therefore, it is still a big challenge to incorporate any phrase pair in the phrase table into NMT system to alleviate the unfaithfulness problem.", "A natural question arises that whether it is more beneficial to incorporate a phrase translation table than the translation lexicons. From Table 1, we can conclude that both translation lexicons and phrase translation table can improve NMT system's translation quality. In CH-EN translation, Arthur improves the baseline NMT system with 0.81 BLEU points, while our method improves the baseline NMT system with 2.23 BLEU points. In EN-JA translation, Arthur improves the baseline NMT system with 0.73 BLEU points, while our method improves the baseline NMT system with 1.96 BLEU points. Therefore, it is very obvious that phrase information is more effective than lexicon information when we use them to improve the NMT system."]}
{"question_id": "6656a9472499331f4eda45182ea697a4d63e943c", "predicted_answer": "", "predicted_evidence": ["Output: word recommendation set INLINEFORM0 [1] INLINEFORM1 Get all suffixes of INLINEFORM2 (denote each suffix by INLINEFORM3 ) Get all prefixes of each target phrase in candidate target phrase set (denote every prefix by INLINEFORM4 ) each suffix INLINEFORM5 and each prefix INLINEFORM6 INLINEFORM7 Add the next word of INLINEFORM8 into INLINEFORM9 INLINEFORM10", "Fig. 4 shows an illustrative example. In this example, baseline NMT mistranslates \u201cdianli (electricity) anquan (safe)\u201d into \u201ccoal\u201d. Arthur partially fixes this error and it can correctly translate \u201cdianli (electrical)\u201d into \u201celectrical\u201d, but the source word \u201canquan (safe)\u201d is still missed. Fortunately, this mistake is fixed by our proposed method. The reason behind this is that Arthur uses information from translation lexicons, which makes the system only fix the translation mistake of an individual lexicon (in this example, it is \u201cdianli (electrical)\u201d), while our method uses the information from phrases, which makes the system can not only obtain the correct translation of the individual lexicon but also capture local lexicon reordering and fixed collocation etc.", "In the definition of the prefix and suffix, we also allow them to be an empty string. By doing so, we can add the first word of each phrase into the word recommendation set, since the suffix of INLINEFORM0 and the prefix of any target phrase always contain a match part INLINEFORM1 . The reason we add the first word of the phrase into recommendation set is that we hope our methods can still recommend some possible words when NMT has finished the translation of one phrase and begins to translate another new one, or predicts the first target word of the whole sentence.", "Incorporating translation lexicons. BIBREF6 , BIBREF17 attempted to integrate NMT with the probabilistic translation lexicons. BIBREF16 moved forward further by incorporating a bilingual dictionaries in NMT.", "2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 ."]}
{"question_id": "430ad71a0fd715a038f3c0fe8d7510e9730fba23", "predicted_answer": "", "predicted_evidence": ["Now we already know which word is worthy to recommend. In order to facilitate the calculation of the bonus value (section 3.2), we also need to maintain the origin of each recommendation word. Here, the origin of a recommendation word contains two parts: 1) the phrase pair this word belongs to and 2) the phrase translation probability between the source and target phrases. Formally, for a recommendation word INLINEFORM0 , we can denote it by: DISPLAYFORM0", "3) Arthur: It is the state-of-the-art method which incorporates discrete translation lexicons into NMT model BIBREF6 . We choose automatically learned lexicons and bias method. We implement the method on the base of the baseline attention-based NMT system. Hyper parameter INLINEFORM0 is 0.001, the same as that reported in their work.", "In this section, we describe the experiments to evaluate our proposed methods.", "Table 1 reports the detailed translation results for different methods. Comparing the first two rows in Table 1, it is very obvious that the attention-based NMT system Baseline substantially outperforms the phrase-based SMT system Moses on both CH-EN translation and EN-JA translation. The average improvement for CH-EN and EN-JA translation is up to 3.99 BLEU points (32.71 vs. 28.72) and 3.59 BLEU (25.99 vs. 22.40) points, respectively.", "In Fig. 3, we show an illustrative example of CH-EN translation. In this example, our method is able to obtain a correct translation while the baseline is not. Specifically, baseline NMT system mistranslates \u201cjinkou dafu xiahua (the sharp decline in imports)\u201d into \u201cimport of imports\u201d, and incorrectly translates \u201cmaoyi shuncha (trade surplus)\u201d into \u201ctrade\u201d. But these two mistakes are fixed by our method, because there are two phrase translation pairs (\u201cjinkou dafu xiahua\u201d to \u201cthe sharp decline in imports\u201d and \u201cmaoyi shuncha\u201d to \u201ctrade surplus\u201d) in the phrase table, and the correct translations are obtained due to our recommendation method."]}
{"question_id": "b79ff0a50bf9f361c5e5fed68525283856662076", "predicted_answer": "", "predicted_evidence": ["[t] Construct recommendation word set Input: candidate target phrase set; already generated partial translation INLINEFORM0", "The past several years have witnessed a significant progress in Neural Machine Translation (NMT). Most NMT methods are based on the encoder-decoder architecture BIBREF0 , BIBREF1 , BIBREF2 and can achieve promising translation performance in a variety of language pairs BIBREF3 , BIBREF4 , BIBREF5 .", "In Fig. 3, we show an illustrative example of CH-EN translation. In this example, our method is able to obtain a correct translation while the baseline is not. Specifically, baseline NMT system mistranslates \u201cjinkou dafu xiahua (the sharp decline in imports)\u201d into \u201cimport of imports\u201d, and incorrectly translates \u201cmaoyi shuncha (trade surplus)\u201d into \u201ctrade\u201d. But these two mistakes are fixed by our method, because there are two phrase translation pairs (\u201cjinkou dafu xiahua\u201d to \u201cthe sharp decline in imports\u201d and \u201cmaoyi shuncha\u201d to \u201ctrade surplus\u201d) in the phrase table, and the correct translations are obtained due to our recommendation method.", "However, recent studies BIBREF6 , BIBREF7 show that NMT often generates words that make target sentences fluent, but unfaithful to the source sentences. In contrast, traditional Statistical Machine Translation (SMT) methods tend to rarely make this kind of mistakes. Fig. 1 shows an example that NMT makes mistakes when translating the phrase \u201cjinkou dafu xiahua (the sharp decline in imports)\u201d and the phrase \u201cmaoyi shuncha (the trade surplus)\u201d, but SMT can produce correct results when translating these two phrases. BIBREF6 argues that the reason behind this is the use of distributed representations of words in NMT makes systems often generate words that seem natural in the context, but do not reflect the content of the source sentence. Traditional SMT can avoid this problem as it produces the translations based on phrase mappings.", "Our phrase translation table is learned directly from parallel data by Moses BIBREF22 . To ensure the quality of the phrase pair, in all experiments, the phrase translation table is filtered as follows: 1) out-of-vocabulary words in the phrase table are replaced by UNK; 2) we remove the phrase pairs whose words are all punctuations and UNK; 3) for a source phrase, we retain at most 10 target phrases having the highest phrase translation probabilities."]}
{"question_id": "d66c31f24f582c499309a435ec3c688dc3a41313", "predicted_answer": "", "predicted_evidence": ["Fully connected units in the DSSM architecture were subsequently replaced with Convolutional Neural Networks (CNNs) BIBREF14 , BIBREF15 and Recurrent Neural Networks (RNNs) BIBREF16 to respect word ordering. In an alternate approach, which articulated the idea of interaction models, BIBREF17 introduced the Deep Relevance Matching Model (DRMM) which leverages an interaction matrix to capture local term matching within neural approaches which has been successfully extended by MatchPyramid BIBREF18 and other techniques BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 . Nevertheless, these interaction methods require memory and computation proportional to the number of words in the document and hence are prohibitively expensive for online inference. In addition, Duet BIBREF24 combines the approaches of DSSM and DRMM to balance the importance of semantic and lexical matching.", "Ranking: The goal of this task is to order a set of documents by relevance, defined as purchase count conditioned on the query. The set of documents contains purchased and impressed products. We report standard information retrieval ranking metrics, such as Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR).", "Products have multiple attributes, like title, brand, and color, that are material to the matching process. We evaluated architectures to embed every attribute independently and concatenate them to obtain the final product representation. However, large variability in the accuracy and availability of structured data across products led to 5% lower recall than simply concatenating the attributes. Hence, we decided to use an ordered bag of words of these attributes.", "We use 11 months of search logs as training data and 1 month as evaluation. We sample 54 billion query-product training pairs. We preprocess these sampled pairs to 650 million rows by grouping the training data by query-product pairs over the entire time period and using the aggregated counts as weights for the pairs. We also decrease the training time by 3X by preprocessing the training data into tokens and using mmap to store the tokens. More details on our best practices for reducing training time can be found in Section SECREF6 .", "To generate a fixed length embedding for the query ( INLINEFORM0 ) and the product ( INLINEFORM1 ) from individual word embeddings, we use average pooling after observing little difference (<0.5%) in both MAP and Recall@100 relative to recurrent approaches like LSTM and GRU (see Table TABREF27 ). Average pooling also requires far less computation, reducing training time and inference latency. We reconciled this departure from state-of-the-art solutions for Question Answering and other NLP tasks through an analysis that showed that, unlike web search, both query and product information tend to be shorter, without long-range dependencies. Additionally, product search queries do not contain stop words and typically require every query word (or its synonym) to be present in the product."]}
{"question_id": "c47312f2ca834ee75fa9bfbf912ea04239064117", "predicted_answer": "", "predicted_evidence": ["For a given customer query, each product is in exactly one of three categories: purchased, impressed but not purchased, or random. For each query, we target a ratio of 6 impressed and 7 random products for every query-product purchase. We sample this way to train the model for both matching and ranking, although in this paper we focus on matching. Intuitively, matching should differentiate purchased and impressed products from random ones; ranking should differentiate purchased products from impressed ones.", "To generate a fixed length embedding for the query ( INLINEFORM0 ) and the product ( INLINEFORM1 ) from individual word embeddings, we use average pooling after observing little difference (<0.5%) in both MAP and Recall@100 relative to recurrent approaches like LSTM and GRU (see Table TABREF27 ). Average pooling also requires far less computation, reducing training time and inference latency. We reconciled this departure from state-of-the-art solutions for Question Answering and other NLP tasks through an analysis that showed that, unlike web search, both query and product information tend to be shorter, without long-range dependencies. Additionally, product search queries do not contain stop words and typically require every query word (or its synonym) to be present in the product.", "We choose the most frequent words to build our vocabulary, referred to as INLINEFORM0 . Each token in the vocabulary is assigned a unique numeric token id, while remaining tokens are assigned 0 or a hashing based identifier. Queries are lowercased, split on whitespace, and converted into a sequence of token ids. We truncate the query tokens at the 99th percentile by length. Token vectors that are smaller than the predetermined length are padded to the right.", "Fully connected units in the DSSM architecture were subsequently replaced with Convolutional Neural Networks (CNNs) BIBREF14 , BIBREF15 and Recurrent Neural Networks (RNNs) BIBREF16 to respect word ordering. In an alternate approach, which articulated the idea of interaction models, BIBREF17 introduced the Deep Relevance Matching Model (DRMM) which leverages an interaction matrix to capture local term matching within neural approaches which has been successfully extended by MatchPyramid BIBREF18 and other techniques BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 . Nevertheless, these interaction methods require memory and computation proportional to the number of words in the document and hence are prohibitively expensive for online inference. In addition, Duet BIBREF24 combines the approaches of DSSM and DRMM to balance the importance of semantic and lexical matching. Despite obtaining state-of-the-art results for ranking, these methods report limited success on ad hoc retrieval tasks BIBREF24 and only achieve a sub-50% Recall@100 and MAP on our product matching dataset, as shown with the ARC-II and Match Pyramid baselines in Table TABREF30 .", "At a high level, as shown in Figure FIGREF4 , a product search engine works as follows: a customer issues a query, which is passed to a lexical matching engine (typically an inverted index BIBREF0 , BIBREF1 ) to retrieve all products that contain words in the query, producing a match set. The match set passes through stages of ranking, wherein top results from the previous stage are re-ranked before the most relevant items are finally displayed. It is imperative that the match set contain a relevant and diverse set of products that match the customer intent in order for the subsequent rankers to succeed. However, inverted index-based lexical matching falls short in several key aspects:"]}
{"question_id": "5499440674f0e4a9d6912b9ac29fa1f7b7cd5253", "predicted_answer": "", "predicted_evidence": ["Table TABREF17 shows that for four out of the six criteria (MC 1, MC 4, MC 5, and MC 6) the top-k sentences model outperforms baseline 1 as well the bottom-k and the random-k sentences models by a significant margin. Furthermore, for three of the six criteria (MC 4, MC 5, and MC 6) the top-k sentences model also outperforms the baseline 2 model (model which utilized all full text). This seems to confirm our hypothesis that semantic relatedness of sentences to the criteria descriptions helps in identifying sentences discussing the criteria. These seems to be the case especially given that for three of the six criteria the top-k sentences model outperforms the model which utilizes all full text (baseline 2) despite being given less information to learn from (selected sentences only in the case of the top-k sentences model vs. all full text in the case of the baseline 2 model).", "A number of previous studies have focused on unsupervised extraction of relations such as protein-protein interactions (PPI) from biomedical texts. For example, BIBREF15 have utilized several techniques, namely kernel-based pattern clustering and dependency parsing, to extract PPI from biomedical texts. BIBREF16 have introduced a system for unsupervised extraction of entities and relations between these entities from clinical texts written in Italian, which utilized a thesaurus for extraction of entities and clustering methods for relation extraction. BIBREF17 also used clinical texts and proposed a generative model for unsupervised relation extraction. Another approach focusing on relation extraction has been proposed by BIBREF18 . Their approach is based on constructing a graph which is used to construct domain-independent patterns for extracting protein-protein interactions.", "The creation of the database followed the study protocol design set forth in BIBREF3 , which is composed of six minimum criteria (MC, Table TABREF5 ). An example of information pertaining to the criteria is shown in Figure FIGREF2 . Only studies which met all six minimum criteria were considered guideline-like (GL) and were included in a follow-up detailed study and the final database BIBREF1 . However, of the 670 publications initially considered for inclusion, only 93 ( INLINEFORM0 14%) were found to contain studies which met all six MC and could therefore be included in the final database; the remaining 577 publications could not be used in the final reference set. Therefore, significant time and resources could be saved by automating the identification and extraction of the MC.", "In this paper we presented a method for unsupervised identification of text segments relevant to specific sought after information being extracted from scientific documents. Our method is entirely unsupervised and only requires the current document itself and the input descriptions instead of corpus linked to this document. The method utilizes short descriptions of the information being extracted from the documents and the ability of word embeddings to capture word context. Consequently, it is domain independent and can potentially be applied to another set of documents and criteria with minimal effort. We have used the method on a corpus of toxicology documents and a set of guideline protocol criteria needed to be extracted from the documents. We have shown the identified text segments are very accurate. Furthermore, a binary classifier trained to identify publications that met the criteria performed better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting our intuition that our method is able to accurately identify relevant text segments from full text documents.", "In this section we describe the method we have used for retrieving text segments related to the criteria described in the previous section. The intuition is based off question answering systems. We treat the criteria descriptions (Table TABREF5 ) as the question and the text segments within the publication that discusses the criteria as the answer. Given a full text publication, the goal is to find the text segments most likely to contain the answer."]}
{"question_id": "de313b5061fc22e8ffef1706445728de298eae31", "predicted_answer": "", "predicted_evidence": ["For two of the criteria (MC 2 and MC 3) this is not the case and the top-k sentences model performs worse than both other models in the case of MC 3 and worse than the random-k model in the case of MC 2. One possible explanation for this is class imbalance. In the case of MC 2, only 33 out of 592 publications (5.57%) represent negative examples (Table TABREF17 ). As the top-k sentences model picks only sentences closely related to MC 2, it is possible that due to the class imbalance the top sentences don't contain enough negative examples to learn from.", "For two of the criteria (MC 2 and MC 3) this is not the case and the top-k sentences model performs worse than both other models in the case of MC 3 and worse than the random-k model in the case of MC 2. One possible explanation for this is class imbalance. In the case of MC 2, only 33 out of 592 publications (5.57%) represent negative examples (Table TABREF17 ). As the top-k sentences model picks only sentences closely related to MC 2, it is possible that due to the class imbalance the top sentences don't contain enough negative examples to learn from. On the other hand, the bottom-k and random-k sentences models may select text not necessarily related to the criteria but potentially containing linguistic patterns which the model learns to associate with the criteria; for example, certain chemicals may require the use of a certain study protocol which may not be aligned with the MC and the model may key in on the appearance of these chemicals in text rather than the appearance of MC indicators.", "We hypothesize that the use of word embedding features will allow us to detect relevant words which are not present in the criteria descriptions. BIBREF10 have shown that an important feature of Word2Vec embeddings is that similar words will have similar vectors because they appear in similar contexts. We utilize this feature to calculate similarity between the criteria descriptions and text segments (such as sentences) extracted from each document. A high-level overview of our approach is shown in Figure FIGREF9 .", "To avoid selecting the same sentences across the three models we removed documents which contained less than INLINEFORM0 sentences (Table TABREF17 , row Number of documents shows how many documents satisfied this condition). In all of the experiments presented in this section, the publication full text was tokenized, lower-cased, stemmed, and stop words were removed. All models used a Bernoulli Na\u00efve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed. The complete results obtained from leave-one-out cross validation are shown in Table TABREF17 . In all cases we report classification accuracy. In the case of the random-k sentences model the accuracy was averaged over 10 runs of the model.", "Candidate segments: For each document we select the top INLINEFORM0 text segments (sentences in the first step and 5-grams in the second step) most similar to the description."]}
{"question_id": "47b7bc232af7bf93338bd3926345e23e9e80c0c1", "predicted_answer": "", "predicted_evidence": ["With the exception of the first study (experiment 1), which had group sizes of 12, all other studies had group sizes of 8.", "There are two main contributions of this work. We present an unsupervised method that employs representation learning to identify text segments from publication full text which are relevant to/contain specific sought after information (such as number of dose groups). In addition, we explore a new dataset which hasn't been previously used in the field of information extraction.", "Animals were killed 24 h after being injected and their uteri were removed and weighed.", "For two of the criteria (MC 2 and MC 3) this is not the case and the top-k sentences model performs worse than both other models in the case of MC 3 and worse than the random-k model in the case of MC 2. One possible explanation for this is class imbalance. In the case of MC 2, only 33 out of 592 publications (5.57%) represent negative examples (Table TABREF17 ). As the top-k sentences model picks only sentences closely related to MC 2, it is possible that due to the class imbalance the top sentences don't contain enough negative examples to learn from. On the other hand, the bottom-k and random-k sentences models may select text not necessarily related to the criteria but potentially containing linguistic patterns which the model learns to associate with the criteria; for example, certain chemicals may require the use of a certain study protocol which may not be aligned with the MC and the model may key in on the appearance of these chemicals in text rather than the appearance of MC indicators.", "A typical approach to automated identification of relevant information in biomedical texts is to infer a prediction model from labeled training data \u2013 such a model can then be used to assign predicted labels to new data instances. However, obtaining training data for creating such prediction models can be very costly as it involves the step which these models are trying to automate \u2013 manual data extraction. Furthermore, depending on the task at hand, the types of information being extracted may vary significantly. For example, in systematic reviews of randomized controlled trials this information generally includes the patient group, the intervention being tested, the comparison, and the outcomes of the study (PICO elements) BIBREF4 . In toxicology research the extraction may focus on routes of exposure, dose, and necropsy timing BIBREF1 . Previous work has largely focused on identifying specific pieces of information such as biomedical events BIBREF6 or PICO elements BIBREF0 . However, depending on the domain and the end goal of the extraction, these may be insufficient to comprehensively describe a given study."]}
{"question_id": "0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "predicted_answer": "", "predicted_evidence": ["We use the following method to retrieve the most relevant text segments:", "The remainder of this paper is organized as follows. In the following section we provide more details of the task and the dataset used in this study. In Section SECREF3 we describe our approach. In Section SECREF4 we evaluate our model and discuss our results. In Section SECREF5 we compare our work to existing approaches. Finally, in Section SECREF6 we provide ideas for further study.", "In this section we present studies most similar to our work. We focus on unsupervised methods for information extraction from biomedical texts.", "Many methods for biomedical data annotation and extraction exist which utilize labeled data and supervised learning approaches ( BIBREF12 and BIBREF6 provided a good overview of a number of these methods); however, unsupervised approaches in this area are much scarcer. One such approach has been introduced by BIBREF13 , who have proposed a model for unsupervised Named Entity Recognition. Similar to our approach, their model is based on calculating the similarity between vector representations of candidate phrases and existing entities. However, their vector representations are created using a combination of TF-IDF weights and word context information, and their method relies on a terminology. More recently, BIBREF14 have utilized Word2Vec and Doc2Vec embeddings for unsupervised sentiment classification in medical discharge summaries.", "After weaning on pnd 21, the dams were euthanized by CO2 asphyxiation and the juvenile females were individually housed."]}
{"question_id": "1397b1c51f722a4ee2b6c64dc9fc6afc8bd3e880", "predicted_answer": "", "predicted_evidence": ["To account for the variations in language that can be used to describe the criteria, we represent words as vectors generated using Word2Vec BIBREF7 . The following two excerpts show two different ways MC 6 was described in text:", "Therefore, in this paper we focus on unsupervised methods for identifying text segments (such as sentences or fixed length sequences of words) relevant to the information being extracted. We develop a model that can be used to identify text segments from text documents without labeled data and that only requires the current document itself, rather than an entire training corpus linked to the target document. More specifically, we utilize representation learning methods BIBREF7 , where words or phrases are embedded into the same vector space. This allows us to compute semantic relatedness among text fragments, in particular sentences or text segments in a given document and a short description of the type of information being extracted from the document, by using similarity measures in the feature space. The model has the potential to speed up identification of relevant segments in text and therefore to expedite annotation of domain specific information without reliance on costly labeled data.", "This section provides additional details and results. Figures FIGREF19 , FIGREF20 , and FIGREF21 show example annotations generated for criteria MC 4, MC 5, and MC 6.", "All animals were euthanized by exposure to ethyl ether 24 h after the final treatment.", "There are a number of things we plan on investigating next. In our initial experiment we have utilized criteria descriptions which were not designed to be used by our model. One possible improvement of our method could be replacing the current descriptions with example sentences taken from the documents containing the sought after information. We also plan on testing our method on an annotated dataset, for example using existing annotated PICO element datasets BIBREF24 ."]}
{"question_id": "230f127e83ac62dd65fccf6b1a4960cf0f7316c7", "predicted_answer": "", "predicted_evidence": ["In a range test, the network is trained for several epochs with the learning rate linearly increased from an initial rate. For instance, the range test for the IWSLT2014 (DE2EN) dataset was run for 35 epochs, with the initial learning rate set to some small values, e.g., $1 \\times 10^{-5}$ for Adam and increased linearly over the 35 epochs. Given the range test curve, e.g., Figure FIGREF7, the base learning rate is set to the point where the loss starts to decrease while the maximum learning rate is selected as the point where the loss starts to plateau or to increase. As shown in Figure FIGREF7, the base learning rate is selected as the initial learning rate for the range test, since there is a steep loss using the initial learning rate. The max learning rate is the point where the loss stagnates. For the step size, following the guideline given in BIBREF17 to select the step size between 2-10 times the number of iterations in an epoch and set the step size to 4.5 epochs.", "We observe the qualitative different range test curves for CV and NMT datasets. As we can see from Figures FIGREF7 and FIGREF21. The CV range test curve looks more well defined in terms of choosing the max learning rate from the point where the curve starts to be ragged. For NMT, the range curve exhibits a smoother, more plateau characteristic. From Figure FIGREF7, one may be tempted to exploit the plateau characteristic by choosing a larger learning rate on the extreme right end (before divergence occurs) as the triangular policy's max learning rate. From our experiments and empirical observations, this often leads to the loss not converging due to excessive learning rate. It is better to be more conservative and choose the point where the loss stagnates as the max learning rate for the triangular policy.", "The other hyperparameter to take care of is the learning rate decay rate, shown in Figure FIGREF8. For the various optimizers, the learning rate is usually decayed to a small value to ensure convergence. There are various commonly used decay schemes such as piece-wise constant step function, inverse (reciprocal) square root. This study adopts two learning rate decay policies:", "Scripts and data are available at https://github.com/nlp-team/CL_NMT.", "The purpose of this section is to demonstrate the effects of applying CLR and various batch sizes to train NMT models. The experiments are performed on two translation directions (DE $\\rightarrow $ EN and FR $\\rightarrow $ EN) for IWSLT2014 and IWSLT2017 BIBREF25."]}
{"question_id": "75c221920bee14a6153bd5f4c1179591b2f48d59", "predicted_answer": "", "predicted_evidence": ["Explore the use of cyclical learning rates for NMT. As far as we know, this is the first time cyclical learning rate policy has been applied to NMT;", "In contrast, the rich body of works in Neural Machine Translation (NMT) and other Natural Language Processing (NLP) related tasks have been largely left untouched. Recall that CV deep learning networks and NMT deep learning networks are very different. For instance, the convolutional network that forms the basis of many successful CV deep learning networks is translation invariant, e.g., in a face recognition network, the convolutional filters produce the same response even when the same face is shifted or translated. In contrast, Recurrent Neural Networks (RNN) BIBREF12, BIBREF13 and transformer-based deep learning networks BIBREF14, BIBREF15 for NMT are specifically looking patterns in sequences. There is no guarantee that the results from the CV based studies can be carried across to NMT. There is also a lack of awareness in the NMT community when it comes to optimizers and other related issues such as learning rate policy and batch size.", "A hypothesis we hold is that NMT training under CLR may result in a better local minimum than that achieved by training with the default learning rate schedule. A comparison experiment is performed for training NMT models for \u201cIWSLT2014-de-en\" corpus using CLR and INV with a range of initial learning rates on two optimizers (Adam and SGD), respectively. It can be observed that both Adam and SGD are very sensitive to the initial learning rate under the default INV schedule before CLR is applied (as shown in Figures FIGREF15 and FIGREF16). In general, SGD prefers a bigger initial learning rate when CLR is not applied. The initial learning rate of Adam is more concentrated towards the central range.", "There are two reasons proposed in BIBREF17 on why CLR works. The theoretical perspective proposed is that the increasing learning rate helps the optimizer to escape from saddle point plateaus. As pointed out in BIBREF28, the difficulty in optimizing deep learning networks is due to saddle points, not local minima. The other more intuitive reason is that the learning rates covered in CLR are likely to include the optimal learning rate, which will be used throughout the training. Leveraging the visualization techniques proposed by BIBREF16, we take a peek at the error surface, optimizer trajectory and learning rate. The first thing to note is the smoothness of the error surface. This is perhaps not so surprising given the abundance of skip connections in transformer-based networks. Referring to Figure FIGREF25 (c), we see the cyclical learning rate greatly amplifying Adam's learning rate in flatter region while nearer the local minimum, the cyclical learning rate policy does not harm convergence to the local minimum.", "The theoretical perspective proposed is that the increasing learning rate helps the optimizer to escape from saddle point plateaus. As pointed out in BIBREF28, the difficulty in optimizing deep learning networks is due to saddle points, not local minima. The other more intuitive reason is that the learning rates covered in CLR are likely to include the optimal learning rate, which will be used throughout the training. Leveraging the visualization techniques proposed by BIBREF16, we take a peek at the error surface, optimizer trajectory and learning rate. The first thing to note is the smoothness of the error surface. This is perhaps not so surprising given the abundance of skip connections in transformer-based networks. Referring to Figure FIGREF25 (c), we see the cyclical learning rate greatly amplifying Adam's learning rate in flatter region while nearer the local minimum, the cyclical learning rate policy does not harm convergence to the local minimum. This is in contrast to Figure FIGREF25 (a) and (b), where although the adaptive nature of the learning rate in Adam helps to move quickly across flatter region, the effect is much less pronounced without the cyclical learning rate."]}
{"question_id": "4eb42c5d56d695030dd47ea7f6d65164924c4017", "predicted_answer": "", "predicted_evidence": ["Then, we create the sets of words $\\Psi _{d}$, $\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$, $f^{\\text{e}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, and $f^{\\text{t}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, respectively,", "The encoder is a series of bi-directional gated recurrent units (bi-GRUs) BIBREF10, similarly to BIBREF3. The output dimensionality for the GRU layers (forward and backward GRUs have same dimensionality) is $\\lbrace 256, 256, 256\\rbrace $. The output of the encoder is processed by an attention mechanism and its output is given as an input to the decoder. The attention mechanism is a feed-forward neural network (FNN) and the decoder a GRU. Then, the output of the decoder is given as an input to another FNN with a softmax non-linearity, which acts as a classifier and outputs the probability distribution of words for the $i$-th time-step. To optimize the parameters of the employed method, we use five times each audio sample, using its five different captions as targeted outputs each time.", "The research leading to these results has received funding from the European Research Council under the European Union\u2019s H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND. Part of the computations leading to these results were performed on a TITAN-X GPU donated by NVIDIA to K. Drossos. The authors also wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.", "That is, we end up with 50 000 sets of splits of 60%, 20%, 20%, corresponding to development, evaluation, and testing splits, respectively. We want to score each of these sets of splits, in order to select the split with the smallest amount of words that deviate from the ideal split for each of these 50 000 sets of splits. We calculate the frequency of appearance of each word in the development, evaluation, and testing splits, $f^{\\text{d}}_{w}$, $f^{\\text{e}}_{w}$, and $f^{\\text{t}}_{w}$, respectively. Then, we create the sets of words $\\Psi _{d}$, $\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},", "Then, we create the sets of words $\\Psi _{d}$, $\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$, $f^{\\text{e}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, and $f^{\\text{t}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, respectively, where $f^{\\text{Ev}}_{w} = f_{w} - f^{\\text{Dev}}_{w}$."]}
{"question_id": "eff9192e05d23e9a67d10be0c89a7ab2b873995b", "predicted_answer": "", "predicted_evidence": ["where $\\alpha _{d}=1/f^{\\text{Dev}}_{w}$ and $\\alpha _{e}=1/0.5f^{\\text{Ev}}_{w}$. We sort all 50 000 sets of splits according to $\\Gamma $ and in ascending fashion, and we pick the top one. This set of splits is the final split for the Clotho dataset, containing 2893 audio samples and 14465 captions in development split, 1045 audio samples and 5225 captions in evaluation split, and 1043 audio samples and 5215 captions in the testing split. The development and evaluation splits are freely available online2. The testing split is withheld for potential usage in scientific challenges. A fully detailed description of the Clotho dataset can be found online.", "$\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$, $f^{\\text{e}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, and $f^{\\text{t}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, respectively, where $f^{\\text{Ev}}_{w} = f_{w} - f^{\\text{Dev}}_{w}$.", "Then, we create the sets of words $\\Psi _{d}$, $\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$, $f^{\\text{e}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, and $f^{\\text{t}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, respectively, where $f^{\\text{Ev}}_{w} = f_{w} - f^{\\text{Dev}}_{w}$.", "We want to score each of these sets of splits, in order to select the split with the smallest amount of words that deviate from the ideal split for each of these 50 000 sets of splits. We calculate the frequency of appearance of each word in the development, evaluation, and testing splits, $f^{\\text{d}}_{w}$, $f^{\\text{e}}_{w}$, and $f^{\\text{t}}_{w}$, respectively. Then, we create the sets of words $\\Psi _{d}$, $\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$, $f^{\\text{e}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},", "where $\\alpha _{d}=1/f^{\\text{Dev}}_{w}$ and $\\alpha _{e}=1/0.5f^{\\text{Ev}}_{w}$. We sort all 50 000 sets of splits according to $\\Gamma $ and in ascending fashion, and we pick the top one. This set of splits is the final split for the Clotho dataset, containing 2893 audio samples and 14465 captions in development split, 1045 audio samples and 5225 captions in evaluation split, and 1043 audio samples and 5215 captions in the testing split. The development and evaluation splits are freely available online2. The testing split is withheld for potential usage in scientific challenges. A fully detailed description of the Clotho dataset can be found online. In Figure FIGREF12 is a histogram of the percentage of words ($f^{d}_{w}/f_{w}$, $f^{e}_{w}/f_{w}$, and $f^{t}_{w}/f_{w}$) in the three different splits."]}
{"question_id": "87523fb927354ddc8ad1357a81f766b7ea95f53c", "predicted_answer": "", "predicted_evidence": ["where $\\alpha _{d}=1/f^{\\text{Dev}}_{w}$ and $\\alpha _{e}=1/0.5f^{\\text{Ev}}_{w}$. We sort all 50 000 sets of splits according to $\\Gamma $ and in ascending fashion, and we pick the top one. This set of splits is the final split for the Clotho dataset, containing 2893 audio samples and 14465 captions in development split, 1045 audio samples and 5225 captions in evaluation split, and 1043 audio samples and 5215 captions in the testing split. The development and evaluation splits are freely available online2. The testing split is withheld for potential usage in scientific challenges. A fully detailed description of the Clotho dataset can be found online. In Figure FIGREF12 is a histogram of the percentage of words ($f^{d}_{w}/f_{w}$, $f^{e}_{w}/f_{w}$, and $f^{t}_{w}/f_{w}$) in the three different splits.", "This means that if a word appears only at the captions of one $\\mathbf {x}^{o}$, then this word will be appearing only at one of the splits. Having a word appearing only in training split leads to sub-optimal learning procedure, because resources are spend to words unused in validation and testing. If a word is not appearing in the training split, then the evaluation procedure suffers by having to evaluate on words not known during training. For that reason, for each $\\mathbf {x}^{o}$ we construct the set of words $\\mathbb {S}_{a}^{o}$ from $\\mathbb {C}^{\\prime o}$. Then, we merge all $\\mathbb {S}_{a}^{o}$ to the bag $\\mathbb {S}_{T}$ and we identify all words that appear only once (i.e. having a frequency of one) in $\\mathbb {S}_{T}$. We employ an extra annotator (not from AMT) which has access only to the captions of $\\mathbf {x}^{o}$, and has the instructions to change the all words in $\\mathbb {S}_{T}$ with frequency of one, with other synonym words in $\\mathbb {S}_{T}$ and (if necessary) rephrase the caption.", "The research leading to these results has received funding from the European Research Council under the European Union\u2019s H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND. Part of the computations leading to these results were performed on a TITAN-X GPU donated by NVIDIA to K. Drossos. The authors also wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.", "In this work we present a novel dataset for audio captioning, named Clotho, that contains 4981 audio samples and five captions for each file (totaling to 24 905 captions). During the creating of Clotho care has been taken in order to promote diversity of captions, eliminate words that appear only once and named entities, and provide data splits that do not hamper the training or evaluation process. Also, there is an example of the usage of Clotho, using a method proposed at the original work of audio captioning. The baseline results indicate that the baseline method started learning the content of the input audio, but more tuning is needed in order to express the content properly. Future work includes the employment of Clotho and development of novel methods for audio captioning.", "$\\Psi _{e}$, and $\\Psi _{t}$, having the words with $f^{\\text{d}}_{w} \\notin [f^{\\text{Dev}}_{w}- \\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$, $f^{\\text{e}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, and $f^{\\text{t}}_{w} \\notin [f^{\\text{Ev}}_{w}- \\delta _{w},f^{\\text{Ev}}_{w}+\\delta _{w}]$, respectively, where $f^{\\text{Ev}}_{w} = f_{w} - f^{\\text{Dev}}_{w}$. Finally, we calculate the sum of the weighted distance of frequencies of words from the $f^{\\text{Dev}}_{w}\\pm \\delta _{w}$ or $f^{\\text{Ev}}_{w}\\pm \\delta _{w}$ range (for words being in the development split or not, respectively), $\\Gamma $, as"]}
{"question_id": "9e9aa8af4b49e2e1e8cd9995293a7982ea1aba0e", "predicted_answer": "", "predicted_evidence": ["We collect the set of audio samples $\\mathbb {X}_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{i}\\rbrace _{i=1}^{N_{\\text{init}}}$, with $N_{\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\mathbf {x}_{\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\text{ s}\\le d({\\mathbf {x}_{\\text{init}}^{i}})\\le 300$ s (where $d(\\mathbf {x})$ is the duration of $\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g.", "In this paper we present the freely available audio captioning dataset Clotho, with 4981 audio samples and 24 905 captions. All audio samples are from Freesound platform BIBREF8, and are of duration from 15 to 30 seconds. Each audio sample has five captions of eight to 20 words length, collected by AMT and a specific protocol for crowdsourcing audio annotations, which ensures diversity and reduced grammatical errors BIBREF0. During annotation no other information but the audio signal was available to the annotators, e.g. video or word tags. The rest of the paper is organized as follows. Section SECREF2 presents the creation of Clotho, i.e. gathering and processing of the audio samples and captions, and the splitting of the data to development, evaluation, and testing splits.", "The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions.", "We randomly sample $10^6$ sets (with overlap) of $N_{\\text{med}}=5000$ files from $\\mathbb {X}^{\\prime }_{\\text{init}}$, and keep the set that has the maximum entropy for $\\mathbb {T}_{\\text{0.01}}$. This process results in $\\mathbb {X}_{\\text{med}}=\\lbrace \\mathbf {x}_{\\text{init}}^{z}\\rbrace _{z=1}^{N_{\\text{med}}}$, having the most uniform tag distribution and, hence, the most diverse content. The resulting distribution of the tags in $\\mathbb {T}_{\\text{0.01}}$ is illustrated in Figure FIGREF5. The 10 most common tags are: ambient, water, nature, birds, noise, rain, city, wind, metal, and people.", "The output dimensionality for the GRU layers (forward and backward GRUs have same dimensionality) is $\\lbrace 256, 256, 256\\rbrace $. The output of the encoder is processed by an attention mechanism and its output is given as an input to the decoder. The attention mechanism is a feed-forward neural network (FNN) and the decoder a GRU. Then, the output of the decoder is given as an input to another FNN with a softmax non-linearity, which acts as a classifier and outputs the probability distribution of words for the $i$-th time-step. To optimize the parameters of the employed method, we use five times each audio sample, using its five different captions as targeted outputs each time. We optimize jointly the parameters of the encoder, attention mechanism, decoder, and the classifier, using 150 epochs, the cross entropy loss, and Adam optimizer BIBREF11 with proposed hyper-parameters. Also, in each batch we pad the captions of the batch to the longest in the same batch, using the end-of-sequence token, and the input audio features to the longest ones, by prepending zeros."]}
{"question_id": "1fa9b6300401530738995f14a37e074c48bc9fd8", "predicted_answer": "", "predicted_evidence": ["Score distribution Due to the caption pairs are generated from different images, strong bias towards low scores is expected (see Figure FIGREF3 ). We measured the score distribution in the two subsets separately and jointly, and see that the two subsets follow same distribution. As expected, the most frequent score is 0 (Table TABREF2 ), but the dataset still shows wide range of similarity values, with enough variability.", "In the future we plan to re-annotate the dataset with scores which are based on both the text and the image, in order to shed light on the interplay of images and text when understanding text.", "Subset 2015 The subset is derived from Image Descriptions dataset, which is a subset of 8k-picture of Flickr. 8k-Flicker is a benchmark collection for sentence-based image description, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We obtained 445 pairs (out of 750 in the original).", "The complementarity of visual and text representations for improved language understanding have been shown also on word representations, where embeddings have been combined with visual or perceptual input to produce grounded representations of words BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 . These improved representation models have outperformed traditional text-only distributional models on a series of word similarity tasks, showing that visual information coming from images is complementary to textual information.", "As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:"]}
{"question_id": "9d98975ab0b75640b2c83e29e1438c76a959fbde", "predicted_answer": "", "predicted_evidence": ["The dataset is derived from a subset of the caption pairs already annotated in the Semantic Textual Similarity Task (see below). We selected some caption pairs with their similarity annotations, and added the images corresponding to each caption. While the human annotators had access to only the text, we provide the system with both the caption and corresponding image, to check whether the visual representations can be exploited by the system to solve a text understanding and inference task.", "Results Table TABREF4 shows the results of the single and combined models. Among single models, as expected, dam obtains the highest Pearson correlation ( INLINEFORM0 ). Interestingly, the results show that images alone are valid to predict caption similarity (0.61 INLINEFORM1 ). Results also show that image and sentence representations are complementary, with the best results for a combination of DAM and RESNET50 representations. These results confirm our hypotheses, and more generally, show indications that in systems that work with text describing the real world, the representation of the real world helps to better understand the text and do better inferences.", "Experimental setting We split the vSTS dataset into development and test partitions, sampling 50% at random, while preserving the overall score distributions. In addition, we used part of the text-only STS benchmark dataset as a training set, discarding the examples that overlap with vSTS.", "As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:", "STS Models We checked four models of different complexity and modalities. The baseline is a word overlap model (overlap), in which input texts are tokenized with white space, vectorized according to a word index, and similarity is computed as the cosine of the vectors. We also calculated the centroid of Glove word embeddings BIBREF17 (caverage) and then computed the cosine as a second text-based model. The third text-based model is the state of the art Decomposable Attention Model BIBREF18 (dam), trained on the STS benchmark dataset as explained above. Finally, we use the top layer of a pretrained resnet50 model BIBREF19 to represent the images associated to text, and use the cosine for computing the similarity of a pair of images (resnet50)."]}
{"question_id": "cc8bcea4052bf92f249dda276acc5fd16cac6fb4", "predicted_answer": "", "predicted_evidence": ["Experiments confirmed our hypotheses: image representations are useful for caption similarity and they are complementary to textual representations, as results improve significantly when two modalities are combined together.", "Model combinations We combined the predictions of text based models with the predictions of the image based model (see Table TABREF4 for specific combinations). Models are combined using addition ( INLINEFORM0 ), multiplication ( INLINEFORM1 ) and linear regression (LR) of the two outputs. We use 10-fold cross-validation on the development test for estimating the parameters of the linear regressor.", "This research was partially supported by the Spanish MINECO (TUNER TIN2015-65308-C5-1-R and MUSTER PCIN-2015-226).", "", "In this paper we present Visual Semantic Textual Similarity (vSTS), a dataset which allows to study whether better sentence representations can be built when having access to corresponding images, e.g. a caption and its image, in contrast with having access to the text alone. This dataset is based on a subset of the STS benchmark BIBREF1 , more specifically, the so called STS-images subset, which contains pairs of captions. Note that the annotations are based on the textual information alone. vSTS extends the existing subset with images, and aims at being a standard dataset to test the contribution of visual information when evaluating sentence representations."]}
{"question_id": "35f48b8f73728fbdeb271b170804190b5448485a", "predicted_answer": "", "predicted_evidence": ["In another strand of related work, tasks that combine representations of multiple modalities have gained increasing attention, including image-caption retrieval, video and text alignment, caption generation, and visual question answering. A common approach is to learn image and text embeddings that share the same space so that sentence vectors are close to the representation of the images they describe BIBREF3 , BIBREF4 . BIBREF5 provides an approach that learns to align images with descriptions. Joint spaces are typically learned combining various types of deep learning networks such us recurrent networks or convolutional networks, with some attention mechanism BIBREF6 , BIBREF7 , BIBREF8 .", "Subset 2015 The subset is derived from Image Descriptions dataset, which is a subset of 8k-picture of Flickr. 8k-Flicker is a benchmark collection for sentence-based image description, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We obtained 445 pairs (out of 750 in the original).", "The dataset is derived from a subset of the caption pairs already annotated in the Semantic Textual Similarity Task (see below). We selected some caption pairs with their similarity annotations, and added the images corresponding to each caption. While the human annotators had access to only the text, we provide the system with both the caption and corresponding image, to check whether the visual representations can be exploited by the system to solve a text understanding and inference task.", "We introduced the vSTS dataset, which contains caption pairs with human similarity annotations, where the systems can also access the actual images. The dataset aims at being a standard dataset to test the contribution of visual information when evaluating the similarity of sentences.", "In this paper we present Visual Semantic Textual Similarity (vSTS), a dataset which allows to study whether better sentence representations can be built when having access to corresponding images, e.g. a caption and its image, in contrast with having access to the text alone. This dataset is based on a subset of the STS benchmark BIBREF1 , more specifically, the so called STS-images subset, which contains pairs of captions. Note that the annotations are based on the textual information alone. vSTS extends the existing subset with images, and aims at being a standard dataset to test the contribution of visual information when evaluating sentence representations."]}
{"question_id": "16edc21a6abc89ee2280dccf1c867c2ac4552524", "predicted_answer": "", "predicted_evidence": ["Subset 2015 The subset is derived from Image Descriptions dataset, which is a subset of 8k-picture of Flickr. 8k-Flicker is a benchmark collection for sentence-based image description, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We obtained 445 pairs (out of 750 in the original).", "Experiments confirmed our hypotheses: image representations are useful for caption similarity and they are complementary to textual representations, as results improve significantly when two modalities are combined together.", "As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:", "We introduced the vSTS dataset, which contains caption pairs with human similarity annotations, where the systems can also access the actual images. The dataset aims at being a standard dataset to test the contribution of visual information when evaluating the similarity of sentences.", "STS Models We checked four models of different complexity and modalities. The baseline is a word overlap model (overlap), in which input texts are tokenized with white space, vectorized according to a word index, and similarity is computed as the cosine of the vectors. We also calculated the centroid of Glove word embeddings BIBREF17 (caverage) and then computed the cosine as a second text-based model. The third text-based model is the state of the art Decomposable Attention Model BIBREF18 (dam), trained on the STS benchmark dataset as explained above. Finally, we use the top layer of a pretrained resnet50 model BIBREF19 to represent the images associated to text, and use the cosine for computing the similarity of a pair of images (resnet50)."]}
{"question_id": "3b8da74f5b359009d188cec02adfe4b9d46a768f", "predicted_answer": "", "predicted_evidence": ["Moreover, as mentioned before, RoBERTa could adapt to different masking strategies and acquires richer semantic representations with the dynamic masking strategy. In contrast, BERT and ERNIE use the static masking strategy in every epoch. In addition, the results in this paper show that the F1 value of ERNIE is slightly lower than BERT. We infer that ERNIE may introduce segmentation errors when performing entity-level and phrase-level masking.", "Named Entity Recognition (NER) is a basic and important task in Natural Language Processing (NLP). It aims to recognize and classify named entities, such as person names and location namesBIBREF0. Extracting named entities from unstructured data can benefit many NLP tasks, for example Knowledge Graph (KG), Decision-making Support System (DSS), and Question Answering system. Researchers used rule-based and machine learning methods for the NER in the early yearsBIBREF1BIBREF2. Recently, with the development of deep learning, deep neural networks have improved the performance of NER tasksBIBREF3BIBREF4. However, it may still be inefficient to use deep neural networks because the performance of these methods depends on the quality of labeled data in training sets while creating annotations for unstructured data is especially difficultBIBREF5. Therefore, researchers hope to find an efficient method to extract semantic and syntactic knowledge from a large amount of unstructured data, which is also unlabeled.", "It aims to recognize and classify named entities, such as person names and location namesBIBREF0. Extracting named entities from unstructured data can benefit many NLP tasks, for example Knowledge Graph (KG), Decision-making Support System (DSS), and Question Answering system. Researchers used rule-based and machine learning methods for the NER in the early yearsBIBREF1BIBREF2. Recently, with the development of deep learning, deep neural networks have improved the performance of NER tasksBIBREF3BIBREF4. However, it may still be inefficient to use deep neural networks because the performance of these methods depends on the quality of labeled data in training sets while creating annotations for unstructured data is especially difficultBIBREF5. Therefore, researchers hope to find an efficient method to extract semantic and syntactic knowledge from a large amount of unstructured data, which is also unlabeled. Then, apply the semantic and syntactic knowledge to improve the performance of NLP task effectively.", "In this paper, we exploit four pre-training models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa) for the NER task. Firstly, we introduce the architecture and pre-training tasks of these pre-training models. Then, we apply the pre-training models to the target task through a fine-tuning approach. During fine-tuning, we add a fully connection layer and a CRF layer after the output of pre-training models. Results showed that using the pre-training models significantly improved the performance of recognition. Moreover, results provided a basis that the structure and pre-training tasks in RoBERTa model are more suitable for NER tasks.", "The purpose of this paper is to introduce the structure and pre-training tasks of four common pre-trained models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa), and how to apply these models to a NER task by fine-tuning. Moreover, we also conduct experiments on the MSRA-2006 dataset to test the effects of different pre-training models on the NER task, and discuss the reasons for these results from the model architecture and pre-training tasks respectively."]}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "predicted_answer": "", "predicted_evidence": ["Named entity recognition (NER) is the basic task of the NLP, such as information extraction and data mining. The main goal of the NER is to extract entities (persons, places, organizations and so on) from unstructured documents. Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF.", "As mentioned above, the performance of deep learning methods depends on the quality of labeled training sets. Therefore, researchers have proposed pre-training models to improve the performance of the NLP tasks through a large number of unlabeled data. Recent research on pre-training models has mainly focused on BERT. For example, R. Qiao $et$ $al$. and N. Li $et$ $al$. BIBREF13BIBREF14 used BERT and ELMO respectively to improve the performance of entity recognition in chinese clinical records. E. Alsentzer $et$ $al$. , L. Yao $et$ $al$. and K. Huang $et$ $al$. BIBREF15BIBREF16BIBREF17 used domain-specific corpus to train BERT(the model structure and pre-training tasks are unchanged), and used this model for a domain-specific task, obtaining the result of SOTA.", "Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task. Ma and Hovy BIBREF11 used a combination of the Convolutional Neural Networks (CNN) and the LSTM-CRF to recognize entities. Chiu and Nichols BIBREF12 improved the BiLSTM-CNN model and tested it on the CoNLL-2003 corpus.", "Like ERNIE, RoBERTa has the same model structure as BERT, with 12 Transformer layers, 768 hidden units, and 12 self-attention heads.", "ERNIE2.0 is a continual pre-training framework. It could incrementally build and train a large variety of pre-training tasks through continual multi-task learning BIBREF19."]}
{"question_id": "37e6ce5cfc9d311e760dad8967d5085446125408", "predicted_answer": "", "predicted_evidence": ["It aims to recognize and classify named entities, such as person names and location namesBIBREF0. Extracting named entities from unstructured data can benefit many NLP tasks, for example Knowledge Graph (KG), Decision-making Support System (DSS), and Question Answering system. Researchers used rule-based and machine learning methods for the NER in the early yearsBIBREF1BIBREF2. Recently, with the development of deep learning, deep neural networks have improved the performance of NER tasksBIBREF3BIBREF4. However, it may still be inefficient to use deep neural networks because the performance of these methods depends on the quality of labeled data in training sets while creating annotations for unstructured data is especially difficultBIBREF5. Therefore, researchers hope to find an efficient method to extract semantic and syntactic knowledge from a large amount of unstructured data, which is also unlabeled. Then, apply the semantic and syntactic knowledge to improve the performance of NLP task effectively.", "In this section, we first introduce the four pre-trained models (BERT, ERNIE, ERNIE 2.0-tiny, RoBERTa), including their model structures and pre-training tasks. Then we introduce how to use them for the NER task through fine-tuning.", "Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task. Ma and Hovy BIBREF11 used a combination of the Convolutional Neural Networks (CNN) and the LSTM-CRF to recognize entities. Chiu and Nichols BIBREF12 improved the BiLSTM-CNN model and tested it on the CoNLL-2003 corpus.", "Named entity recognition (NER) is the basic task of the NLP, such as information extraction and data mining. The main goal of the NER is to extract entities (persons, places, organizations and so on) from unstructured documents. Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task.", "The main goal of the NER is to extract entities (persons, places, organizations and so on) from unstructured documents. Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task. Ma and Hovy BIBREF11 used a combination of the Convolutional Neural Networks (CNN) and the LSTM-CRF to recognize entities."]}
{"question_id": "6683008e0a8c4583058d38e185e2e2e18ac6cf50", "predicted_answer": "", "predicted_evidence": ["It aims to recognize and classify named entities, such as person names and location namesBIBREF0. Extracting named entities from unstructured data can benefit many NLP tasks, for example Knowledge Graph (KG), Decision-making Support System (DSS), and Question Answering system. Researchers used rule-based and machine learning methods for the NER in the early yearsBIBREF1BIBREF2. Recently, with the development of deep learning, deep neural networks have improved the performance of NER tasksBIBREF3BIBREF4. However, it may still be inefficient to use deep neural networks because the performance of these methods depends on the quality of labeled data in training sets while creating annotations for unstructured data is especially difficultBIBREF5. Therefore, researchers hope to find an efficient method to extract semantic and syntactic knowledge from a large amount of unstructured data, which is also unlabeled. Then, apply the semantic and syntactic knowledge to improve the performance of NLP task effectively.", "This section discusses the experimental results in detail. We will analyze the different model structures and pre-training tasks on the effect of the NER task.", "Recent theoretical developments have revealed that word embeddings have shown to be effective for improving many NLP tasks. The Word2Vec and Glove models represent a word as a word embedding, where similar words have similar word embeddingsBIBREF6. However, the Word2Vec and Glove models can not solve the problem of polysemy. Researchers have proposed some pre-training models, such as BERT, ERNIE, and RoBERTa, to learn contextualized word embeddings from unstructured text corpusBIBREF7BIBREF8BIBREF9. These models not only solve the problem of polysemy but also obtain more accurate word representations. Therefore, researchers pay more attention to how to apply these pre-training models to improve the performance of NLP tasks.", "ERNIE has the same model structure as BERT-base, which uses 12 Transformer encoder layers, 768 hidden units and 12 attention heads.", "ERNIE2.0 is a continual pre-training framework. It could incrementally build and train a large variety of pre-training tasks through continual multi-task learning BIBREF19."]}
{"question_id": "7bd24920163a4801b34d0a50aed957ba8efed0ab", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF26, the results were consistent with those on ABSA. From the results, BERT-Attention and BERT-LSTM perform better than vanilla BERT$_{\\tiny \\textsc {BASE}}$. Furthermore, MT-DNN-Attention and MT-DNN-LSTM outperform vanilla MT-DNN on Dev set, and are slightly inferior to vanilla MT-DNN on Test set. As a whole, our pooling strategies generally improve the vanilla BERT-based model, which draws the same conclusion as on ABSA.", "Given a pair of sentences, the goal is to predict whether a sentence is an entailment, contradiction, or neutral with respect to the other sentence.", "Some of the most prominent examples are ELMo BIBREF2, GPT BIBREF3 and BERT BIBREF4. BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. The pre-trained BERT model can then be fine-tuned on downstream task with task-specific training data. Sun et al., sun2019utilizing utilize BERT for ABSA task by constructing a auxiliary sentences, Xu et al., xu2019bert propose a post-training approach for ABSA task, and Liu et al., liu2019multi combine multi-task learning and pre-trained BERT to improve the performance of various NLP tasks. However, these BERT-based studies follow the canonical way of fine-tuning: append just an additional output layer after BERT structure. This fine-tuning approach ignores the rich semantic knowledge contained in the intermediate layers. Due to the multi-layer structure of BERT, different layers capture different levels of representations for the specific task after fine-tuning.", "Some of the most prominent examples are ELMo BIBREF2, GPT BIBREF3 and BERT BIBREF4. BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. The pre-trained BERT model can then be fine-tuned on downstream task with task-specific training data. Sun et al., sun2019utilizing utilize BERT for ABSA task by constructing a auxiliary sentences, Xu et al., xu2019bert propose a post-training approach for ABSA task, and Liu et al., liu2019multi combine multi-task learning and pre-trained BERT to improve the performance of various NLP tasks. However, these BERT-based studies follow the canonical way of fine-tuning: append just an additional output layer after BERT structure. This fine-tuning approach ignores the rich semantic knowledge contained in the intermediate layers.", "Aspect based sentiment analysis (ABSA) is an important task in natural language processing. It aims at collecting and analyzing the opinions toward the targeted aspect in an entire text. In the past decade, ABSA has received great attention due to a wide range of applications BIBREF0, BIBREF1. Aspect-level (also mentioned as \u201ctarget-level\u201d) sentiment classification as a subtask of ABSA BIBREF0 aims at judging the sentiment polarity for a given aspect. For example, given a sentence \u201cI hated their service, but their food was great\u201d, the sentiment polarities for the target \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively."]}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "predicted_answer": "", "predicted_evidence": ["where $W_h^T$ and $\\mathbf {q}$ are learnable weights.", "Since the sizes of ABSA datasets are small and there is no validation set, the results between two consecutive epochs may be significantly different. In order to conduct fair and rigorous experiments, we use 10-fold cross-validation for ABSA task, which achieves quite stable results. The final result is obtained as the average of 10 individual experiments.", "We use three popular datasets in ABSA task: Restaurant reviews and Laptop reviews from SemEval 2014 Task 4 BIBREF5, and ACL 14 Twitter dataset BIBREF6.", "Intuitively, attention operation can learn the contribution of each $h_{\\tiny \\textsc {CLS}}^i$. We use a dot-product attention module to dynamically combine all intermediates:", "Some of the most prominent examples are ELMo BIBREF2, GPT BIBREF3 and BERT BIBREF4. BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. The pre-trained BERT model can then be fine-tuned on downstream task with task-specific training data. Sun et al., sun2019utilizing utilize BERT for ABSA task by constructing a auxiliary sentences, Xu et al., xu2019bert propose a post-training approach for ABSA task, and Liu et al., liu2019multi combine multi-task learning and pre-trained BERT to improve the performance of various NLP tasks. However, these BERT-based studies follow the canonical way of fine-tuning: append just an additional output layer after BERT structure. This fine-tuning approach ignores the rich semantic knowledge contained in the intermediate layers. Due to the multi-layer structure of BERT, different layers capture different levels of representations for the specific task after fine-tuning."]}
{"question_id": "a7a433de17d0ee4dd7442d7df7de17e508baf169", "predicted_answer": "", "predicted_evidence": ["In this work, we explore the potential of utilizing BERT intermediate layers and propose two effective pooling strategies to enhance the performance of fine-tuning of BERT. Experimental results demonstrate the effectiveness and generality of the proposed approach.", "Main contributions of this paper can be summarized as follows:", "It is the first to explore the potential of utilizing intermediate layers of BERT and we design two effective information pooling strategies to solve aspect based sentiment analysis task.", "Aspect based sentiment analysis (ABSA) is an important task in natural language processing. It aims at collecting and analyzing the opinions toward the targeted aspect in an entire text. In the past decade, ABSA has received great attention due to a wide range of applications BIBREF0, BIBREF1. Aspect-level (also mentioned as \u201ctarget-level\u201d) sentiment classification as a subtask of ABSA BIBREF0 aims at judging the sentiment polarity for a given aspect. For example, given a sentence \u201cI hated their service, but their food was great\u201d, the sentiment polarities for the target \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively.", "Pre-trained language models can leverage large amounts of unlabeled data to learn the universal language representations, which provide an effective solution for the above problem. Some of the most prominent examples are ELMo BIBREF2, GPT BIBREF3 and BERT BIBREF4. BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. The pre-trained BERT model can then be fine-tuned on downstream task with task-specific training data. Sun et al., sun2019utilizing utilize BERT for ABSA task by constructing a auxiliary sentences, Xu et al., xu2019bert propose a post-training approach for ABSA task, and Liu et al., liu2019multi combine multi-task learning and pre-trained BERT to improve the performance of various NLP tasks."]}
{"question_id": "abfa3daaa984dfe51289054f4fb062ce93f31d19", "predicted_answer": "", "predicted_evidence": ["where $W_h^T$ and $\\mathbf {q}$ are learnable weights.", "In this section, we present our methods for BERT-based model fine-tuning on three ABSA datasets. To show the generality, we also conduct experiments on a large and popular NLI task. We also apply the same strategy to existing state-of-the-art BERT-based models and demonstrate the effectiveness of our approaches.", "Pre-trained language models can leverage large amounts of unlabeled data to learn the universal language representations, which provide an effective solution for the above problem. Some of the most prominent examples are ELMo BIBREF2, GPT BIBREF3 and BERT BIBREF4. BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. The pre-trained BERT model can then be fine-tuned on downstream task with task-specific training data. Sun et al., sun2019utilizing utilize BERT for ABSA task by constructing a auxiliary sentences, Xu et al., xu2019bert propose a post-training approach for ABSA task, and Liu et al., liu2019multi combine multi-task learning and pre-trained BERT to improve the performance of various NLP tasks. However, these BERT-based studies follow the canonical way of fine-tuning: append just an additional output layer after BERT structure.", "Pre-trained language models can leverage large amounts of unlabeled data to learn the universal language representations, which provide an effective solution for the above problem. Some of the most prominent examples are ELMo BIBREF2, GPT BIBREF3 and BERT BIBREF4. BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. The pre-trained BERT model can then be fine-tuned on downstream task with task-specific training data. Sun et al., sun2019utilizing utilize BERT for ABSA task by constructing a auxiliary sentences, Xu et al., xu2019bert propose a post-training approach for ABSA task, and Liu et al., liu2019multi combine multi-task learning and pre-trained BERT to improve the performance of various NLP tasks.", "This section briefly describes three ABSA datasets and SNLI dataset. Statistics of these datasets are shown in Table TABREF15."]}
{"question_id": "1702985a3528e876bb19b8e223399729d778b4e4", "predicted_answer": "", "predicted_evidence": ["Language is evolving with the flattening world order and the pervasiveness of the social media in fusing culture and bridging relationships at a click. One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3.", "Language is evolving with the flattening world order and the pervasiveness of the social media in fusing culture and bridging relationships at a click. One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions.", "The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains.", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text."]}
{"question_id": "f44a9ed166a655df1d54683c91935ab5e566a04f", "predicted_answer": "", "predicted_evidence": ["One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text.", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "Language is evolving with the flattening world order and the pervasiveness of the social media in fusing culture and bridging relationships at a click. One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3.", "The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains.", "VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9. The possibility of improving sentiment detection via label transfer from monolingual to synthetic code-switched text has been well executed with significant improvements in sentiment labelling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs BIBREF5"]}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "predicted_answer": "", "predicted_evidence": ["One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text.", "Language is evolving with the flattening world order and the pervasiveness of the social media in fusing culture and bridging relationships at a click. One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions.", "VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9. The possibility of improving sentiment detection via label transfer from monolingual to synthetic code-switched text has been well executed with significant improvements in sentiment labelling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs BIBREF5", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers."]}
{"question_id": "d14118b18ee94dafe170439291e20cb19ab7a43c", "predicted_answer": "", "predicted_evidence": ["But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.", "One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9.", "One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text.", "Language is evolving with the flattening world order and the pervasiveness of the social media in fusing culture and bridging relationships at a click. One of the consequences of the conversational evolution is the intrasentential code switching, a language alternation in a single discourse between two languages, where the switching occurs within a sentence BIBREF0. The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3."]}
{"question_id": "d922eaa5aa135c1ae211827c6a599b4d69214563", "predicted_answer": "", "predicted_evidence": ["GRU has two gates, a reset gate INLINEFORM0 , and an update gate INLINEFORM1 . Intuitively, the reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. We use Keras GRNN implementation to setup our experiments. We note that GRU units are a concatenation of GRU layers in each task.", "The rest of this paper is structured as followings: section SECREF2 introduce our neural net model, in section SECREF3 we explain the experimental setup and data that is been used for training and development sets, section SECREF4 discuss the results and analyze the errors, section SECREF5 describe related works, section SECREF6 conclude our study and discuss future direction.", "Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.", "GRU- has been widely used in the literature to model sequential problems. RNN applies the same set of weights recursively as follow: DISPLAYFORM0", "Attention layer - GRUs update their hidden state h(t) as they process a sequence and the final hidden state holds the summation of all other history information. Attention layer BIBREF2 modifies this process such that representation of each hidden state is an output in each GRU unit to analyze whether this is an important feature for prediction."]}
{"question_id": "ff668c7e890064756cdd2f9621e1cedb91eef1d0", "predicted_answer": "", "predicted_evidence": ["Processing ELMo and attention is computationally very expensive, among our models GRU-att-ELMo+F has the longest training time and GRU-att-fasttext has the fastest training time. Results are shown in table TABREF21 and table refemoresultss", "We achieved the best results combining ELMo with contextual information, and achieve %85.54 f-score overall, including class others. In this task we achieved %56.04 f-score overall for emotion classes, which indicates our model needs to improve the identification of emotion. Table TABREF22 shows our model performance on each emotion tag. The results show a low performance of the model for emotion tag happy, which is due to our data being out of domain. Most of the confusion and errors are happened among the emotion categories, which suggest further investigation and improvement. We achieved %90.48, %60.10, %60.19, %49.38 f-score for class others, angry, sad, and happy respectfully.", "The results indicates the impact of contextual information using different embeddings, which are different in feature representation. Results of class happy without contextual features has %44.16 by GRU-att-ELMo model, and %49.38 by GRU-att-ELMo+F.", "Data pre-processing - we tokenize all the data. For tweets we replace all the URLs, image URLs, hashtags, @users with specific anchors. Based on the popularity of each emoticon per each emotion tag, we replace them with the corresponding emotion tag. We normalized all the repeated characters, finally caps words are replaced with lower case but marked as caps words.", "Sentiment and objective Information (SOI)- relativity of subjectivity and sentiment with emotion are well studied in the literature. To craft these features we use SentiwordNet BIBREF5 , we create sentiment and subjective score per word in each sentences. SentiwordNet is the result of the automatic annotation of all the synsets of WORDNET according to the notions of positivity, negativity, and neutrality. Each synset s in WORDNET is associated to three numerical scores Pos(s), Neg(s), and Obj(s) which indicate how positive, negative, and objective (i.e., neutral) the terms contained in the synset are. Different senses of the same term may thus have different opinion-related properties. These scores are presented per sentence and their lengths are equal to the length of each sentence. In case that the score is not available, we used a fixed score 0.001."]}
{"question_id": "d3cfbe497a30b750a8de3ea7f2cecf4753a4e1f9", "predicted_answer": "", "predicted_evidence": ["GRU has two gates, a reset gate INLINEFORM0 , and an update gate INLINEFORM1 . Intuitively, the reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. We use Keras GRNN implementation to setup our experiments. We note that GRU units are a concatenation of GRU layers in each task.", "The rest of this paper is structured as followings: section SECREF2 introduce our neural net model, in section SECREF3 we explain the experimental setup and data that is been used for training and development sets, section SECREF4 discuss the results and analyze the errors, section SECREF5 describe related works, section SECREF6 conclude our study and discuss future direction.", "In recent studies, deep learning models have achieved top performances in emotion detection and classification. Access to large amount of data has contributed to these high results. Numerous efforts have been dedicated to build emotion classification models, and successful results have been reported. In this work, we combine several popular emotional data sets in different genres, plus the one given for this task to train the emotion model we developed. We introduce a multigenre training mechanism, our intuition to combine different genres are a) to augment more training data, b) to generalize detection of emotion. We utilize Portable textual information such as subjectivity, sentiment, and presence of emotion words, because emotional sentences are subjective and affectual states like sentiment are strong indicator for presence of emotion.", "Gates Recurrent Neural Network (GRU) BIBREF0 , BIBREF1 and attention layer are used in sequential NLP problems and successful results are reported in different studies. Figure FIGREF11 shows the diagram of our model.", "Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results."]}
{"question_id": "73d87f6ead32653a518fbe8cdebd81b4a3ffcac0", "predicted_answer": "", "predicted_evidence": ["For a formal description of unfolding we address layers with indices INLINEFORM0 . The special layer 0 has a single neuron for modelling bias vectors. Layer 1 holds the input neurons and layer INLINEFORM1 is the output layer. We denote the size of a layer in the individual models as INLINEFORM2 . When combining INLINEFORM3 networks, the layer size INLINEFORM4 in the unfolded network is increased by factor INLINEFORM5 if INLINEFORM6 is an inner layer, and equal to INLINEFORM7 if INLINEFORM8 is the input or output layer. We denote the weight matrix between two layers INLINEFORM9 in the INLINEFORM10 -th individual model ( INLINEFORM11 ) as INLINEFORM12 , and the corresponding weight matrix in the unfolded network as INLINEFORM13 . We explicitly allow INLINEFORM14 and INLINEFORM15 to be non-consecutive or reversed to be able to model recurrent networks.", "Therefore, we introduce a novel algorithm based on linear combinations of neurons which can be applied either during training (data-bound) or directly on the weight matrices without using training data (data-free). We report that with a mix of the presented shrinking methods we are able to reduce the size of the unfolded network to the size of the single NMT network while keeping the boost in BLEU score from the ensemble. Depending on the aggressiveness of shrinking, we report either a gain of 2.2 BLEU at the same decoding speed, or a 3.4 INLINEFORM1 CPU decoding speed up with only a minor drop in BLEU compared to the original single NMT system. Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints. Unfolding and shrinking address these problems directly.", "The special layer 0 has a single neuron for modelling bias vectors. Layer 1 holds the input neurons and layer INLINEFORM1 is the output layer. We denote the size of a layer in the individual models as INLINEFORM2 . When combining INLINEFORM3 networks, the layer size INLINEFORM4 in the unfolded network is increased by factor INLINEFORM5 if INLINEFORM6 is an inner layer, and equal to INLINEFORM7 if INLINEFORM8 is the input or output layer. We denote the weight matrix between two layers INLINEFORM9 in the INLINEFORM10 -th individual model ( INLINEFORM11 ) as INLINEFORM12 , and the corresponding weight matrix in the unfolded network as INLINEFORM13 . We explicitly allow INLINEFORM14 and INLINEFORM15 to be non-consecutive or reversed to be able to model recurrent networks. We use the zero-matrix if layers INLINEFORM16 and INLINEFORM17 are not connected. The construction of the unfolded weight matrix INLINEFORM18 from the individual matrices INLINEFORM19 depends on whether the connected layers are inner layers or not.", "The first concept of our approach is called unfolding. Unfolding is an alternative to ensembling of multiple neural networks with the same topology. Rather than averaging their predictions, unfolding constructs a single large neural net out of the individual models which has the same number of input and output neurons but larger inner layers. Our main motivation for unfolding is to obtain a single network with ensemble level performance which can be shrunk with the techniques in Sec. SECREF3 .", "Suppose we ensemble two single layer feedforward neural nets as shown in Fig. FIGREF1 . Normally, ensembling is implemented by performing an isolated forward pass through the first network (Fig. SECREF2 ), another isolated forward pass through the second network (Fig. SECREF3 ), and averaging the activities in the output layers of both networks. This can be simulated by merging both networks into a single large network as shown in Fig. SECREF4 . The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network. A single pass through the combined network yields the same output as the ensemble if the output layer is linear (up to a factor 2). The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2. This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention."]}
{"question_id": "fda47c68fd5f7b44bd539f83ded5882b96c36dd7", "predicted_answer": "", "predicted_evidence": ["We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets.", "Unfolded NMT networks approximate but do not exactly match the output of the ensemble due to two reasons. First, the unfolded network synchronizes the attentions of the individual models. Each decoding step in the unfolded network computes a single attention weight vector. In contrast, ensemble decoding would compute one attention weight vector for each of the INLINEFORM0 input models. A second difference is that the ensemble decoder first applies the softmax at the output layer, and then averages the prediction probabilities. The unfolded network averages the neuron activities (i.e. the logits) first, and then applies the softmax function. Interestingly, as shown in Sec. SECREF4 , these differences do not have any impact on the BLEU score but yield potential speed advantages of unfolding since the computationally expensive softmax layer is only applied once.", "Although we find our data-free approach to be a substantial improvement over the methods of Srinivas and Babu sparsify-datafree on NMT networks, it still leads to a non-negligible decline in BLEU score when applied to recurrent GRU layers. Our data-free method uses the incoming weights to identify similar neurons, i.e. neurons expected to have similar activities. This works well enough for simple layers, but the interdependencies between the states and the gates inside gated layers like GRUs or LSTMs are complex enough that redundancies cannot be found simply by looking for similar weights. In the spirit of Babaeizadeh et al. sparsify-noiseout, our data-bound version records neuron activities during training to estimate INLINEFORM0 . We compensate for the removal of the INLINEFORM1 -th neuron by using a linear combination of the output of remaining neurons with similar activity patterns.", "Ensembling consistently outperforms single NMT by a large margin. However, the decoding speed is significantly worse since the decoder needs to apply INLINEFORM0 NMT models rather than only one. Therefore, a recent line of research transfers the idea of knowledge distillation BIBREF11 , BIBREF12 to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system (the teacher) BIBREF13 , BIBREF14 . This paper presents an alternative to knowledge distillation as we aim to speed up decoding to be comparable to single NMT while retaining the boost in translation accuracy from the ensemble. In a first step, we describe how to construct a single large neural network which imitates the output of an ensemble of multiple networks with the same topology. We will refer to this process as unfolding.", "GPU-based decoding with the unfolded network is often much faster than ensemble decoding since more work can be done on the GPU. In a second step, we explore methods to reduce the size of the unfolded network. This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy BIBREF15 , BIBREF16 , BIBREF17 . Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices. More importantly, the decoding speed on all platforms benefits greatly from the reduced number of neurons. We find that the dimensionality of linear embedding layers in the NMT network can be reduced heavily by low-rank matrix approximation based on singular value decomposition (SVD). This suggest that high dimensional embedding layers may be needed for training, but do not play an important role for decoding. The NMT network, however, also consists of complex layers like gated recurrent units BIBREF18 and attention BIBREF19 ."]}
{"question_id": "643645e02ffe8fde45918615ec92013a035d1b92", "predicted_answer": "", "predicted_evidence": ["Our data-free method uses the incoming weights to identify similar neurons, i.e. neurons expected to have similar activities. This works well enough for simple layers, but the interdependencies between the states and the gates inside gated layers like GRUs or LSTMs are complex enough that redundancies cannot be found simply by looking for similar weights. In the spirit of Babaeizadeh et al. sparsify-noiseout, our data-bound version records neuron activities during training to estimate INLINEFORM0 . We compensate for the removal of the INLINEFORM1 -th neuron by using a linear combination of the output of remaining neurons with similar activity patterns. In each layer, we prune 40 neurons each 450 training iterations until the target layer size is reached. Let INLINEFORM2 be the matrix which holds the records of neuron activities in the layer since the last removal.", "Suppose we ensemble two single layer feedforward neural nets as shown in Fig. FIGREF1 . Normally, ensembling is implemented by performing an isolated forward pass through the first network (Fig. SECREF2 ), another isolated forward pass through the second network (Fig. SECREF3 ), and averaging the activities in the output layers of both networks. This can be simulated by merging both networks into a single large network as shown in Fig. SECREF4 . The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network. A single pass through the combined network yields the same output as the ensemble if the output layer is linear (up to a factor 2). The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2. This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention.", "For a formal description of unfolding we address layers with indices INLINEFORM0 . The special layer 0 has a single neuron for modelling bias vectors. Layer 1 holds the input neurons and layer INLINEFORM1 is the output layer. We denote the size of a layer in the individual models as INLINEFORM2 . When combining INLINEFORM3 networks, the layer size INLINEFORM4 in the unfolded network is increased by factor INLINEFORM5 if INLINEFORM6 is an inner layer, and equal to INLINEFORM7 if INLINEFORM8 is the input or output layer. We denote the weight matrix between two layers INLINEFORM9 in the INLINEFORM10 -th individual model ( INLINEFORM11 ) as INLINEFORM12 , and the corresponding weight matrix in the unfolded network as INLINEFORM13 . We explicitly allow INLINEFORM14 and INLINEFORM15 to be non-consecutive or reversed to be able to model recurrent networks. We use the zero-matrix if layers INLINEFORM16 and INLINEFORM17 are not connected.", "We denote the size of a layer in the individual models as INLINEFORM2 . When combining INLINEFORM3 networks, the layer size INLINEFORM4 in the unfolded network is increased by factor INLINEFORM5 if INLINEFORM6 is an inner layer, and equal to INLINEFORM7 if INLINEFORM8 is the input or output layer. We denote the weight matrix between two layers INLINEFORM9 in the INLINEFORM10 -th individual model ( INLINEFORM11 ) as INLINEFORM12 , and the corresponding weight matrix in the unfolded network as INLINEFORM13 . We explicitly allow INLINEFORM14 and INLINEFORM15 to be non-consecutive or reversed to be able to model recurrent networks. We use the zero-matrix if layers INLINEFORM16 and INLINEFORM17 are not connected. The construction of the unfolded weight matrix INLINEFORM18 from the individual matrices INLINEFORM19 depends on whether the connected layers are inner layers or not. The complete formula is listed in Fig.", "Neuron INLINEFORM0 is selected for removal if (1) there is another neuron INLINEFORM1 which has a very similar set of incoming weights and if (2) INLINEFORM2 has a small outgoing weight vector. Their criterion is data-free since it does not require any training data. For further details we refer to Srinivas and Babu sparsify-datafree."]}
{"question_id": "a994cc18046912a8c9328dc572f4e4310736c0e2", "predicted_answer": "", "predicted_evidence": ["Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De.", "In the spirit of Babaeizadeh et al. sparsify-noiseout, our data-bound version records neuron activities during training to estimate INLINEFORM0 . We compensate for the removal of the INLINEFORM1 -th neuron by using a linear combination of the output of remaining neurons with similar activity patterns. In each layer, we prune 40 neurons each 450 training iterations until the target layer size is reached. Let INLINEFORM2 be the matrix which holds the records of neuron activities in the layer since the last removal. For example, for the decoder GRU layer, a batch size of 80, and target sentence lengths of 20, INLINEFORM3 has INLINEFORM4 rows and INLINEFORM5 (the number of neurons in the layer) columns. Similarly to Eq. EQREF10 we find interpolation weights INLINEFORM6 using the method of least squares on the following linear system. DISPLAYFORM0", "The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al.", "The standard attention-based NMT network architecture BIBREF19 includes three linear layers: the embedding layer in the encoder, and the output and feedback embedding layers in the decoder. We have found that linear layers are particularly easy to shrink using low-rank matrix approximation. As before we denote the incoming weight matrix as INLINEFORM0 and the outgoing weight matrix as INLINEFORM1 . Since the layer is linear, we could directly connect the previous layer with the next layer using the product of both weight matrices INLINEFORM2 . However, INLINEFORM3 may be very large. Therefore, we approximate INLINEFORM4 as a product of two low rank matrices INLINEFORM5 and INLINEFORM6 ( INLINEFORM7 ) where INLINEFORM8 is the desired layer size. A very common way to find such a matrix factorization is using truncated singular value decomposition (SVD). The layer is eventually shrunk by replacing INLINEFORM9 with INLINEFORM10 and INLINEFORM11 with INLINEFORM12 .", "The data-bound algorithm runs gradient-based optimization on the unfolded network. We use the AdaGrad BIBREF20 step rule, a small learning rate of 0.0001, and aggressive step clipping at 0.05 to avoid destroying useful weights which were learned in the individual networks prior to the construction of the unfolded network."]}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "predicted_answer": "", "predicted_evidence": ["Eventually we extracted conversation topics by running a spectral clustering algorithm on the word-to-word similarity matrix INLINEFORM0 with INLINEFORM1 vocabulary size and elements defined as the INLINEFORM2 cosine similarity between word vectors. Here INLINEFORM3 is a vector of a word INLINEFORM4 in the embedding, INLINEFORM5 is the dot product of vectors, and INLINEFORM6 is the INLINEFORM7 norm of a vector. This definition allows for negative entries in the matrix to cluster, which were set to null in our case. This is consistent with the goal of the clustering procedure as negative similarities shouldn't encode dissimilarity between pairs of words but orthogonality between the embeddings. This procedure was run for 50, 100 and 200 clusters and allowed the homogeneous distribution of words among clusters (hard clustering). The best results were obtained with 100 topics in the topic model.", "Here INLINEFORM3 is a vector of a word INLINEFORM4 in the embedding, INLINEFORM5 is the dot product of vectors, and INLINEFORM6 is the INLINEFORM7 norm of a vector. This definition allows for negative entries in the matrix to cluster, which were set to null in our case. This is consistent with the goal of the clustering procedure as negative similarities shouldn't encode dissimilarity between pairs of words but orthogonality between the embeddings. This procedure was run for 50, 100 and 200 clusters and allowed the homogeneous distribution of words among clusters (hard clustering). The best results were obtained with 100 topics in the topic model. Finally, we manually labeled topics based on the words assigned to them, and computed the topic-to-topic correlation matrix shown in Fig. FIGREF18 . There, after block diagonalization, we found clearly correlated groups of topics which could be associated to larger topical areas such as communication, advertisement or soccer.", "Here INLINEFORM3 is a vector of a word INLINEFORM4 in the embedding, INLINEFORM5 is the dot product of vectors, and INLINEFORM6 is the INLINEFORM7 norm of a vector. This definition allows for negative entries in the matrix to cluster, which were set to null in our case. This is consistent with the goal of the clustering procedure as negative similarities shouldn't encode dissimilarity between pairs of words but orthogonality between the embeddings. This procedure was run for 50, 100 and 200 clusters and allowed the homogeneous distribution of words among clusters (hard clustering). The best results were obtained with 100 topics in the topic model. Finally, we manually labeled topics based on the words assigned to them, and computed the topic-to-topic correlation matrix shown in Fig. FIGREF18 .", "As a result we could compute a representative topic distribution for each user, defined as a vector of normalized usage frequency of words from each topic. Also note that the topic distribution for a given user was automatically obtained as it depends only on the set of tweets and the learned topic clusters without further parametrization.", "To find users with a representative home location we followed the method published in BIBREF24 , BIBREF25 . As a bottom line, we concentrated on INLINEFORM0 users who posted at least five geolocated tweets with valid GPS coordinates, with at least three of them within a valid census cell (for definition see later), and over a longer period than seven days. Applying these filters we obtained 1,000,064 locations from geolocated tweets. By focusing on the geolocated users, we kept those with limited mobility, i.e., with median distance between locations not greater than 30 km, with tweets posted at places and times which did not require travel faster than 130 INLINEFORM1 (maximum speed allowed within France), and with no more than three tweets within a two seconds window. We further filtered out tweets with coordinates corresponding to locations referring to places (such as \u201cParis\" or \u201cFrance\"). Thus, we removed locations that didn't exactly correspond to GPS-tagged tweets and also users which were most likely bots."]}
{"question_id": "2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "predicted_answer": "", "predicted_evidence": ["In this work we proposed a novel methodology for the inference of the SES of Twitter users. We built our models combining information obtained from numerous sources, including Twitter, census data, LinkedIn and Google Maps. We developed precise methods of home location inference from geolocation, novel annotation of remotely sensed images of living environments, and effective combination of datasets collected from multiple sources. As new scientific results, we demonstrated that within the French Twitter space, the utilization of words in different topic categories, identified via advanced semantic analysis of tweets, can discriminate between people of different income. More importantly, we presented a proof-of-concept that our methods are competitive in terms of SES inference when compared to other methods relying on domain specific information.", "Despite these shortcomings, using all the three datasets we were able to infer SES with performances close to earlier reported results, which were based on more thoroughly annotated datasets. Our results, and our approach of using open, crawlable, or remotely sensed data highlights the potential of the proposed methodologies.", "We hence used it for our predictions in the remainder of this study. We found that the LinkedIn data was the best, with INLINEFORM0 , to train a model to predict SES of people based on their semantic features. It provided a INLINEFORM1 increase in performance as compared to the census based inference with INLINEFORM2 , and INLINEFORM3 relative to expert annotated data with INLINEFORM4 . Thus we can conclude that there seem to be a trade-off between scalability and prediction quality, as while the occupation dataset provided the best results, it seems unlikely to be subject to any upscaling due to the high cost of obtaining a clean dataset. Relying on location to estimate SES seems to be more likely to benefit from such an approach, though at the cost of an increased number of mislabelled users in the dataset. Moreover, the annotator's estimation of SES using Street View at each home location seems to be hindered by the large variability of urban features. Note that even though inter-agreement is 76%, the Cohen's kappa score for annotator inter-agreement is low at 0.169.", "Finally, motivated by recent remote sensing techniques, we sought to estimate SES via the analysis of the urban environment around the inferred home locations. Similar methodology has been lately reported by the remote sensing community BIBREF34 to predict socio-demographic features of a given neighborhood by analyzing Google Street View images to detect different car models, or to predict poverty rates across urban areas in Africa from satellite imagery BIBREF35 . Driven by this line of work, we estimated the SES of geolocated Twitter users as follows:", "Finally, it should also be noted that following recent work by Aletras and Chamberlain in BIBREF21 , we tested our model by extending the feature set with the node2vec embedding of users computed from the mutual mention graph of Twitter. Nevertheless, in our setting, it did not increase the overall predictive performance of the inference pipeline. We hence didn't include in the feature set for the sake of simplicity."]}
{"question_id": "892ee7c2765b3764312c3c2b6f4538322efbed4e", "predicted_answer": "", "predicted_evidence": ["We provide in Section SECREF2 an overview of the related literature to contextualize the novelty of our work. In Section SECREF3 we provide a detailed description of the data collection and combination methods. In Section SECREF4 we introduce the features extracted to solve the SES inference problem, with results summarized in Section SECREF5 . Finally, in Section SECREF6 and SECREF7 we conclude our paper with a brief discussion of the limitations and perspectives of our methods.", "INLINEFORM0 Occupation data: LinkedIn as a professional online social network is predominantly used by people from IT, business, management, marketing or other expert areas, typically associated with higher education levels and higher salaries. Moreover, we could observe only users who shared their professional profiles on Twitter, which may further biased our training set. In terms of occupational-salary classification, the data in BIBREF32 was collected in 2010 thus may not contain more recent professions. These biases may induce limits in the representativeness of our training data and thus in the predictions' precision. However, results based on this method of SES annotation performed best in our measurements, indicating that professions are among the most predictive features of SES, as has been reported in BIBREF9 .", "This dataset contains 2100 INLINEFORM1 INLINEFORM2 aerial RGB images over 21 classes of different land use (for a pair of sample images see Fig. FIGREF12 b). To classify land use a CaffeNet architecture was trained which reached an accuracy over INLINEFORM3 . Here, we instantiated a ResNet50 network using keras BIBREF39 pre-trained on ImageNet BIBREF40 where all layers except the last five were frozen. The network was then trained with 10-fold cross validation achieving a INLINEFORM4 accuracy after the first 100 epochs. We used this model to classify images of the estimated home location satellite views (cf. Figure FIGREF12 a) and kept those which were identified as residential areas (see Fig. FIGREF12 b, showing the activation of the two first hidden layers of the trained model). This way INLINEFORM5 inferred home locations were discarded.", "For each socioeconomic dataset, we trained our models by using 75% of the available data for training and the remaining 25% for testing. During the training phase, the training data undergoes a INLINEFORM0 -fold inner cross-validation, with INLINEFORM1 , where all splits are computed in a stratified manner to get the same ratio of lower to higher SES users. The four first blocks were used for inner training and the remainder for inner testing. This was repeated ten times for each model so that in the end, each model's performance on the validation set was averaged over 50 samples. For each model, the parameters were fine-tuned by training 500 different models over the aforementioned splits. The selected one was that which gave the best performance on average, which was then applied to the held-out test set. This is then repeated through a 5-fold outer cross-validation.", "Our first method to associate SES to geolocated users builds on an open census income dataset at intra-urban level for France BIBREF27 . Obtained from 2010 French tax returns, it was released in December 2016 by the National Institute of Statistics and Economic Studies (INSEE) of France. This dataset collects detailed socioeconomic information of individuals at the census block level (called IRIS), which are defined as territorial cells with varying size but corresponding to blocks of around INLINEFORM0 inhabitants, as shown in Fig. FIGREF7 for greater Paris. For each cell, the data records the deciles of the income distribution of inhabitants. Note that the IRIS data does not provide full coverage of the French territory, as some cells were not reported to avoid identification of individuals (in accordance with current privacy laws), or to avoid territorial cells of excessive area. Nevertheless, this limitation did not hinder our results significantly as we only considered users who posted at least three times from valid IRIS cells, as explained in Section SECREF3 ."]}
{"question_id": "c68946ae2e548ec8517c7902585c032b3f3876e6", "predicted_answer": "", "predicted_evidence": ["We thank J-Ph. Magu\u00e9, J-P. Chevrot, D. Seddah, D. Carnino and E. De La Clergerie for constructive discussions and for their advice on data management and analysis. We are grateful to J. Altn\u00e9der and M. Hunyadi for their contributions as expert architects for data annotation.", "INLINEFORM0 Occupation data: LinkedIn as a professional online social network is predominantly used by people from IT, business, management, marketing or other expert areas, typically associated with higher education levels and higher salaries. Moreover, we could observe only users who shared their professional profiles on Twitter, which may further biased our training set. In terms of occupational-salary classification, the data in BIBREF32 was collected in 2010 thus may not contain more recent professions. These biases may induce limits in the representativeness of our training data and thus in the predictions' precision. However, results based on this method of SES annotation performed best in our measurements, indicating that professions are among the most predictive features of SES, as has been reported in BIBREF9 .", "Finally, it should also be noted that following recent work by Aletras and Chamberlain in BIBREF21 , we tested our model by extending the feature set with the node2vec embedding of users computed from the mutual mention graph of Twitter. Nevertheless, in our setting, it did not increase the overall predictive performance of the inference pipeline. We hence didn't include in the feature set for the sake of simplicity.", "INLINEFORM0 Annotated home locations: The remote sensing annotation was done by experts and their evaluation was based on visual inspection and biased by some unavoidable subjectivity. Although their annotations were cross-referenced and found to be consistent, they still contained biases, like over-representative middle classes, which somewhat undermined the prediction task based on this dataset.", "This procedure was applied to each of our datasets. The obtained results are shown in Fig. FIGREF21 and in Table TABREF22 ."]}
{"question_id": "7557f2c3424ae70e2a79c51f9752adc99a9bdd39", "predicted_answer": "", "predicted_evidence": ["For each socioeconomic dataset, we trained our models by using 75% of the available data for training and the remaining 25% for testing. During the training phase, the training data undergoes a INLINEFORM0 -fold inner cross-validation, with INLINEFORM1 , where all splits are computed in a stratified manner to get the same ratio of lower to higher SES users. The four first blocks were used for inner training and the remainder for inner testing. This was repeated ten times for each model so that in the end, each model's performance on the validation set was averaged over 50 samples. For each model, the parameters were fine-tuned by training 500 different models over the aforementioned splits. The selected one was that which gave the best performance on average, which was then applied to the held-out test set. This is then repeated through a 5-fold outer cross-validation.", "As a result we could compute a representative topic distribution for each user, defined as a vector of normalized usage frequency of words from each topic. Also note that the topic distribution for a given user was automatically obtained as it depends only on the set of tweets and the learned topic clusters without further parametrization.", "In this work we proposed a novel methodology for the inference of the SES of Twitter users. We built our models combining information obtained from numerous sources, including Twitter, census data, LinkedIn and Google Maps. We developed precise methods of home location inference from geolocation, novel annotation of remotely sensed images of living environments, and effective combination of datasets collected from multiple sources. As new scientific results, we demonstrated that within the French Twitter space, the utilization of words in different topic categories, identified via advanced semantic analysis of tweets, can discriminate between people of different income. More importantly, we presented a proof-of-concept that our methods are competitive in terms of SES inference when compared to other methods relying on domain specific information.", "To obtain meaningful linguistic data we pre-processed the incoming tweet streams in several ways. As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet.", "Due to the ambiguous naming of jobs and to acknowledge permanent/non-permanent, senior/junior contract types we followed three strategies for the matching. In INLINEFORM0 of the cases we directly associated the reported job titles to regular expressions of an occupation. In INLINEFORM1 of the cases we used string sequencing methods borrowed from DNA-sequencing BIBREF33 to associate reported and official names of occupations with at least INLINEFORM2 match. For the remaining INLINEFORM3 of users we directly inspected profiles. The distribution of estimated salaries reflects the expected income heterogeneities as shown in Fig. FIGREF15 . Users were eventually assigned to one of two SES classes based on whether their salary was higher or lower than the average value of the income distribution. Also note, that LinkedIn users may not be representative of the whole population. We discuss this and other types of poential biases in Section SECREF6 ."]}
{"question_id": "b03249984c26baffb67e7736458b320148675900", "predicted_answer": "", "predicted_evidence": ["Eventually we extracted conversation topics by running a spectral clustering algorithm on the word-to-word similarity matrix INLINEFORM0 with INLINEFORM1 vocabulary size and elements defined as the INLINEFORM2 cosine similarity between word vectors. Here INLINEFORM3 is a vector of a word INLINEFORM4 in the embedding, INLINEFORM5 is the dot product of vectors, and INLINEFORM6 is the INLINEFORM7 norm of a vector. This definition allows for negative entries in the matrix to cluster, which were set to null in our case. This is consistent with the goal of the clustering procedure as negative similarities shouldn't encode dissimilarity between pairs of words but orthogonality between the embeddings. This procedure was run for 50, 100 and 200 clusters and allowed the homogeneous distribution of words among clusters (hard clustering). The best results were obtained with 100 topics in the topic model. Finally, we manually labeled topics based on the words assigned to them, and computed the topic-to-topic correlation matrix shown in Fig.", "For each socioeconomic dataset, we trained our models by using 75% of the available data for training and the remaining 25% for testing. During the training phase, the training data undergoes a INLINEFORM0 -fold inner cross-validation, with INLINEFORM1 , where all splits are computed in a stratified manner to get the same ratio of lower to higher SES users. The four first blocks were used for inner training and the remainder for inner testing. This was repeated ten times for each model so that in the end, each model's performance on the validation set was averaged over 50 samples. For each model, the parameters were fine-tuned by training 500 different models over the aforementioned splits. The selected one was that which gave the best performance on average, which was then applied to the held-out test set. This is then repeated through a 5-fold outer cross-validation.", "In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task. Training a decision tree learning algorithm involves the generation of a series of rules, split points or nodes ordered in a tree-like structure enabling the prediction of a target output value based on the values of the input features. More specifically, XGBoost, as an ensemble technique, is trained by sequentially adding a high number of individually weak but complementary classifiers to produce a robust estimator: each new model is built to be maximally correlated with the negative gradient of the loss function associated with the model assembly BIBREF44 . To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.", "In order to filter out inferred home locations not in urban/residential areas, we downloaded via Google Maps Static API BIBREF36 a satellite view in a INLINEFORM0 radius around each coordinate (for a sample see Fig. FIGREF12 a). To discriminate between residential and non-residential areas, we built on land use classifier BIBREF37 using aerial imagery from the UC Merced dataset BIBREF38 . This dataset contains 2100 INLINEFORM1 INLINEFORM2 aerial RGB images over 21 classes of different land use (for a pair of sample images see Fig. FIGREF12 b). To classify land use a CaffeNet architecture was trained which reached an accuracy over INLINEFORM3 . Here, we instantiated a ResNet50 network using keras BIBREF39 pre-trained on ImageNet BIBREF40 where all layers except the last five were frozen. The network was then trained with 10-fold cross validation achieving a INLINEFORM4 accuracy after the first 100 epochs.", "To associate a single income value to each user, we identified the cell of their estimated home locations and assigned them with the median of the corresponding income distribution. Thus we obtained an average socioeconomic indicator for each user, which was distributed heterogeneously in accordance with Pareto's law BIBREF28 . This is demonstrated in Fig. FIGREF15 a, where the INLINEFORM0 cumulative income distributions as the function of population fraction INLINEFORM1 appears as a Lorentz-curve with area under the diagonal proportional to socioeconomic inequalities. As an example, Fig. FIGREF7 depicts the spatial distribution of INLINEFORM2 users with inferred home locations in IRIS cells located in central Paris and colored as the median income."]}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "predicted_answer": "", "predicted_evidence": ["Eventually we extracted conversation topics by running a spectral clustering algorithm on the word-to-word similarity matrix INLINEFORM0 with INLINEFORM1 vocabulary size and elements defined as the INLINEFORM2 cosine similarity between word vectors. Here INLINEFORM3 is a vector of a word INLINEFORM4 in the embedding, INLINEFORM5 is the dot product of vectors, and INLINEFORM6 is the INLINEFORM7 norm of a vector. This definition allows for negative entries in the matrix to cluster, which were set to null in our case. This is consistent with the goal of the clustering procedure as negative similarities shouldn't encode dissimilarity between pairs of words but orthogonality between the embeddings. This procedure was run for 50, 100 and 200 clusters and allowed the homogeneous distribution of words among clusters (hard clustering). The best results were obtained with 100 topics in the topic model. Finally, we manually labeled topics based on the words assigned to them, and computed the topic-to-topic correlation matrix shown in Fig.", "The precise inference of SES would contribute to overcome several scientific challenges and could potentially have several commercial applications BIBREF7 . Further, robust SES inference would provide unique opportunities to gain deeper insights on socioeconomic inequalities BIBREF8 , social stratification BIBREF2 , and on the driving mechanisms of network evolution, such as status homophily or social segregation.", "FIGREF12 a). To discriminate between residential and non-residential areas, we built on land use classifier BIBREF37 using aerial imagery from the UC Merced dataset BIBREF38 . This dataset contains 2100 INLINEFORM1 INLINEFORM2 aerial RGB images over 21 classes of different land use (for a pair of sample images see Fig. FIGREF12 b). To classify land use a CaffeNet architecture was trained which reached an accuracy over INLINEFORM3 . Here, we instantiated a ResNet50 network using keras BIBREF39 pre-trained on ImageNet BIBREF40 where all layers except the last five were frozen. The network was then trained with 10-fold cross validation achieving a INLINEFORM4 accuracy after the first 100 epochs. We used this model to classify images of the estimated home location satellite views (cf. Figure FIGREF12 a) and kept those which were identified as residential areas (see Fig.", "To demonstrate how discriminative the identified topics were in terms of the SES of users we associated to each user the 9th decile value of the income distribution corresponding to the census block of their home location and computed for each labelled topic the average income of users depending on whether or not they mentioned the given topic. Results in Fig. FIGREF19 demonstrates that topics related to politics, technology or culture are more discussed by people with higher income, while other topics associated to slang, insults or informal abbreviations are more used by people of lower income. These observable differences between the average income of people, who use (or not) words from discriminative topics, demonstrates well the potential of word topic clustering used as features for the inference of SES. All in all, each user in our dataset was assigned with a 1117 feature vector encoding the lexical and semantic profile she displayed on Twitter. We did not apply any further feature selection as the distribution of importance of features appeared rather smooth (not shown here).", "By focusing on the geolocated users, we kept those with limited mobility, i.e., with median distance between locations not greater than 30 km, with tweets posted at places and times which did not require travel faster than 130 INLINEFORM1 (maximum speed allowed within France), and with no more than three tweets within a two seconds window. We further filtered out tweets with coordinates corresponding to locations referring to places (such as \u201cParis\" or \u201cFrance\"). Thus, we removed locations that didn't exactly correspond to GPS-tagged tweets and also users which were most likely bots. Home location was estimated by the most frequent location for a user among all coordinates he visited. This way we obtained INLINEFORM2 users, each associated with a unique home location. Finally, we collected the latest INLINEFORM3 tweets from the timeline of all of geolocated users using the Twitter public API BIBREF17 . Note, that by applying these consecutive filters we obtained a more representative population as the Gini index, indicating overall socioeconomic inequalities, was INLINEFORM4 before filtering become INLINEFORM5 due to the filtering methods, which is closer to the value reported by the World Bank ( INLINEFORM6 ) BIBREF26 ."]}
{"question_id": "08c0d4db14773cbed8a63e69381a2265e85f8765", "predicted_answer": "", "predicted_evidence": ["By focusing on the geolocated users, we kept those with limited mobility, i.e., with median distance between locations not greater than 30 km, with tweets posted at places and times which did not require travel faster than 130 INLINEFORM1 (maximum speed allowed within France), and with no more than three tweets within a two seconds window. We further filtered out tweets with coordinates corresponding to locations referring to places (such as \u201cParis\" or \u201cFrance\"). Thus, we removed locations that didn't exactly correspond to GPS-tagged tweets and also users which were most likely bots. Home location was estimated by the most frequent location for a user among all coordinates he visited. This way we obtained INLINEFORM2 users, each associated with a unique home location. Finally, we collected the latest INLINEFORM3 tweets from the timeline of all of geolocated users using the Twitter public API BIBREF17 . Note, that by applying these consecutive filters we obtained a more representative population as the Gini index, indicating overall socioeconomic inequalities, was INLINEFORM4 before filtering become INLINEFORM5 due to the filtering methods, which is closer to the value reported by the World Bank ( INLINEFORM6 ) BIBREF26 .", "In this work we combined multiple datasets collected from various sources. Each of them came with some bias due to the data collection and post-treatment methods or the incomplete set of users. These biases may limit the success of our inference, thus their identification is important for the interpretation and future developments of our framework.", "Using the user profile information and tweets collected from every account's timeline, we built a feature set for each user, similar to Lampos et al. BIBREF9 . We categorized features into two sets, one containing shallow features directly observable from the data, while the other was obtained via a pipeline of data processing methods to capture semantic user features.", "Eventually we extracted conversation topics by running a spectral clustering algorithm on the word-to-word similarity matrix INLINEFORM0 with INLINEFORM1 vocabulary size and elements defined as the INLINEFORM2 cosine similarity between word vectors. Here INLINEFORM3 is a vector of a word INLINEFORM4 in the embedding, INLINEFORM5 is the dot product of vectors, and INLINEFORM6 is the INLINEFORM7 norm of a vector. This definition allows for negative entries in the matrix to cluster, which were set to null in our case. This is consistent with the goal of the clustering procedure as negative similarities shouldn't encode dissimilarity between pairs of words but orthogonality between the embeddings. This procedure was run for 50, 100 and 200 clusters and allowed the homogeneous distribution of words among clusters (hard clustering). The best results were obtained with 100 topics in the topic model.", "For each socioeconomic dataset, we trained our models by using 75% of the available data for training and the remaining 25% for testing. During the training phase, the training data undergoes a INLINEFORM0 -fold inner cross-validation, with INLINEFORM1 , where all splits are computed in a stratified manner to get the same ratio of lower to higher SES users. The four first blocks were used for inner training and the remainder for inner testing. This was repeated ten times for each model so that in the end, each model's performance on the validation set was averaged over 50 samples. For each model, the parameters were fine-tuned by training 500 different models over the aforementioned splits. The selected one was that which gave the best performance on average, which was then applied to the held-out test set. This is then repeated through a 5-fold outer cross-validation."]}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "predicted_answer": "", "predicted_evidence": ["Original English: I presided over a region crossed by heavy traffic from all over Europe...what is more, in 2002, two Member States of the European Union appealed to the European Court of Justice...", "", "French: however (Comparison.Contrast)", "Czech: therefore (Contingency.Cause) after all", "Case 4: Implicit relations can co-occur with marked discourse relations BIBREF17 , and multiple translations help discover these instances, for example:"]}
{"question_id": "26844cec57df6ff0f02245ea862af316b89edffe", "predicted_answer": "", "predicted_evidence": ["German: therefore (Contingency.Cause)", "", "We compare the explicitations obtained from translations into three different languages, and find that instances where at least two back-translations agree yield the best quality, significantly outperforming a version of the model that does not use additional data, or uses data from just one language. A qualitative analysis furthermore shows that the strength of the method partially stems from being able to learn additional discourse cues which are typically translated consistently, and suggests that our method may also be used for identifying multiple relations holding between two arguments.", "Czech: in addition (Expansion.Conjunction)", ""]}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "predicted_answer": "", "predicted_evidence": ["", "", "French: but (Comparison.Contrast) there is more", "", "Czech: therefore (Contingency.Cause)"]}
{"question_id": "4d824b49728649432371ecb08f66ba44e50569e0", "predicted_answer": "", "predicted_evidence": ["A linear classifier classifies extraction candidates as correct and incorrect extractions, based on consistency and correctness scores INLINEFORM0 and INLINEFORM1 and potentially other features. Candidates classified as incorrect are discarded.", "e.g. ts_tick_abs (US_Unemployment, 4.9%), or", "We would like to thank my managers Alex Bozic, Tim Phelan and Joshwini Pereira for supporting this project, as well as David Rosenberg from the CTO's office for providing access to GPU infrastructure.", "There are two sources of noise in our application's database supervision. First, there is a high rate of false positives. It is not rare for the parser to generate an extraction candidate ts_tick_abs (TS symbol, numerical value) in which the numerical value fits into the time series of the time series symbol, but the extraction is nonetheless incorrect. False negatives are also a problem: many financial time series are sparse and are rarely observed. As a result, it is common for differences between reference numerical values and extracted numerical values to be large even for correct extractions.", "We presented an architecture for information extraction from text using a combination of an existing parser and a deep neural network. The architecture can boost the precision of a high-recall information extraction system. To train the neural network, we use measures of consistency between extracted data and existing databases as a form of noisy supervision. The architecture resulted in substantial improvements over a mature and highly tuned constraint-based information extraction system for financial language text. While we used time series databases to derive measures of consistency for candidate extractions, our set-up can easily be applied to a variety of other information extraction tasks for which potentially noisy reference data is available."]}
{"question_id": "02a5acb484bda77ef32a13f5d93d336472cf8cd4", "predicted_answer": "", "predicted_evidence": ["We assume that the noise in the source of supervision INLINEFORM0 is limited in magnitude, e.g. INLINEFORM1 . We moreover assume that there are no strong patterns in the distribution of the noise: if the noise correlates with certain attributes of the candidate-extraction, the pseudo-likelihoods INLINEFORM2 might no longer be a good estimate of the candidate extraction's probability of being a correct extraction.", "To meet the application domain's high accuracy requirements, marrying constraints with statistical models is often beneficial, see e.g. BIBREF0 , BIBREF1 . Many quantities appearing in information extraction problems are by definition constrained in the numerical values they can assume (e.g. unemployment numbers cannot be negative numbers, while changes in unemployment numbers can be negative). The inclusion of such constraints may significantly boost data efficiency. Constraints can be complex in nature, and may involve multiple entities belonging to an extraction candidate generated by the parser. At Bloomberg, we found the system for information extraction described in this paper especially useful to extract time series (TS) data. As an example, consider numerical relations of the form", "Unstructured textual data is abundant in the financial domain (see e.g. Figure FIGREF2 ). This information is by definition not in a format that lends itself to immediate processing. Hence, information extraction is an essential step in business applications that require fast, accurate, and low-cost information processing. In the financial domain, these applications include the creation of time series databases for macroeconomic forecasting or financial analysis, as well as the real-time extraction of time series data to inform algorithmic trading strategies. Bloomberg has had information extraction systems for financial language text for nearly a decade.", "We found that even with only 256 hidden LSTM cells, the neural network described in the previous section significantly outperformed a 2-layer fully connected network with n-grams based on document text and parser annotations as input.", "e.g. ts_tick_abs (US_Unemployment, -0.2%)."]}
{"question_id": "863d8d32a1605402e11f0bf63968a14bcfd15337", "predicted_answer": "", "predicted_evidence": ["To meet the application domain's high accuracy requirements, marrying constraints with statistical models is often beneficial, see e.g. BIBREF0 , BIBREF1 . Many quantities appearing in information extraction problems are by definition constrained in the numerical values they can assume (e.g. unemployment numbers cannot be negative numbers, while changes in unemployment numbers can be negative). The inclusion of such constraints may significantly boost data efficiency. Constraints can be complex in nature, and may involve multiple entities belonging to an extraction candidate generated by the parser. At Bloomberg, we found the system for information extraction described in this paper especially useful to extract time series (TS) data. As an example, consider numerical relations of the form", "Unstructured textual data is abundant in the financial domain (see e.g. Figure FIGREF2 ). This information is by definition not in a format that lends itself to immediate processing. Hence, information extraction is an essential step in business applications that require fast, accurate, and low-cost information processing. In the financial domain, these applications include the creation of time series databases for macroeconomic forecasting or financial analysis, as well as the real-time extraction of time series data to inform algorithmic trading strategies. Bloomberg has had information extraction systems for financial language text for nearly a decade.", "e.g. ts_tick_abs (US_Unemployment, 4.9%), or", "ts_tick_rel (TS symbol, change in num. value),", "We assume that the noise in the source of supervision INLINEFORM0 is limited in magnitude, e.g. INLINEFORM1 . We moreover assume that there are no strong patterns in the distribution of the noise: if the noise correlates with certain attributes of the candidate-extraction, the pseudo-likelihoods INLINEFORM2 might no longer be a good estimate of the candidate extraction's probability of being a correct extraction."]}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "predicted_answer": "", "predicted_evidence": ["In Fig. 4 a and b we show the temporal variability of $\\overline{L}^{\\Lambda }_{\\mathrm {cn}}(t)$ and $\\overline{L}^{\\Lambda }_{\\mathrm {cp}}(t)$ (respectively) computed for the whole Twitter user set ( $\\Gamma =all$ , solid line) and for geolocated users ( $\\Gamma =geo$ , dashed lines). Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night.", "Despite these findings, one has to acknowledge the multiple limitations affecting this work: First of all, although Twitter is a broadly adopted service in most technologically enabled societies, it commonly provides a biased sample in terms of age and socioeconomic status as older or poorer people may not have access to this technology. In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the \"homogenization\" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results.", "In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the \"homogenization\" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results. Nonetheless, this INSEE dataset provides still the most comprehensive available information on socioeconomic status over the whole country. For limiting such risk of bias, we analyzed the potential effect of the confounding variables on distribution and cross-correlations of SES indicators.", "Our first dataset consists of a large data corpus collected from the online news and social networking service, Twitter. On it, users can post and interact with messages, \"tweets\", restricted to 140 characters. Tweets may come with several types of metadata including information about the author's profile, the detected language, where and when the tweet was posted, etc. Specifically, we recorded 170 million tweets written in French, posted by $2.5$ million users in the timezones GMT and GMT+1 over three years (between July 2014 to May 2017). These tweets were obtained via the Twitter powertrack API feeds provided by Datasift and Gnip with an access rate varying between $15-25\\%$ .", "Via a detailed multidimensional correlation study we concluded that (a) socioeconomic indicators and linguistic variables are significantly correlated. i.e. people with higher socioeconomic status are more prone to use more standard variants of language and a larger vocabulary set, while people on the other end of the socioeconomic spectrum tend to use more non-standard terms and, on average, a smaller vocabulary set; (b) Spatial position was also found to be a key feature of standard language use as, overall, people from the North tended to use more non-standard terms and a smaller vocabulary set compared to people from the South; a more fine-grained analysis reveals that the spatial variability of language is determined to a greater extent locally by the socioeconomic status; (c) In terms of temporal activity, standard language was more likely to be used during the daytime while non-standard variants were predominant during the night."]}
{"question_id": "90756bdcd812b7ecc1c5df2298aa7561fd2eb02c", "predicted_answer": "", "predicted_evidence": ["In Fig. 4 a and b we show the temporal variability of $\\overline{L}^{\\Lambda }_{\\mathrm {cn}}(t)$ and $\\overline{L}^{\\Lambda }_{\\mathrm {cp}}(t)$ (respectively) computed for the whole Twitter user set ( $\\Gamma =all$ , solid line) and for geolocated users ( $\\Gamma =geo$ , dashed lines). Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night.", "Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night. However, after measuring the average income of active users in a given hour over a week, we obtained an even more sophisticated picture. It turned out that people active during the day have higher average income (warmer colors in Fig. 4 ) than people active during the night (colder colors in Fig. 4 ). Thus the variability of standard language patterns was largely explained by the changing overall composition of active Twitter users during different times of day and the positive correlation between socioeconomic status and the usage of higher linguistic standards (that we have seen earlier).", "To obtain meaningful linguistic data we preprocessed the incoming tweet stream in several ways. As our central question here deals with the variability of the language, repeated tweets do not bring any additional information to our study. Therefore, as an initial filtering step, we decided to remove retweets. Next, in order to facilitate the detection of the selected linguistic markers we removed any URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) from each tweet. These expressions were not considered to be semantically meaningful and their filtering allowed to further increase the speed and accuracy of our linguistic detection methods when run across the data. In addition we completed a last step of textual preprocessing by down-casing and stripping the punctuation out of the tweets body. POS-taggers such as MElt BIBREF25 were also tested but they provided no significant improvement in the detection of the linguistic markers.", "In order to measure linguistic similarities between a pair of users $u$ and $v$ , we simply computed the $|L^{u}_{*}-L^{v}_{*}|$ absolute difference of their corresponding individual linguistic variable $*\\in \\lbrace \\mathrm {cn},\\mathrm {cp},vs\\rbrace $ . This measure appeared with a minimum of 0 and associated smaller values to more similar pairs of users. To identify the effects of status homophily and the social network, we proceeded by computing the similarity distribution in four cases: for connected users from the same socioeconomic class; for disconnected randomly selected pairs of users from the same socioeconomic class; for connected users in the network; and randomly selected pairs of disconnected users in the network. Note that in each case the same number of user pairs were sampled from the network to obtain comparable averages.", "Our first dataset consists of a large data corpus collected from the online news and social networking service, Twitter. On it, users can post and interact with messages, \"tweets\", restricted to 140 characters. Tweets may come with several types of metadata including information about the author's profile, the detected language, where and when the tweet was posted, etc. Specifically, we recorded 170 million tweets written in French, posted by $2.5$ million users in the timezones GMT and GMT+1 over three years (between July 2014 to May 2017). These tweets were obtained via the Twitter powertrack API feeds provided by Datasift and Gnip with an access rate varying between $15-25\\%$ ."]}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "predicted_answer": "", "predicted_evidence": ["In order to measure linguistic similarities between a pair of users $u$ and $v$ , we simply computed the $|L^{u}_{*}-L^{v}_{*}|$ absolute difference of their corresponding individual linguistic variable $*\\in \\lbrace \\mathrm {cn},\\mathrm {cp},vs\\rbrace $ . This measure appeared with a minimum of 0 and associated smaller values to more similar pairs of users. To identify the effects of status homophily and the social network, we proceeded by computing the similarity distribution in four cases: for connected users from the same socioeconomic class; for disconnected randomly selected pairs of users from the same socioeconomic class; for connected users in the network; and randomly selected pairs of disconnected users in the network. Note that in each case the same number of user pairs were sampled from the network to obtain comparable averages.", "Despite these findings, one has to acknowledge the multiple limitations affecting this work: First of all, although Twitter is a broadly adopted service in most technologically enabled societies, it commonly provides a biased sample in terms of age and socioeconomic status as older or poorer people may not have access to this technology. In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the \"homogenization\" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results.", "Note that in each case the same number of user pairs were sampled from the network to obtain comparable averages. This number was naturally limited by the number of connected users in the smallest socioeconomic class, and were chosen to be $10,000$ in each cases. By comparing the distributions shown in Fig. 5 we concluded that (a) connected users (red and yellow bars) were the most similar in terms of any linguistic marker. This similarity was even greater when the considered tie was connecting people from the same socioeconomic group; (b) network effects can be quantified by comparing the most similar connected (red bar) and disconnected (light blue bar) users from the same socioeconomic group. Since the similarity between disconnected users here is purely induced by status homophily, the difference of these two bars indicates additional effects that cannot be explained solely by status homophily.", "Via a detailed multidimensional correlation study we concluded that (a) socioeconomic indicators and linguistic variables are significantly correlated. i.e. people with higher socioeconomic status are more prone to use more standard variants of language and a larger vocabulary set, while people on the other end of the socioeconomic spectrum tend to use more non-standard terms and, on average, a smaller vocabulary set; (b) Spatial position was also found to be a key feature of standard language use as, overall, people from the North tended to use more non-standard terms and a smaller vocabulary set compared to people from the South; a more fine-grained analysis reveals that the spatial variability of language is determined to a greater extent locally by the socioeconomic status; (c) In terms of temporal activity, standard language was more likely to be used during the daytime while non-standard variants were predominant during the night.", "Other approaches have also relied on sources of socioeconomic information such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions BIBREF19 . Despite the relative success of these methods, their common limitation is to provide observations and predictions based on a carefully hand-picked small set of users, letting alone the problem of socioeconomic status inference on larger and more heterogeneous populations. Our work stands out from this well-established line of research by expanding the definition of socioeconomic status to include several demographic features as well as by pinpointing potential home location to individual users with an unprecedented accuracy. Identifying socioeconomic status and the network effects of homophily BIBREF20 is an open question BIBREF21 . However, recent results already showed that status homophily, i.e. the tendency of people of similar socioeconomic status are better connected among themselves, induce structural correlations which are pivotal to understand the stratified structure of society BIBREF22 ."]}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "predicted_answer": "", "predicted_evidence": ["Finally we would like to emphasize two scientific merits of the paper. On one side, based on a very large sample, we confirm and clarify results from the field of sociolinguistics and we highlight new findings. We thus confirm clear correlations between the variable realization of the negative particle in French and three indices of socioeconomic status. This result challenges those among the sociolinguistic studies that do not find such correlation. Our data also suggested that the language used in the southern part of France is more standard. Understanding this pattern fosters further investigations within sociolinguistics. We finally established that the linguistic similarity of socially connected people is partially explained by status homophily but could be potentially induced by social influences passing through the network of links or other terms of homophilic correlations. Beyond scientific merit, we can identify various straightforward applications of our results. The precise inference of socioeconomic status of individuals from online activities is for instance still an open question, which carries a huge potential in marketing design and other areas.", "The present work meets most of these challenges. It constructs the largest dataset of French tweets enriched with census sociodemographic information existent to date to the best of our knowledge. From this dataset, we observed variation of two grammatical cues and an index of vocabulary size in users located in France. We study how the linguistic cues correlated with three features reflective of the socioeconomic status of the users, their most representative location and their daily periods of activity on Twitter. We also observed whether connected people are more linguistically alike than disconnected ones. Multivariate analysis shows strong correlations between linguistic cues and socioeconomic status as well as a broad spatial pattern never observed before, with more standard language variants and lexical diversity in the southern part of the country. Moreover, we found an unexpected daily cyclic evolution of the frequency of standard variants. Further analysis revealed that the observed cycle arose from the ever changing average economic status of the population of users present in Twitter through the day. Finally, we were able to establish that linguistic similarity between connected people does arises partially but not uniquely due to status homophily (users with similar socioeconomic status are linguistically similar and tend to connect).", "To do so, first we took the geolocated Twitter users in France and partitioned them into nine socioeconomic classes using their inferred income $S_\\mathrm {inc}^u$ . Partitioning was done first by sorting users by their $S^u_\\mathrm {inc}$ income to calculate their $C(S^u_\\mathrm {inc})$ cumulative income distribution function. We defined socioeconomic classes by segmenting $C(S^u_\\mathrm {inc})$ such that the sum of income is the same for each classes (for an illustration of our method see Fig. 6 a in the Appendix). We constructed a social network by considering mutual mention links between these users (as introduced in Section \"Data Description\" ).", "Another potentially important factor determining language variability is the time of day when users are active in Twitter BIBREF39 , BIBREF40 . The temporal variability of standard language usage can be measured for a dynamical quantity like the $L_{\\mathrm {cn}}(t)$ rate of correct negation. To observe its periodic variability (with a $\\Delta T$ period of one week) over an observation period of $T$ (in our case 734 days), we computed", "$$L^u_\\mathrm {vs}=\\frac{N^u_\\mathrm {vs}}{N^u_{tw}} \\hspace{14.45377pt} \\mbox{and} \\hspace{14.45377pt} \\overline{L}^{i}_\\mathrm {vs}=\\frac{\\sum _{u\\in i}N^u_\\mathrm {vs}}{N_i},$$   (Eq. 22)"]}
{"question_id": "3746aaa1a81d9c725bc7a4a67086634c11998d39", "predicted_answer": "", "predicted_evidence": ["This module provides: image segmentations, object semantic attributes and text descriptions.", "This module provides: stereo sound frames for agents w.r.t. environmental sound sources.", "Size (\u201csmall,\u201d \u201cmedium,\u201d or \u201clarge\u201d) calculated by comparing an object's mesh volume to a histogram of other objects of the same category.", "The acoustic engine is implemented using EVERT, which handles real-time acoustic ray-tracing based on the house and object 3D geometry. EVERT also supports multiple microphones and sound sources, distance-dependent sound attenuation, frequency-dependent material absorption and reflection (walls muffle sounds, metallic surfaces reflect acoustics, etc.), and air-absorption based on atmospheric conditions (temperature, pressure, humidity, etc.). Sounds may be instantiated artificially or based on the environment (i.e. a TV with static noise or an agent's surface-dependent footsteps).", "[leftmargin=*]"]}
{"question_id": "143409d16125790c8db9ed38590a0796e0b2b2e2", "predicted_answer": "", "predicted_evidence": ["We hypothesise that breaking this linearity, and allowing a more local fit to the training data will undermine the global structure that the analogy predictions exploit.", "In a controversial essay, BIBREF0 draws the distinction between two types of generalisation: interpolation and extrapolation; with the former being predictions made between the training data points, and the latter being generalisation outside this space. He goes on to claim that deep learning is only effective at interpolation, but that human like learning and behaviour requires extrapolation.", "We have used the development of the scientific understanding of planetary motion as a repeated example of the possibility of uncovering global structures that support extrapolation, throughout our discussion. Kepler and Newton found laws that went beyond simply maximising the fit to the known set of planetary bodies to describe regularities that held for every body, terrestrial and heavenly.", "It is certainly true that extrapolation is hard, but there appear to be clear real-world examples. For example, in 1705, using Newton's then new inverse square law of gravity, Halley predicted the return of a comet 75 years in the future. This prediction was not only possible for a new celestial object for which only a limited amount of data was available, but was also effective on an orbital period twice as long as any of those known to Newton. Pre-Newtonian models required a set of parameters (deferents, epicycles, equants, etc.) for each body and so would struggle to generalise from known objects to new ones. Newton's theory of gravity, in contrast, not only described celestial orbits but also predicted the motion of bodies thrown or dropped on Earth.", "On Twitter, Thomas Diettrich rebutted this claim with the response that no methods extrapolate; that what appears to be extrapolation from X to Y is interpolation in a representation that makes X and Y look the same."]}
{"question_id": "8ba582939823faae6822a27448ea011ab6b90ed7", "predicted_answer": "", "predicted_evidence": ["It is certainly true that extrapolation is hard, but there appear to be clear real-world examples. For example, in 1705, using Newton's then new inverse square law of gravity, Halley predicted the return of a comet 75 years in the future. This prediction was not only possible for a new celestial object for which only a limited amount of data was available, but was also effective on an orbital period twice as long as any of those known to Newton. Pre-Newtonian models required a set of parameters (deferents, epicycles, equants, etc.) for each body and so would struggle to generalise from known objects to new ones. Newton's theory of gravity, in contrast, not only described celestial orbits but also predicted the motion of bodies thrown or dropped on Earth.", "In fact, we can often use a symbol effectively with no prior data. For example, a language user that has never have encountered the symbol Socrates before may nonetheless be able to leverage their syntactic, semantic and inferential skills to conclude that Socrates is mortal contradicts Socrates is not mortal.", "On Twitter, Thomas Diettrich rebutted this claim with the response that no methods extrapolate; that what appears to be extrapolation from X to Y is interpolation in a representation that makes X and Y look the same.", "BIBREF0 links his experiment to the systematic ways in which the meaning and use of a word in one context is related to its meaning and use in another BIBREF1 , BIBREF2 . These regularities allow us to extrapolate from sometimes even a single use of a word to understand all of its other uses.", "One tool for thinking about this dichotomy is the equivalent kernel BIBREF15 , which measures the extent to which a given prediction is influenced by nearby training examples. Typically, models with highly local equivalent kernels - e.g. splines, sigmoids and random forests - are preferred over non-local models - e.g. polynomials - in the context of general curve fitting BIBREF16 ."]}
{"question_id": "65c7a2b734dab51c4c81f722527424ff33b023f8", "predicted_answer": "", "predicted_evidence": ["The major contributions of our work are:", "We trained phrase-based SMT systems using the Moses system BIBREF31 , with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA BIBREF32 for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character, OS and BPE-unit level models. Subword level representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000). This setting has been shown to have minimal impact on translation quality BIBREF33 .", "Our major observations are described below (based on BLEU scores):", "The term, related languages, refers to languages that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time BIBREF0 . Examples of languages related by common ancestry are Slavic and Indo-Aryan languages. Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas BIBREF1 . Examples of such linguistic areas are the Indian subcontinent BIBREF2 , Balkan BIBREF3 and Standard Average European BIBREF4 linguistic areas. Genetic as well as contact relationship lead to related languages sharing vocabulary and structural features.", "INLINEFORM0 It is worth mentioning that BPE units provide a substantial benefit over OS units when translation involves a morphologically rich language. In translations involving Malayalam, Tamil and Telugu, average accuracy improvement of 6.25% were observed."]}
{"question_id": "11ef46187a5bf15e89d63220fdeaecbeb92d818e", "predicted_answer": "", "predicted_evidence": ["The above mentioned results for BPE units do not explore optimal values of the number of merge operations. This is the only hyper-parameter that has to be selected for BPE. We experimented with number of merge operations ranging from 1000 to 4000 and the translation results for these are shown in Table TABREF25 . Selecting the optimal value of merge operations lead to a modest, average increase of 1.6% and maximum increase of 3.5% in the translation accuracy over B INLINEFORM0 across different language pairs .", "The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step BIBREF8 , BIBREF9 , a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder BIBREF10 , so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made.", "The LeBLEU scores also show the same trends as the BLEU scores.", "The term, related languages, refers to languages that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time BIBREF0 . Examples of languages related by common ancestry are Slavic and Indo-Aryan languages. Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas BIBREF1 . Examples of such linguistic areas are the Indian subcontinent BIBREF2 , Balkan BIBREF3 and Standard Average European BIBREF4 linguistic areas. Genetic as well as contact relationship lead to related languages sharing vocabulary and structural features.", "INLINEFORM0 BPE units also show modest improvement over the recently proposed orthographic syllables over most language pairs (average improvement of 2.6% and maximum improvement of up to 11%). The improvements are not statistically significant for most language pairs. The only exceptions are Bengali-Hindi, Punjabi-Hindi and Malay-Indonesian - all these languages pairs have relatively less morphological affixing (Bengali-Hindi, Punjabi-Hindi) or are registers of the same language (Malay-Indonesian). For Bengali-Hindi and Punjabi-Hindi, the BPE unit translation accuracies are quite close to OS level accuracies. Since OS level models have been shown to be better than character level models BIBREF7 , BPE units are better than character level models by transitivity."]}
{"question_id": "45aab23790161cbc55f78e16fdf5678a3f5b4b92", "predicted_answer": "", "predicted_evidence": ["We discuss why BPE is a promising method for learning subword units (subsections SECREF7 and SECREF8 ) and describe how we trained our BPE unit level translation models (subsections SECREF9 and SECREF10 ).", "Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, Indonesian-Malay, Spanish-Catalan with modest success BIBREF11 , BIBREF12 , BIBREF13 . Unigram-level learning provides very little context for learning translation models BIBREF14 . The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit BIBREF13 . These results were demonstrated primarily for very close European languages. kunchukuttan2016orthographic proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable.", "INLINEFORM0 It is worth mentioning that BPE units provide a substantial benefit over OS units when translation involves a morphologically rich language. In translations involving Malayalam, Tamil and Telugu, average accuracy improvement of 6.25% were observed.", "Since we are concerned with low resource scenarios, a desirable property of subword units is robustness of the translation models to change of translation domain. kunchukuttan2016orthographic have shown that OS level models are robust to domain change. Since BPE units are learnt from a specific corpus, it is not guaranteed that they would also be robust to domain changes. To study the behaviour of BPE unit trained models, we also tested the translation models trained on tourism & health domains on an agriculture domain test set of 1000 sentences (see Table TABREF27 for results). In this cross-domain translation scenario, the BPE level model outperforms the OS-level and word-level models for most language pairs. The Konkani-Marathi pair alone shows a degradation using the OS level model. The BPE model is almost on par with the OS level model for Telugu-Malayalam and Hindi-Malayalam.", "This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size are studied. We also report initial results with a joint bilingual BPE model."]}
{"question_id": "bf5e80f1ab4eae2254b4f4d7651969a3cf945fb4", "predicted_answer": "", "predicted_evidence": ["We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems, and the evaluation methodology.", "Recently, subword level models have also generated interest for neural machine translation (NMT) systems. The motivation is the need to limit the vocabulary of neural MT systems in encoder-decoder architectures BIBREF15 . It is in this context that Byte Pair Encoding, a data compression method BIBREF5 , was adapted to learn subword units for NMT BIBREF6 . Other subword units for NMT have also been proposed: character BIBREF16 , Huffman encoding based units BIBREF17 , wordpieces BIBREF18 , BIBREF19 . Our hypothesis is that such subword units learnt from corpora are particularly suited for translation between related languages. In this paper, we test this hypothesis by using BPE to learn subword units.", "The improved performance of BPE units compared to word-level and morpheme-level representations is easy to explain: with a limited vocabulary they address the problem of data sparsity. But character level models also have a limited vocabulary, yet they do not improve translation performance except for very close languages. Character level models learn character mappings effectively, which is sufficient for translating related languages which are very close to each other (translation is akin to transliteration in these cases). But they are not sufficient for translating related languages that are more divergent. In this case, translating cognates, morphological affixes, non-cognates etc. require a larger context. So, BPE and OS units \u2014 which provide more context \u2014 outperform character units.", "Modelling lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies related languages share many words with similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages.", "The major contributions of our work are:"]}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "predicted_answer": "", "predicted_evidence": ["Table TABREF7 provides a summary of the results. We compare the results of our methods of the pre-trained BERT, using both the headline and text body, and the Coh-Mertix approach, to the language-based baseline with Multinomial Naive Bayes from BIBREF1. Both the semantic cues with BERT and the linguistic cues with Coh-Metrix significantly outperform the baseline on the F1 score. The two-tailed paired t-test with a 0.05 significance level was used for testing statistical significance of performance differences. The best result is given by the BERT model. Overall, these results provide an answer to research question RQ1 regarding the existence of semantic and linguistic difference between fake news and satire.", "We addressed the challenge of identifying nuances between fake news and satire. Inspired by the humor and social message aspects of satire articles, we tested two classification approaches based on a state-of-the-art contextual language model, and linguistic features of textual coherence. Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire. In particular, both methods achieved a significantly better performance than the baseline language-based method. Lastly, we studied the feature importance of our linguistic-based method to help shed light on the nuances between fake news and satire. For instance, we observed that satire articles are more sophisticated, or less easy to read, than fake news articles.", "Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression.", "We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.", "The efforts by social media platforms to reduce the exposure of users to misinformation have resulted, on several occasions, in flagging legitimate satire stories. To avoid penalizing publishers of satire, which is a protected form of speech, the platforms have begun to add more nuance to their flagging systems. Facebook, for instance, added an option to mark content items as \u201cSatire\u201d, if \u201cthe content is posted by a page or domain that is a known satire publication, or a reasonable person would understand the content to be irony or humor with a social message\u201d BIBREF0. This notion of humor and social message is also echoed in the definition of satire by Oxford dictionary as \u201cthe use of humour, irony, exaggeration, or ridicule to expose and criticize people's stupidity or vices, particularly in the context of contemporary politics and other topical issues\u201d."]}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "predicted_answer": "", "predicted_evidence": ["The efforts by social media platforms to reduce the exposure of users to misinformation have resulted, on several occasions, in flagging legitimate satire stories. To avoid penalizing publishers of satire, which is a protected form of speech, the platforms have begun to add more nuance to their flagging systems. Facebook, for instance, added an option to mark content items as \u201cSatire\u201d, if \u201cthe content is posted by a page or domain that is a known satire publication, or a reasonable person would understand the content to be irony or humor with a social message\u201d BIBREF0. This notion of humor and social message is also echoed in the definition of satire by Oxford dictionary as \u201cthe use of humour, irony, exaggeration, or ridicule to expose and criticize people's stupidity or vices, particularly in the context of contemporary politics and other topical issues\u201d.", "The rest of paper is organized as follows: in section SECREF2, we briefly review studies on fake news and satire articles which are the most relevant to our work. In section SECREF3, we present the methods we use to investigate semantic and linguistic differences between fake and satire articles. Next, we evaluate these methods and share insights on nuances between fake news and satire in section SECREF4. Finally, we conclude the paper in section SECREF5 and outline next steps and future work.", "Moreover, in response to these efforts to demote misinformation, fake news purveyors have begun to masquerade as legitimate satire sites, for instance, carrying small badges at the footer of each page denoting the content as satire BIBREF1. The disclaimers are usually small such that the stories are still being spread as though they were real news BIBREF2.", "This gives rise to the challenge of classifying fake news versus satire based on the content of a story. While previous work BIBREF1 have shown that satire and fake news can be distinguished with a word-based classification approach, our work is focused on the semantic and linguistic properties of the content. Inspired by the distinctive aspects of satire with regard to humor and social message, our hypothesis is that using semantic and linguistic cues can help to capture these nuances.", "The distinction between fake news and satire carries implications with regard to the exposure of content on social media platforms. While fake news stories are algorithmically suppressed in the news feed, the satire label does not decrease the reach of such posts. This also has an effect on the experience of users and publishers. For users, incorrectly classifying satire as fake news may deprive them from desirable entertainment content, while identifying a fake news story as legitimate satire may expose them to misinformation. For publishers, the distribution of a story has an impact on their ability to monetize content."]}
{"question_id": "71b07d08fb6ac8732aa4060ae94ec7c0657bb1db", "predicted_answer": "", "predicted_evidence": ["Moreover, in response to these efforts to demote misinformation, fake news purveyors have begun to masquerade as legitimate satire sites, for instance, carrying small badges at the footer of each page denoting the content as satire BIBREF1. The disclaimers are usually small such that the stories are still being spread as though they were real news BIBREF2.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable.", "To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels.", "The rest of paper is organized as follows: in section SECREF2, we briefly review studies on fake news and satire articles which are the most relevant to our work. In section SECREF3, we present the methods we use to investigate semantic and linguistic differences between fake and satire articles. Next, we evaluate these methods and share insights on nuances between fake news and satire in section SECREF4. Finally, we conclude the paper in section SECREF5 and outline next steps and future work."]}
{"question_id": "812c974311747f74c3aad23999bfef50539953c8", "predicted_answer": "", "predicted_evidence": ["Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable.", "The efforts by social media platforms to reduce the exposure of users to misinformation have resulted, on several occasions, in flagging legitimate satire stories. To avoid penalizing publishers of satire, which is a protected form of speech, the platforms have begun to add more nuance to their flagging systems. Facebook, for instance, added an option to mark content items as \u201cSatire\u201d, if \u201cthe content is posted by a page or domain that is a known satire publication, or a reasonable person would understand the content to be irony or humor with a social message\u201d BIBREF0. This notion of humor and social message is also echoed in the definition of satire by Oxford dictionary as \u201cthe use of humour, irony, exaggeration, or ridicule to expose and criticize people's stupidity or vices, particularly in the context of contemporary politics and other topical issues\u201d.", "In the following sub sections, we evaluate our classification model and share insights on the nuances between fake news and satire, while addressing our two research questions.", "Moreover, in response to these efforts to demote misinformation, fake news purveyors have begun to masquerade as legitimate satire sites, for instance, carrying small badges at the footer of each page denoting the content as satire BIBREF1. The disclaimers are usually small such that the stories are still being spread as though they were real news BIBREF2.", "The most relevant work to ours is that of Golbeck et al. BIBREF1. They introduced a dataset of fake news and satirical articles, which we also employ in this work. The dataset includes the full text of 283 fake news stories and 203 satirical stories, that were verified manually, and such that each fake news article is paired with a rebutting article from a reliable source. Albeit relatively small, this data carries two desirable properties. First, the labeling is based on the content and not the source, and the stories spread across a diverse set of sources. Second, both fake news and satire articles focus on American politics and were posted between January 2016 and October 2017, minimizing the possibility that the topic of the article will influence the classification."]}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "predicted_answer": "", "predicted_evidence": ["The efforts by social media platforms to reduce the exposure of users to misinformation have resulted, on several occasions, in flagging legitimate satire stories. To avoid penalizing publishers of satire, which is a protected form of speech, the platforms have begun to add more nuance to their flagging systems. Facebook, for instance, added an option to mark content items as \u201cSatire\u201d, if \u201cthe content is posted by a page or domain that is a known satire publication, or a reasonable person would understand the content to be irony or humor with a social message\u201d BIBREF0. This notion of humor and social message is also echoed in the definition of satire by Oxford dictionary as \u201cthe use of humour, irony, exaggeration, or ridicule to expose and criticize people's stupidity or vices, particularly in the context of contemporary politics and other topical issues\u201d.", "Moreover, in response to these efforts to demote misinformation, fake news purveyors have begun to masquerade as legitimate satire sites, for instance, carrying small badges at the footer of each page denoting the content as satire BIBREF1. The disclaimers are usually small such that the stories are still being spread as though they were real news BIBREF2.", "To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate.", "Our main research questions are therefore, RQ1) are there semantic and linguistic differences between fake news and satire stories that can help to tell them apart?; and RQ2) can these semantic and linguistic differences contribute to the understanding of nuances between fake news and satire beyond differences in the language being used?", "For future work, we plan to study additional linguistic cues, and specifically humor related features, such as absurdity and incongruity, which were shown to be good indicators of satire in previous work. Another interesting line of research would be to investigate techniques of identifying whether a story carries a political or social message, for example, by comparing it with timely news information."]}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "predicted_answer": "", "predicted_evidence": ["We thank Brendan O'Connor, Swabha Swayamdipta, and Brian Roark for feedback on drafts of this paper, and Jan Buys, Phil Blunsom, and Yue Zhang for help with data preparation. This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114; it was also supported in part by Contract No. W911NF-15-1-0543 with the DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.", "reduce repeatedly pops completed subtrees or terminal symbols from the stack until an open nonterminal is encountered, and then this open NT is popped and used as the label of a new constituent that has the popped subtrees as its children. This new completed constituent is pushed onto the stack as a single composite item. A single reduce operation can thus create constituents with an unbounded number of children.", "We introduced recurrent neural network grammars, a probabilistic model of phrase-structure trees that can be trained generatively and used as a language model or a parser, and a corresponding discriminative model that can be used as a parser. Apart from out-of-vocabulary preprocessing, the approach requires no feature design or transformations to treebank data. The generative model outperforms every previously published parser built on a single supervised generative model in English, and a bit behind the best-reported generative model in Chinese. As language models, RNNGs outperform the best single-sentence language models.", "The representation of the algorithm state at time INLINEFORM0 , INLINEFORM1 , is computed by combining the representation of the generator's three data structures: the output buffer ( INLINEFORM2 ), represented by an embedding INLINEFORM3 , the stack ( INLINEFORM4 ), represented by an embedding INLINEFORM5 , and the history of actions ( INLINEFORM6 ) taken by the generator, represented by an embedding INLINEFORM7 , INLINEFORM8", "INLINEFORM0 introduces an \u201copen nonterminal\u201d X onto the top of the stack. Open nonterminals are written as a nonterminal symbol preceded by an open parenthesis, e.g., \u201c(VP\u201d, and they represent a nonterminal whose child nodes have not yet been fully constructed. Open nonterminals are \u201cclosed\u201d to form complete constituents by subsequent reduce operations."]}
{"question_id": "4c7ec282697f4f6646eb1c19f46bbaf8670b0de6", "predicted_answer": "", "predicted_evidence": ["All experiments are implemented on the hardware with Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz and NVIDIA Tesla P100.", "Our postprocessing mechanism is mainly based on the FAQ evaluation rules. After model prediction, we remove triplets whose entity-relation types are against the given schemas. For entities contained in book title mark, we complement them if they are incomplete. Date type entities are also complemented to the finest grain. These are implemented by regular expression matching.", "Results on SKE dataset are presented in Table 1. The baseline model is based on the Google BERT, use hard label embedding and train on only SKE dataset without NER pretraining. As shown in table 1, the F1 score increase from 0.864 to 0.871 when combined with our enhanced BERT. NER pretraining using the extra corpus, soft label embedding and auxiliary sentence-level relation classification prediction also improve the F1 score. Combined all of these contributions, we achieve F1-score 0.876 with the single model on test set 1.", "By solving Eq DISPLAY_FORM11 we can obtain the optimal sequence tags:", "Previous works show that introducing extra data for distant supervised learning usually boost the model performance. For this task, we collect a large-scale Baidu Baike corpus (about 6 million sentences) for NER pre-training. As shown in figure FIGREF12, each sample contains the content and its title. These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data. Experimental results show that it improves performance."]}
{"question_id": "07104dd36a0e7fdd2c211ad710de9a605495b697", "predicted_answer": "", "predicted_evidence": ["Previous works show that introducing extra data for distant supervised learning usually boost the model performance. For this task, we collect a large-scale Baidu Baike corpus (about 6 million sentences) for NER pre-training. As shown in figure FIGREF12, each sample contains the content and its title. These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data. Experimental results show that it improves performance.", "We evaluate our method on the SKE dataset used in this competition, which is the largest schema-based Chinese information extraction dataset in the industry, containing more than 430,000 SPO triples in over 210,000 real-world Chinese sentences, bounded by a pre-specified schema with 50 types of predicates. All sentences in SKE Dataset are extracted from Baidu Baike and Baidu News Feeds. The dataset is divided into a training set (170k sentences), a development set (20k sentences) and a testing set (20k sentences). The training set and the development set are to be used for training and are available for free download. The test set is divided into two parts, the test set 1 is available for self-verification, the test set 2 is released one week before the end of the competition and used for the final evaluation.", "$\\cdot $ whether the entity boundary is consistent with the word segmentation results", "The performance of this method is limited by the word segmentation accuracy because it can not extract entities beyond the word segmentation results. Li et al. BIBREF12 (2019) cast the task as a multi-turn question answering problem, i.e., the extraction of entities and relations is transformed to the task of identifying answer spans from the context. This framework provides an elegant way to capture the hierarchical dependency of tags. However, it is also of low computational efficiency since it needs to scan all entity template questions and corresponding relation template questions for a single sentence. Bekoulis et al. (2017) BIBREF13 propose a joint neural model which performs entity recognition and relation extraction simultaneously, without the need of any manually extracted features or the use of any external tool. They model the entity recognition task using a CRF (Conditional Random Fields) layer and the relation extraction task as a multi-head selection problem since one entity can have multiple relations.", "Many earlier entity-relation extraction systems BIBREF3, BIBREF4, BIBREF5 adopt pipelined framework: they first conduct entity extraction and then predict the relations between each entity pair. The pipelined framework has the flexibility of integrating different data sources and learning algorithms, but their disadvantages are obvious. First, they suffer significantly from error propagation, the error of the entity extraction stage will be propagated to the relation classification stage. Second, they ignore the relevance of entity extraction and relation classification. As shown in Figure FIGREF3, entity contained in book title marks can be a song or book, its relation to a person can be singer or writer. Once the relationship has been confirmed, the entity type can be easily identified, and vice versa. For example, if we know the relationship is singer, then the entity type should be a song."]}
{"question_id": "3e88fcc94d0f451e87b65658751834f6103b2030", "predicted_answer": "", "predicted_evidence": ["Original google BERT is pre-trained using two unsupervised tasks, masked language model (MLM) and next sentence prediction (NSP). MLM task enables the model to capture the discriminative contextual feature. NSP task makes it possible to understand the relationship between sentence pairs, which is not directly captured by language modeling. We further design a semantic-enhanced task to enhance the performance of BERT. It incorporate previous sentence prediction and document level prediction. We pre-train BERT by combining MLM, NSP and the semantic-enhanced task together.", "$\\cdot $ whether the entity boundary is consistent with the word segmentation results", "(1) BERT BIBREF14 is introduced as a feature extraction layer in place of BiLSTM. We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.", "The pipelined framework has the flexibility of integrating different data sources and learning algorithms, but their disadvantages are obvious. First, they suffer significantly from error propagation, the error of the entity extraction stage will be propagated to the relation classification stage. Second, they ignore the relevance of entity extraction and relation classification. As shown in Figure FIGREF3, entity contained in book title marks can be a song or book, its relation to a person can be singer or writer. Once the relationship has been confirmed, the entity type can be easily identified, and vice versa. For example, if we know the relationship is singer, then the entity type should be a song. Entity extraction and relation classification can benefit from each other so it will harm the performance if we consider them separately. Third, the pipelined framework results in low computational efficiency. After the entity extraction stage, each entity pair should be passed to the relation classification model to identify their relation.", "(3) Soft label embedding is proposed to effectively transmit information between entity recognition and relation extraction."]}
{"question_id": "c8cf20afd75eb583aef70fcb508c4f7e37f234e1", "predicted_answer": "", "predicted_evidence": ["Similar concerns are growing at an unprecedented rate in the media, with reports of Apple's Iphone X face unlock feature failing to differentiate between two different Asian people BIBREF17 and automatic soap dispensers which reportedly do not recognize black hands BIBREF18 . Machine bias, the phenomenon by which trained statistical models unbeknownst to their creators grow to reflect controversial societal asymmetries, is growing into a pressing concern for the modern times, invites us to ask ourselves whether there are limits to our dependence on these techniques \u2013 and more importantly, whether some of these limits have already been traversed. In the wave of algorithmic bias, some have argued for the creation of some kind of agency in the likes of the Food and Drug Administration, with the sole purpose of regulating algorithmic discrimination BIBREF19 .", "Chomsky and Norvig's debate BIBREF12 is a microcosm of the two leading standpoints about the future of science in the face of increasingly sophisticated statistical models. Are we, as Chomsky seems to argue, jeopardizing science by relying on statistical tools to perform predictions instead of perfecting traditional science models, or are these tools, as Norvig argues, components of the scientific standard since its conception? Currently there are no satisfactory resolutions to this conundrum, but perhaps statistical models pose an even greater and more urgent threat to our society.", "In spite of the recent commercial success of automated translation tools (or perhaps stemming directly from it), machine translation has amounted a significant deal of criticism. Noted philosopher and founding father of generative linguistics Noam Chomsky has argued that the achievements of machine translation, while successes in a particular sense, are not successes in the sense that science has ever been interested in: they merely provide effective ways, according to Chomsky, of approximating unanalyzed data BIBREF11 , BIBREF12 . Chomsky argues that the faith of the MT community in statistical methods is absurd by analogy with a standard scientific field such as physics BIBREF11 :", "Since then, recent worrisome results in machine learning have somewhat supported Schiebinger's view. Not only Google photos' statistical image labeling algorithm has been found to classify dark-skinned people as gorillas BIBREF14 and purportedly intelligent programs have been suggested to be negatively biased against black prisoners when predicting criminal behavior BIBREF15 but the machine learning revolution has also indirectly revived heated debates about the controversial field of physiognomy, with proposals of AI systems capable of identifying the sexual orientation of an individual through its facial characteristics BIBREF16 . Similar concerns are growing at an unprecedented rate in the media, with reports of Apple's Iphone X face unlock feature failing to differentiate between two different Asian people BIBREF17 and automatic soap dispensers which reportedly do not recognize black hands BIBREF18 . Machine bias, the phenomenon by which trained statistical models unbeknownst to their creators grow to reflect controversial societal asymmetries, is growing into a pressing concern for the modern times, invites us to ask ourselves whether there are limits to our dependence on these techniques \u2013 and more importantly, whether some of these limits have already been traversed.", "On a 2014 article, Londa Schiebinger suggested that scientific research fails to take gender issues into account, arguing that the phenomenon of male defaults on new technologies such as Google Translate provides a window into this asymmetry BIBREF13 . Since then, recent worrisome results in machine learning have somewhat supported Schiebinger's view. Not only Google photos' statistical image labeling algorithm has been found to classify dark-skinned people as gorillas BIBREF14 and purportedly intelligent programs have been suggested to be negatively biased against black prisoners when predicting criminal behavior BIBREF15 but the machine learning revolution has also indirectly revived heated debates about the controversial field of physiognomy, with proposals of AI systems capable of identifying the sexual orientation of an individual through its facial characteristics BIBREF16 ."]}
{"question_id": "3567241b3fafef281d213f49f241071f1c60a303", "predicted_answer": "", "predicted_evidence": ["Although the idea of automated translation can in principle be traced back to as long as the 17th century with Ren\u00e9 Descartes proposal of an \u201cuniversal language\u201d BIBREF0 , machine translation has only existed as a technological field since the 1950s, with a pioneering memorandum by Warren Weaver BIBREF1 , BIBREF2 discussing the possibility of employing digital computers to perform automated translation. The now famous Georgetown-IBM experiment followed not long after, providing the first experimental demonstration of the prospects of automating translation by the means of successfully converting more than sixty Russian sentences into English BIBREF3 . Early systems improved upon the results of the Georgetown-IBM experiment by exploiting Noam Chomsky's theory of generative linguistics, and the field experienced a sense of optimism about the prospects of fully automating natural language translation.", "Although the idea of automated translation can in principle be traced back to as long as the 17th century with Ren\u00e9 Descartes proposal of an \u201cuniversal language\u201d BIBREF0 , machine translation has only existed as a technological field since the 1950s, with a pioneering memorandum by Warren Weaver BIBREF1 , BIBREF2 discussing the possibility of employing digital computers to perform automated translation. The now famous Georgetown-IBM experiment followed not long after, providing the first experimental demonstration of the prospects of automating translation by the means of successfully converting more than sixty Russian sentences into English BIBREF3 . Early systems improved upon the results of the Georgetown-IBM experiment by exploiting Noam Chomsky's theory of generative linguistics, and the field experienced a sense of optimism about the prospects of fully automating natural language translation. As is customary with artificial intelligence, the initial optimistic stage was followed by an extended period of strong disillusionment with the field, of which the catalyst was the influential 1966 ALPAC (Automatic Language Processing Advisory Committee) report( BIBREF4 .", "The now famous Georgetown-IBM experiment followed not long after, providing the first experimental demonstration of the prospects of automating translation by the means of successfully converting more than sixty Russian sentences into English BIBREF3 . Early systems improved upon the results of the Georgetown-IBM experiment by exploiting Noam Chomsky's theory of generative linguistics, and the field experienced a sense of optimism about the prospects of fully automating natural language translation. As is customary with artificial intelligence, the initial optimistic stage was followed by an extended period of strong disillusionment with the field, of which the catalyst was the influential 1966 ALPAC (Automatic Language Processing Advisory Committee) report( BIBREF4 . Such research was then disfavoured in the United States, making a re-entrance in the 1970s before the 1980s surge in statistical methods for machine translation BIBREF5 , BIBREF6 . Statistical and example-based machine translation have been on the rise ever since BIBREF7 , BIBREF8 , BIBREF9 , with highly successful applications such as Google Translate (recently ported to a neural translation technology BIBREF10 ) amounting to over 200 million users daily.", "To simplify our dataset, we have decided to focus our work on job positions \u2013 which, we believe, are an interesting window into the nature of gender bias \u2013, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics' detailed occupations table BIBREF31 , from the United States Department of Labor. The values inside, however, had to be expanded since each line contained multiple occupations and sometimes very specific ones. Fortunately this table also provided a percentage of women participation in the jobs shown, for those that had more than 50 thousand workers. We filtered some of these because they were too generic ( \u201cComputer occupations, all other\u201d, and others) or because they had gender specific words for the profession (\u201chost/hostess\u201d, \u201cwaiter/waitress\u201d). We then separated the curated jobs into broader categories (Artistic, Corporate, Theatre, etc.) as shown in Table TABREF3 .", "Similar concerns are growing at an unprecedented rate in the media, with reports of Apple's Iphone X face unlock feature failing to differentiate between two different Asian people BIBREF17 and automatic soap dispensers which reportedly do not recognize black hands BIBREF18 . Machine bias, the phenomenon by which trained statistical models unbeknownst to their creators grow to reflect controversial societal asymmetries, is growing into a pressing concern for the modern times, invites us to ask ourselves whether there are limits to our dependence on these techniques \u2013 and more importantly, whether some of these limits have already been traversed. In the wave of algorithmic bias, some have argued for the creation of some kind of agency in the likes of the Food and Drug Administration, with the sole purpose of regulating algorithmic discrimination BIBREF19 ."]}
{"question_id": "d5d48b812576470edbf978fc18c00bd24930a7b7", "predicted_answer": "", "predicted_evidence": ["In this context, all experiments reported here offer an analysis of a \u201cscreenshot\u201d of that tool as of August 2018, the moment they were carried out. A preprint version of this paper was posted the in well-known Cornell University-based arXiv.org open repository on September 6, 2018. The manuscript soon enjoyed a significant amount of media coverage, featuring on The Register BIBREF33 , Datanews BIBREF34 , t3n BIBREF35 , among others, and more recently on Slator BIBREF36 and Jornal do Comercio BIBREF37 . On December 6, 2018 the company's policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation (Figure FIGREF30 ). According to the company, this decision is part of a broader goal of promoting fairness and reducing biases in machine learning.", "The now famous Georgetown-IBM experiment followed not long after, providing the first experimental demonstration of the prospects of automating translation by the means of successfully converting more than sixty Russian sentences into English BIBREF3 . Early systems improved upon the results of the Georgetown-IBM experiment by exploiting Noam Chomsky's theory of generative linguistics, and the field experienced a sense of optimism about the prospects of fully automating natural language translation. As is customary with artificial intelligence, the initial optimistic stage was followed by an extended period of strong disillusionment with the field, of which the catalyst was the influential 1966 ALPAC (Automatic Language Processing Advisory Committee) report( BIBREF4 . Such research was then disfavoured in the United States, making a re-entrance in the 1970s before the 1980s surge in statistical methods for machine translation BIBREF5 , BIBREF6 . Statistical and example-based machine translation have been on the rise ever since BIBREF7 , BIBREF8 , BIBREF9 , with highly successful applications such as Google Translate (recently ported to a neural translation technology BIBREF10 ) amounting to over 200 million users daily.", "On a 2014 article, Londa Schiebinger suggested that scientific research fails to take gender issues into account, arguing that the phenomenon of male defaults on new technologies such as Google Translate provides a window into this asymmetry BIBREF13 . Since then, recent worrisome results in machine learning have somewhat supported Schiebinger's view. Not only Google photos' statistical image labeling algorithm has been found to classify dark-skinned people as gorillas BIBREF14 and purportedly intelligent programs have been suggested to be negatively biased against black prisoners when predicting criminal behavior BIBREF15 but the machine learning revolution has also indirectly revived heated debates about the controversial field of physiognomy, with proposals of AI systems capable of identifying the sexual orientation of an individual through its facial characteristics BIBREF16 . Similar concerns are growing at an unprecedented rate in the media, with reports of Apple's Iphone X face unlock feature failing to differentiate between two different Asian people BIBREF17 and automatic soap dispensers which reportedly do not recognize black hands BIBREF18 .", "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian \u201c\u0151 egy \u00e1pol\u00f3n\u0151\u201d, where \u201c\u00e1pol\u00f3n\u0151\u201d translates to \u201cnurse\u201d and \u201c\u0151\u201d is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result \u201cshe's a nurse\u201d on Google Translate. As Figure FIGREF1 clearly shows, the same template yields a male pronoun when \u201cnurse\u201d is replaced by \u201cengineer\u201d. The same basic template can be ported to all other gender neutral languages, as depicted in Table TABREF4 . Given the success of Google Translate, which amounts to 200 million users daily, we have chosen to exploit its API to obtain the desired thermometer of gender bias.", "All adjectives were obtained from the top one thousand most frequent words in this category as featured in the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, but it was necessary to manually curate them because a substantial fraction of these adjectives cannot be applied to human subjects. Also because the sentiment associated with each adjective is not as easily accessible as for example the occupation category of each job position, we performed a manual selection of a subset of such words which we believe to be meaningful to this study. These words are presented in Table TABREF6 . We made all code and data used to generate and compile the results presented in the following sections publicly available in the following Github repository: https://github.com/marceloprates/Gender-Bias. Note however that because the Google Translate algorithm can change, unfortunately we cannot guarantee full reproducibility of our results. All experiments reported here were conducted on April 2018."]}
{"question_id": "643527e94e8eed1e2229915fcf8cd74d769173fc", "predicted_answer": "", "predicted_evidence": ["As is shown in Fig. FIGREF15, suppose there are $N$ languages, and each has a corresponding dataset, i.e., $\\lbrace D_{1}, D_{2},...,D_{N}\\rbrace $. Since our task is to predict the exercise accuracy of language learners on each language, we can regard these predictions as different tasks. Therefore, there are $N$ tasks.", "The experimental results are shown in Fig. FIGREF20. It can be found that our method outperforms all the state-of-the-art baselines when the training data of a language dataset is insufficient, which is a huge improvement compared with the existing methods. For example, as shown in AUC/en_es in Fig. FIGREF20, using only 1K training data, our multi-task learning method still could get the AUC score of 0.738, while the AUC score of ours-MTL is only 0.640, and existing RNN, GBDT and LR methods are 0.659, 0.658 and 0.650 respectively. Therefore, the performance of introducing the multi-task learning increases by nearly ten percent. Moreover, to achieve the same performance as our multi-task learning on 1K training data, the methods without multi-task learning require more than 10K training data, which is ten times more than ours.", "The experiments above show that our method has a huge advantage over the existing methods in low-resource scenarios. In this section, we will observe the performance of our method in the non-low-resource scenario.", "FIGREF20. It can be found that our method outperforms all the state-of-the-art baselines when the training data of a language dataset is insufficient, which is a huge improvement compared with the existing methods. For example, as shown in AUC/en_es in Fig. FIGREF20, using only 1K training data, our multi-task learning method still could get the AUC score of 0.738, while the AUC score of ours-MTL is only 0.640, and existing RNN, GBDT and LR methods are 0.659, 0.658 and 0.650 respectively. Therefore, the performance of introducing the multi-task learning increases by nearly ten percent. Moreover, to achieve the same performance as our multi-task learning on 1K training data, the methods without multi-task learning require more than 10K training data, which is ten times more than ours. Thus, multi-task learning utilizes data from all language-learning datasets simultaneously and effectively alleviate the problem of lacking data in a single language-learning dataset.", "As shown in Table TABREF30, it can be found that although the improvement is not very big, our method surpasses all existing methods on all three datasets and refreshes the best scores on all three datasets. Especially for the smallest dataset fr_en, our method obtains the most improvement than ours-MTL. As for the largest dataset en_es, our method also improves the AUC score by 0.003 over the best existing method GBDT+RNN. Therefore, our method also gains improvement slightly in the non-low-resource scenario."]}
{"question_id": "bfd55ae9630a08a9e287074fff3691dfbffc3258", "predicted_answer": "", "predicted_evidence": ["The final output of the context encoder is generated by a single-layer MLP, and the concatenation of $g_{t}$, $\\hat{g}_{t}$ and $\\tilde{g}_{t}$ is fed as the input. The process is formulated as", "ours-MTL It is our encoder-decoder model without multi-task learning. Thus, we will separately train a model for each language-learning dataset.", "The char level CNN context encoder can be similarly formulated as", "Our task is to build a model based on users' exercises, and further to predict word-level label sequence of future exercises.", "where $P(\\cdot )$ is the probability, $s(\\cdot )$ is the trained classifier, $x_{1}$ is the instance randomly extracted from positive samples, and $x_{2}$ is the instance randomly extracted from negative samples."]}
{"question_id": "3a06d40a4bf5ba6e26d9138434e9139a014deb40", "predicted_answer": "", "predicted_evidence": ["where the activation function of $MLP_{decoder}$ is sigmoid function.", "AUC is calculated as:", "where $r^{context}_{t}$ is the final context representation of the word $w_{t}$.", "GBDT Here, we use NYU's method BIBREF7, which is the best method among all tree ensemble methods. It uses an ensemble of GBDTs with existing features of dataset and manually constructed features based on psychological theories.", "In the training process, one mini batch contains data of $N$ datasets and they will all be sent to the same meta encoder and decoder, but will be sent to their corresponding context encoder according to their language type. Thus, the final loss with $N$ tasks is calculated as"]}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "predicted_answer": "", "predicted_evidence": ["Knowledge bases of tables, or \u201ctable stores\u201d, have recently been proposed as a semi-structured knowledge formalism for question answering that balances the cost of manually crafting highly-structured knowledge bases with the difficulties in acquiring this knowledge from free text BIBREF14 , BIBREF15 , BIBREF16 . The methods for question answering over tables generally take the form of constructing chains of multiple table rows that lead from terms in the question to terms in the answer, while the tables themselves are generally either collected from the web, automatically generated by extracting relations from free text, or manually constructed.", "Natural language sentences: QA models use a variety of different representations for inference, from semantic roles and syntactic dependencies to discourse and embeddings. Following Khashabi et al. Khashabi:2016TableILP, we make use of a specific form of table representation that includes \u201cfiller\u201d columns that allow each row to be directly read off as a stand-alone natural language sentence, and serve as input to any model. Examples of these filler columns can be seen in Figure 2 .", "One of the central shortcomings of question answering models is that while solvers are steadily increasing the proportion of questions they answer correctly, most solvers generally lack the capacity to provide human-readable explanations or justifications for why those answers are correct. This \u201cexplainable inference\u201d task is seen as a limitation of current machine learning models in general (e.g. Ribeiro et al., Ribeiro2016), but is critical for domains such as science or medicine where user trust and detecting potentially costly errors are important. More than this, evidence from the cognitive and pedagogy literature suggests that explanations (when tutoring others) and self-explanations (when engaged in self-directed learning) are an important aspect of learning, helping humans better generalize the knowledge they have learned BIBREF9 , BIBREF10 , BIBREF11 .", "Each explanation sentence is represented as a single row from a semi-structured table defined around a particular relation. Our tablestore includes 62 such tables, each centered around a particular relation such as taxonomy, meronymy, causality, changes, actions, requirements, or affordances, and a number of tables specified around specific properties, such as average lifespans of living things, the magnetic properties of materials, or the nominal durations of certain processes (like the Earth orbiting the Sun). The initial selection of table relations was drawn from a list of 21 common relations required for science explanations identified by Jansen et al. jansen2016:COLING on a smaller corpus, and expanded as new knowledge types were identified. Subsets of example tables are included in Figure 2 . Each explanation in this corpus contains an average of 6.3 rows.", "Building QA solvers that generate explanations for their answers is a challenging task, requiring a number of inference capacities. Central among these is the idea of information aggregation, or the idea that explanations for a given question are rarely found in a contiguous passage of text, and as such inference methods must generally assemble many separate pieces of knowledge from different sources in order to arrive at a correct answer. Previous estimates BIBREF2 suggest elementary science questions require an average of 4 pieces of knowledge to answer and explain those answers (here our analysis suggests this is closer to 6), but inference methods tend to have difficulty aggregating more than 2 pieces of knowledge from free-text together due to the semantic or contextual \u201cdrift\u201d associated with this aggregation BIBREF12 . Because of the difficulty in assembling training data for the information aggregation task, some have approached explanation generation as a distant supervision problem, with explanation quality modelled as a latent variable BIBREF7 , BIBREF13 ."]}
{"question_id": "7d34cdd9cb1c988e218ce0fd59ba6a3b5de2024a", "predicted_answer": "", "predicted_evidence": ["In terms of question answering, the ability to provide compelling human-readable explanations for answers to questions has been proposed as a complementary metric to assess QA performance alongside the proportion of questions answered correctly. Jansen et al. jansen2017framing developed a QA system for elementary science that answers questions by building and ranking explanation graphs built from aggregating multiple sentences read from free text corpora, including study guides and dictionaries. Because of the difficulty in constructing gold explanations to serve as training data, the explanations built with this system were constructed by modeling explanation quality as a latent variable machine learning problem. First, sentences were decomposed into sentence graphs based on clausal and prepositional boundaries, then assembled into multi-sentence \u201cexplanation graphs\u201d. Questions were answered by ranking these candidate explanation graphs, using answer correctness as well as features that capture the connectivity of key-terms in the graphs as a proxy for explanation quality. Jansen at al. jansen2017framing showed that it is possible to learn to generate high quality explanations for 60% of elementary science questions using this method, an increase of 15% over a baseline that retrieved single continuous passages of text as answer justifications.", "Computable explanations: Explanations should be represented at different levels of structure (explanation, then sentences, then relations within sentences). The knowledge links between explanation sentences should be explicit through lexical overlap, which can be used to form an \u201cexplanation graph\u201d that describes how each sentence is linked in an explanation.", "We author explanation graphs for a corpus of 2,201 elementary science questions (3rd through 5th grade) from the AI2 Science Questions V2 corpus, consisting of both standardized exam questions from 12 US states, as well as the separate AI2 Science Questions Mercury dataset, a set of questions licensed from a student assessment entity. Each question is a 4-way multiple choice question, and only those questions that do not involve diagram interpretation (a separate spatial task) are included. Approximately 20% of explanations required specialized domain knowledge (for example, spatial or mathematical knowledge) that did not easily lend itself to explanation using our formalism, resulting in a corpus of 1,680 questions and explanations.", "Elementary science exams contain a variety of complex and challenging inference problems BIBREF1 , BIBREF2 , with nearly 70% of questions requiring some form of causal, process, or model-based reasoning to solve and produce an explanation for. In spite of these exams being taken by millions of students each year, elementary students tend not to be fast or voluminous readers by adult standards, making this a surprisingly low-resource domain for grade-appropriate study guides and other materials. The questions also tend to require world knowledge expressed in grade-appropriate language (like that bears have fur and that fur keeps animals warm) to solve. Because of these requirements and limitations, table stores for elementary science QA tend to be manually or semi-automatically constructed, and comparatively small.", "In terms of automatic generation, though relations are often represented as $<subject, relation, argument>$ triples, Yin et al. yin:2015answering create a large table containing 120M n-tuple relations using OpenIE BIBREF18 , arguing that the extra expressivity afforded by these more detailed relations allows their system to answer more complex questions. Yin et al. use this to successfully reason over the WebQuestions dataset, as well as their own set of questions with more complex prepositional and adverbial constraints."]}
{"question_id": "83db51da819adf6faeb950fe04b4df942a887fb5", "predicted_answer": "", "predicted_evidence": ["Unlike many tasks in NLP, our goal is not to explicitly maximize accuracy. The framework is that we may only review a certain percentage of documents, given this, we want to maximize the probability than an alert will be caught. I.e., the cost of a false-positive is negligible, while we consider false negatives to be more serious. Conversely, this same information could be used to set a percentage of documents required to be read in order to have have some degree of certainty that an alert is flagged. If we encode all alerts with the value 1 and all normal documents with a value of 0, any neural network model will serve as a statistical mechanism in which an alert that was not used in training will, a priori, be given a score by the engine from a distribution of numbers between 0 and 1 which is skewed towards 1 while normal documents will also have scores from another distribution skewed towards 0.", "Recurrent neural networks are behind many of the most recent advances in NLP. We have depicted the general structure of an unfolded recurrent unit in figure FIGREF4 . A single unit takes a sequence of inputs, denoted INLINEFORM0 below, which affects a set of internal states of the node, denoted INLINEFORM1 , to produce an output, INLINEFORM2 . A single unit either outputs a single variable, which is the output of the last node, or a sequence of the same length of the input sequence, INLINEFORM3 , which may be used as the input into another recurrent unit.", "The embedding we created reflects the imperfect manner in which students use words BIBREF26 . For example, while the words 'happems' and 'ocures' are both incorrectly spelled versions of 'happens' and 'occurs' respectively, our embedding exhibits a high cosine similarity between the word vectors of the correct and incorrect versions. The embedding we created was an embedding into 200 dimensional space with a vocabulary consisting of 1.12 million words. Using spelling dictionaries we approximate that the percentage of correctly spelled words in the vocabulary of this embedding is approximately 7%, or roughly 80,000 words, while the remaining 93% are either misspellings, made up words or words from other languages. Lastly, due to the prevalence of words that are concatenated (due to a missing space), we split up any word with a Levenstein distance that is greater than two from our vocabulary into smaller words that are in the vocabulary. This ensures that any sentence is tokenized into a list of elements, almost all of which have valid embeddings.", "ct = ft ct-1 + it yt,", "we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses. Further study is required to approximate what percentage are Tier A and Tier B."]}
{"question_id": "7e7471bc24970c6f23baff570be385fd3534926c", "predicted_answer": "", "predicted_evidence": ["rt = g (Wr xt + Ur ht-1 + br),", "A layer of these recurrent units is a collection of independent units, each of which may pick up a different aspect of the series. A recurrent layer, consisting of INLINEFORM0 independent recurrent units, has the ability to take the most important/prevalent features and summarize those features in a vector of length INLINEFORM1 . When we feed the sequence of outputs of one recurrent layer into another recurrent layer, we call this a stacked recurrent layer. Analogous to the types of features observed in stacking convolutional and dense layers in convolutional neural networks BIBREF30 , it is suspected that stacking recurrent layers allows a neural network to model more semantically complex features of a text BIBREF31 , BIBREF32 .", "The second type of recurrent unit we consider is the LSTM, which appeared in the literature before the GRU and contains more parameters BIBREF18 . It was created to address the vanishing gradient problem and differs from the gated recurrent unit in that it has more parameters, hence, may be regarded as more powerful. ft = g (Wf xt + Uf ht-1 + bf),", "We do this by using standard algorithms BIBREF25 on a large corpus of student responses (approximately 160 million responses). The embedding we created reflects the imperfect manner in which students use words BIBREF26 . For example, while the words 'happems' and 'ocures' are both incorrectly spelled versions of 'happens' and 'occurs' respectively, our embedding exhibits a high cosine similarity between the word vectors of the correct and incorrect versions. The embedding we created was an embedding into 200 dimensional space with a vocabulary consisting of 1.12 million words. Using spelling dictionaries we approximate that the percentage of correctly spelled words in the vocabulary of this embedding is approximately 7%, or roughly 80,000 words, while the remaining 93% are either misspellings, made up words or words from other languages. Lastly, due to the prevalence of words that are concatenated (due to a missing space), we split up any word with a Levenstein distance that is greater than two from our vocabulary into smaller words that are in the vocabulary.", "When it comes to neural network design, there are two dominant types of neural networks in NLP; convolutional neural networks (CNN) and recurrent neural networks (RNN) BIBREF15 . Since responses may be of an arbitrary length different recurrent neural networks are more appropriate tools for classifying alerts BIBREF16 . The most common types of cells used in the design of recurrent neural networks are Gated Recurrent Units (GRU)s BIBREF17 and Long-Short-Term-Memory (LSTM) units BIBREF18 . The latter were originally designed to overcome the vanishing gradient problem BIBREF19 . The GRU has some interesting properties which simplify the LSTM unit and the two types of units can give very similar results BIBREF20 . We also consider stacked versions, bidirectional variants BIBREF21 and the effect of an attention mechanism BIBREF22 ."]}
{"question_id": "ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "predicted_answer": "", "predicted_evidence": ["The problem of depression and violence in our schools is one that has recently garnered high levels of media attention. This type of problem is not confined to the scope of educational research, but this type of anomaly detection is also applicable to social media platforms where there are posts that indicate potential cases of users alluding to suicide, depression, using hate-speech and engaging in cyberbullying. The program on which this study concerns is in place and has contributed to the detection an intervention of cases of depression and violence across America. This study itself has led to a dramatic increase in our ability to detect such cases.", "We should also mention that the above results do not represent the state-of-the-art, since we were able to take simple aggregated results from the models to produce better statistics at each threshold level than our best model. This can be done in a similar manner to the work of BIBREF23 , however, this is a topic we leave for a future paper. It is also unclear as to whether traditional sentiment analysis provides additional information from which better estimates may be possible.", "To give an approximation of the effect of each of the attributes we endowed our models with, we can average over the effectiveness of each model with and without each attribute in question. It is clear that that stacking two layers of recurrent units, each with half as many cells, offers the greatest boost in effectiveness, followed by the difference in recurrent structures followed by the use of attention. Using bidirectional units seems to give the smallest increase, but given the circumstances, any positive increase could potentially save lives.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses."]}
{"question_id": "7f958017cbb08962c80e625c2fd7a1e2375f27a3", "predicted_answer": "", "predicted_evidence": ["This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses.", "ot = g (Wo xt + Uo ht-1 + bo),", "These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system. Our goal is not to produce a classification, but rather to use the same methods developed in AES, ASAS and sentiment analysis BIBREF8 , BIBREF9 to identify some percentage of responses that fit patterns seen in known alerts and send them to be assessed by a team of reviewers.", "The American Institutes for Research has a hand-scoring team specifically devoted to verifying whether a given response satisfies the requirements of being an alert. At the beginning of this program, we had very few examples of student responses that satisfied the above requirements, moreover, given the diverse nature of what constitutes an alert, the alerts we did have did not span all the types of responses we considered to be worthy of attention. As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses. We observed that the manner in which the students engaged with the our essay platform in cases of alerts mimicked the way in which students used online forums in a sufficiently similar manner for the data to faithfully represent real alerts.", "Unlike many tasks in NLP, our goal is not to explicitly maximize accuracy. The framework is that we may only review a certain percentage of documents, given this, we want to maximize the probability than an alert will be caught. I.e., the cost of a false-positive is negligible, while we consider false negatives to be more serious. Conversely, this same information could be used to set a percentage of documents required to be read in order to have have some degree of certainty that an alert is flagged. If we encode all alerts with the value 1 and all normal documents with a value of 0, any neural network model will serve as a statistical mechanism in which an alert that was not used in training will, a priori, be given a score by the engine from a distribution of numbers between 0 and 1 which is skewed towards 1 while normal documents will also have scores from another distribution skewed towards 0. The thresholds values where we set are values in which all scores given by the engine above the cut-off are considered possible alerts while all below are considered normal."]}
{"question_id": "4130651509403becc468bdbe973e63d3716beade", "predicted_answer": "", "predicted_evidence": ["The responses themselves are drawn from a wide range of free-form text responses to questions and student comments from a semantically diverse range of topics, including many that are emotive in nature. For example, the semantic differences between an essay on gun-control and a student talking about getting a gun can be very subtle. Sometimes our systems include essays on emotive topics because the difference in language between such essays and alerts can be very small. Students often use phrases like \u201ckill me now\" as hyperbole out of frustration rather than a genuine desire to end ones life, e.g., \"this test is so boring, kill me now\". To minimize false positives, the engine should attempt to evaluate context, not just operate on key words or phrases.", "rt = g (Wr xt + Ur ht-1 + br),", "The American Institutes for Research has a hand-scoring team specifically devoted to verifying whether a given response satisfies the requirements of being an alert. At the beginning of this program, we had very few examples of student responses that satisfied the above requirements, moreover, given the diverse nature of what constitutes an alert, the alerts we did have did not span all the types of responses we considered to be worthy of attention. As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses.", "The collections of variables associated with the state of the recurrent units, which are denoted INLINEFORM0 in figure FIGREF4 , and their relations between the inputs, INLINEFORM1 , and the outputs are what distinguishes simple recurrent units, GRUs and LSTM units. In our case, INLINEFORM2 is a sequence of word-vectors. The underlying formulas for gated recurrent units are specified by the initial condition INLINEFORM3 and zt = g (Wz xt + Uz ht-1 + bz),", "We should also mention that the above results do not represent the state-of-the-art, since we were able to take simple aggregated results from the models to produce better statistics at each threshold level than our best model. This can be done in a similar manner to the work of BIBREF23 , however, this is a topic we leave for a future paper. It is also unclear as to whether traditional sentiment analysis provides additional information from which better estimates may be possible."]}
{"question_id": "6edef748370e63357a57610b5784204c9715c0b4", "predicted_answer": "", "predicted_evidence": ["These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses.", "The last mechanism we wish to test is an attention mechanism BIBREF22 . The key to attention mechanisms is that we apply weights to the sequences, INLINEFORM0 , outputted by the recurrent layer, not just the final output. This means that the attention is a function of the intermediate states of the recurrent layer as well as the final output. This may be useful when identifying when key phrases are mentioned for example. This weighted sequence is sent to a soft-max layer to create a context vector. The attention vector is then multiplied by INLINEFORM1 to produce resulting attention vector, INLINEFORM2 . We have implemented the following attention mechanism ht = ct ht,", "When it comes to neural network design, there are two dominant types of neural networks in NLP; convolutional neural networks (CNN) and recurrent neural networks (RNN) BIBREF15 . Since responses may be of an arbitrary length different recurrent neural networks are more appropriate tools for classifying alerts BIBREF16 . The most common types of cells used in the design of recurrent neural networks are Gated Recurrent Units (GRU)s BIBREF17 and Long-Short-Term-Memory (LSTM) units BIBREF18 . The latter were originally designed to overcome the vanishing gradient problem BIBREF19 . The GRU has some interesting properties which simplify the LSTM unit and the two types of units can give very similar results BIBREF20 . We also consider stacked versions, bidirectional variants BIBREF21 and the effect of an attention mechanism BIBREF22 . This study has been designed to guide the creation of our desired final production model, which may include higher stacking, dropouts (both regular and recurrent) and may be an ensemble of various networks tuned to different types of responses BIBREF23 .", "As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses. We observed that the manner in which the students engaged with the our essay platform in cases of alerts mimicked the way in which students used online forums in a sufficiently similar manner for the data to faithfully represent real alerts. This additional data also provided crucial examples of classes of alerts found too infrequently in student data for a valid classification. This initial data allowed us to build preliminary models and hence build better engines."]}
{"question_id": "6b302280522c350c4d1527d8c6ebc5b470f9314c", "predicted_answer": "", "predicted_evidence": ["The collections of variables associated with the state of the recurrent units, which are denoted INLINEFORM0 in figure FIGREF4 , and their relations between the inputs, INLINEFORM1 , and the outputs are what distinguishes simple recurrent units, GRUs and LSTM units. In our case, INLINEFORM2 is a sequence of word-vectors. The underlying formulas for gated recurrent units are specified by the initial condition INLINEFORM3 and zt = g (Wz xt + Uz ht-1 + bz),", "We do this by using standard algorithms BIBREF25 on a large corpus of student responses (approximately 160 million responses). The embedding we created reflects the imperfect manner in which students use words BIBREF26 . For example, while the words 'happems' and 'ocures' are both incorrectly spelled versions of 'happens' and 'occurs' respectively, our embedding exhibits a high cosine similarity between the word vectors of the correct and incorrect versions. The embedding we created was an embedding into 200 dimensional space with a vocabulary consisting of 1.12 million words. Using spelling dictionaries we approximate that the percentage of correctly spelled words in the vocabulary of this embedding is approximately 7%, or roughly 80,000 words, while the remaining 93% are either misspellings, made up words or words from other languages. Lastly, due to the prevalence of words that are concatenated (due to a missing space), we split up any word with a Levenstein distance that is greater than two from our vocabulary into smaller words that are in the vocabulary.", "As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses. We observed that the manner in which the students engaged with the our essay platform in cases of alerts mimicked the way in which students used online forums in a sufficiently similar manner for the data to faithfully represent real alerts. This additional data also provided crucial examples of classes of alerts found too infrequently in student data for a valid classification. This initial data allowed us to build preliminary models and hence build better engines.", "At the beginning of this program, we had very few examples of student responses that satisfied the above requirements, moreover, given the diverse nature of what constitutes an alert, the alerts we did have did not span all the types of responses we considered to be worthy of attention. As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses. We observed that the manner in which the students engaged with the our essay platform in cases of alerts mimicked the way in which students used online forums in a sufficiently similar manner for the data to faithfully represent real alerts. This additional data also provided crucial examples of classes of alerts found too infrequently in student data for a valid classification.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses."]}
{"question_id": "7da138ec43a88ea75374c40e8491f7975db29480", "predicted_answer": "", "predicted_evidence": ["This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses.", "We do this by using standard algorithms BIBREF25 on a large corpus of student responses (approximately 160 million responses). The embedding we created reflects the imperfect manner in which students use words BIBREF26 . For example, while the words 'happems' and 'ocures' are both incorrectly spelled versions of 'happens' and 'occurs' respectively, our embedding exhibits a high cosine similarity between the word vectors of the correct and incorrect versions. The embedding we created was an embedding into 200 dimensional space with a vocabulary consisting of 1.12 million words. Using spelling dictionaries we approximate that the percentage of correctly spelled words in the vocabulary of this embedding is approximately 7%, or roughly 80,000 words, while the remaining 93% are either misspellings, made up words or words from other languages. Lastly, due to the prevalence of words that are concatenated (due to a missing space), we split up any word with a Levenstein distance that is greater than two from our vocabulary into smaller words that are in the vocabulary.", "These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system.", "Since the programs inception, we have greatly expanded our collection of training data, which is summarized below in Table TABREF3 . While we have accumulated over 1.11 million essay responses, which include many types of essays over a range of essay topics, student age ranges, styles of writing as well as a multitude of types of alerts, we find that many of them are mapped to the same set of words after applying our preprocessing steps. When we disregard duplicate responses after preprocessing, our training sample consists of only 866,137 unique responses.", "Since natural languages contain so many rules, it is inconceivable that we could simply list all possible combinations of words that would constitute an alert. This means that the only feasible models we create are statistical in nature. Just as mathematicians use elementary functions like polynomials or periodic functions to approximate smooth functions, recurrent neural networks are used to fit classes of sequences. Character-level language models are typically useful in predicting text BIBREF27 , speech recognition BIBREF28 and correcting spelling, in contrast it is generally accepted that semantic details are encoded by word-embedding based language models BIBREF29 ."]}
{"question_id": "d5d4504f419862275a532b8e53d0ece16e0ae8d1", "predicted_answer": "", "predicted_evidence": ["Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text.", "Recently, there has been renewed interest in multimodal machine learning problems. BIBREF19 demonstrate an effective image captioning system that uses a CNN to encode an image which is used as the input to an LSTM BIBREF20 decoder, producing the output caption. This encoder-decoder architecture forms the basis for successful approaches to other multimodal problems such as visual question answering BIBREF21 . Another body of work focuses on the problem of unifying information from different modes of information. BIBREF22 propose to concatenate together the output of a text-based distributional model (such as word2vec BIBREF23 ) with an encoding produced from a CNN applied to images of the word. BIBREF24 demonstrate an alternative approach to concatenation, where instead the a word embedding is learned that minimizes a joint loss function involving context-prediction and image reconstruction losses.", "For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation.", "In this section, we formulate a novel extraction model for the task that builds upon the architectures used recently in tasks such as image captioning, question answering, VQA, etc. The model is composed of three separate modules: (1) an encoding module that uses modern neural architectures to jointly embed the query, text, and images into a common latent space, (2) a fusion module that combines these embedded vectors using an attribute-specific attention mechanism to a single dense vector, and (3) a similarity-based value decoder which produces the final value prediction. We provide an overview of this architecture in Figure 3 .", "There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation."]}
{"question_id": "f1e70b63c45ab0fc35dc63de089c802543e30c8f", "predicted_answer": "", "predicted_evidence": ["BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB. This is essentially the approach used to construct the knowledge base of attribute-value pairs used in our work, which is automatically performed by Diffbot's Product API.", "Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "The introduction of large curated datasets has driven progress in many fields of machine learning. Notable examples include: The Penn Treebank BIBREF5 for syntactic parsing models, Imagenet BIBREF7 for object recognition, Flickr30k BIBREF16 and MS COCO BIBREF17 for image captioning, SQuAD BIBREF6 for question answering and VQA BIBREF18 for visual question answering. Despite the interest in related tasks, there is currently no publicly available dataset for attribute extraction, let alone multimodal attribute extraction. This creates a high barrier to entry as anyone interested in attribute extraction must go through the expensive and time-consuming process of acquiring a dataset. Furthermore, there is no way to compare the effectiveness of different techniques. Our dataset aims to address this concern.", "There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation."]}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "predicted_answer": "", "predicted_evidence": ["The introduction of large curated datasets has driven progress in many fields of machine learning. Notable examples include: The Penn Treebank BIBREF5 for syntactic parsing models, Imagenet BIBREF7 for object recognition, Flickr30k BIBREF16 and MS COCO BIBREF17 for image captioning, SQuAD BIBREF6 for question answering and VQA BIBREF18 for visual question answering. Despite the interest in related tasks, there is currently no publicly available dataset for attribute extraction, let alone multimodal attribute extraction. This creates a high barrier to entry as anyone interested in attribute extraction must go through the expensive and time-consuming process of acquiring a dataset. Furthermore, there is no way to compare the effectiveness of different techniques. Our dataset aims to address this concern.", "Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text.", "Recently, there has been renewed interest in multimodal machine learning problems. BIBREF19 demonstrate an effective image captioning system that uses a CNN to encode an image which is used as the input to an LSTM BIBREF20 decoder, producing the output caption. This encoder-decoder architecture forms the basis for successful approaches to other multimodal problems such as visual question answering BIBREF21 . Another body of work focuses on the problem of unifying information from different modes of information. BIBREF22 propose to concatenate together the output of a text-based distributional model (such as word2vec BIBREF23 ) with an encoding produced from a CNN applied to images of the word. BIBREF24 demonstrate an alternative approach to concatenation, where instead the a word embedding is learned that minimizes a joint loss function involving context-prediction and image reconstruction losses.", "We evaluate on a subset of the MAE dataset consisting of the 100 most common attributes, covering roughly 50% of the examples in the overall MAE dataset. To determine the relative effectiveness of the different modes of information, we train image and text only versions of the model described above. Following the suggestions in BIBREF15 we use a 600 unit single layer in our text convolutions, and a 5 word window size. We apply dropout to the output of both the image and text CNNs before feeding the output through fully connected layers to obtain the image and text embeddings. Employing a coarse grid search, we found models performed best using a large embedding dimension of $k=1024$ . Lastly, we explore multimodal models using both the Concat and the GMU strategies. To evaluate models we use the hits@ $k$ metric on the values.", "There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation."]}
{"question_id": "4e0df856b39055a9ba801cc9c8e56d5b069bda11", "predicted_answer": "", "predicted_evidence": ["BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB. This is essentially the approach used to construct the knowledge base of attribute-value pairs used in our work, which is automatically performed by Diffbot's Product API.", "We evaluate on a subset of the MAE dataset consisting of the 100 most common attributes, covering roughly 50% of the examples in the overall MAE dataset. To determine the relative effectiveness of the different modes of information, we train image and text only versions of the model described above. Following the suggestions in BIBREF15 we use a 600 unit single layer in our text convolutions, and a 5 word window size. We apply dropout to the output of both the image and text CNNs before feeding the output through fully connected layers to obtain the image and text embeddings. Employing a coarse grid search, we found models performed best using a large embedding dimension of $k=1024$ . Lastly, we explore multimodal models using both the Concat and the GMU strategies. To evaluate models we use the hits@ $k$ metric on the values.", "To asses the difficulty of the task and the dataset, we first conduct a human evaluation study using Mechanical Turk that demonstrates that all available modes of information are useful for detecting values. We also train and provide results for a variety of machine learning models on the dataset. We observe that a simple most-common value classifier, which always predicts the most-common value for a given attribute, provides a very difficult baseline for more complicated models to beat (33% accuracy). In our current experiments, we are unable to train an image-only classifier that can outperform this simple model, despite using modern neural architectures such as VGG-16 BIBREF8 and Google's Inception-v3 BIBREF9 . However, we are able to obtain significantly better performance using a text-only classifier (59% accuracy). We hope to improve and obtain more accurate models in further research.", "In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data.", "Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer."]}
{"question_id": "bbc6d0402cae16084261f8558cebb4aa6d5b1ea5", "predicted_answer": "", "predicted_evidence": ["In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data.", "Model predictions for the example shown in Figure 1 are given in Table 2 , along with their similarity scores. Observe that the predictions made by the current image baseline model are almost identical to the most-common value model. This suggests that our current image baseline model is essentially ignoring all of the image related information and instead learning to predict common values.", "BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB. This is essentially the approach used to construct the knowledge base of attribute-value pairs used in our work, which is automatically performed by Diffbot's Product API.", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest.", "For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation."]}
{"question_id": "a7e03d24549961b38e15b5386d9df267900ef4c8", "predicted_answer": "", "predicted_evidence": ["In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data.", "Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer.", "We evaluate on a subset of the MAE dataset consisting of the 100 most common attributes, covering roughly 50% of the examples in the overall MAE dataset. To determine the relative effectiveness of the different modes of information, we train image and text only versions of the model described above. Following the suggestions in BIBREF15 we use a 600 unit single layer in our text convolutions, and a 5 word window size. We apply dropout to the output of both the image and text CNNs before feeding the output through fully connected layers to obtain the image and text embeddings. Employing a coarse grid search, we found models performed best using a large embedding dimension of $k=1024$ . Lastly, we explore multimodal models using both the Concat and the GMU strategies. To evaluate models we use the hits@ $k$ metric on the values.", "There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values."]}
{"question_id": "036c400424357457e42b22df477b7c3cdc2eefe9", "predicted_answer": "", "predicted_evidence": ["Model predictions for the example shown in Figure 1 are given in Table 2 , along with their similarity scores. Observe that the predictions made by the current image baseline model are almost identical to the most-common value model. This suggests that our current image baseline model is essentially ignoring all of the image related information and instead learning to predict common values.", "To our knowledge, we are the first to study the problem of attribute extraction from multimodal data. However the problem of attribute extraction from text is well studied. BIBREF1 treat attribute extraction of retail products as a form of named entity recognition. They predefine a list of attributes to extract and train a Na\u00efve Bayes model on a manually labeled seed dataset to extract the corresponding values. BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description.", "There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation.", "In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data.", "Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer."]}
{"question_id": "63eda2af88c35a507fbbfda0ec1082f58091883a", "predicted_answer": "", "predicted_evidence": ["There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "Model predictions for the example shown in Figure 1 are given in Table 2 , along with their similarity scores. Observe that the predictions made by the current image baseline model are almost identical to the most-common value model. This suggests that our current image baseline model is essentially ignoring all of the image related information and instead learning to predict common values.", "Recently, there has been renewed interest in multimodal machine learning problems. BIBREF19 demonstrate an effective image captioning system that uses a CNN to encode an image which is used as the input to an LSTM BIBREF20 decoder, producing the output caption. This encoder-decoder architecture forms the basis for successful approaches to other multimodal problems such as visual question answering BIBREF21 . Another body of work focuses on the problem of unifying information from different modes of information. BIBREF22 propose to concatenate together the output of a text-based distributional model (such as word2vec BIBREF23 ) with an encoding produced from a CNN applied to images of the word. BIBREF24 demonstrate an alternative approach to concatenation, where instead the a word embedding is learned that minimizes a joint loss function involving context-prediction and image reconstruction losses.", "We evaluate on a subset of the MAE dataset consisting of the 100 most common attributes, covering roughly 50% of the examples in the overall MAE dataset. To determine the relative effectiveness of the different modes of information, we train image and text only versions of the model described above. Following the suggestions in BIBREF15 we use a 600 unit single layer in our text convolutions, and a 5 word window size. We apply dropout to the output of both the image and text CNNs before feeding the output through fully connected layers to obtain the image and text embeddings. Employing a coarse grid search, we found models performed best using a large embedding dimension of $k=1024$ . Lastly, we explore multimodal models using both the Concat and the GMU strategies. To evaluate models we use the hits@ $k$ metric on the values."]}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "predicted_answer": "", "predicted_evidence": ["Intuitively, the two policy gradient methods described in this section have strong relationships to MLE, since training signals are based on the gradients of caption log-likelihoods. We illustrate the training settings of MLE and the two proposed methods in Figure FIGREF8. In MLE, we train the model using positive captions only and treat all positive captions equally, as illustrated in Figure FIGREF8a: the parameters are updated by the gradients of log-likelihoods of ground-truth captions $c_\\mathrm {GT}$. The on-policy policy gradient method (Eq. (DISPLAY_FORM14)) instead computes the gradients of reward-weighted log-likelihoods of sample captions $c_s$ over all possible captions. By sampling from the policy distribution (on-policy), we may sample captions whose true rating scores are not known (not in the dataset).", "where $\\mathcal {J}_\\mathrm {MLE}$ is the average log-likelihood of ground-truth captions in $\\mathcal {D}_\\mathrm {IC}$, and $\\alpha $ is a hyper-parameter that balances the regularization effect.", "There have been multiple attempts to define metrics that evaluate the quality of generated captions. Several studies proposed automatic metrics using ground-truth captions. A few of them are adopted from machine translation community and are based on $n$-gram matches between ground-truth and generated captions; BLEU BIBREF16 and ROUGE BIBREF17 measures precision and recall based on $n$-gram matches, respectively, while METEOR BIBREF18 incorporates alignments between $n$-gram matches. In the context of evaluating image caption quality specifically, CIDEr BIBREF8 and SPICE BIBREF19 utilize more corpus-level and semantic signals to measure matches between generated and ground-truth captions. Aside from these handcrafted metrics, a recent study proposes to learn an automatic metric from a captioning dataset BIBREF1, while another uses semantic similarity between object labels identified in the image and the words in the caption BIBREF20. To overcome the limitations imposed by the automatic metrics, several studies evaluate their models using human judgments BIBREF0, BIBREF2, BIBREF15, BIBREF14.", "In the other type of evaluation, we measure the relative improvement of a model against the Baseline model; Three professional raters are shown the input image and two captions (anonymized and randomly shuffled with respect to their left/right position) side-by-side. One of the captions is from a candidate model and the other always from Baseline. We ask for relative judgments on three dimensions \u2013 Informativeness, Correctness and Fluency, using their corresponding questions shown in Table TABREF32. Each of these dimensions allows a 5-way choice, shown below together with their corresponding scores:", "In addition, we train a caption rating estimator for the OnPG method using the Caption-Quality dataset. The rating estimator extracts the same types of visual features as the captioning model above, and embeds the input caption with a pretrained BERT encoder BIBREF33. We concatenate all these features after projecting into a common embedding space and predict the human ratings of the input image/caption pair. To feed the generated captions from the captioning model directly into the rating estimator, we share the vocabulary (but not the token embeddings) between the two models. We fix the pretrained image feature extraction modules in both models during training, as well as the BERT encoder of the rating estimator."]}
{"question_id": "0b1b8e1b583242e5be9b7be73160630a0d4a96b2", "predicted_answer": "", "predicted_evidence": ["We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.", "In the first type of evaluation, 6 distinct raters are asked to judge each image caption as good or bad. They are shown the image and caption with the \u201cGoodness\u201d question prompt shown in Table TABREF32. The bad or good rating is translated to 0 or 1, respectively. We measure \u201caverage\u201d goodness score as the average of all the ratings over the test set. We also report a \u201cvoting\u201d score which is the average of the binarized score for each caption based on majority voting. Note that both the \u201caverage\u201d and \u201cvoting\u201d scores are in the range $[0, 1]$, where higher values denote better model performance.", "By sampling from the policy distribution (on-policy), we may sample captions whose true rating scores are not known (not in the dataset). The on-policy method thus approximates the rating function by a rating estimator $\\tilde{r}(c|I)$, depicted by the background gradient in Figure FIGREF8b. However, the mismatch between the true rating function and the estimator (depicted by the gap between solid and dashed lines) can degenerate the quality of the resulting captioning model. On the other hand, the off-policy method focuses on the captions with true rating scores in the dataset, by changing the sampling distribution. In contrast to MLE, where each sample is viewed as equally correct and important, the off-policy method weights each caption by its rating, and therefore includes captions with negative feedback, as illustrated in Figure FIGREF8c. Note that, in the off-policy method, the baseline determines the threshold for positive/negative feedback; captions with ratings below the baseline are explicitly penalized, while the others are positively rewarded.", "In the other type of evaluation, we measure the relative improvement of a model against the Baseline model; Three professional raters are shown the input image and two captions (anonymized and randomly shuffled with respect to their left/right position) side-by-side. One of the captions is from a candidate model and the other always from Baseline. We ask for relative judgments on three dimensions \u2013 Informativeness, Correctness and Fluency, using their corresponding questions shown in Table TABREF32. Each of these dimensions allows a 5-way choice, shown below together with their corresponding scores:", "Since the dataset provides ratings only for a small set of images and captions, we do not have a generic reward function for random image-caption pairs. Therefore, it is not straightforward to apply policy gradient method that requires a reward for randomly sampled captions. To address this challenge, we use an off-policy technique and force the network to sample captions for which ratings are available in the dataset. We evaluate the effectiveness of our method using human evaluation studies on the T2 test set used for the Conceptual Captions Challenge, using both a similar human evaluation methodology and an additional, multi-dimensional side-by-side human evaluation strategy. Additionally, the human raters in our evaluation study are different from the ones that provided the caption ratings in BIBREF13, thereby ensuring that the results are independent of using a specific human-evaluator pool. The results of our human evaluations indicate that the proposed method improves the image captioning quality, by effectively leveraging both the positive and negative signals from the captions ratings dataset."]}
{"question_id": "830f9f9499b06fb4ac3ce2f2cf035127b4f0ec63", "predicted_answer": "", "predicted_evidence": ["Table TABREF38 shows the goodness scores from the single-caption evaluation. Both \u201caverage\u201d and \u201cvoting\u201d metrics clearly indicate that OffPG significantly improves over Baseline, while the other methods achieve only marginal gains, all of which are within the error range. Baseline$+(t)$ models use only 1.5% and 2.2% additional data, at $t=0.7$ and $t=0.5$, respectively, with insignificant impact. Moreover, these methods only maximize the likelihood of the additional captions, which are already generated with high likelihood by previous models trained on the same dataset, which results in self-reinforcement. In contrast, the policy gradient methods are allowed to utilize the negative feedback to directly penalize incorrect captions. However, OnPG fails to improve the quality, most likely because it relies on a noisy caption ratings estimator that fails to generalize well over the large space of possible captions.", "There have been multiple attempts to define metrics that evaluate the quality of generated captions. Several studies proposed automatic metrics using ground-truth captions. A few of them are adopted from machine translation community and are based on $n$-gram matches between ground-truth and generated captions; BLEU BIBREF16 and ROUGE BIBREF17 measures precision and recall based on $n$-gram matches, respectively, while METEOR BIBREF18 incorporates alignments between $n$-gram matches. In the context of evaluating image caption quality specifically, CIDEr BIBREF8 and SPICE BIBREF19 utilize more corpus-level and semantic signals to measure matches between generated and ground-truth captions. Aside from these handcrafted metrics, a recent study proposes to learn an automatic metric from a captioning dataset BIBREF1, while another uses semantic similarity between object labels identified in the image and the words in the caption BIBREF20. To overcome the limitations imposed by the automatic metrics, several studies evaluate their models using human judgments BIBREF0, BIBREF2, BIBREF15, BIBREF14.", "Intuitively, the two policy gradient methods described in this section have strong relationships to MLE, since training signals are based on the gradients of caption log-likelihoods. We illustrate the training settings of MLE and the two proposed methods in Figure FIGREF8. In MLE, we train the model using positive captions only and treat all positive captions equally, as illustrated in Figure FIGREF8a: the parameters are updated by the gradients of log-likelihoods of ground-truth captions $c_\\mathrm {GT}$. The on-policy policy gradient method (Eq. (DISPLAY_FORM14)) instead computes the gradients of reward-weighted log-likelihoods of sample captions $c_s$ over all possible captions. By sampling from the policy distribution (on-policy), we may sample captions whose true rating scores are not known (not in the dataset).", "In our experiments, we use the Caption-Quality dataset BIBREF13, recently introduced for the purpose of training quality-estimation models for image captions. We re-purpose this data as our caption ratings dataset $\\mathcal {D}_\\mathrm {CR}$. The dataset is divided into training, validation and test splits containing approximately 130K, 7K and 7K rated captions, respectively. Each image has an average of 4.5 captions (generated by different models that underwent evaluation evaluation). The captions are individually rated by asking raters the question \u201cIs this a good caption for the image?\u201d, with the answers \u201cNO\u201d or \u201cYES\u201d mapped to a 0 or 1 score, respectively.", "Figure FIGREF45 shows the results of these ablation experiments, for which we performed side-by-side comparisons over a 200-image subset from the T2 dataset. The results indicate that a very small $\\alpha $ limits the impact of the additional signal for both models, since the regularization effect from the original loss term becomes too strong. By allowing updates using policy gradient with a larger $\\alpha $ value, OffPG improves the performances along all three dimensions, whereas the performance of OnPG starts degrading at higher $\\alpha $ values. At $\\alpha =100$, OnPG drastically suffers from mode collapse and ends up generating a single caption for every image. This mode collapse is a result of poor generalization of the rating estimator: the collapsed captions are structurally ill-formed (e.g., an empty string, or a string with simply a period `.'), but they receive high rating estimates ($>0.9$) from the estimator. Although we can (and did) introduce some heuristics to avoid some of these failure cases in the estimator, we observe that OnPG training would continue to suffer from the estimator failing to generalize well over the vast space of possible captions."]}
{"question_id": "a606bffed3bfeebd1b66125be580f908244e5d92", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF45 shows the results of these ablation experiments, for which we performed side-by-side comparisons over a 200-image subset from the T2 dataset. The results indicate that a very small $\\alpha $ limits the impact of the additional signal for both models, since the regularization effect from the original loss term becomes too strong. By allowing updates using policy gradient with a larger $\\alpha $ value, OffPG improves the performances along all three dimensions, whereas the performance of OnPG starts degrading at higher $\\alpha $ values. At $\\alpha =100$, OnPG drastically suffers from mode collapse and ends up generating a single caption for every image. This mode collapse is a result of poor generalization of the rating estimator: the collapsed captions are structurally ill-formed (e.g., an empty string, or a string with simply a period `.'), but they receive high rating estimates ($>0.9$) from the estimator. Although we can (and did) introduce some heuristics to avoid some of these failure cases in the estimator, we observe that OnPG training would continue to suffer from the estimator failing to generalize well over the vast space of possible captions.", "Figure FIGREF45 shows the results of these ablation experiments, for which we performed side-by-side comparisons over a 200-image subset from the T2 dataset. The results indicate that a very small $\\alpha $ limits the impact of the additional signal for both models, since the regularization effect from the original loss term becomes too strong. By allowing updates using policy gradient with a larger $\\alpha $ value, OffPG improves the performances along all three dimensions, whereas the performance of OnPG starts degrading at higher $\\alpha $ values. At $\\alpha =100$, OnPG drastically suffers from mode collapse and ends up generating a single caption for every image. This mode collapse is a result of poor generalization of the rating estimator: the collapsed captions are structurally ill-formed (e.g., an empty string, or a string with simply a period `.'), but they receive high rating estimates ($>0.9$) from the estimator.", "Each model is evaluated by the average rating scores from 3 distinct raters. As a result, we obtain 3 values for each model in the range $[-1, 1]$, where a negative score means a performance degradation in the given dimension with respect to Baseline. For every human evaluation, we report confidence intervals based on bootstrap resampling BIBREF35.", "By allowing updates using policy gradient with a larger $\\alpha $ value, OffPG improves the performances along all three dimensions, whereas the performance of OnPG starts degrading at higher $\\alpha $ values. At $\\alpha =100$, OnPG drastically suffers from mode collapse and ends up generating a single caption for every image. This mode collapse is a result of poor generalization of the rating estimator: the collapsed captions are structurally ill-formed (e.g., an empty string, or a string with simply a period `.'), but they receive high rating estimates ($>0.9$) from the estimator. Although we can (and did) introduce some heuristics to avoid some of these failure cases in the estimator, we observe that OnPG training would continue to suffer from the estimator failing to generalize well over the vast space of possible captions. This observation is similar to the mode collapsing phenomenon seen when training generative adversarial networks (GANs), but even more severe as the estimator in OnPG is fixed (unlike the discriminators in GANs which are trained simultaneously).", "There have been multiple attempts to define metrics that evaluate the quality of generated captions. Several studies proposed automatic metrics using ground-truth captions. A few of them are adopted from machine translation community and are based on $n$-gram matches between ground-truth and generated captions; BLEU BIBREF16 and ROUGE BIBREF17 measures precision and recall based on $n$-gram matches, respectively, while METEOR BIBREF18 incorporates alignments between $n$-gram matches. In the context of evaluating image caption quality specifically, CIDEr BIBREF8 and SPICE BIBREF19 utilize more corpus-level and semantic signals to measure matches between generated and ground-truth captions. Aside from these handcrafted metrics, a recent study proposes to learn an automatic metric from a captioning dataset BIBREF1, while another uses semantic similarity between object labels identified in the image and the words in the caption BIBREF20."]}
{"question_id": "f8fe4049bea86d0518d1881f32049e60526d0f34", "predicted_answer": "", "predicted_evidence": ["While specifying an MRE, the user can interact with the visualization and editor views to make sure the MRE expresses the intent. The color-sensitive text view in Figure FIGREF29 shows the highlighted tag matches after the user called the MRE simulator using the Tagtypes menu.", "The MBF match visualizer shows color sensitive text view, the tag list view, and the tag description view. The tag description view presents the details of the selected tag along with the relevant tag type information. The user can edit the tags using a context sensitive menus. MERF GUI also allows manual tag types and corresponding tags that are not based on morphological features. This enables building reference corpora without help from the morphological analyzer.", "A morphology-based Boolean formula (MBF) is of the following form.", "Up to our knowledge, INLINEFORM0 provides the first light Arabic WordNet based on the lexicon of Sarf. The sets INLINEFORM1 and INLINEFORM2 denote all English words, Arabic words, and Arabic lexicon words, respectively. Recall that INLINEFORM3 and INLINEFORM4 denote the set of glosses and stems in the morphological analyzer, respectively. We have INLINEFORM5 and INLINEFORM6 . Function INLINEFORM7 maps Arabic stems to subsets of related English glosses, where INLINEFORM8 denotes the power set of INLINEFORM9 which is the set of all subsets of INLINEFORM10 . Function INLINEFORM11 maps Arabic lexicon words to subsets of relevant Arabic stems.", ""]}
{"question_id": "a9eb8039431e2cb885cfcf96eb58c0675b36b3bd", "predicted_answer": "", "predicted_evidence": ["As we can see, the auxiliary predictor increased the number of tags by $33\\%$ relatively, significantly increasing the number of mentions of type organization and person \u2013 which happen to be the least frequent tags.", "Following the proposed method, we generate SESAME, a dataset for Portuguese NER. Although not a gold standard dataset, it allows for training of data-hungry predictors in a weakly-supervised fashion, alleviating the need for manually-annotated data. We show experimentally that SESAME can be used to train competitive NER predictors, or improve the performance of NER models when used alongside gold-standard data. We hope to increase interest in the study of automatic generation of silver-standard datasets, aimed at distant learning of complex models. Although SESAME is a dataset for the Portuguese language, the underlying method can be applied to virtually any language that is covered by Wikipedia.", "In particular, for the First HAREM $F_1$ score: (1) as the corpus defines multiple tags for the same segments of the text, the evaluation also accepts multiple correct answers; (2) partial matches are considered and positively impact the score.", "The HAREM corpus follows a different format than the one of SESAME: it uses a markup structure, without a proper tokenization of sentences and words. To circumvent this, we convert it to BIO format by applying the same tokenization process used for generating our dataset.", "Table TABREF47 shows a size comparison between SESAME and popular datasets for Portuguese NER."]}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "predicted_answer": "", "predicted_evidence": ["Table TABREF49 presents the proportion of matched and detected mentions for each entity type \u2013 recall that tagged mentions have either been matched to DBpedia (hence have been manually annotated) or have been detected by the auxiliary NER system Polyglot.", "Raising information relevant to the calibration and evaluation of model performance e.g. proportion of each entity type and of each annotation source (DBpedia or auxiliary NER system)", "Domain specific (e.g. chemistry, math)", "The next step consists of detecting mentions to entities in the raw text. To do this, we tag character segments that exactly match one of the known names of an entity. For instance, we can tag two different entities in the following text:", "To circumvent mentioned entities which are not present in DBpedia, we use an auxiliary NER system to detect such mentions. More specifically, we use the Polyglot BIBREF12 system, a model trained on top of a dataset generated from Wikipedia."]}
{"question_id": "a82686c054b96f214521e468b17f0435e6cdf7cf", "predicted_answer": "", "predicted_evidence": ["The standard evaluation metric for NER is the $F_1$ score:", "SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43.", "As we can see, the auxiliary predictor increased the number of tags by $33\\%$ relatively, significantly increasing the number of mentions of type organization and person \u2013 which happen to be the least frequent tags.", "SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.", "Lists, (e.g. unbulled list, flatlist, bulleted list)"]}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "predicted_answer": "", "predicted_evidence": ["Supported by Foundation Research Funds for the Central Universities (Program No.2662017JC049) and State Scholarship Fund (NO.261606765054).", "As knowledge sharing and Q&A platforms continue to gain a greater popularity, the released dataset ZhihuLive-DB could greatly help researchers in related fields. However, current data and attributes are relatively unitary in ZhihuLive-DB. The malicious comment and assessment on SNS platforms are also very important issues to be taken into consideration. In our future work, we will gather richer dataset, and integrate malicious comments detector into our data-driven approach.", "The review scores are used as labels in our experiments, our task is to precisely estimate the scores with MTNet. Since the data-driven methods are based on crowd wisdom on Zhihu Live platform, they don't need any additional labeling work, and ensure the reliability of the scores of judgment as well.", "Finally, we get 15-dimension feature vector as the input for conventional (non-deep learning based) regressors.", "We train our MTNet with Adam optimizer for 20 epochs. We set batch size as 8, and weight decay as 1e-5, we adopt 3 branched layers in MTNet. Detailed configuration is shown in Table TABREF21 . We use ReLU in shared layers, and relu6 in branched layers to prevent information loss. Our proposed MTNet achieves 0.2250 on MAE and 0.3216 on RMSE, respectively."]}
{"question_id": "2ae66798333b905172e2c0954e9808662ab7f221", "predicted_answer": "", "predicted_evidence": ["As knowledge sharing and Q&A platforms continue to gain a greater popularity, the released dataset ZhihuLive-DB could greatly help researchers in related fields. However, current data and attributes are relatively unitary in ZhihuLive-DB. The malicious comment and assessment on SNS platforms are also very important issues to be taken into consideration. In our future work, we will gather richer dataset, and integrate malicious comments detector into our data-driven approach.", "The results are listed in Table TABREF37 . Our method achieves the best performance in contrast to the compared baseline regressors.", "Deep neural network can learn more abstract features via stacked layers. Deep learning has empowered many AI tasks (like computer vision BIBREF6 and natural language processing BIBREF9 ) in an end-to-end fashion. We apply deep learning to our Zhihu Live quality evaluation problem. Furthermore, we also compare our MTNet algorithm with baseline models with carefully designed features.", "We calculate the correlation between each regressor and label as: INLINEFORM0 .", "We normalize the numerical values with minimum subtraction and range division to ensure values [0, 1] intervals."]}
{"question_id": "9d80ad8cf4d5941a32d33273dc5678195ad1e0d2", "predicted_answer": "", "predicted_evidence": ["Table TABREF22 shows two examples of quality prediction on the validation data of WMT2018 QE task for English-Czech. In the first example, the model without POS tags and baseline features is biased towards predicting \u201cOK\u201d tags, while the model with full features can detect the reordering error. In the second example, the target word \u201cpanelu\u201d is a variant of the reference word \u201cpanel\u201d. The target word \u201cznaky\u201d is the plural noun of the reference \u201cznak\u201d. Thus, their POS tags have some subtle differences. Note the target word \u201czmnit\u201d and its aligned source word \u201cchange\u201d are both verbs. We can observe that POS tags can help the model capture such syntactic variants.", "During training, we find that the model can easily overfit the training data, which yields poor performance on the test and validation sets. To make the model more stable on the unseen data, we apply dropout to the word embeddings, POS embeddings, vectors after the convolutional layers and the stacked recurrent layers. In Figure FIGREF24 , we examine the accuracies dropout rates in INLINEFORM0 . We find that adding dropout alleviates overfitting issues on the training set. If we reduce the dropout rate to INLINEFORM1 , which means randomly setting some values to zero with probability INLINEFORM2 , the training F1-Multi increases rapidly and the validation F1-multi score is the lowest among all the settings. Preliminary results proved best for a dropout rate of INLINEFORM3 , so we use this in all the experiments.", "The authors thank Andre Martins for his advice regarding the word-level QE task.", "We concatenate the 31 baseline features extracted by the Marmot toolkit with the last 50 feed-forward hidden features. The baseline features are listed in Table TABREF13 . We then apply a softmax layer on the combined features to predict the binary labels.", "The output of the one-dimensional convolution layer, INLINEFORM0 , is then concatenated with the embedding of POS tags of the target words, as well as its aligned source words, to provide a more direct signal to the following recurrent layers."]}
{"question_id": "bd817a520a62ddd77e65e74e5a7e9006cdfb19b3", "predicted_answer": "", "predicted_evidence": ["This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.", "The authors thank Andre Martins for his advice regarding the word-level QE task.", "After we obtain the representation of the source-target word pair by the convolution layer, we follow a similar architecture as BIBREF6 to refine the representation of the word pairs using feed-forward and recurrent networks.", "The output of the one-dimensional convolution layer, INLINEFORM0 , is then concatenated with the embedding of POS tags of the target words, as well as its aligned source words, to provide a more direct signal to the following recurrent layers.", "We concatenate the 31 baseline features extracted by the Marmot toolkit with the last 50 feed-forward hidden features. The baseline features are listed in Table TABREF13 . We then apply a softmax layer on the combined features to predict the binary labels."]}
{"question_id": "c635295c2b77aaab28faecca3b5767b0c4ab3728", "predicted_answer": "", "predicted_evidence": ["Quality estimation (QE) refers to the task of measuring the quality of machine translation (MT) system outputs without reference to the gold translations BIBREF0 , BIBREF1 . QE research has grown increasingly popular due to the improved quality of MT systems, and potential for reductions in post-editing time and the corresponding savings in labor costs BIBREF2 , BIBREF3 . QE can be performed on multiple granularities, including at word level, sentence level, or document level. In this paper, we focus on quality estimation at word level, which is framed as the task of performing binary classification of translated tokens, assigning \u201cOK\u201d or \u201cBAD\u201d labels.", "Early work on this problem mainly focused on hand-crafted features with simple regression/classification models BIBREF4 , BIBREF5 . Recent papers have demonstrated that utilizing recurrent neural networks (RNN) can result in large gains in QE performance BIBREF6 . However, these approaches encode the context of the target word by merely concatenating its left and right context words, giving them limited ability to control the interaction between the local context and the target word.", "In this paper, we propose a neural architecture, Context Encoding Quality Estimation (CEQE), for better encoding of context in word-level QE. Specifically, we leverage the power of both (1) convolution modules that automatically learn local patterns of surrounding words, and (2) hand-crafted features that allow the model to make more robust predictions in the face of a paucity of labeled data. Moreover, we further utilize stacked recurrent neural networks to capture the long-term dependencies and global context information from the whole sentence.", "In Table TABREF21 , we show the ablation study of the features used in our model on English-German, German-English, and English-Czech. For each language pair, we show the performance of CEQE without adding the corresponding components specified in the second column respectively. The last row shows the performance of the complete CEQE with all the components. As the baseline features released in the WMT2018 QE Shared Task for English-Latvian are incomplete, we train our CEQE model without using such features. We can glean several observations from this data:", "The QE module receives as input a tuple INLINEFORM0 , where INLINEFORM1 is the source sentence, INLINEFORM2 is the translated sentence, and INLINEFORM3 is a set of word alignments. It predicts as output a sequence INLINEFORM4 , with each INLINEFORM5 . The overall architecture is shown in Figure FIGREF2"]}
{"question_id": "7f8fc3c7d59aba80a3e7c839db6892a1fc329210", "predicted_answer": "", "predicted_evidence": ["A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component.", "As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5).", "This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component. The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset).", "Named Entity Recognition (NER) is an important step in most of the natural language processing (NLP) pipelines. It is designed to robustly handle proper names, which is essential for many applications. Although a seemingly simple task, it faces a number of challenges in noisy datasets and it is still considered an emerging research area BIBREF0 , BIBREF1 . Despite recent efforts, we still face limitations at identifying entities and (consequently) correctly classifying them. Current state-of-the-art NER systems typically have about 85-90% accuracy on news text - such as articles (CoNLL03 shared task data set) - but they still perform poorly (about 30-50% accuracy) on short texts, which do not have implicit linguistic formalism (e.g. punctuation, spelling, spacing, formatting, unorthodox capitalisation, emoticons, abbreviations and hashtags) BIBREF2 , BIBREF3 , BIBREF4 , BIBREF1 .", "It is designed to robustly handle proper names, which is essential for many applications. Although a seemingly simple task, it faces a number of challenges in noisy datasets and it is still considered an emerging research area BIBREF0 , BIBREF1 . Despite recent efforts, we still face limitations at identifying entities and (consequently) correctly classifying them. Current state-of-the-art NER systems typically have about 85-90% accuracy on news text - such as articles (CoNLL03 shared task data set) - but they still perform poorly (about 30-50% accuracy) on short texts, which do not have implicit linguistic formalism (e.g. punctuation, spelling, spacing, formatting, unorthodox capitalisation, emoticons, abbreviations and hashtags) BIBREF2 , BIBREF3 , BIBREF4 , BIBREF1 . Furthermore, the lack of external knowledge resources is an important gap in the process regardless of writing style BIBREF5 ."]}
{"question_id": "2d92ae6b36567e7edb6afdd72f97b06ac144fbdf", "predicted_answer": "", "predicted_evidence": ["A major reason is that they rely heavily on hand-crafted features and domain-specific knowledge. In terms of architecture, NER algorithms may also be designed based on generative (e.g., Naive Bayes) or discriminative (e.g., MaxEnt) models. Furthermore, sequence models (HMMs, CMM, MEMM and CRF) are a natural choice to design such systems. A more recent study proposed by Lample et al., 2016 BIBREF9 used neural architectures to solve this problem. Similarly in terms of architecture, Al-Rfou et al., 2015 BIBREF10 had also proposed a model (without dependency) that learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Chiu and Nichols, 2015 BIBREF11 proposed a neural network architecture that automatically detects word and character-level features using a hybrid bidirectional LSTM and CNN. Thus, these models work without resorting to any language-specific knowledge or resources such as gazetteers.", "Furthermore, sequence models (HMMs, CMM, MEMM and CRF) are a natural choice to design such systems. A more recent study proposed by Lample et al., 2016 BIBREF9 used neural architectures to solve this problem. Similarly in terms of architecture, Al-Rfou et al., 2015 BIBREF10 had also proposed a model (without dependency) that learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Chiu and Nichols, 2015 BIBREF11 proposed a neural network architecture that automatically detects word and character-level features using a hybrid bidirectional LSTM and CNN. Thus, these models work without resorting to any language-specific knowledge or resources such as gazetteers. They, however, focused on newswire to improve current state-of-the-art systems and not on the microblogs context, in which they are naturally harder to outperform due to the aforementioned issues.", "To the best of our knowledge, this is the first report of a NER architecture which aims to provide a priori information based on clusters of images and text features.", "Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), \u201cfaces\u201d from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection.", "where INLINEFORM0 and INLINEFORM1 represent the INLINEFORM2 and INLINEFORM3 position of INLINEFORM4 and INLINEFORM5 , respectively. INLINEFORM6 represents the n-gram of POS tag. INLINEFORM7 and INLINEFORM8 ( INLINEFORM9 ) represent the total objects found by a classifier INLINEFORM10 for a given class INLINEFORM11 ( INLINEFORM12 ) (where N is the total of retrieved images INLINEFORM15 ). INLINEFORM16 and INLINEFORM17 represent the distance between the two higher predictions ( INLINEFORM18 ), i.e. INLINEFORM19 . Finally, INLINEFORM20 represents the sum of all predictions made by all INLINEFORM21 classifiers INLINEFORM22 ( INLINEFORM23 ). - Training (E): the outcomes of D.1 and D.2 ( INLINEFORM26 ) are used as input features to our final classifier."]}
{"question_id": "a5df7361ae37b9512fb57cb93efbece9ded8cab1", "predicted_answer": "", "predicted_evidence": ["Over the past few years, the problem of recognizing named entities in natural language texts has been addressed by several approaches and frameworks BIBREF7 , BIBREF8 . Existing approaches basically adopt look-up strategies and use standard local features, such as part-of-speech tags, previous and next words, substrings, shapes and regex expressions, for instance. The main drawback is the performance of those models with noisy data, such as Tweets. A major reason is that they rely heavily on hand-crafted features and domain-specific knowledge. In terms of architecture, NER algorithms may also be designed based on generative (e.g., Naive Bayes) or discriminative (e.g., MaxEnt) models. Furthermore, sequence models (HMMs, CMM, MEMM and CRF) are a natural choice to design such systems.", "In the first step (A), we simply apply POS Tagging and Shallow Parsing to filter out tokens except for those tagged as INLINEFORM0 or INLINEFORM1 and their INLINEFORM2 (local context). Afterwards, we use the search engine (B) to query and cache (C) the top INLINEFORM3 texts and images associated to each term INLINEFORM4 , where INLINEFORM5 is the set resulting of the pre-processing step (A) for each (partial or complete) sentence INLINEFORM6 . This resulting data (composed of excerpts of texts and images from web pages) is used to predict a possible class for a given term. These outcomes are then used in the first two levels (D.1 and D.2) of our approach: the Computer Vision and Text Analytics components, respectively, which we introduce as follows:", "A more recent study proposed by Lample et al., 2016 BIBREF9 used neural architectures to solve this problem. Similarly in terms of architecture, Al-Rfou et al., 2015 BIBREF10 had also proposed a model (without dependency) that learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Chiu and Nichols, 2015 BIBREF11 proposed a neural network architecture that automatically detects word and character-level features using a hybrid bidirectional LSTM and CNN. Thus, these models work without resorting to any language-specific knowledge or resources such as gazetteers. They, however, focused on newswire to improve current state-of-the-art systems and not on the microblogs context, in which they are naturally harder to outperform due to the aforementioned issues. According to Derczynski et al., 2015 BIBREF1 some approaches have been proposed for Twitter, but they are mostly still in development and often not freely available.", "Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical.", "The main insight underlying this work is that we can produce a NER model which performs similarly to state-of-the-art approaches but without relying on any specific resource or encoded rule. To this aim, we propose a multi-level architecture which intends to produce biased indicators to a certain class (LOC, PER or ORG). These outcomes are then used as input features for our final classifier. We perform clustering on images and texts associated to a given term INLINEFORM0 existing in complete or partial sentences INLINEFORM1 (e.g., \u201cnew york\u201d or \u201ceinstein\u201d), leveraging the global context obtained from the Web providing valuable insights apart from standard local features and hand-coded information. fig:architecture gives an overview of the proposed architecture."]}
{"question_id": "915e4d0b3cb03789a20380ead961d473cb95bfc3", "predicted_answer": "", "predicted_evidence": ["The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), \u201cfaces\u201d from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection. These datasets produces the training data for our set of supervised classifiers (1 for ORG, 1 for PER and 10 for LOC).", "Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component. The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset). While initial results in a gold standard dataset showed the potential of the approach, we also plan to integrate these outcomes into a Sequence Labeling (SL) system, including neural architectures such as LSTM, which are more suitable for such tasks as NER or POS.", "A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected.", "In this paper we presented a novel architecture for NER that expands the feature set space based on feature clustering of images and texts, focused on microblogs. Due to their terse nature, such noisy data often lack enough context, which poses a challenge to the correct identification of named entities. To address this issue we have presented and evaluated a novel approach using the Ritter dataset, showing consistent results over state-of-the-art models without using any external resource or encoded rule, achieving an average of 0.59 F1. The results slightly outperformed state-of-the-art models which do not rely on encoded rules (0.49 and 0.54 F1), suggesting the viability of using the produced metadata to also boost existing NER approaches. A further important contribution is the ability to handle single tokens and misspelled words successfully, which is of utmost importance in order to better understand short texts.", "The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset). While initial results in a gold standard dataset showed the potential of the approach, we also plan to integrate these outcomes into a Sequence Labeling (SL) system, including neural architectures such as LSTM, which are more suitable for such tasks as NER or POS. We argue that this can potentially reduce the existing (significant) gap in NER performance on microblogs."]}
{"question_id": "c01a8b42fd27b0a3bec717ededd98b6d085a0f5c", "predicted_answer": "", "predicted_evidence": ["To the best of our knowledge, this is the first report of a NER architecture which aims to provide a priori information based on clusters of images and text features.", "In this paper, we propose a joint clustering architecture that aims at minimizing the current gap between world knowledge and knowledge available in open domain knowledge bases (e.g., Freebase) for NER systems, by extracting features from unstructured data sources. To this aim, we use images and text from the web as input data. Thus, instead of relying on encoded information and manually annotated resources (the major limitation in NER architectures) we focus on a multi-level approach for discovering named entities, combining text and image features with a final classifier based on a decision tree model. We follow an intuitive and simple idea: some types of images are more related to people (e.g. faces) whereas some others are more related to organisations (e.g. logos), for instance. This principle is applied similarly to the text retrieved from websites: keywords for search engines representing names and surnames of people will often return similarly related texts, for instance.", "A more recent study proposed by Lample et al., 2016 BIBREF9 used neural architectures to solve this problem. Similarly in terms of architecture, Al-Rfou et al., 2015 BIBREF10 had also proposed a model (without dependency) that learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Chiu and Nichols, 2015 BIBREF11 proposed a neural network architecture that automatically detects word and character-level features using a hybrid bidirectional LSTM and CNN. Thus, these models work without resorting to any language-specific knowledge or resources such as gazetteers. They, however, focused on newswire to improve current state-of-the-art systems and not on the microblogs context, in which they are naturally harder to outperform due to the aforementioned issues. According to Derczynski et al., 2015 BIBREF1 some approaches have been proposed for Twitter, but they are mostly still in development and often not freely available.", "A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component.", "where INLINEFORM0 and INLINEFORM1 represent the INLINEFORM2 and INLINEFORM3 position of INLINEFORM4 and INLINEFORM5 , respectively. INLINEFORM6 represents the n-gram of POS tag. INLINEFORM7 and INLINEFORM8 ( INLINEFORM9 ) represent the total objects found by a classifier INLINEFORM10 for a given class INLINEFORM11 ( INLINEFORM12 ) (where N is the total of retrieved images INLINEFORM15 ). INLINEFORM16 and INLINEFORM17 represent the distance between the two higher predictions ( INLINEFORM18 ), i.e. INLINEFORM19 . Finally, INLINEFORM20 represents the sum of all predictions made by all INLINEFORM21 classifiers INLINEFORM22 ( INLINEFORM23 ). - Training (E): the outcomes of D.1 and D.2 ( INLINEFORM26 ) are used as input features to our final classifier. We implemented a simple Decision Tree (non-parametric supervised learning method) algorithm for learning simple decision rules inferred from the data features (since it does not require any assumptions of linearity in the data and also works well with outliers, which are expected to be found more often in a noisy environment, such as the Web of Documents)."]}
{"question_id": "8e113fd9661bc8af97e30c75a20712f01fc4520a", "predicted_answer": "", "predicted_evidence": ["FL is rich of various linguistic phenomena like \u2018metonymy\u2019 reference to an entity stands for another of the same domain, a more general case of \u2018synonymy\u2019; and \u2018metaphors\u2019 systematic interchange between entities from different abstract domains BIBREF19. Besides the philosophical considerations, theories and debates about the exact nature of FL, findings from the neuroscience research domain present clear evidence on the presence of differentiating FL processing patterns in the human brain BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF14, even for woman-man attraction situations! BIBREF24. A fact that makes FL processing even more challenging and difficult to tackle. Indeed, this is the case of pragmatic FL phenomena like irony and sarcasm that main intention of in most of the cases, are characterized by an oppositeness to the literal language context. It is crucial to distinguish between the literal meaning of an expression considered as a whole from its constituents\u2019 words and phrases.", "Relevant efforts focus on tracking the sentiment polarity of single utterances, which in most cases is loaded with a lot of subjectivity and a degree of vagueness BIBREF10. Contemporary research in the field utilizes data from social media resources (e.g., Facebook, Twitter) as well as other short text references in blogs, forums etc BIBREF11. However, users of social media tend to violate common grammar and vocabulary rules and even use various figurative language forms to communicate their message. In such situations, the sentiment inclination underlying the literal content of the conveyed concept may significantly differ from its figurative context, making SA tasks even more puzzling. Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13.", "Several approaches search words on large dictionaries which demand large computational times and can be considered as impractical BIBREF0, BIBREF34", "However, users of social media tend to violate common grammar and vocabulary rules and even use various figurative language forms to communicate their message. In such situations, the sentiment inclination underlying the literal content of the conveyed concept may significantly differ from its figurative context, making SA tasks even more puzzling. Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user\u2019s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc. Hand crafted and sentiment-oriented features, indicative of emotion polarity, are utilized to represent respective excerpt cases.", "In such situations, the sentiment inclination underlying the literal content of the conveyed concept may significantly differ from its figurative context, making SA tasks even more puzzling. Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user\u2019s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc. Hand crafted and sentiment-oriented features, indicative of emotion polarity, are utilized to represent respective excerpt cases. The formed data are then fed traditional machine learning classifiers (e.g."]}
{"question_id": "35e0e6f89b010f34cfb69309b85db524a419c862", "predicted_answer": "", "predicted_evidence": ["predicting next word within a sequence. The ELMo model was exhaustingly trained on 30 million sentences corpus BIBREF78, with a two layered bidirectional LSTM architecture, aiming to predict both next and previous words, introducing the concept of contextual embeddings. The final embeddings vector is produced by a task specific weighted sum of the two directional hidden layers of LSTM models. Another contextual approach for creating embedding vector representations is proposed in BIBREF79 where, complete sentences, instead of words, are mapped to a latent vector space. The approach provides two variations of Universal Sentence Encoder (USE) with some trade-offs in computation and accuracy. The first approach consists of a computationally intensive transformer that resembles a transformer network BIBREF80, proved to achieve higher performance figures. In contrast, the second approach provides a light-weight model that averages input embedding weights for words and bi-grams by utilizing of a Deep Average Network (DAN) BIBREF81. The output of the DAN is passed through a feedforward neural network in order to produce the sentence embeddings.", "Deep Learning approaches. Although several DL methodologies, such as recurrent neural networks (RNNs), are able to capture hidden dependencies between terms within text passages and can be considered as content-based, we grouped all DL studies for readability purposes. Word Embeddings, i.e., learned mappings of words to real valued vectors BIBREF54, play a key role in the success of RNNs and other DL neural architectures that utilize pre-trained word embeddings to tackle FL. In fact, the combination of word embeddings with Convolutional Neural Networks (CNN), so called CNN-LSTM units, was introduced by Kumar BIBREF55 and Ghosh & Veale BIBREF56 achieving state-of-the-art performance. Attentive RNNs exhibit also good performance when matched with pre-trained Word2Vec embeddings BIBREF57, and contextual information BIBREF58. Following the same approach an LSTM based intra-attention was introduced in BIBREF59 that achieved increased performance.", "Actually, the proposed leaning model is based on a hybrid DL neural architecture that utilizes pre-trained transformer models and feed the hidden representations of the transformer into a Recurrent Convolutional Neural Network (RCNN), similar to BIBREF92. In particular, we employed the RoBERTa base model with 12 hidden states and 12 attention heads, and used its output hidden states as an embedding layer to a RCNN. As already stated, contradictions and long-time dependencies within a sentence may be used as strong identifiers of FL expressions. RNNs are often used to capture time relationships between words, however they are strongly biased, i.e. later words are tending to be more dominant that previous ones BIBREF92. This problem can be alleviated with CNNs, which, as unbiased models, can determine semantic relationships between words with max-pooling. Nevertheless, contextual information in CNNs is depended totally on kernel sizes. Thus, we appropriately modified the RCNN model presented in BIBREF92 in order to capture unbiased recurrent informative relationships within text, and we implemented a Bidirectional LSTM (BiLSTM) layer, which is fed with RoBERTa\u2019s final hidden layer weights.", "Recently, the detection of ironic and sarcastic meanings from respective literal ones have raised scientific interest due to the intrinsic difficulties to differentiate between them. Apart from English language, irony and sarcasm detection have been widely explored on other languages as well, such as Italian BIBREF38, Japanese BIBREF39, Spanish BIBREF40, Greek BIBREF41 etc. In the review analysis that follows we group related approaches according to the their adopted key concepts to handle FL.", "An ensemble of a shallow classifier with lexical, pragmatic and semantic features, utilizing a Bidirectional LSTM model is presented in BIBREF61. In a subsequent study BIBREF35, the researchers engineered a soft attention LSTM model coupled with a CNN. Contextual DL approaches are also employed, utilizing pre-trained along with user embeddings structured from previous posts BIBREF62 or, personality embeddings passed through CNNs BIBREF63. ELMo embeddings BIBREF64 are utilized in BIBREF65. In our previous approach we implemented an ensemble deep learning classifier (DESC) BIBREF0, capturing content and semantic information. In particular, we employed an extensive feature set of a total 44 features leveraging syntactic, demonstrative, sentiment and readability information from each text along with Tf-idf features. In addition, an attentive bidirectional LSTM model trained with GloVe pre-trained word embeddings was utilized to structure an ensemble classifier processing different text representations. DESC model performed state-of-the-art results on several FL tasks."]}
{"question_id": "992e67f706c728bc0e534f974c1656da10e7a724", "predicted_answer": "", "predicted_evidence": ["However, users of social media tend to violate common grammar and vocabulary rules and even use various figurative language forms to communicate their message. In such situations, the sentiment inclination underlying the literal content of the conveyed concept may significantly differ from its figurative context, making SA tasks even more puzzling. Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user\u2019s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc.", "Multi-head self-attention layers are calculated in parallel facing the computational costs of regular attention layers used by previous seq2seq network architectures. In BIBREF18 the authors presented a model that is founded on findings from various previous studies (e.g., BIBREF83, BIBREF84, BIBREF64, BIBREF49, BIBREF80), which achieved state-of-the-art results on eleven NLP tasks, called BERT - Bidirectional Encoder Representations from Transformers. The BERT training process is split in two phases, the unsupervised pre-training phase and the fine-tuning phase using labelled data for down-streaming tasks. In contrast with previous proposed models (e.g., BIBREF64, BIBREF49), BERT uses masked language models (MLMs) to enable pre-trained deep bidirectional representations. In the pre-training phase the model is trained with a large amount of unlabeled data from Wikipedia, BookCorpus BIBREF85 and WordPiece BIBREF86 embeddings.", "The Semantic Evaluation Workshop-2015 BIBREF66 proposed a joint task to evaluate the impact of FL in sentiment analysis on ironic, sarcastic and metaphorical tweets, with a number of submissions achieving highly performance results. The ClaC team BIBREF67 exploited four lexicons to extract attributes as well as syntactic features to identify sentiment polarity. The UPF team BIBREF68 introduced a regression classification methodology on tweet features extracted with the use of the widely utilized SentiWordNet and DepecheMood lexicons. The LLT-PolyU team BIBREF69 used semi-supervised regression and decision trees on extracted uni-gram and bi-gram features, coupled with features that capture potential contradictions at short distances. An SVM-based classifier on extracted n-gram and Tf-idf features was used by the Elirf team BIBREF70 coupled with specific lexicons such as Affin, Patter and Jeffrey 10.", "Despite the achieved breakthroughs, the BERT model suffers from several drawbacks. Firstly, BERT, as all language models using Transformers, assumes (and pre-supposes) independence between the masked words from the input sequence, and neglects all the positional and dependency information between words. In other words, for the prediction of a masked token both word and position embeddings are masked out, even if positional information is a key-aspect of NLP BIBREF87. In addition, the [MASK] token which, is substituted with masked words, is mostly absent in fine-tuning phase for down-streaming tasks, leading to a pre-training fine-turning discrepancy. To address the cons of BERT, a permutation language model was introduced, so-called XLnet, trained to predict masked tokens in a non-sequential random order, factorizing likelihood in an autoregressive manner without the independence assumption and without relying on any input corruption BIBREF88. In particular, a query stream is used that extends embedding representations to incorporate positional information about the masked words.", "Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user\u2019s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc. Hand crafted and sentiment-oriented features, indicative of emotion polarity, are utilized to represent respective excerpt cases. The formed data are then fed traditional machine learning classifiers (e.g. SVM, Random Forest, multilayer perceptrons) or DL techniques and respective complex neural architectures, in order to induce analytical models that are able to capture the underlying sentiment content and polarity of passages BIBREF15, BIBREF16, BIBREF17."]}
{"question_id": "61e96abdc924c34c6b82a587168ea3d14fe792d1", "predicted_answer": "", "predicted_evidence": ["In this study, we propose the first transformer based methodology, leveraging the pre-trained RoBERTa model combined with a recurrent convolutional neural network, to tackle figurative language in social media. Our network is compared with all, to the best of our knowledge, published approaches under four different benchmark dataset. In addition, we aim to minimize preprocessing and engineered feature extraction steps which are, as we claim, unnecessary when using overly trained deep learning methods such as transformers. In fact, hand crafted features along with preprocessing techniques such as stemming and tagging on huge datasets containing thousands of samples are almost prohibited in terms of their computation cost. Our proposed model, RCNN-RoBERTa, achieve state-of-the-art performance under six metrics over four benchmark dataset, denoting that transfer learning non-literal forms of language. Moreover, RCNN-RoBERTa model outperforms all other state-of-the-art approaches tested including BERT, XLnet, ELMo, and USE under all metric, some by a large factor.", "Similarly, using several lexical resources BIBREF34, and syntactic and sentiment related features BIBREF37, the respective researchers explored differences between sarcastic and ironic expressions. Affective and structural features are also employed to predict irony with conventional machine learning classifiers (DT, SVM, Na\u00efve Bayes/NB) in BIBREF51. In a follow-up study BIBREF30, a knowledge-based k-NN classifier was fed with a feature set that captures a wide range of linguistic phenomena (e.g., structural, emotional). Significant results were achieved in BIBREF36, were a combination of lexical, semantic and syntactic features passed through an SVM classifier that outperformed LSTM deep neural network approaches. Apart from local content, several approaches claimed that global context may be essential to capture FL phenomena. In particular, in BIBREF52 it is claimed that capturing previous and following comments on Reddit increases classification performance. Users\u2019 behavioral information seems to be also beneficial as it captures useful contextual information in Twitter post BIBREF32.", "To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d (SemEval-2018) that contains ironic tweets BIBREF95; Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66.", "Actually, the proposed leaning model is based on a hybrid DL neural architecture that utilizes pre-trained transformer models and feed the hidden representations of the transformer into a Recurrent Convolutional Neural Network (RCNN), similar to BIBREF92. In particular, we employed the RoBERTa base model with 12 hidden states and 12 attention heads, and used its output hidden states as an embedding layer to a RCNN. As already stated, contradictions and long-time dependencies within a sentence may be used as strong identifiers of FL expressions. RNNs are often used to capture time relationships between words, however they are strongly biased, i.e. later words are tending to be more dominant that previous ones BIBREF92. This problem can be alleviated with CNNs, which, as unbiased models, can determine semantic relationships between words with max-pooling. Nevertheless, contextual information in CNNs is depended totally on kernel sizes.", "In such situations, the sentiment inclination underlying the literal content of the conveyed concept may significantly differ from its figurative context, making SA tasks even more puzzling. Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user\u2019s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc. Hand crafted and sentiment-oriented features, indicative of emotion polarity, are utilized to represent respective excerpt cases. The formed data are then fed traditional machine learning classifiers (e.g."]}
{"question_id": "ee8a77cddbe492c686f5af3923ad09d401a741b5", "predicted_answer": "", "predicted_evidence": ["In the networked-world era the production of (structured or unstructured) data is increasing with most of our knowledge being created and communicated via web-based social channels BIBREF1. Such data explosion raises the need for efficient and reliable solutions for the management, analysis and interpretation of huge data sizes. Analyzing and extracting knowledge from massive data collections is not only a big issue per-se, but also challenges the data analytics state-of-the-art BIBREF2, with statistical and machine learning methodologies paving the way, and deep learning (DL) taking over and presenting highly accurate solutions BIBREF3. Relevant applications in the field of social media cover a wide spectrum, from the categorization of major disasters BIBREF4 and the identification of suggestions BIBREF5 to inducing users\u2019 appeal to political parties BIBREF6.", "In this study, we propose the first transformer based methodology, leveraging the pre-trained RoBERTa model combined with a recurrent convolutional neural network, to tackle figurative language in social media. Our network is compared with all, to the best of our knowledge, published approaches under four different benchmark dataset. In addition, we aim to minimize preprocessing and engineered feature extraction steps which are, as we claim, unnecessary when using overly trained deep learning methods such as transformers. In fact, hand crafted features along with preprocessing techniques such as stemming and tagging on huge datasets containing thousands of samples are almost prohibited in terms of their computation cost. Our proposed model, RCNN-RoBERTa, achieve state-of-the-art performance under six metrics over four benchmark dataset, denoting that transfer learning non-literal forms of language. Moreover, RCNN-RoBERTa model outperforms all other state-of-the-art approaches tested including BERT, XLnet, ELMo, and USE under all metric, some by a large factor.", "During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier\u2019s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a \u201cBase Bert\u201d model with 12 encoder layers (i.e. transformer blocks), feed-forward networks with 768 hidden units and 12 attention heads, and a \u201cLarge Bert\u201d model with 24 encoder layers 1024 feed-the pre-trained Bert model, an architecture almost identical with the aforementioned Transformer network. A [CLS] token is supplied in the input as the first token, the final hidden state of which is aggregated for classification tasks. Despite the achieved breakthroughs, the BERT model suffers from several drawbacks. Firstly, BERT, as all language models using Transformers, assumes (and pre-supposes) independence between the masked words from the input sequence, and neglects all the positional and dependency information between words. In other words, for the prediction of a masked token both word and position embeddings are masked out, even if positional information is a key-aspect of NLP BIBREF87.", "Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant \u201cSemEval-2014 Sentiment Analysis task 9\u201d BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user\u2019s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc. Hand crafted and sentiment-oriented features, indicative of emotion polarity, are utilized to represent respective excerpt cases. The formed data are then fed traditional machine learning classifiers (e.g. SVM, Random Forest, multilayer perceptrons) or DL techniques and respective complex neural architectures, in order to induce analytical models that are able to capture the underlying sentiment content and polarity of passages BIBREF15, BIBREF16, BIBREF17.", "The intuition behind our proposed RCNN-RoBERTa approach is founded on the following observation: as pre-trained networks are beneficial for several down-streaming tasks, their outputs could be further enhanced if processed properly by other networks. Towards this end, we devised an end-to-end model with minimum training time that utilizes pre-trained RoBERTa weights combined with a RCNN in order to capture contextual information. Actually, the proposed leaning model is based on a hybrid DL neural architecture that utilizes pre-trained transformer models and feed the hidden representations of the transformer into a Recurrent Convolutional Neural Network (RCNN), similar to BIBREF92. In particular, we employed the RoBERTa base model with 12 hidden states and 12 attention heads, and used its output hidden states as an embedding layer to a RCNN. As already stated, contradictions and long-time dependencies within a sentence may be used as strong identifiers of FL expressions. RNNs are often used to capture time relationships between words, however they are strongly biased, i.e."]}
{"question_id": "552b1c813f25bf39ace6cd5eefa56f4e4dd70c84", "predicted_answer": "", "predicted_evidence": ["Each data sample consists of multiple labels, allowing users to utilize the dataset for 2-way, 3-way, and 5-way classification. This enables both high-level and fine-grained fake news classification.", "Having this hierarchy of labels will enable researchers to train for fake news detection at a high level or a more fine-grained one. The 2-way classification determines whether a sample is fake or true. The 3-way classification determines whether a sample is completely true, the sample is fake news with true text (text that is true in the real world), or the sample is fake news with false text. Our final 5-way classification was created to categorize different types of fake news rather than just doing a simple binary or trinary classification. This can help in pinpointing the degree and variation of fake news for applications that require this type of fine-grained detection. The first label is true and the other four are defined within the seven types of fake news BIBREF3. We provide examples from each class for 5-way classification in Figure SECREF3. The 5-way classification labels are explained below:", "A variety of datasets for fake news detection have been published in recent years. These are listed in Table TABREF1, along with their specific characteristics. When comparing these datasets, a few trends can be seen. Most of the datasets are small in size, which can be ineffective for current machine learning models that require large quantities of training data. Only four contain over half a million samples, with CREDBANK and FakeNewsCorpus being the largest with millions of samples BIBREF2. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. However, fake news can be categorized into many different types BIBREF3. Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5.", "Misleading Content: This category consists of information that is intentionally manipulated to fool the audience. Our dataset contains three subreddits in this category: propagandaposters, fakefacts, and savedyouaclick.", "The results are shown in Tables TABREF17 and SECREF3. We found that the multimodal features performed the best, followed by text-only, and image-only in all instances. Thus, having both image and text improves fake news detection. For image and multimodal classification, ResNet50 performed the best followed by VGG16 and EfficientNet. In addition, BERT generally achieved better results than InferSent for multimodal classification. However, for text-only classification InferSent outperformed BERT. The \u201cmaximum\u201d method to merge image and text features yielded the highest accuracy, followed by average, concatenate, and add. Overall, the multimodal model that combined BERT text features and ResNet50 image features through the maximum method performed most optimally."]}
{"question_id": "1100e442e00c9914538a32aca7af994ce42e1b66", "predicted_answer": "", "predicted_evidence": ["True: True content is accurate in accordance with fact. Eight of the subreddits fall into this category, such as usnews and mildlyinteresting. The former consists of posts from various news sites. The latter encompasses real photos with accurate captions. The other subreddits include photoshopbattles, nottheonion, neutralnews, pic, usanews, and upliftingnews.", "Having this hierarchy of labels will enable researchers to train for fake news detection at a high level or a more fine-grained one. The 2-way classification determines whether a sample is fake or true. The 3-way classification determines whether a sample is completely true, the sample is fake news with true text (text that is true in the real world), or the sample is fake news with false text. Our final 5-way classification was created to categorize different types of fake news rather than just doing a simple binary or trinary classification. This can help in pinpointing the degree and variation of fake news for applications that require this type of fine-grained detection. The first label is true and the other four are defined within the seven types of fake news BIBREF3. We provide examples from each class for 5-way classification in Figure SECREF3. The 5-way classification labels are explained below:", "In this paper, we presented a novel dataset for fake news research, Fakeddit. Compared to previous datasets, Fakeddit provides a large quantity of text+image samples with multiple labels for various levels of fine-grained classification. We created detection models that incorporate both modalities of data and conducted experiments, showing that there is still room for improvement in fake news detection. Although we do not utilize submission metadata and comments made by users on the submissions, we anticipate that these features will be useful for further research. We hope that our dataset can be used to advance efforts to combat the ever growing rampant spread of misinformation.", "We used the InferSent model because it performs very well as a universal sentence embeddings generator. For this model, we loaded a vocabulary of 1 million of the most common words in English and used fastText as opposed to ELMO embeddings because fastText can perform relatively well for rare words and words that do not appear in the vocabulary BIBREF20, BIBREF21. We obtained encoded sentence features of length 4096 for each submission title using InferSent.", "Most of the datasets are small in size, which can be ineffective for current machine learning models that require large quantities of training data. Only four contain over half a million samples, with CREDBANK and FakeNewsCorpus being the largest with millions of samples BIBREF2. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. However, fake news can be categorized into many different types BIBREF3. Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5. While some datasets include data from a variety of categories BIBREF6, BIBREF7, many contain data from specific areas, such as politics and celebrity gossip BIBREF8, BIBREF9, BIBREF10, BIBREF11. These data samples may contain limited styles of writing due to this categorization."]}
{"question_id": "82b93ecd2397e417e1e80f93b7cf49c7bd9aeec3", "predicted_answer": "", "predicted_evidence": ["When experimenting with user type embeddings or biases, we group the users into the following types. INLINEFORM0 is the number of training comments posted by user (id) INLINEFORM1 . INLINEFORM2 is the ratio of training comments posted by INLINEFORM3 that were rejected.", "rnn: This is the rnn-based method of our previous work BIBREF0 . It is a chain of gru cells BIBREF4 that transforms the tokens INLINEFORM0 of each comment to the hidden states INLINEFORM1 ( INLINEFORM2 ). Once INLINEFORM3 has been computed, a logistic regression (lr) layer estimates the probability that comment INLINEFORM4 should be rejected: DISPLAYFORM0", "uernn: This is the rnn-based method with user embeddings added. Each user INLINEFORM0 of the training set with INLINEFORM1 is mapped to a user-specific embedding INLINEFORM2 . Users with INLINEFORM3 are mapped to a single `unknown' user embedding. The lr layer is modified as follows; INLINEFORM4 is the embedding of the author of INLINEFORM5 ; and INLINEFORM6 . DISPLAYFORM0", "User-specific information always improves our original rnn-based method (Table TABREF15 ), but the best results are obtained by adding user embeddings (uernn). Figure FIGREF16 visualizes the user embeddings learned by uernn. The two dimensions of Fig. FIGREF16 correspond to the two principal components of the user embeddings, obtained via pca.The colors and numeric labels reflect the rejection rates INLINEFORM0 of the corresponding users. Moving from left to right in Fig. FIGREF16 , the rejection rate increases, indicating that the user embeddings of uernn capture mostly the rejection rate INLINEFORM1 . This rate (a single scalar value per user) can also be captured by the simpler user-specific biases of ubrnn, which explains why ubrnn also performs well (second best results in Table TABREF15 ). Nevertheless, uernn performs better than ubrnn, suggesting that user embeddings capture more information than just a user-specific rejection rate bias.", "Cheng et al. Cheng2015 predict which users will be banned from on-line communities. Their best system uses a Random Forest or lr classifier, with features examining the average readability and sentiment of each user's past posts, the past activity of each user (e.g., number of posts daily, proportion of posts that are replies), and the reactions of the community to the past actions of each user (e.g., up-votes, number of posts rejected). Lee et al. Lee2014 and Napoles et al. Napoles2017b include similar user-specific features in classifiers intended to detect high quality on-line discussions."]}
{"question_id": "2973fe3f5b4bf70ada02ac4a9087dd156cc3016e", "predicted_answer": "", "predicted_evidence": ["Table TABREF19 shows normalized discounted cumulative gain (NDCG) scores for top 5, 10 and 20 ranked documents for each approach. NDCG BIBREF45 is a measure for ranking quality and it penalizes relevant documents appearing in lower ranks by adding a rank-based discount factor. In the table, reranking documents by learning to rank performs better than BM25 overall, however the larger gain is obtained from using titles (BM25 + SEMTitle) by increasing NDCG@20 by 23%. NDCG@5 and NDCG@10 also perform better than BM25 by 23% and 25%, respectively. It is not surprising that SEMTitle produces better performance than SEMAbstract. The current PubMed search interface does not allow users to see abstracts on the results page, hence users click documents mostly based on titles. Nevertheless, it is clear that the abstract-based semantic distance helps achieve better performance.", "Let INLINEFORM0 and INLINEFORM1 be BOW representations of two documents INLINEFORM2 and INLINEFORM3 . Let INLINEFORM4 be a flow matrix, where INLINEFORM5 denotes how much it costs to travel from INLINEFORM6 in INLINEFORM7 to INLINEFORM8 in INLINEFORM9 , and INLINEFORM10 is the number of unique words appearing in INLINEFORM11 and/or INLINEFORM12 . To entirely transform INLINEFORM13 to INLINEFORM14 , we ensure that the entire outgoing flow from INLINEFORM15 equals INLINEFORM16 and the incoming flow to INLINEFORM17 equals INLINEFORM18 . The Word Mover's Distance between INLINEFORM19 and INLINEFORM20 is then defined as the minimum cumulative cost required to move all words from INLINEFORM21 to INLINEFORM22 or vice versa, i.e. DISPLAYFORM0", "In the following subsections, we describe the datasets and experiments, and discuss our results.", "In particular, we use LambdaMART BIBREF25 , BIBREF26 for our experiments. LambdaMART is a pairwise learning to rank approach and is being used for PubMed relevance search. While the simplest approach (pointwise learning) is to train the function INLINEFORM0 directly, pairwise approaches seek to train the model to place correct pairs higher than incorrect pairs, i.e. INLINEFORM1 , where the document INLINEFORM2 is relevant and INLINEFORM3 is irrelevant. INLINEFORM4 indicates a margin. LambdaMART is a boosted tree version of LambdaRank BIBREF26 . An ensemble of LambdaMART, LambdaRank and logistic regression models won the Yahoo! learning to rank challenge BIBREF23 .", "To build the PubMed set, we collected one year's worth of search logs and restricted the set of queries to those where users requested the relevance order and which yielded at least 20 retrieved documents. This set contained many popular but duplicate queries. Therefore, we merged queries and summed up user actions for each of them. That is, for each document stored for each query, we counted the number of times it was clicked in the retrieved set (i.e. abstract click) and the number of times users requested full-text articles (i.e. full-text click). We considered the queries that appeared less than 10 times to be less informative because they were usually very specific, and we could not collect enough user actions for training. After this step, we further filtered out non-informational queries (e.g. author and journal names). As the result, 27,870 queries remained for the final set."]}
{"question_id": "42269ed04e986ec5dc4164bf57ef306aec4a1ae1", "predicted_answer": "", "predicted_evidence": ["While many deep learning solutions have been proposed recently, their slow training and lack of flexibility to adopt various features limit real-world use. However, our approach is more straightforward and can be easily added as a feature in the current PubMed relevance search framework. Proven by our PubMed search results, our semantic measure improves ranking performance without adding much overhead to the system.", "Although our approach outperforms BM25 on TREC, we do not claim that BM25 and other traditional approaches can be completely replaced with the semantic method. We see the semantic approach as a means to narrow the gap between words in documents and those in queries (or users' intentions). This leads to the next experiment using our semantic measure as a feature for ranking in learning to rank.", "For the PubMed dataset, we used learning to rank to combine BM25 and our semantic measure. An advantage of using learning to rank is its flexibility to add more features and optimize performance by learning their importance. PubMed documents are semi-structured, consisting of title, abstract and many more fields. Since our interest lies in text, we only used titles and abstracts, and applied learning to rank in two different ways: 1) to find semantically closest words in titles (BM25 + SEMTitle) and 2) to find semantically closest words in abstracts (BM25 + SEMAbstract). Although our semantic measure alone produces better ranking scores on the TREC set, this does not apply to user queries in PubMed. It is because user queries are often short, including around three words on average, and the semantic measure cannot differentiate documents when they include all query words.", "To build the PubMed set, we collected one year's worth of search logs and restricted the set of queries to those where users requested the relevance order and which yielded at least 20 retrieved documents. This set contained many popular but duplicate queries. Therefore, we merged queries and summed up user actions for each of them. That is, for each document stored for each query, we counted the number of times it was clicked in the retrieved set (i.e. abstract click) and the number of times users requested full-text articles (i.e. full-text click). We considered the queries that appeared less than 10 times to be less informative because they were usually very specific, and we could not collect enough user actions for training. After this step, we further filtered out non-informational queries (e.g. author and journal names). As the result, 27,870 queries remained for the final set.", "where INLINEFORM0 represents the word INLINEFORM1 in INLINEFORM2 . INLINEFORM3 in Eq. ( EQREF7 ) is unknown for queries, therefore we compute INLINEFORM4 based on the document collection. The optimal solution of the expression in Eq. ( EQREF9 ) is to map each word in INLINEFORM5 to the most similar word in INLINEFORM6 based on word embeddings. The time complexity for getting the optimal solution is INLINEFORM7 , where INLINEFORM8 is the number of unique query words and INLINEFORM9 is the number of unique document words. In general, INLINEFORM10 and evaluating the similarity between a query and a document can be implemented in parallel computation. Thus, the document ranking process can be quite efficient."]}
{"question_id": "31a3ec8d550054465e55a26b0136f4d50d72d354", "predicted_answer": "", "predicted_evidence": ["We presented a word embedding approach for measuring similarity between a query and a document. Starting from the Word Mover's Distance, we reinterpreted the model for a query-document search problem. Even with the INLINEFORM0 flow only, the word embedding approach is already efficient and effective. In this setup, the proposed approach cannot distinguish documents when they include all query words, but surprisingly, the word embedding approach shows remarkable performance on the TREC Genomics datasets. Moreover, applied to PubMed user queries and click-through data, our semantic measure allows to further improves BM25 ranking performance. This demonstrates that the semantic measure is an important feature for IR and is closely related to user clicks.", "This research was supported by the Intramural Research Program of the NIH, National Library of Medicine.", "As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches.", "While many deep learning solutions have been proposed recently, their slow training and lack of flexibility to adopt various features limit real-world use. However, our approach is more straightforward and can be easily added as a feature in the current PubMed relevance search framework. Proven by our PubMed search results, our semantic measure improves ranking performance without adding much overhead to the system.", "To build the PubMed set, we collected one year's worth of search logs and restricted the set of queries to those where users requested the relevance order and which yielded at least 20 retrieved documents. This set contained many popular but duplicate queries. Therefore, we merged queries and summed up user actions for each of them. That is, for each document stored for each query, we counted the number of times it was clicked in the retrieved set (i.e. abstract click) and the number of times users requested full-text articles (i.e. full-text click). We considered the queries that appeared less than 10 times to be less informative because they were usually very specific, and we could not collect enough user actions for training. After this step, we further filtered out non-informational queries (e.g. author and journal names). As the result, 27,870 queries remained for the final set."]}
{"question_id": "a7e1b13cc42bfe78d37b9c943de6288e5f00f01b", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches.", "Compared to the TREC Genomics set, the full PubMed set is much larger, including all 27 million documents in PubMed. While the TREC and PubMed sets share essentially the same type of documents, the tested queries are quite different. The queries in TREC are a question type, e.g. \u201cwhat is the role of MMS2 in cancer?\" However, the PubMed set uses actual queries from PubMed users.", "In the following subsections, we describe the datasets and experiments, and discuss our results.", "We presented a word embedding approach for measuring similarity between a query and a document. Starting from the Word Mover's Distance, we reinterpreted the model for a query-document search problem. Even with the INLINEFORM0 flow only, the word embedding approach is already efficient and effective. In this setup, the proposed approach cannot distinguish documents when they include all query words, but surprisingly, the word embedding approach shows remarkable performance on the TREC Genomics datasets. Moreover, applied to PubMed user queries and click-through data, our semantic measure allows to further improves BM25 ranking performance. This demonstrates that the semantic measure is an important feature for IR and is closely related to user clicks.", "Table TABREF17 presents the average precision of tf-idf (TFIDF), BM25, word vector centroid (CENTROID) and our embedding approach on the TREC dataset. Average precision BIBREF43 is the average of the precisions at the ranks where relevant documents appear. Relevance judgements in TREC are based on the pooling method BIBREF44 , i.e. relevance is manually assessed for top ranking documents returned by participating systems. Therefore, we only used the documents that annotators reviewed for our evaluation BIBREF1 ."]}
{"question_id": "49cd18448101da146c3187a44412628f8c722d7b", "predicted_answer": "", "predicted_evidence": ["As an example, figure FIGREF36 presents the simple sentence \u201cAmobee is awesome\u201d with its parsing tree. The leaves are given by INLINEFORM0 -dimensional word vectors together with their POS tagging, syntactic categories (if defined for the leaf) and an entity indicator bit. The computation takes place in the inner nodes; \u201cis\u201d and \u201cawesome\u201d are combined in a node marked by \u201cVP\u201d which is the phrase category. In terms of our terminology, \u201cis\u201d and \u201cawesome\u201d are the INLINEFORM1 nodes, respectively for \u201cVP\u201d node calculation. We define INLINEFORM2 as the cell's state for the left child, in this case the \u201cis\u201d node. Left and right are concatenated as input INLINEFORM3 and the metadata INLINEFORM4 is from the right child while INLINEFORM5 is the metadata from the left child. The second calculation takes place at the root \u201cS\u201d; the input INLINEFORM6 is now a concatenation of \u201cAmobee\u201d word vector, the input INLINEFORM7 holds the INLINEFORM8 output of the previous step in node \u201cVP\u201d; the cell state INLINEFORM9 comes from the \u201cAmobee\u201d node.", "We anticipated the sentiment distribution of the test data would be similar to the training data\u2014as they may be drawn from the same distribution. Therefore we used re-sampling of the training dataset to obtain a skewed dataset such that a logistic regression would predict similar sentiment distributions for both the train and test datasets. Finally we trained a logistic regression on the new dataset and used it on the task A test set. We obtained a macro-averaged recall score of INLINEFORM0 and accuracy of INLINEFORM1 .", "The five models output is concatenated and used as input for the various tasks, as described in SECREF27 .", "Apparently, our assumption about distribution similarity was misguided as one can observe in the next table.", "Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees, much like the Stanford sentiment treebank. The sample contains twenty thousand tweets with sentiment distribution as following:"]}
{"question_id": "e9260f6419c35cbd74143f658dbde887ef263886", "predicted_answer": "", "predicted_evidence": ["The module functions are defined as following: DISPLAYFORM0", "Emojis: removing duplicate emojis, clustering them according to sentiment and replacing them with representative keywords, e.g. \u201chappy-emoji\u201d.", "This paper describes our system and participation in all sub-tasks of SemEval 2017 task 4. Our system consists of two parts: a recurrent neural network trained on a private Twitter dataset, followed by a task-specific combination of model stacking and logistic regression classifiers.", "The goals of these tasks are to classify tweets sentiment regarding a given entity into five classes\u2014very negative, negative, neutral, positive, very positive\u2014(task C) and estimate sentiment distribution over five classes for each entity (task E). The measured metrics are macro-averaged MAE and earth-movers-distance (EMD), respectively.", "NER: using entity recognition annotator, replacing numbers, dates and locations with representative keywords."]}
{"question_id": "2834a340116026d5995e537d474a47d6a74c3745", "predicted_answer": "", "predicted_evidence": ["Training data with lemmatization step, with pre-trained word-vectors of size 25.", "Regex: removing duplicate punctuation marks, replacing URLs with a keyword, removing Camel casing.", "This paper describes our system and participation in all sub-tasks of SemEval 2017 task 4. Our system consists of two parts: a recurrent neural network trained on a private Twitter dataset, followed by a task-specific combination of model stacking and logistic regression classifiers.", "", ""]}
{"question_id": "bd53399be8ff59060792da4c8e42a7fc1e6cbd85", "predicted_answer": "", "predicted_evidence": ["blackWe used BART$_{\\mathrm {LARGE}}$ BIBREF1, which is one of the state-of-the-art models, as the pre-trained seq-to-seq model and RoBERTa$_\\mathrm {BASE}$ BIBREF11 as the initial model of the extractor. In the extractor of CIT, stop words and duplicate tokens are ignored for the XSum dataset.", "The third type uses both the shared encoder and the extractor (\u00a7SECREF39). These models consist of the extractor, shared encoder, and decoder and also follow two steps: first, blackthe extractor extracts the important tokens from the source text, and second, blackthe shared encoder uses them as an input of the seq-to-seq model.", "One reason for the difference can be traced to the quality of the pseudo saliency labels. CNN/DM is a highly extractive dataset, so it is relatively easy to create token alignments for generating pseudo saliency labels, while in contrast, a summary in XSum is highly abstractive and short, which makes it difficult to create pseudo labels with high quality by simple token alignment. To improve the accuracy of summarization in this dataset, we have to improve the quality of the pseudo saliency labels and the accuracy of the saliency model.", "On the other hand, on the XSum dataset, PEGASUS$_\\mathrm {HugeNews}$ improved the ROUGE scores and achieved the best results. In the XSum dataset, summaries often include the expressions that are not written in the source text. Therefore, increasing the pre-training data and learning more patterns were effective. However, by improving the quality of the pseudo saliency labels, we should be able to improve the accuracy of the CIT model.", "In the training, we extracted $X_s$, which maximizes the ROUGE-L scores with the reference summary text. In the test, we used the average number of sentences in $X_{s}$ in the training set as $P$. The loss function of the extractor is $L_\\mathrm {ext} = L_\\mathrm {sal}$, and that of the seq-to-seq model is $L_\\mathrm {abs} = L_\\mathrm {sum}$."]}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "predicted_answer": "", "predicted_evidence": ["Task 2 (Saliency detection) Given the source text $X$ with $L$ words $X$= $(x_1,\\dots ,x_L)$, the output is the saliency score $S = \\lbrace S_1, S_2, ... S_L \\rbrace $.", "From the viewpoint of the loss function, there are two major types of model: those that use the saliency loss (\u00a7SECREF21) and those that do not. We also denote the loss function for the seq-to-seq model as $L_\\mathrm {abs}$ and the loss function for the extractor as $L_\\mathrm {ext}$. black$L_\\mathrm {ext}$ is trained with $L_\\mathrm {sal}$, and $L_\\mathrm {abs}$ is trained with $L_\\mathrm {sum}$ or $L_\\mathrm {sum} + L_\\mathrm {sal}$.", "On the other hand, the extractive result on the XSum dataset was lower. For highly abstractive datasets, there is little overlap between the tokens. We need to consider how to make the high-quality pseudo saliency labels and how to evaluate the similarity of these two sequences.", "BIBREF4, BIBREF3, and BIBREF21 incorporated a sentence- and word-level extractive model in the pointer-generator model. Their models weight the copy probability for the source text by using an extractive model and guide the pointer-generator model to copy important words. BIBREF22 proposed a keyword guided abstractive summarization model. BIBREF23 proposed a sentence extraction and re-writing model that is trained in an end-to-end manner by using reinforcement learning. BIBREF24 proposed a search and rewrite model. BIBREF25 proposed a combination of sentence-level extraction and compression. None of these models are based on a pre-trained model. In contrast, our purpose is to clarify whether combined models are effective or not, and we are the first to investigate the combination of pre-trained seq-to-seq and saliency models. We compared a variety of combinations and clarified which combination is the most effective.", "BIBREF18 used BERT for their sentence-level extractive summarization model. BIBREF19 proposed a new pre-trained model that considers document-level information for sentence-level extractive summarization. Several researchers have published pre-trained encoder-decoder models very recently BIBREF20, BIBREF1, BIBREF2. BIBREF20 pre-trained a Transformer-based pointer-generator model. BIBREF1 pre-trained a standard Transformer-based encoder-decoder model using large unlabeled data and achieved state-of-the-art results. BIBREF8 and BIBREF16 extended the BERT structure to handle seq-to-seq tasks."]}
{"question_id": "cfe21b979a6c851bdafb2e414622f61e62b1d98c", "predicted_answer": "", "predicted_evidence": ["We used ROUGE scores (F1), including ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L), as the evaluation metrics BIBREF12. ROUGE scores were calculated using the files2rouge toolkit.", "The first type uses the shared encoder (\u00a7SECREF26). These models consist of the shared encoder and the decoder, where the shared encoder module blackplays two roles: saliency detection and the encoding of the seq-to-seq model. blackThe saliency scores are used to bias the representation of the seq-to-seq model for several models in this type.", "BIBREF4, BIBREF3, and BIBREF21 incorporated a sentence- and word-level extractive model in the pointer-generator model. Their models weight the copy probability for the source text by using an extractive model and guide the pointer-generator model to copy important words. BIBREF22 proposed a keyword guided abstractive summarization model. BIBREF23 proposed a sentence extraction and re-writing model that is trained in an end-to-end manner by using reinforcement learning. BIBREF24 proposed a search and rewrite model. BIBREF25 proposed a combination of sentence-level extraction and compression. None of these models are based on a pre-trained model. In contrast, our purpose is to clarify whether combined models are effective or not, and we are the first to investigate the combination of pre-trained seq-to-seq and saliency models. We compared a variety of combinations and clarified which combination is the most effective.", "This model combines the CIT and SA, so we also train two saliency models. The SA model is trained in an unsupervised way, the same as the CIT + SE model. The attention score $a_i^t \\in \\mathbb {R}^{L+K}$ is weighted by $S \\in \\mathbb {R}^{L+K}$ with Eq. (DISPLAY_FORM32). The loss function of the extractor is $L_\\mathrm {ext} = L_\\mathrm {sal}$, and that of the seq-to-seq model is $L_\\mathrm {abs} = L_\\mathrm {sum}$.", "where $N_j$ and $X_j$ are the number of tokens and the set of tokens within the $j$-th sentence. Top $P$ sentences are extracted according to the sentence-level saliency score and then concatenated as one text $X_s$. These extracted sentences are then used as the input of the seq-to-seq model."]}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "predicted_answer": "", "predicted_evidence": ["Rouge scores of the combined models on the XSum dataset are shown in Table TABREF52. The CIT model performed the best, although its improvement was smaller than on the CNN/DM dataset. Moreover, the accuracy of the MT, SE + MT, and SEG models decreased on the XSum dataset. These results were very different from those on the CNN/DM dataset.", "where $N_j$ and $X_j$ are the number of tokens and the set of tokens within the $j$-th sentence. Top $P$ sentences are extracted according to the sentence-level saliency score and then concatenated as one text $X_s$. These extracted sentences are then used as the input of the seq-to-seq model.", "A basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network. We define the saliency score of the $l$-th token ($1 \\le l \\le L$) in the source text as", "This model uses the saliency score to weight the shared encoder output. Specifically, the final output $h_{el}^M$ of the shared encoder is weighted as", "This is the first study that has conducted extensive experiments to investigate the effectiveness of incorporating saliency models into the pre-trained seq-to-seq model. From the results, we found that saliency models were effective in finding important parts of the source text, even if the seq-to-seq model is pre-trained on large-scale corpora, especially for generating an highly extractive summary. We also proposed a new combination model, CIT, that outperformed simple fine-tuning and other combination models. Our combination model improved the summarization accuracy without any additional pre-training data and can be applied to any pre-trained model. While recent studies have been conducted to improve summarization accuracy by increasing the amount of pre-training data and developing new pre-training strategies, this study sheds light on the importance of saliency models in abstractive summarization."]}
{"question_id": "2e37eb2a2a9ad80391e57acb53616eab048ab640", "predicted_answer": "", "predicted_evidence": ["On the other hand, on the XSum dataset, PEGASUS$_\\mathrm {HugeNews}$ improved the ROUGE scores and achieved the best results. In the XSum dataset, summaries often include the expressions that are not written in the source text. Therefore, increasing the pre-training data and learning more patterns were effective. However, by improving the quality of the pseudo saliency labels, we should be able to improve the accuracy of the CIT model.", "We used fairseq for the implementation of the seq-to-seq model. For fine-tuning of BART$_\\mathrm {LARGE}$ and the combination models, we used the same parameters as the official code. For fine-tuning of RoBERTa$_\\mathrm {BASE}$, we used Transformers. We set the learning rate to 0.00005 and the batch size to 32.", "Several studies have proposed the combination of a token-level saliency model and a seq-to-seq model, blackwhich is not pre-trained, and reported its effectiveness BIBREF3, BIBREF10. We also use a simple token-level saliency model blackas a basic model in this study.", "There are several pre-trained seq-to-seq models applied for abstractive summarization BIBREF7, BIBREF8, BIBREF2. The models use a simple Transformer-based encoder-decoder model BIBREF9 in which the encoder-decoder model is pre-trained on large unlabeled data.", "blackWe used BART$_{\\mathrm {LARGE}}$ BIBREF1, which is one of the state-of-the-art models, as the pre-trained seq-to-seq model and RoBERTa$_\\mathrm {BASE}$ BIBREF11 as the initial model of the extractor. In the extractor of CIT, stop words and duplicate tokens are ignored for the XSum dataset."]}
{"question_id": "54002c15493d4082d352a66fb9465d65bfe9ddca", "predicted_answer": "", "predicted_evidence": ["BIBREF131 extends an LSTM model for text question processing with an image attention model conditioned on the previous LSTM hidden state, whose input is a concatenation of the current word embedding with the attended image feature. The final LSTM hidden state is regarded as the fused multimodal representation to predict the answer for pointing and grounded VQA. The attention model for sequence-based encoder-decoder model is used to attend to the image features for image captioning BIBREF132. Further for VQA, attention model conditioned on both image and query feature vectors is applied to pinpoint the image regions relevant to the answer BIBREF133. Similarly, stacked attention networks (SANs) are proposed to use multiple layers of attention models to query an image multiple times to infer the answer progressively by simulating a multi-step reasoning procedure BIBREF134. At each layer, a refined query vector is generated and send to the next layer by adding the previous query vector to the attended image vector produced using the current attention model.", "We finish this section by reviewing the relation networks (RN), which has a simple structure that uses an ANN as the function to model the relationship between any pair of visual and textual features, and the resulted output values are accumulated and transformed by another ANN BIBREF258. Though RN merely models the relationship without any form of induction reasoning, it achieves very high VQA accuracy on CLEVR. This inspires a re-thinking of the connection between correlation and induction.", "Although significant progress has been made in the learning of representations for vision or language, it is theoretically insufficient to model a complete set of human concepts using only unimodal data. For example, the concept of \u201cbeautiful music\u201d is clearly grounded in the auditory perceptron and one can be struggled to describe this by natural language or other approaches. Therefore, it is important to learn a joint embedding to leverage the complementarity from multimodal data to represent the concepts better. Both supervised and unsupervised training approaches are of broad interest and can be applied to tasks with different data availability. Meanwhile, by assuming the corresponding representations to have similar neighbourhood structures across modalities, the representation of a concept with zero training sample in one modal can be found based on its representations grounded in other modalities which have training data.", "Another problem that current VQA methods suffers from is the low robustness against linguistic variations from the questions. A data set, VQA-Rephrasings, modified the VQA v2.0 validation set with human authored rephrasing of the questions BIBREF202. A cycle-consistency BIBREF241 based method that improves the linguistic robustness by enforcing consistencies between the original and rephrased questions, and between the true answer and the answers predicted based on the original and rephrased questions. BIBREF242 suggests that attention mechanism can cause VQA models to suffer from counting the object proposals, and an extra model component was proposed as a solution. Moreover, it is the fact that the current VQA methods cannot even read text from images. A method is proposed to address this problem by fusing not text extracted from the image using optical character recognition BIBREF243.", "Text-adaptive GAN BIBREF215 allows semantic modification of input images for birds and flowers via natural language. BIBREF216 enforces to learn the representation content and style as two disentangled variables using a dual inference mechanism based on cycle-consistency for text-to-image synthesis. The success of these methods validate GAN is able to learn some semantic concepts as disentangled representations, as in Section SECREF9. Text2Scene is another interesting work that generates compositional scene representation from natural langauge step-by-step without using GANs BIBREF217. It is shown with minor modifications, Text2Scene can generate cartoon like, semantic layout, and real image like scenes. Dialogue based interaction is studied to control image synthesis, in order to improve complex scene generation progressively BIBREF218, BIBREF219, BIBREF220, BIBREF221, BIBREF222. Meanwhile, text-to-image synthesis is extended to multiple images or videos, where visual consistency is required among the generated images BIBREF223, BIBREF224, BIBREF225."]}
{"question_id": "7caeb5ef6f2985b2cf383cd01765d247c936605f", "predicted_answer": "", "predicted_evidence": ["Removing SC and BN simplifies the model structure and speeds up both training and inference. Removing BN also solves the sequence level training problem discussed in Section SECREF3. More importantly, we always observe as good or better accuracy (WER) with the proposed simplified model structure.", "ResNet BIBREF8 uses shortcut connections (SC) and batch normalization (BN), allowing the training of surprisingly deep architectures with dramatic accuracy improvements. Since its invention, ResNet has dominated the field of computer vision. The later state-of-the-art-model, DenseNet BIBREF9, also uses SC and BN. Besides success in computer vision, ResNet has also performed well in acoustic models for speech recognition BIBREF10, BIBREF11.", "[Row 3 vs. Row 4] [Row 7 vs. Row 8] SELU activation makes the training of very deep models (with no SC&BN) feasible", "[Row 1-4 vs. Row 5-8] Deep CNN models show advantage in terms of WER against shallower DNNs", "Table TABREF23 compares en_US WER of ResNet-50 and SNDCNN-50 with 10000 hours of training data and 7 hours of testing data. In this experiment, the proposed SNDCNN has much better WER than ResNet."]}
{"question_id": "1fcd25e9a63a53451cac9ad2b8a1b529aff44a97", "predicted_answer": "", "predicted_evidence": ["", "ResNet BIBREF8 solves many problems in training very deep CNNs. The key ResNet innovation is the shortcut connections shown in Figure FIGREF1. Figure FIGREF1 is a typical building block of ResNet. The input to the block, $x$, will go through both the original mapping $F(x)$ (weight layers, RELU activations and batch normalization BIBREF3) and the identity shortcut connection. The output, $y$, will be $F(x)+x$. The authors of BIBREF8 hypothesize that the so-called residual mapping of $y=F(x)+x$ should be easier to optimize than the original mapping of $y=F(x)$. The design of the special building block is motivated by the observation in BIBREF6, BIBREF7 that accuracy degrades when more layers are stacked onto an already very deep CNN model. If the added layers can be constructed as identity mappings, the deeper model should not have worse training error than the original shallower model without these added layers.", "We verify the Self-Normalizing property by observing the trend of mean and variance in the SELU activation outputs during training. The model topology is a 50-layer CNN obtained by removing SC and BN from ResNet-50. We call this topology SNDCNN-50. Model parameters are initialized as instructed in BIBREF0. In Figures FIGREF14 and FIGREF15, we plot the mean and variance trend of the 1st, 23rd, 46th, 47th, and 49th layers of SNDCNN-50 and the 23rd layer of SNDCNN-24. The mean and variance are computed across frames within a mini-batch (256 frames). Each data point is obtained by averaging all the units in the same layer. The x-axis is training time, and we collect statistics from 33k mini-batches to draw each curve.", "In the SNDCNN-50 case, we can see that the outputs of 1st and middle (23rd) layers follow the claims in BIBREF0 nicely, but the last several layers do not. We find that the non-self-normalizing phenomenon becomes significant only after the 46th layer. As shown in Figure FIGREF14 and FIGREF15, the 46th layer almost has mean $=$ 0 and variance $=$ 1, but the following layers are worse. We verify that the non-self-normalizing phenomenon is not caused by the depth of the neural network but by the distance to the output layer. The 23rd layer of SNDCNN-24 has the non-self-normalizing phenomenon, similar to the one seen in the 49th layer of SNDCNN-50, while the 23rd layer of SNDCNN-50 has a very nice self-normalizing property. We suspect that the back propagation path has to be long enough to effectively train the neural network's parameters to ensure the self-normalizing property.", "We also tried different kinds of initialization for the network. Our findings indicate that as long as training starts normally, the trend of the mean and variance will follow the patterns seen in Figures FIGREF14 and FIGREF15."]}
{"question_id": "049415676f8323f4af16d349f36fbcaafd7367ae", "predicted_answer": "", "predicted_evidence": ["Sounds as the ground-truth. One difference from the previous work is that we utilize utterances with positive system responses as the positive train set and the dev set, and use those with the negative responses as the negative train set as described in Section SECREF11. We have extracted 3M positive train, 400K negative train, and 600K dev sets from 4M log data with 2,500 most frequent domains as the ground-truths. Pseudo labels are added to 53K out of 3M in the positive train set as described in Section SECREF7.", "We have proposed deriving pseudo labels along with leveraging utterances with negative system responses and self-distillation to improve the performance of domain classification when multiple domains are ground-truths even if only one ground-truth is known in large-scale domain classification. Evaluating on the test utterances with multiple ground-truths from an intelligent conversational system, we have showed that the proposed approach significantly improves the performance of domain classification with hypothesis reranking.", "Table TABREF21 shows the evaluation results of the shortlister and the hypothesis reranker with the proposed approaches. For the shortlisters, we show nDCG$_3$ scores, which are highly correlated with the F1 scores of the rerankers than other metrics since the second and third top shortlister predictions contribute the metric. We find that just using the pseudo labels as the additional targets degrades the performance (2). However, when both the pseudo labels and the negative ground-truths are utilized, we observe significant improvements for both precision and recall (5). In addition, recall is increased when self-distillation is used, which achieves the best F1 score (6).", "Table TABREF22 shows examples of derived pseudo labels from model (6). It demonstrates that the domains capable of processing the utterances can be derived, which helps more correct model training.", "Table TABREF21 shows the evaluation results of the shortlister and the hypothesis reranker with the proposed approaches. For the shortlisters, we show nDCG$_3$ scores, which are highly correlated with the F1 scores of the rerankers than other metrics since the second and third top shortlister predictions contribute the metric. We find that just using the pseudo labels as the additional targets degrades the performance (2). However, when both the pseudo labels and the negative ground-truths are utilized, we observe significant improvements for both precision and recall (5). In addition, recall is increased when self-distillation is used, which achieves the best F1 score (6). Each of utilizing the negative feedback $((1)\\rightarrow (3) \\;\\text{and}\\; (2)\\rightarrow (5))$ and then additional pseudo labels $((3)\\rightarrow (5) \\;\\text{and}\\; (4)\\rightarrow (6))$ show statistically significant improvements with McNemar test for p=0.05 for the final reranker results."]}
{"question_id": "fee498457774d9617068890ff29528e9fa05a2ac", "predicted_answer": "", "predicted_evidence": ["where $\\alpha ^t=1-0.95^t$ and $t$ is the current epoch so that the baseline loss is mainly used in the earlier epochs while the pseudo labels and self-distillation are more contributing in the later epochs following BIBREF23. $\\beta $ is a hyperparameter for utilizing negative ground-truths, which is set to 0.00025 showing the best dev set performance.", "First, we run intent classification and slot filling for the $k$ most confident domains from the shortlister outputs to obtain additional information for those domains BIBREF0. Then, we compose $k$ hypotheses, each of which is a vector consists of the shortlister confidence score, intent score, Viterbi score of slot-filling, domain vector, intent vector, and the summation of the slot vectors. On top of the $k$ hypothesis vectors, a BiLSTM is utilized for representing contextualized hypotheses and a shared feed-forward neural network is used to obtain final confidence score for each hypothesis. We set $k$=3 in our experiments following BIBREF4. We leverage the given ground-truth and the derived pseudo labels from the shortlister at the epoch showing the best dev set performance as target labels for training the reranker. We use hinge loss with margin 0.4 as the loss function.", "We have proposed deriving pseudo labels along with leveraging utterances with negative system responses and self-distillation to improve the performance of domain classification when multiple domains are ground-truths even if only one ground-truth is known in large-scale domain classification. Evaluating on the test utterances with multiple ground-truths from an intelligent conversational system, we have showed that the proposed approach significantly improves the performance of domain classification with hypothesis reranking.", "Knowledge distillation has been shown to improve the model performance by leveraging the prediction confidence scores from another model or from previous epochs BIBREF11, BIBREF12, BIBREF17. Inspired by BIBREF17, we utilize the model at the epoch showing the best dev set performance before the current epoch to obtain the prediction confidence scores as the soft target. The self-distillation in our work can be formulated as follows:", "During the model training, irrelevant domains could be top predicted, and regarding them as additional target labels results in wrong confirmation bias BIBREF19, which causes incorrect model training. To reduce the side effect, we leverage utterances with negative responses in order to discourage the utterances' incorrect predictions. This setting can be considered as a multi-label variant of Positive, Unlabeled, and Biased Negative Data (PUbN) learning BIBREF20."]}
{"question_id": "c626637ed14dee3049b87171ddf326115e59d9ee", "predicted_answer": "", "predicted_evidence": ["As future work, combining our approach with pure semi-supervised learning, and the relation between pseudo labeling and distillation should be further studied.", "where $\\tilde{o_i}$ denotes the model output at the epoch showing the best dev set performance so far. Before taking sigmoid to obtain $\\tilde{o_i}$, we use 16 as the temperature to increase the influence of distillation BIBREF11, which shows the best dev set performance following BIBREF17.", "First, we run intent classification and slot filling for the $k$ most confident domains from the shortlister outputs to obtain additional information for those domains BIBREF0. Then, we compose $k$ hypotheses, each of which is a vector consists of the shortlister confidence score, intent score, Viterbi score of slot-filling, domain vector, intent vector, and the summation of the slot vectors. On top of the $k$ hypothesis vectors, a BiLSTM is utilized for representing contextualized hypotheses and a shared feed-forward neural network is used to obtain final confidence score for each hypothesis. We set $k$=3 in our experiments following BIBREF4. We leverage the given ground-truth and the derived pseudo labels from the shortlister at the epoch showing the best dev set performance as target labels for training the reranker. We use hinge loss with margin 0.4 as the loss function.", "Sounds as the ground-truth. One difference from the previous work is that we utilize utterances with positive system responses as the positive train set and the dev set, and use those with the negative responses as the negative train set as described in Section SECREF11. We have extracted 3M positive train, 400K negative train, and 600K dev sets from 4M log data with 2,500 most frequent domains as the ground-truths. Pseudo labels are added to 53K out of 3M in the positive train set as described in Section SECREF7.", "In this section, we show training and evaluation sets, and experiment results."]}
{"question_id": "b160bfb341f24ae42a268aa18641237a4b3a6457", "predicted_answer": "", "predicted_evidence": ["In this section, we show training and evaluation sets, and experiment results.", "Sounds as the ground-truth. One difference from the previous work is that we utilize utterances with positive system responses as the positive train set and the dev set, and use those with the negative responses as the negative train set as described in Section SECREF11. We have extracted 3M positive train, 400K negative train, and 600K dev sets from 4M log data with 2,500 most frequent domains as the ground-truths. Pseudo labels are added to 53K out of 3M in the positive train set as described in Section SECREF7.", "As future work, combining our approach with pure semi-supervised learning, and the relation between pseudo labeling and distillation should be further studied.", "where $j$ denotes the index corresponding to the negative ground-truth domain. We demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming.", "One issue of the hypothesis reranking is that a training utterance cannot be used if no ground-truth exist in the top $k$ predictions of the shortlister. This is problematic in the multi-label PU setting since correct domains can indeed exist in the top $k$ list but unknown, which makes the training utterance less useful in the reranking. Our pseudo labeling method can address this issue. If correct pseudo labels are derived from the shortlister's top predictions for such utterances, we can use them properly in the reranker training, which was unavailable without them. This allows our approach make more improvement in hypothesis reranking than shortlisting."]}
{"question_id": "c0120d339fcdb3833884622e532e7513d1b2c7dd", "predicted_answer": "", "predicted_evidence": ["The validation result is in Table 4. For clarity, we recorded both BERTScore and ROUGE-1. Note that for ROUGE-1, the character-level comparison was utilized, regardless of the tokenizer that was adopted in the training and inference.", "$\\rightarrow $ \uc624\ub298 \uac15\uc124\ub7c9 / the amount of today's snowfall", "(7) \u201c\uc218\uc601\uc744 \uc628\ucc9c\uc5d0\uc11c\ub294 \ud558\uba74 \uc548\ub429\ub2c8\ub2e4\u201d / \u201cit is prohibited to swim in an onsen\u201d", "We expect the formalization as (7) can be useful for a real-life command to the social robots, and (8) meaningful if smart agents more become human-like beings, though far future. Also, as in the case of two wh-questions (9-10), the nominalization of wh-related features may help the NLU modules to efficiently get the answer of information-seeking questions that are not in a canonical form. Not all the results were nice, especially regarding some intonation-dependent utterances (11) and the most challenging ones that incorporate various OOV/loanwords (12).", ""]}
{"question_id": "f52c9744a371104eb2677c181a7004f7a77d9dd3", "predicted_answer": "", "predicted_evidence": ["", "First, alternative questions, prohibitions, and strong requirements were needed to ensure that we had class balance for each utterance type, or at least a sufficient number for the automation. To do this, we manually wrote 400 intent arguments for each of the three types. In the process of deciding intent arguments, the topic of sentences to be generated was also carefully considered. Specifically, sentences were created at a 1: 1: 1: 1: 4 ratio for mail, schedule, house control, weather, and other free topics. This reflects the topic characteristics of the dataset used in Section 4.1, and its purpose is to build a corpus oriented to the future advancement of smart agents.", "(5) a. put your right foot there", "", "(3) b. i i don't want to see you tomorrow"]}
{"question_id": "867b1bb1e6a38de525be7757d49928a132d0dbd8", "predicted_answer": "", "predicted_evidence": ["This research was supported by Projects for Research and Development of Police science and Technology under Center for Research and Development of Police science and Technology and Korean National Police Agency funded by the Ministry of Science, ICT and Future Planning (PA-J000001-2017-101). Also, this work was supported by the Technology Innovation Program (10076583, Development of free-running speech recognition technologies for embedded robot system) funded By the Ministry of Trade, Industry & Energy (MOTIE, Korea).", "In this regard, we first surveyed a proper evaluation for the automatic and quantitative analysis of the result, respectively. A part of the conclusion is that the automatic analysis of semantic similarity can be executed utilizing and modifying the recent BERT-based scoring system BIBREF32. Such an approach can be adopted regardlessly the label is correctly inferred, and also well reflects the common sense inherited in the pre-trained language models. Moreover, in the case that the label is correct that some format-related tokens (e.g., the method, whether, not to) in the output overlap with the ones in the gold data, the lexical similarity can also be taken into account, probably as an extra point. It can be further represented by ROUGE compared to the gold standard.", "(3) c. how many points you got", "(3) $\\rightarrow $ the number of points that the addressee got", "(9) \u201c\uc624\ub298 \ub208\uc774 \uc5bc\ub9c8\ub098 \uc624\ub2c8\u201d / \u201chow much does it snow today\u201d"]}
{"question_id": "6167618e0c53964f3a706758bdf5e807bc5d7760", "predicted_answer": "", "predicted_evidence": ["The advancements in the field of deep learning have certainly helped to develop systems for the task of Image Question Answering. Krizhevsky et al BIBREF1 proposed the AlexNet model, which created a revolution in the computer vision domain. The paper introduced the concept of Convolution Neural Networks (CNN) to the mainstream computer vision application. Later many authors have worked on CNN, which has resulted in robust, deep learning models like VGGNet BIBREF2, Inception BIBREF3, ResNet BIBREF4, and etc. Similarly, the recent advancements in natural language processing area based on deep learning have improved the text understanding prforance as well. The first major algorithm in the context of text processing is considered to be the Recurrent Neural Networks (RNN) BIBREF5 which introduced the concept of prior context for time series based data. This architecture helped the growth of machine text understanding which gave new boundaries to machine translation, text classification and contextual understanding.", "KVQA: The recent interest in common-sense questions has led to the development of world Knowledge based VQA dataset BIBREF10. The dataset contains questions targeting various categories of nouns and also require world knowledge to arrive at a solution. Questions in this dataset require multi-entity, multi-relation, and multi- hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. The dataset contains 24,000 images with 183,100 question-answer pairs employing around 18K proper nouns. The KVQA samples are shown in Fig. SECREF2 in 2nd row and 2nd column.", "Visual Question Answering (VQA) refers to a challenging task which lies at the intersection of image understanding and language processing. The VQA task has witnessed a significant progress in the recent years by the machine intelligence community. The aim of VQA is to develop a system to answer specific questions about an input image. The answer could be in any of the following forms: a word, a phrase, binary answer, multiple choice answer, or a fill in the blank answer. Agarwal et al. BIBREF0 presented a novel way of combining computer vision and natural language processing concepts of to achieve Visual Grounded Dialogue, a system mimicking the human understanding of the environment with the use of visual observation and language understanding.", "Visual7W: The Visual7W dataset BIBREF8 is also based on the MS-COCO dataset. It contains 47,300 COCO images with 327,939 question-answer pairs. The dataset also consists of 1,311,756 multiple choice questions and answers with 561,459 groundings. The dataset mainly deals with seven forms of questions (from where it derives its name): What, Where, When, Who, Why, How, and Which. It is majorly formed by two types of questions. The \u2018telling\u2019 questions are the ones which are text-based, giving a sort of description. The \u2018pointing\u2019 questions are the ones that begin with \u2018Which,\u2019 and have to be correctly identified by the bounding boxes among the group of plausible answers.", "VQA Dataset: The Visual Question Answering (VQA) dataset BIBREF0 is one of the largest datasets collected from the MS-COCO BIBREF18 dataset. The VQA dataset contains at least 3 questions per image with 10 answers per question. The dataset contains 614,163 questions in the form of open-ended and multiple choice. In multiple choice questions, the answers can be classified as: 1) Correct Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently, VQA V2 dataset BIBREF0 is released with additional confusing images. The VQA sample images and questions are shown in Fig. SECREF2 in 1st row and 1st column."]}
{"question_id": "78a0c25b83cdeaeaf0a4781f502105a514b2af0e", "predicted_answer": "", "predicted_evidence": ["KVQA: The recent interest in common-sense questions has led to the development of world Knowledge based VQA dataset BIBREF10. The dataset contains questions targeting various categories of nouns and also require world knowledge to arrive at a solution. Questions in this dataset require multi-entity, multi-relation, and multi- hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. The dataset contains 24,000 images with 183,100 question-answer pairs employing around 18K proper nouns. The KVQA samples are shown in Fig. SECREF2 in 2nd row and 2nd column.", "VQA Dataset: The Visual Question Answering (VQA) dataset BIBREF0 is one of the largest datasets collected from the MS-COCO BIBREF18 dataset. The VQA dataset contains at least 3 questions per image with 10 answers per question. The dataset contains 614,163 questions in the form of open-ended and multiple choice. In multiple choice questions, the answers can be classified as: 1) Correct Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently, VQA V2 dataset BIBREF0 is released with additional confusing images. The VQA sample images and questions are shown in Fig. SECREF2 in 1st row and 1st column.", "The advancements in the field of deep learning have certainly helped to develop systems for the task of Image Question Answering. Krizhevsky et al BIBREF1 proposed the AlexNet model, which created a revolution in the computer vision domain. The paper introduced the concept of Convolution Neural Networks (CNN) to the mainstream computer vision application. Later many authors have worked on CNN, which has resulted in robust, deep learning models like VGGNet BIBREF2, Inception BIBREF3, ResNet BIBREF4, and etc. Similarly, the recent advancements in natural language processing area based on deep learning have improved the text understanding prforance as well. The first major algorithm in the context of text processing is considered to be the Recurrent Neural Networks (RNN) BIBREF5 which introduced the concept of prior context for time series based data. This architecture helped the growth of machine text understanding which gave new boundaries to machine translation, text classification and contextual understanding.", "Visual7W: The Visual7W dataset BIBREF8 is also based on the MS-COCO dataset. It contains 47,300 COCO images with 327,939 question-answer pairs. The dataset also consists of 1,311,756 multiple choice questions and answers with 561,459 groundings. The dataset mainly deals with seven forms of questions (from where it derives its name): What, Where, When, Who, Why, How, and Which. It is majorly formed by two types of questions. The \u2018telling\u2019 questions are the ones which are text-based, giving a sort of description. The \u2018pointing\u2019 questions are the ones that begin with \u2018Which,\u2019 and have to be correctly identified by the bounding boxes among the group of plausible answers.", "DAQUAR: DAQUAR stands for Dataset for Question Answering on Real World Images, released by Malinowski et al. BIBREF7. It is the first dataset released for the IQA task. The images are taken from NYU-Depth V2 dataset BIBREF17. The dataset is small with a total of 1449 images. The question bank includes 12468 question-answer pairs with 2483 unique questions. The questions have been generated by human annotations and confined within 9 question templates using annotations of the NYU-Depth dataset."]}
{"question_id": "08202b800a946b8283c2684e23b51c0ec1e8b2ac", "predicted_answer": "", "predicted_evidence": ["Differential Networks BIBREF19: This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN BIBREF21. The differential modules BIBREF29 are used to refine the features in both text and images. GRU BIBREF30 is used for question feature extraction. Finally, it is combined with an attention module to classify the answers. The Differential Networks architecture is illustrated in Fig. FIGREF5.", "Krizhevsky et al BIBREF1 proposed the AlexNet model, which created a revolution in the computer vision domain. The paper introduced the concept of Convolution Neural Networks (CNN) to the mainstream computer vision application. Later many authors have worked on CNN, which has resulted in robust, deep learning models like VGGNet BIBREF2, Inception BIBREF3, ResNet BIBREF4, and etc. Similarly, the recent advancements in natural language processing area based on deep learning have improved the text understanding prforance as well. The first major algorithm in the context of text processing is considered to be the Recurrent Neural Networks (RNN) BIBREF5 which introduced the concept of prior context for time series based data. This architecture helped the growth of machine text understanding which gave new boundaries to machine translation, text classification and contextual understanding. Another major breakthrough in the domain was the introduction of Long-Short Term Memory (LSTM) architecture BIBREF6 which improvised over the RNN by introducing a context cell which stores the prior relevant information.", "Tally-QA: Very recently, in 2019, the Tally-QA BIBREF9 dataset is proposed which is the largest dataset of object counting in the open-ended task. The dataset includes both simple and complex question types which can be seen in Fig. SECREF2. The dataset is quite large in numbers as well as it is 2.5 times the VQA dataset. The dataset contains 287,907 questions, 165,000 images and 19,000 complex questions. The Tally-QA samples are shown in Fig. SECREF2 in 2nd row and 1st column.", "KVQA: The recent interest in common-sense questions has led to the development of world Knowledge based VQA dataset BIBREF10. The dataset contains questions targeting various categories of nouns and also require world knowledge to arrive at a solution. Questions in this dataset require multi-entity, multi-relation, and multi- hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. The dataset contains 24,000 images with 183,100 question-answer pairs employing around 18K proper nouns. The KVQA samples are shown in Fig. SECREF2 in 2nd row and 2nd column.", "Visual Madlibs: The Visual Madlibs dataset BIBREF15 presents a different form of template for the Image Question Answering task. One of the forms is the fill in the blanks type, where the system needs to supplement the words to complete the sentence and it mostly targets people, objects, appearances, activities and interactions. The Visual Madlibs samples are shown in Fig. SECREF2 in 1st row and 2nd column."]}
{"question_id": "00aea97f69290b496ed11eb45a201ad28d741460", "predicted_answer": "", "predicted_evidence": ["The vanilla VQA model BIBREF0 used a combination of VGGNet BIBREF2 and LSTM BIBREF6. This model has been revised over the years, employing newer architectures and mathematical formulations. Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system. In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset BIBREF0, DAQUAR BIBREF7, Visual7W BIBREF8 and most recent datasets up to 2019 include Tally-QA BIBREF9 and KVQA BIBREF10.", "The vanilla VQA model BIBREF0 used a combination of VGGNet BIBREF2 and LSTM BIBREF6. This model has been revised over the years, employing newer architectures and mathematical formulations. Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system. In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset BIBREF0, DAQUAR BIBREF7, Visual7W BIBREF8 and most recent datasets up to 2019 include Tally-QA BIBREF9 and KVQA BIBREF10. Next, we discuss the state-of-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA BIBREF0, Stacked Attention Networks BIBREF11 and Pythia v1.0 BIBREF12.", "Stacked Attention Networks BIBREF11: This model introduced the attention using the softmax output of the intermediate question feature. The attention between the features are stacked which helps the model to focus on the important portion of the image.", "VQA Dataset: The Visual Question Answering (VQA) dataset BIBREF0 is one of the largest datasets collected from the MS-COCO BIBREF18 dataset. The VQA dataset contains at least 3 questions per image with 10 answers per question. The dataset contains 614,163 questions in the form of open-ended and multiple choice. In multiple choice questions, the answers can be classified as: 1) Correct Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently, VQA V2 dataset BIBREF0 is released with additional confusing images. The VQA sample images and questions are shown in Fig. SECREF2 in 1st row and 1st column.", "Tally-QA: Very recently, in 2019, the Tally-QA BIBREF9 dataset is proposed which is the largest dataset of object counting in the open-ended task. The dataset includes both simple and complex question types which can be seen in Fig. SECREF2. The dataset is quite large in numbers as well as it is 2.5 times the VQA dataset. The dataset contains 287,907 questions, 165,000 images and 19,000 complex questions. The Tally-QA samples are shown in Fig. SECREF2 in 2nd row and 1st column."]}
{"question_id": "4e1293592e41646a6f5f0cb00c75ee8de14eb668", "predicted_answer": "", "predicted_evidence": ["The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.", "VQA Dataset: The Visual Question Answering (VQA) dataset BIBREF0 is one of the largest datasets collected from the MS-COCO BIBREF18 dataset. The VQA dataset contains at least 3 questions per image with 10 answers per question. The dataset contains 614,163 questions in the form of open-ended and multiple choice. In multiple choice questions, the answers can be classified as: 1) Correct Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently, VQA V2 dataset BIBREF0 is released with additional confusing images. The VQA sample images and questions are shown in Fig. SECREF2 in 1st row and 1st column.", "CLEVR: CLEVR BIBREF16 is a synthetic dataset to test the visual understanding of the VQA systems. The dataset is generated using three objects in each image, namely cylinder, sphere and cube. These objects are in two different sizes, two different materials and placed in eight different colors. The questions are also synthetically generated based on the objects placed in the image. The dataset also accompanies the ground-truth bounding boxes for each object in the image.", "KVQA: The recent interest in common-sense questions has led to the development of world Knowledge based VQA dataset BIBREF10. The dataset contains questions targeting various categories of nouns and also require world knowledge to arrive at a solution. Questions in this dataset require multi-entity, multi-relation, and multi- hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. The dataset contains 24,000 images with 183,100 question-answer pairs employing around 18K proper nouns. The KVQA samples are shown in Fig. SECREF2 in 2nd row and 2nd column.", "Visual7W: The Visual7W dataset BIBREF8 is also based on the MS-COCO dataset. It contains 47,300 COCO images with 327,939 question-answer pairs. The dataset also consists of 1,311,756 multiple choice questions and answers with 561,459 groundings. The dataset mainly deals with seven forms of questions (from where it derives its name): What, Where, When, Who, Why, How, and Which. It is majorly formed by two types of questions. The \u2018telling\u2019 questions are the ones which are text-based, giving a sort of description. The \u2018pointing\u2019 questions are the ones that begin with \u2018Which,\u2019 and have to be correctly identified by the bounding boxes among the group of plausible answers."]}
{"question_id": "15aeda407ae3912419fd89211cdb98989d9cde58", "predicted_answer": "", "predicted_evidence": ["Our work is built on the recently proposed MAML framework BIBREF4 , which we describe briefly here. MAML aims to learn the learners (for the tasks) and the meta-learner in the few-shot meta-learning setup BIBREF13 , BIBREF14 , BIBREF15 . Formally, it considers a model that is represented by a function INLINEFORM0 with parameters INLINEFORM1 . When the model adapts to a new task INLINEFORM2 , the model changes the parameters from one INLINEFORM3 to the next, where a task contains INLINEFORM4 training examples and one or more test examples (K-shot learning). MAML updates the parameters INLINEFORM5 via one or a few iterations of gradient descent based on the training examples of task INLINEFORM6 . For example, for one gradient update,", "Few-shot learning generally resolves the data deficiency problem by recognizing novel classes from very few labeled examples. This limitation in the size of samples (only one or very few examples) challenges the standard fine-tuning method in DL. Early studies in this field BIBREF3 applied data augmentation and regularization techniques to alleviate the overfitting problem caused by data scarcity but only to a limited extent. Instead, researchers have been inspired by human learning to explore meta-learning BIBREF4 to leverage the distribution over similar tasks. Contemporary approaches to few-shot learning often decompose the training procedure into an auxiliary meta-learning phase, which includes many sub-tasks, following the principle that the testing and training conditions must match. They extract some transferable knowledge by switching the task from one mini-batch to the next. Moreover, the few-shot model is able to classify data into new classes with just a small labeled support set.", "Deep learning (DL) has achieved great success in many fields such as computer vision, speech recognition, and machine translation BIBREF0 , BIBREF1 , BIBREF2 thanks to the advancements in optimization techniques, larger datasets, and streamlined designs of deep neural architectures. However, DL is notorious for requiring large labeled datasets, which limits the scalability of a deep model to new classes owing to the cost of annotation. Humans, however, are readily able to learn and distinguish new classes rapidly with only a few examples. This gap between human and machine learning provides opportunities for DL development and applications.", "Recently, a variety of techniques were proposed for training general-purpose language representation models using an enormous amount of unannotated text, such as ELMo BIBREF10 and generative pretrained transformer (GPT) BIBREF11 . Pretrained models can be fine-tuned on natural language processing (NLP) tasks and have achieved significant improvements over training on task-specific annotated data. More recently, a pretraining technique named bidirectional encoder representations from transformers (BERT) BIBREF12 was proposed and has enabled the creation of state-of-the-art models for a wide variety of NLP tasks, including question answering (SQuAD v1.1) and natural language inference, among others.", "It should be noted that human beings are intelligent to leverage learned knowledge about the world in understanding language. BIBREF23 think human beings have a universal grammar, and our daily language system is only a formal expression of this universal grammar. In other words, there are deep structures related to concepts and superficial structures related to speech and symbols in a language. Moreover, neuroscience research has proposed a prominent idea that language processing may offer such a principle that the brain contains partially separate systems for processing syntax and semantics. The part of the prefrontal cortex responsible for language production, called Broca\u2019s area, is thought to be important for parsing syntactic information and applying selective attention to help a separate comprehension system interpret the semantics BIBREF24 . Our idea for few-shot learning in NLP is somewhat similar to this assumption as the pretraining stage may learn common syntax information across tasks, and the meta-learning stage may learn semantic knowledge, which is task specific."]}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "predicted_answer": "", "predicted_evidence": ["We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset. All model parameters are updated by backpropagation using Adam with a learning rate of 0.01. We regularize our network using dropout with a rate of 0.3 tuned using the development set.", "We use the multiple tasks with the multi-domain sentiment classification BIBREF19 dataset ARSC. This dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set. We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 .", "It should be noted that human beings are intelligent to leverage learned knowledge about the world in understanding language. BIBREF23 think human beings have a universal grammar, and our daily language system is only a formal expression of this universal grammar. In other words, there are deep structures related to concepts and superficial structures related to speech and symbols in a language. Moreover, neuroscience research has proposed a prominent idea that language processing may offer such a principle that the brain contains partially separate systems for processing syntax and semantics. The part of the prefrontal cortex responsible for language production, called Broca\u2019s area, is thought to be important for parsing syntactic information and applying selective attention to help a separate comprehension system interpret the semantics BIBREF24 . Our idea for few-shot learning in NLP is somewhat similar to this assumption as the pretraining stage may learn common syntax information across tasks, and the meta-learning stage may learn semantic knowledge, which is task specific.", "In this study, we attempt to analyze language representation pretraining for few-shot text classification empirically. We combine the MAML algorithm with the pretraining strategy to disentangle the task-agnostic and task-specific representation learning. Results show that our model outperforms conventional state-of-the-art few-shot text classification models. In the future, we plan to apply our method to other NLP scenarios.", "INLINEFORM0"]}
{"question_id": "c24f7c030010ad11e71ef4912fd79093503f3a8d", "predicted_answer": "", "predicted_evidence": ["Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set. We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset. All model parameters are updated by backpropagation using Adam with a learning rate of 0.01.", "These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set. We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset.", "Given the pretrained language representations, we construct episodes to compute the gradients and update our model in each training iteration. The training episode is formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each selected class to act as the support set INLINEFORM0 with a subset of the remaining examples to serve as the query set INLINEFORM1 . Training with such episodes is achieved by feeding the support set INLINEFORM2 to the model and updating its parameters to minimize the loss in the query set INLINEFORM3 . We call this strategy as episode-based meta training. The adaptation of meta-learning using the MAML framework with pretrained language representations is summarized in Algorithm SECREF4 , called P-MAML. The use of episodes makes the training procedure more faithful to the test environment, thereby improving generalization. It is worth noting that there are exponentially many possible meta tasks to train the model with, thus making it difficult to overfit.", "From the results shown in Table TABREF9 , we observe that our approach achieves the best results amongst all meta-learning models. Compared with ROBUSTTC-FSL and Induction-Network-Routing, which adopt several metric methods and dynamic routing algorithms, our approach still provides more advantages. We believe the performance of our model can be further improved by adopting additional mechanisms like adaptive metrics, which will be part of our future work. Note that, our approach is very simple and independent of the encoder choices, and can, therefore, be easily adapted to fit other encoder architectures for sophisticated NLP tasks.", "Few-shot classification is a task in which a classifier must adapt and accommodate new classes that are not seen in training, given only a few examples of each of these new classes. We have a large labeled training set with a set of defined classes INLINEFORM0 . However, after training, our ultimate goal is to produce classifiers on the testing set with a disjoint set of new classes INLINEFORM1 for which only a small labeled support set will be available. If the support set contains INLINEFORM2 labeled examples for each of the INLINEFORM3 unique classes, the target few-shot problem is called a INLINEFORM4 -way INLINEFORM5 -shot problem. Usually, INLINEFORM6 is a too small sample set to train a supervised classification model. Therefore, we aim to perform meta-learning on the training set in order to extract transferrable knowledge that will allow us to perform better few-shot learning on the support set to classify the test set more successfully."]}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "predicted_answer": "", "predicted_evidence": ["We used the Adam optimizer BIBREF11 with $\\beta _1=0.9$ , $\\beta _2=0.98$ , and $\\epsilon = 10^{-9}$ . We used the same warmup and decay strategy for learning rate as Vaswani et al. vaswani2017, with 4,000 warmup steps. During training, we employed label smoothing of value $\\epsilon _{ls} = 0.1$ BIBREF12 . For evaluation, we used beam search with a beam size of 4 and length penalty $\\alpha = 0.6$ BIBREF7 .", "For future work, we plan to extend this mechanism to consider arbitrary directed, labeled graph inputs to the Transformer. We are also interested in nonlinear compatibility functions to combine input representations and edge representations. For both of these extensions, a key consideration will be determining efficient implementations.", "$$z_i = \\sum _{j=1}^{n} \\alpha _{ij} (x_jW^V + a^V_{ij})$$   (Eq. 6)", "The first term is identical to eq. ( 4 ), and can be computed as described above. For the second term involving relative position representations, tensor reshaping can be used to compute $n$ parallel multiplications of $bh \\times d_z$ and $d_z \\times n$ matrices. Each matrix multiplication computes contributions to $e_{ij}$ for all heads and batches, corresponding to a particular sequence position. Further reshaping allows adding the two terms. The same approach can be used to efficiently compute eq. ( 6 ).", "We performed several experiments modifying various aspects of our model. All of our experiments in this section use the base model configuration without any absolute position representations. BLEU scores are calculated on the WMT English-to-German task using the development set, newstest2013."]}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "predicted_answer": "", "predicted_evidence": ["We compared our model using only relative position representations to the baseline Transformer BIBREF3 with sinusoidal position encodings. We generated baseline results to isolate the impact of relative position representations from any other changes to the underlying library and experimental configuration.", "For a sequence of length $n$ and $h$ attention heads, we reduce the space complexity of storing relative position representations from $O(hn^2d_a)$ to $O(n^2d_a)$ by sharing them across each heads. Additionally, relative position representations can be shared across sequences. Therefore, the overall self-attention space complexity increases from $O(bhnd_z)$ to $O(bhnd_z + n^2d_a)$ . Given $d_a = d_z$ , the size of the relative increase depends on $\\frac{n}{bh}$ .", "In this paper we presented an extension to self-attention that can be used to incorporate relative position information for sequences, which improves performance for machine translation.", "We propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully-connected graph.", "For all experiments, we split tokens into a 32,768 word-piece vocabulary BIBREF7 . We batched sentence pairs by approximate length, and limited input and output tokens per batch to 4096 per GPU. Each resulting training batch contained approximately 25,000 source and 25,000 target tokens."]}
{"question_id": "d48b5e4a7cf1f96c5b939ba9b46350887c5e5268", "predicted_answer": "", "predicted_evidence": ["We also, importantly, modify eq. ( 4 ) to consider edges when determining compatibility:", "$$z_i = \\sum _{j=1}^{n} \\alpha _{ij} (x_jW^V)$$   (Eq. 3)", "We used the Adam optimizer BIBREF11 with $\\beta _1=0.9$ , $\\beta _2=0.98$ , and $\\epsilon = 10^{-9}$ . We used the same warmup and decay strategy for learning rate as Vaswani et al. vaswani2017, with 4,000 warmup steps. During training, we employed label smoothing of value $\\epsilon _{ls} = 0.1$ BIBREF12 . For evaluation, we used beam search with a beam size of 4 and length penalty $\\alpha = 0.6$ BIBREF7 .", "Each attention head operates on an input sequence, $x = (x_1, \\ldots , x_n)$ of $n$ elements where $x_i \\in \\mathbb {R}^{d_x}$ , and computes a new sequence $z = (z_1, \\ldots , z_n)$ of the same length where $z_i \\in \\mathbb {R}^{d_z}$ .", "The primary motivation for using simple addition to incorporate edge representations in eq. ( 6 ) and eq. ( 7 ) is to enable an efficient implementation described in \"Efficient Implementation\" ."]}
{"question_id": "de344aeb089affebd15a8c370ae9ab5734e99203", "predicted_answer": "", "predicted_evidence": ["Apart from the features, most of the teams used machine learning algorithms like SVM, Na\u00efve Bayes. It is observed that the deep learning models are quite successful for many NLP tasks. CFIL team have used the deep learning framework however the deep learning based system did not perform well as compared to machine learning based system. The main reason for the above may be that the training datasets provided are not sufficient to built a deep learning model.", "Any of the six language tags is used to annotate the language to each of the words and these are HI (Hindi), EN (English), BN (Bengali), UN(Universal), MIX (Mix of two languages), EMT (emoticons). MIX words are basically the English words with Hindi or Bengali suffix, for example, Delhite (in Delhi). Sometimes, the words are joined together by mistake due to the typing errors, for example, jayegiTension (tension will go away). UN words are basically symbols, hashtags, or name etc. The statistics of training and test tweets for Bengali and Hindi code-mixed datasets are provided in Table TABREF23 . Some examples of HI-EN and BN-EN datasets with sentiment tags are given below.", "Tagged: Irrfan/EN Khan/EN hollywood/EN e/BN abar/BN dekha/BN debe/BN ,/UN trailer/EN ta/BN toh/BN awesome/EN ar/BN acting/EN o/BN enjoyable/EN ./UN", "Translation: Irrfan Khan will be seen in Hollywood again, trailer is awesome and acting is also enjoyable.", "a tweet does not have either Bengali or Hindi words."]}
{"question_id": "84327a0a9321bf266e22d155dfa94828784595ce", "predicted_answer": "", "predicted_evidence": ["BI-EN: Ei movie take bar bar dekheo er matha mundu kichui bojha jaye na. Everything boddo confusing and amar mote not up to the mark. (negative)", "Tagged: Irrfan/EN Khan/EN hollywood/EN e/BN abar/BN dekha/BN debe/BN ,/UN trailer/EN ta/BN toh/BN awesome/EN ar/BN acting/EN o/BN enjoyable/EN ./UN", "BI-EN: Irrfan Khan hollywood e abar dekha debe, trailer ta toh awesome ar acting o enjoyable. (positive)", "a tweet is incomplete, i.e. there is not much information available in the tweet.", "Translation: Irrfan Khan will be seen in Hollywood again, trailer is awesome and acting is also enjoyable."]}
{"question_id": "c2037887945abbdf959389dc839a86bc82594505", "predicted_answer": "", "predicted_evidence": ["Tagged: Irrfan/EN Khan/EN hollywood/EN e/BN abar/BN dekha/BN debe/BN ,/UN trailer/EN ta/BN toh/BN awesome/EN ar/BN acting/EN o/BN enjoyable/EN ./UN", "The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .", "The paper is organized as following manner. Section SECREF2 describes the NLP in Indian languages mainly related to code-mixing and sentiment analysis. The detailed statistics of the dataset and evaluation are described in Section SECREF3 . The baseline systems and participant's system description are described in Section SECREF4 . Finally, conclusion and future research are drawn in Section SECREF5 .", "Translation: Irrfan Khan will be seen in Hollywood again, trailer is awesome and acting is also enjoyable.", "This subsection describes the details of systems submitted for the shared task. Six teams have submitted their system details and those are described below in order of decreasing f-score."]}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "predicted_answer": "", "predicted_evidence": ["Social media has become the voice of many people over decades and it has special relations with real time events. The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts.", "Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts. Language Identification in Code-Switched Data was one of the shared tasks which covered four language pairs such as Spanish-English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English. In the case of Indian languages, Mixed Script Information Retrieval BIBREF9 shared task at FIRE-2015 was organized for eight code-mixed Indian languages such as Bangla, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, and Telugu mixed with English.", "According to Census of India, there are 22 scheduled languages and more than 100 non scheduled languages in India. There are 462 million internet users in India and most people know more than one language. They express their feelings or emotions using more than one languages, thus generating a new code-mixed/code-switched language. The problem of code-mixing and code-switching are well studied in the field of NLP BIBREF0 , BIBREF1 . Information extraction from Indian internet user-generated texts become more difficult due to this multilingual nature. Much research has been conducted in this field such as language identification BIBREF2 , BIBREF3 , part-of-speech tagging BIBREF4 . Joshi et al.", "Translation: Irrfan Khan will be seen in Hollywood again, trailer is awesome and acting is also enjoyable.", "The paper is organized as following manner. Section SECREF2 describes the NLP in Indian languages mainly related to code-mixing and sentiment analysis. The detailed statistics of the dataset and evaluation are described in Section SECREF3 . The baseline systems and participant's system description are described in Section SECREF4 . Finally, conclusion and future research are drawn in Section SECREF5 ."]}
{"question_id": "5b2839bef513e5d441f0bb8352807f673f4b2070", "predicted_answer": "", "predicted_evidence": ["The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .", "Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts. Language Identification in Code-Switched Data was one of the shared tasks which covered four language pairs such as Spanish-English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English. In the case of Indian languages, Mixed Script Information Retrieval BIBREF9 shared task at FIRE-2015 was organized for eight code-mixed Indian languages such as Bangla, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, and Telugu mixed with English.", "The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts. Language Identification in Code-Switched Data was one of the shared tasks which covered four language pairs such as Spanish-English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English.", "Information extraction from Indian internet user-generated texts become more difficult due to this multilingual nature. Much research has been conducted in this field such as language identification BIBREF2 , BIBREF3 , part-of-speech tagging BIBREF4 . Joshi et al. JoshiPSV16 have performed sentiment analysis in Hindi-English (HI-EN) code-mixed data and almost no work exists on sentiment analysis of Bengali-English (BN-EN) code-mixed texts. The Sentiment Analysis of Indian Language (Code-Mixed) (SAIL _Code-Mixed) is a shared task at ICON-2017. Two most popular code-mixed languages namely Hindi and Bengali mixed with English were considered for the sentiment identification task. A total of 40 participants registered for the shared task and only nine teams have submitted their predicted outputs. Out of nine unique submitted systems for evaluation, eight teams submitted fourteen runs for HI-EN dataset whereas seven teams submitted nine runs for BN-EN dataset.", "JU_KS team used n-gram and sentiment lexicon based features. Small sentiment lexicons are manually prepared for both English and Bengali words. However, no sentiment lexicon is used for Hindi language. Bengali sentiment lexicon consists of a collection of 1700 positive and 3750 negative words whereas English sentiment lexicon consists of 2006 positive and 4783 negative words. Finally, Na\u00efve Bayes multinomial is used to classify and system results are presented in Table TABREF29 ."]}
{"question_id": "2abf916bc03222d3b2a3d66851d87921ff35c0d2", "predicted_answer": "", "predicted_evidence": ["Tagged: Irrfan/EN Khan/EN hollywood/EN e/BN abar/BN dekha/BN debe/BN ,/UN trailer/EN ta/BN toh/BN awesome/EN ar/BN acting/EN o/BN enjoyable/EN ./UN", "Translation: Irrfan Khan will be seen in Hollywood again, trailer is awesome and acting is also enjoyable.", "Translation: Friends, when will #railbudget2015 start?", "Tagged: yaaaro/HI yeah/EN #railbudget2015/EN kitne/HI baaje/HI start/EN hooga/EN ?/UN", "Tagged: Ei/BN movie/EN take/BN bar/BN bar/BN dekheo/BN er/BN matha/BN mundu/BN kichui/BN bojha/BN jaye/BN na/BN ./UN Everything/EN boddo/BN confusing/EN and/EN amar/BN mote/BN not/EN up/EN to/EN the/EN mark/EN ./UN"]}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "predicted_answer": "", "predicted_evidence": ["Discussion Our evaluation results suggest that the verbalization of these languages is a non-trivial task that can be approached by using a bottom-up approach. As expected, the verbalization of short expressions leads to sentences which read as if they have been generated by a human. However, due to the complexity of the semantics that can be expressed by the languages at hand, long expressions can sound mildly artificial. Our results however also suggest that although the text generated can sound artificial, it is still clear enough to enable non-expert users to achieve results that are comparable to those achieved by experts. Hence, our first conclusion is that our framework clearly serves its purpose. Still, potential improvements can be derived from the results achieved during the experiments. In particular, we will consider the used of attention-based encoder-decoder networks to improve the fluency of complex sentences.", "$\\rho $(s p o) $\\Rightarrow $ subj($\\rho $(p),$\\rho $(s))$\\,\\wedge \\,$dobj($\\rho $(p),$\\rho $(o))", "FIGREF39), the fluency was assigned a average score of 3.47 by experts and 3.0 by non-experts (see Fig. FIGREF39). What these results suggest is that (1) our framework generates sentences that are close to that which a domain expert would also generate (adequacy). However (2) while the sentence is grammatically sufficient for the experts, it is regarded by non-domain experts (which were mostly linguists, i.e., the worst-case scenario for such an evaluation) as being grammatically passably good but still worthy of improvement. The completeness rating achieves a score of 4.31 on average (see Fig. FIGREF39). This was to be expected as we introduced a rule to shorten the description of resources that contain more than 5 triples which share a common subject and predicate. Finally, we measured how well the users and experts were able to understand the meaning of the text generated by our approach.", "OWL 2 ontologies consist of Entities, Expressions and Axioms as introduced in subsec:owl. While both expressions and axioms can be mapped to RDF, i.e. into a set of RDF triples, using this mapping and applying the triple-based verbalization on it would lead to a non-human understandable text in many cases. For example, the intersection of two classes :A and :B can be represented in RDF by the six triples", "The goal of LD2NL is to provide an integrated system which generates a complete and correct NL representation for the most common used SW modeling languages RDF and OWL, and SPARQL. In terms of the standard model of NL generation proposed by Reiter & Dale BIBREF19, our steps mainly play the role of the micro-planner, with focus on aggregation, lexicalization, referring expressions and linguistic realization. In the following, we present our approach to formalizing NL sentences for each of the supported languages."]}
{"question_id": "7de0b2df60d3161dd581ed7915837d460020bc11", "predicted_answer": "", "predicted_evidence": ["Table TABREF11 depicts the benchmarked micro-averaged precision of classification prediction of the articles in the Shinra Dataset. The results initially demonstrate that the dataset is not a super easy one as the Binary Logistic Regression model is not achieving very high accuracy scores. Besides, the lower scores for Japanese in comparison to the other languages is demonstrating the higher difficulty of classification of the larger number of classes for all the models.", "In all of our experiments, we have used Adam optimizer BIBREF16 with a learning rate of $1e-3$ and have performed gradient clipping BIBREF17 of 5.0. We have initialized all of the network parameters with random values between $(-0.1, 0.1)$. We have done training on mini-batches of size 32, and to have a fair comparison, all the experiments have been conducted with 30,000 steps (batches) of randomly shuffled training instances to train the model parameters. The hidden layer size of all the models in each layer has also been set to 384.", "The evaluation measure would then be the micro-averaged precision BIBREF14 of the predicted labels. In addition, to prevent the domination of more frequent classes on the training procedure, we suggest weighted gradient back-propagation. The back-propagation weight of each article would be calculated using $w = \\frac{N}{\\sum _{n=1}^{N}{f(l_n)}}$ where $N$ is the number of labels assigned to the article (with a maximum of 6) and $f(l_n)$ counts the total train-set articles to which label $l_n$ has been assigned. The loss function used for training all the models has been Binary Cross Entropy Loss averaged over all the possible classes.", "We have performed the evaluation in a 10 fold cross validation manner in each fold of which 80% of the data has been used for training, 10% for validation and model selection, and 10% for testing. In addition, classes with a frequency less than 20 in the dataset have been ignored in the train/test procedure.", "Last but not least, the overall precision scores depict that the currently available models struggle with larger more complex annotated sets of Wikipedia articles."]}
{"question_id": "0a3a7e412682ce951329c37b06343d2114acad9d", "predicted_answer": "", "predicted_evidence": ["Table TABREF11 depicts the benchmarked micro-averaged precision of classification prediction of the articles in the Shinra Dataset. The results initially demonstrate that the dataset is not a super easy one as the Binary Logistic Regression model is not achieving very high accuracy scores. Besides, the lower scores for Japanese in comparison to the other languages is demonstrating the higher difficulty of classification of the larger number of classes for all the models.", "We implemented all the models suggested in \u00a7SECREF3 using PyTorch framework. For part-of-speech tagging the title and first sentences of the articles mentioned in the feature selection schema (Figure FIGREF7) and also normalization and tokenization of the articles, we used Hazm Toolkit for Farsi, Mecab Toolkit BIBREF15 for Japanese, and TreeTagger Toolkit for English, French, and German.", "Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy BIBREF8, containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set.", "In the collection of the dataset articles, we targeted only Japanese Wikipedia articles, since our annotators were fluent Japanese speakers. The articles were selected from Japanese Wikipedia with the condition of being hyperlinked at least 100 times from other articles in Wikipedia. We also considered the Goodness scoring measures mentioned in BIBREF9 to remove some of the unuseful articles. The collected dataset contained 120,333 Japanese Wikipedia articles in different areas, covering 141 out of 200 ENE labels.", "Figure FIGREF7 summarizes the final unified schema for categorization of the Wikipedia articles in Shinra Dataset."]}
{"question_id": "74cc0300e22f60232812019011a09df92bbec803", "predicted_answer": "", "predicted_evidence": ["However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language. There has been substantial work on sentiment and opinion analysis (e.g., BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 , BIBREF39 , BIBREF40 ) and recognition of specific emotions in text BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , which could be incorporated in future extensions of our work. We also hope to examine more closely the relationship of this work to previous work aimed at the identification of nasty vs. nice arguments in the IAC BIBREF45 , BIBREF8 .", "For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data.", "Human lives are being lived online in transformative ways: people can now ask questions, solve problems, share opinions, or discuss current events with anyone they want, at any time, in any location, on any topic. The purposes of these exchanges are varied, but a significant fraction of them are argumentative, ranging from hot-button political controversies (e.g., national health care) to religious interpretation (e.g., Biblical exegesis). And while the study of the structure of arguments has a long lineage in psychology BIBREF0 and rhetoric BIBREF1 , large shared corpora of natural informal argumentative dialogues have only recently become available.", "Table TABREF20 shows examples of learned NP Prep patterns with the preposition \"of\" in the fact class and \"for\" in the feel class. The \"of\" preposition in the factual arguments often attaches to objective terminology. The \"for\" preposition in the feeling-based arguments is commonly used to express advocacy (e.g., demand for) or refer to affected population groups (e.g., treatment for). Interestingly, these phrases are subtle indicators of feeling-based arguments rather than explicit expressions of emotion or sentiment.", "factual responses may try to bolster their argument by providing statistics related to a position, giving historical or scientific background, or presenting specific examples or data. There is clearly a relationship between a proposition being factual versus objective or veridical, although each of these different labelling tasks may elicit differences from annotators BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 ."]}
{"question_id": "865811dcf63a1dd3f22c62ec39ffbca4b182de31", "predicted_answer": "", "predicted_evidence": ["Section SECREF2 describes the manual annotations for factual and feeling in the IAC corpus. Section SECREF5 then describes how we generate lexico-syntactic patterns that occur in both types of argument styles. We use a weakly supervised pattern learner in a bootstrapping framework to automatically generate lexico-syntactic patterns from both annotated and unannotated debate posts. Section SECREF3 evaluates the precision and recall of the factual and feeling patterns learned from the annotated texts and after bootstrapping on the unannotated texts. We also present results for a supervised learner with bag-of-word features to assess the difficulty of this task. Finally, Section SECREF4 presents analyses of the linguistic expressions found by the pattern learner and presents several observations about the different types of linguistic structures found in factual and feeling based argument styles. Section SECREF5 discusses related research, and Section SECREF6 sums up and proposes possible avenues for future work.", "The high-precision patterns are then used in the bootstrapping framework to identify more factual and feeling texts from the 11,561 unannotated posts, also from 4forums.com. For each round of bootstrapping, the current set of factual and feeling patterns are matched against the unannotated texts, and posts that match at least 3 patterns associated with a given class are assigned to that class. As shown in Figure FIGREF10 , the Bootstrapped Data Balancer then randomly selects a balanced subset of the newly classified posts to maintain the same proportion of factual vs. feeling documents throughout the bootstrapping process. These new documents are added to the set of labeled documents, and the bootstrapping process repeats. We use the same threshold values to select new high-precision patterns for all iterations.", "For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data.", "Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label.", "The feeling responses may seem to lack argumentative merit, but previous work on argumentation describes situations in which such arguments can be effective, such as the use of emotive arguments to draw attention away from the facts, or to frame a discussion in a particular way BIBREF18 , BIBREF19 . Furthermore, work on persuasion suggest that feeling based arguments can be more persuasive in particular circumstances, such as when the hearer shares a basis for social identity with the source (speaker) BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . However none of this work has documented the linguistic patterns that characterize the differences in these argument types, which is a necessary first step to their automatic recognition or classification. Thus the goal of this paper is to use computational methods for pattern-learning on conversational arguments to catalog linguistic expressions and stylistic properties that distinguish Factual from Emotional arguments in these on-line debate forums."]}
{"question_id": "9e378361b6462034aaf752adf04595ef56370b86", "predicted_answer": "", "predicted_evidence": ["Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label.", "feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts.", "The IAC corpus is a freely available annotated collection of 109,553 forum posts (11,216 discussion threads). In such forums, conversations are started by posting a topic or a question in a particular category, such as society, politics, or religion BIBREF6 . Forum participants can then post their opinions, choosing whether to respond directly to a previous post or to the top level topic (start a new thread). These discussions are essentially dialogic; however the affordances of the forum such as asynchrony, and the ability to start a new thread rather than continue an existing one, leads to dialogic structures that are different than other multiparty informal conversations BIBREF25 . An additional source of dialogic structure in these discussions, above and beyond the thread structure, is the use of the quote mechanism, which is an interface feature that allows participants to optionally break down a previous post into the components of its argument and respond to each component in turn.", "Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .", "Natural informal dialogues exhibit a much broader range of argumentative styles than found in traditional work on argumentation BIBREF2 , BIBREF0 , BIBREF3 , BIBREF4 . Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification BIBREF5 , BIBREF6 , argument summarization BIBREF7 , sarcasm detection BIBREF8 , and work on the detailed structure of arguments BIBREF9 , BIBREF10 , BIBREF11 . Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents BIBREF12 , and in systems that support human argumentative behavior BIBREF13 ."]}
{"question_id": "667dce60255d8ab959869eaf8671312df8c0004b", "predicted_answer": "", "predicted_evidence": ["Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process.", "In this paper, we use observed differences in argumentation styles in online debate forums to extract patterns that are highly correlated with factual and emotional argumentation. From an annotated set of forum post responses, we are able extract high-precision patterns that are associated with the argumentation style classes, and we are then able to use these patterns to get a larger set of indicative patterns using a bootstrapping methodology on a set of unannotated posts.", "To learn patterns from texts labeled as fact or feeling arguments, we use the AutoSlog-TS BIBREF27 extraction pattern learner, which is freely available for research. AutoSlog-TS is a weakly supervised pattern learner that requires training data consisting of documents that have been labeled with respect to different categories. For our purposes, we provide AutoSlog-TS with responses that have been labeled as either fact or feeling.", "We evaluate the effectiveness of the learned patterns by applying them to the test set of 586 posts (347 fact and 239 feeling posts, maintaining the original ratio of fact to feel data in train). We classify each post as factual or feeling using the same procedure as during bootstrapping: a post is labeled as factual or feeling if it matches at least three high-precision patterns for that category. If a document contains three patterns for both categories, then we leave it unlabeled. We ran the bootstrapping algorithm for four iterations.", "Table TABREF11 shows that recall increases after each bootstrapping iteration, demonstrating that the patterns learned from the unannotated texts yield substantial gains in coverage over those learned only from the annotated texts. Recall increases from 22.8% to 40.9% for fact, and from 8.0% to 18.8% for feel. The precision for the factual class is reasonably good, but the precision for the feeling class is only moderate. However, although precision typically decreases during boostrapping due to the addition of imperfectly labeled data, the precision drop during bootstrapping is relatively small."]}
{"question_id": "d5e716c1386b6485e63075e980f80d44564d0aa2", "predicted_answer": "", "predicted_evidence": ["In this paper, we use observed differences in argumentation styles in online debate forums to extract patterns that are highly correlated with factual and emotional argumentation. From an annotated set of forum post responses, we are able extract high-precision patterns that are associated with the argumentation style classes, and we are then able to use these patterns to get a larger set of indicative patterns using a bootstrapping methodology on a set of unannotated posts.", "Other types of language data also typically contains a mixture of subjective and objective sentences, e.g. Wiebe et al. wiebeetal2001a,wiebeetalcl04 found that 44% of sentences in a news corpus were subjective. Our work is also related to research on distinguishing subjective and objective text BIBREF33 , BIBREF34 , BIBREF14 , including bootstrapped pattern learning for subjective/objective sentence classification BIBREF15 . However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language.", "Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .", "The IAC corpus is a freely available annotated collection of 109,553 forum posts (11,216 discussion threads). In such forums, conversations are started by posting a topic or a question in a particular category, such as society, politics, or religion BIBREF6 . Forum participants can then post their opinions, choosing whether to respond directly to a previous post or to the top level topic (start a new thread). These discussions are essentially dialogic; however the affordances of the forum such as asynchrony, and the ability to start a new thread rather than continue an existing one, leads to dialogic structures that are different than other multiparty informal conversations BIBREF25 . An additional source of dialogic structure in these discussions, above and beyond the thread structure, is the use of the quote mechanism, which is an interface feature that allows participants to optionally break down a previous post into the components of its argument and respond to each component in turn.", "This work was funded by NSF Grant IIS-1302668-002 under the Robust Intelligence Program. The collection and annotation of the IAC corpus was supported by an award from NPS-BAA-03 to UCSC and an IARPA Grant under the Social Constructs in Language Program to UCSC by subcontract from the University of Maryland."]}
{"question_id": "1fd31fdfff93d65f36e93f6919f6976f5f172197", "predicted_answer": "", "predicted_evidence": ["Other types of language data also typically contains a mixture of subjective and objective sentences, e.g. Wiebe et al. wiebeetal2001a,wiebeetalcl04 found that 44% of sentences in a news corpus were subjective. Our work is also related to research on distinguishing subjective and objective text BIBREF33 , BIBREF34 , BIBREF14 , including bootstrapped pattern learning for subjective/objective sentence classification BIBREF15 . However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language.", "Initially, we give the labeled training data to AutoSlog-TS, which generates patterns and associated statistics. The next step identifies high-precision patterns that can be used to label some of the unannotated texts as factual or feeling. We define two thresholds: INLINEFORM0 to represent a minimum frequency value, and INLINEFORM1 to represent a minimum probability value. We found that using only a small set of patterns (when INLINEFORM2 is set to a high value) achieves extremely high precision, yet results in a very low recall. Instead, we adopt a strategy of setting a moderate probability threshold to identify reasonably reliable patterns, but labeling a text as factual or feeling only if it contains at least a certain number different patterns for that category, INLINEFORM3 .", "In this paper, we use observed differences in argumentation styles in online debate forums to extract patterns that are highly correlated with factual and emotional argumentation. From an annotated set of forum post responses, we are able extract high-precision patterns that are associated with the argumentation style classes, and we are then able to use these patterns to get a larger set of indicative patterns using a bootstrapping methodology on a set of unannotated posts.", "This work was funded by NSF Grant IIS-1302668-002 under the Robust Intelligence Program. The collection and annotation of the IAC corpus was supported by an award from NPS-BAA-03 to UCSC and an IARPA Grant under the Social Constructs in Language Program to UCSC by subcontract from the University of Maryland.", "Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process."]}
{"question_id": "d2d9c7177728987d9e8b0c44549bbe03c8c00ef2", "predicted_answer": "", "predicted_evidence": ["Discoverability: The resources for African languages that do exist are hard to find. Often one needs to be associated with a specific academic institution in a specific country to gain access to the language data available for that country. This reduces the ability of countries and institutions to combine their knowledge and datasets to achieve better performance and innovations. Often the existing research itself is hard to discover since they are often published in smaller African conferences or journals, which are not electronically available nor indexed by research tools such as Google Scholar.", "We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work.", "In general, the Transformer model outperformed the ConvS2S model for all of the languages, sometimes achieving 10 BLEU points or more over the ConvS2S models. The results also show that the translations using BPE tokenisation outperformed translations using standard word-based tokenisation. The relative performance of Transformer to ConvS2S models agrees with what has been seen in existing NMT literature BIBREF20 . This is also the case when using BPE tokenisation as compared to standard word-based tokenisation techniques BIBREF21 .", "This section details published research for machine translation for the South African languages. The existing research is technically incomparable to results published in this paper, because their datasets (in particular their test sets) are not published. Table TABREF1 shows the BLEU scores provided by the existing work.", "Low availability of resources for African languages hinders the ability for researchers to do machine translation. Institutes such as the South African Centre for Digital Language Resources (SADiLaR) are attempting to change that by providing an open platform for technologies and resources for South African languages BIBREF7 . This, however, only addresses the 11 official languages of South Africa and not the greater problems within Africa."]}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "predicted_answer": "", "predicted_evidence": ["Africa has over 2000 languages across the continent BIBREF0 . South Africa itself has 11 official languages. Unlike many major Western languages, the multitude of African languages are very low-resourced and the few resources that exist are often scattered and difficult to obtain.", "Low availability of resources for African languages hinders the ability for researchers to do machine translation. Institutes such as the South African Centre for Digital Language Resources (SADiLaR) are attempting to change that by providing an open platform for technologies and resources for South African languages BIBREF7 . This, however, only addresses the 11 official languages of South Africa and not the greater problems within Africa.", "Machine translation of African languages would not only enable the preservation of such languages, but also empower African citizens to contribute to and learn from global scientific, social and educational conversations, which are currently predominantly English-based BIBREF1 . Tools, such as Google Translate BIBREF2 , support a subset of the official South African languages, namely English, Afrikaans, isiZulu, isiXhosa and Southern Sotho, but do not translate the remaining six official languages.", "This paper aims to address some of the above problems as follows: We trained models to translate English to Afrikaans, isiZulu, N. Sotho, Setswana and Xitsonga, using modern NMT techniques. We have published the code, datasets and results for the above experiments on GitHub, and in doing so promote reproducibility, ensure discoverability and create a baseline leader board for the five languages, to begin to address the lack of benchmarks.", "The difficulties hindering the progress of machine translation of African languages are discussed below."]}
{"question_id": "175cddfd0bcd77b7327b62f99e57d8ea93f8d8ba", "predicted_answer": "", "predicted_evidence": ["Unfortunately, in addition to being low-resourced, progress in machine translation of African languages has suffered a number of problems. This paper discusses the problems and reviews existing machine translation research for African languages which demonstrate those problems. To try to solve the highlighted problems, we train models to perform machine translation of English to Afrikaans, isiZulu, Northern Sotho (N. Sotho), Setswana and Xitsonga, using state-of-the-art neural machine translation (NMT) architectures, namely, the Convolutional Sequence-to-Sequence (ConvS2S) and Transformer architectures.", "Machine translation of African languages would not only enable the preservation of such languages, but also empower African citizens to contribute to and learn from global scientific, social and educational conversations, which are currently predominantly English-based BIBREF1 . Tools, such as Google Translate BIBREF2 , support a subset of the official South African languages, namely English, Afrikaans, isiZulu, isiXhosa and Southern Sotho, but do not translate the remaining six official languages.", "Africa has over 2000 languages across the continent BIBREF0 . South Africa itself has 11 official languages. Unlike many major Western languages, the multitude of African languages are very low-resourced and the few resources that exist are often scattered and difficult to obtain.", "Table TABREF23 shows that the ConvS2S model translated the sentence very successfully. The word \u201ckhumo\u201d directly means \u201cwealth\u201d or \u201criches\u201d. A better synonym would be \u201cletseno\u201d, meaning income or \u201cletlotlo\u201d which means monetary assets. The Transformer model only had a single misused word (translated \u201cshortage\u201d into \u201cnecessity\u201d), but otherwise translated successfully. The attention map visualization in Figure FIGREF18 suggests that the attention mechanism has learnt that the sentence structure of Setswana is the same as English.", "Focus: According to BIBREF8 , African society does not see hope for indigenous languages to be accepted as a more primary mode for communication. As a result, there are few efforts to fund and focus on translation of these languages, despite their potential impact."]}
{"question_id": "f0afc116809b70528226d37190e8e79e1e9cd11e", "predicted_answer": "", "predicted_evidence": ["Reproducibility: The data and code of existing research are rarely shared, which means researchers cannot reproduce the results properly. Examples of papers that do not publicly provide their data and code are described in Section SECREF4 .", "In general, the Transformer model outperformed the ConvS2S model for all of the languages, sometimes achieving 10 BLEU points or more over the ConvS2S models. The results also show that the translations using BPE tokenisation outperformed translations using standard word-based tokenisation. The relative performance of Transformer to ConvS2S models agrees with what has been seen in existing NMT literature BIBREF20 . This is also the case when using BPE tokenisation as compared to standard word-based tokenisation techniques BIBREF21 .", "The source code and the data used are available at https://github.com/LauraMartinus/ukuxhumana.", "BIBREF6 used unsupervised word segmentation with phrase-based statistical machine translation models. These models translate from English to Afrikaans, N. Sotho, Xitsonga and isiZulu. The parallel corpora were created by crawling online sources and official government data and aligning these sentences using the HunAlign software package. Large monolingual datasets were also used.", "The Tensor2Tensor implementation of Transformer was used BIBREF18 . The models were trained on a Google TPU, using Tensor2Tensor's recommended parameters for training, namely, a batch size of 2048, an Adafactor optimizer with learning rate warm-up of 10K steps, and a max sequence length of 64. The model was trained for 125K steps. Each dataset was encoded using the Tensor2Tensor data generation algorithm which invertibly encodes a native string as a sequence of subtokens, using WordPiece, an algorithm similar to BPE BIBREF19 . Beam search was used to decode the test data, with a beam width of 4."]}
{"question_id": "3588988f2230f3329d7523fbb881b20bf177280d", "predicted_answer": "", "predicted_evidence": ["ilpnlgextend: Nephropathia epidemica results in infections. It often originates from bank voles from the puumala virus.", "Similarly to Section \"Experiments with the Disease Ontology\" , $W_{max}$ was set to the number of words of the longest aggregated sentence in the experiments of our previous work BIBREF10 , where pipeline was allowed to combine up to three simple (expressing one fact each) sentences to form an aggregated one. In ilpnlgextend, we used different values for $\\lambda _1$ ( $\\lambda _2 = 1 - \\lambda _1$ ), setting $m = 3$ , as in Section \"Experiments with the Wine Ontology\" . In pipeline and pipelineshort*, we used $M = 2, 3, \\dots , 7$ . For each $M$ value, the texts of pipeline for the 76 individuals and classes were generated 10 times (not 3, unlike all the previous experiments with pipeline); each time, we used one of the different alternative sentence plans for each relation and one of the different alternative nl names for the individual or class the text was being generated for, since pipeline cannot select among alternative nl names (and sentence plans) by itself.", "We then generated texts for the 200 classes again, this time with pipeline, pipelineshort* (both with $M = 2, 3, \\dots , 7$ , $W_{max} = 54$ ) and ilpnlgextend ( $m$ = 4, $W_{max} = 54$ ); we modified pipeline and pipelineshort to count words (instead of elements) when comparing against ilpnlgextend, which is why we report $W_{max}$ in all three systems. Similarly to how $B_{max}$ was previously selected, $W_{max}=54$ was the number of words of the longest aggregated sentence in the experiments of Evaggelakaki Evaggelakaki2014. Figures 12 and 13 show the new facts per word ratios, for the texts of the 200 classes. In Figure 12 , for $\\lambda _1 \\le 0.06$ , ilpnlgextend produces empty texts, because it focuses on minimizing the lengths of the texts.", "In Fig. 14 , for $\\lambda _1 < 0.04$ , ilpnlgextend produces empty texts, because it focuses on minimizing the length of each text. For $\\lambda _1 \\ge 0.08$ , it performs clearly better than the other systems. For $\\lambda _1 = 0.12$ , it obtains the highest facts per word ratio by selecting the facts and sentence plans that lead to the shortest (in words) aggregated sentences, and nl names that indirectly express facts (not requiring separate sentences). For greater values of $\\lambda _1$ , ilpnlgextend selects additional facts whose sentence plans do not aggregate that well or that cannot be indirectly expressed via nl names, which is why the ratio of ilpnlgextend declines. We note that the highest average facts per word ratio of ilpnlgapprox (0.37, for $\\lambda _1 = 0.12$ ) of Fig. 14 is higher than the highest average ratio (0.33, for $\\lambda _1 = 0.3$ ) we had obtained in Section \"Experiments with the Wine Ontology\" with ilpnlg (Fig.", "We generated texts with ilpnlg, pipeline, and pipelineshort, for $M = 2, 3, 4, \\dots , 7$ in the two pipeline systems, and different values of $\\lambda _1$ ( $\\lambda _2 = 1 - \\lambda _1$ ) in ilpnlg. All three systems were allowed to form aggregated sentences with up to $B_{max} = 30$ distinct elements; this was the number of distinct elements of the longest aggregated sentence in the experiments of Evaggelakaki Evaggelakaki2014, where pipeline was allowed to combine up to three simple (expressing one fact each) sentences to form an aggregated one. There are 3.7 available facts ( $|F|$ ) on average and a maximum of 7 facts for each one of the 200 classes. In ilpnlg, we set $m = 4$ , which was the maximum number of (aggregated) sentences in the texts of pipeline and pipelineshort."]}
{"question_id": "78f8dad0f1acf024f69b3218b2d204b8019bb0d2", "predicted_answer": "", "predicted_evidence": ["sorry. what should i have said instead? H: no but i have kids B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: what do you want for christmas? B: i love to bake cakes H: what? (reward:1) B: oops! sorry. what should i have said instead? H: i want a pony for christmas B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: popsicle, lawnmower, mom. H: hi, are you having a good day so far? B: i'm here . i'm doing well , just waiting for the hurricane to hit . H: oh im sorry. where do you live? (reward:5) B: miami . . .", "(reward:1) B: oops! sorry. what should i have said instead? H: i want a pony for christmas B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: popsicle, lawnmower, mom. H: hi, are you having a good day so far? B: i'm here . i'm doing well , just waiting for the hurricane to hit . H: oh im sorry. where do you live? (reward:5) B: miami . . . i live in little cuba , but i wish i lived in europe . H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead?", "(reward:5) B: miami . . . i live in little cuba , but i wish i lived in europe . H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car? B: i only drive toyota cars H: what's the fastest you've ever driven a car? (reward:2) B: a yellow toyota camry convertible . H: that doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: how fast you have driven before B: thanks!", "if you need ideas for a topic, here are a few random words: popsicle, lawnmower, mom. H: hi, are you having a good day so far? B: i'm here . i'm doing well , just waiting for the hurricane to hit . H: oh im sorry. where do you live? (reward:5) B: miami . . . i live in little cuba , but i wish i lived in europe . H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car?", "can you pick a new topic for us to talk about now? H: what music do you listen to? B: listen about lectures about space and stuff H: so you don't like to listen to music at all? (reward:4) B: i mostly listen to jazz H: jazz is a fun genre. i play the trombone (reward:5) B: classical jazz is my very favorite music H: do you play any instruments? (reward:2) B: classical music . mozart piano music . H: i did not ask that (reward:1) B: oops! sorry. what should i have said instead? H: the instrument that you play B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: what are your plans today? B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: clown, drip, whatever."]}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "predicted_answer": "", "predicted_evidence": ["H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car? B: i only drive toyota cars H: what's the fastest you've ever driven a car? (reward:2) B: a yellow toyota camry convertible . H: that doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: how fast you have driven before B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you like staying up late?", "who do you like to listen to? B: i mostly listen to smooth jazz and pop music H: oh smooth jazz eh? do you like kenny g? (reward:5) B: i like pop jazz and rap H: you already said that! (reward:1) B: oops! sorry. what should i have said instead? H: you could say \"kenny g is a little too smooth for my tastes.\" B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: i think i'm going to just eat cereal with breakfast. what did you have to eat? B: bacon bacon bacon bacon bacon bacon bacon H: haha bacon is pretty good. i like the maple smoked stuff. (reward:3) B: onion green peppers mushrooms cream cheese shredded cheese . H: that sounds like an awesome bagel. how often do you eat breakfast?", "where do you live? (reward:5) B: miami . . . i live in little cuba , but i wish i lived in europe . H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car? B: i only drive toyota cars H: what's the fastest you've ever driven a car? (reward:2) B: a yellow toyota camry convertible . H: that doesnt make sense (reward:1) B: oops! sorry. what should i have said instead?", "sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car? B: i only drive toyota cars H: what's the fastest you've ever driven a car? (reward:2) B: a yellow toyota camry convertible . H: that doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: how fast you have driven before B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you like staying up late? B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: stout, mop, through.", "H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car? B: i only drive toyota cars H: what's the fastest you've ever driven a car? (reward:2) B: a yellow toyota camry convertible . H: that doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: how fast you have driven before B: thanks! i'll remember that. can you pick a new topic for us to talk about now?"]}
{"question_id": "1128a600a813116cba9a2cf99d8568ae340f327a", "predicted_answer": "", "predicted_evidence": ["For single-task learning, we compare our Meta-LSTMs with three models.", "where $\\hat{\\textbf {h}}^{(s)}_t$ and $\\textbf {h}^{(k)}_t$ are the hidden states of the shared meta LSTM and the $k$ -th task-specific basic LSTM respectively; $\\theta ^{(s)}_m$ and $\\theta ^{(k)}_b$ denote their parameters. The superscript $(s)$ indicates the parameters or variables are shared across the different tasks.", "LSTM: the standard LSTM with one hidden layer;", "Update the parameters for this task by taking a gradient step with respect to this mini-batch.", "For multi-task learning, we compare our Meta-LSTMs with the generic shared-private sharing scheme."]}
{"question_id": "d64fa192a7e9918c6a22d819abad581af0644c7d", "predicted_answer": "", "predicted_evidence": ["For multi-task learning, we can assign a basic network to each task, while sharing a meta network among tasks. The meta network captures the meta (shared) knowledge of different tasks. The meta network can learn at the \u201cmeta-level\u201d of predicting parameters for the basic task-specific network.", "LSTM: the standard LSTM with one hidden layer;", "SSP-MTL: Stacked shared-private sharing scheme, introduced in Section 2.", "In this paper, we introduce a novel knowledge sharing scheme for multi-task learning. The difference from the previous models is the mechanisms of sharing information among several tasks. We design a meta network to store the knowledge shared by several related tasks. With the help of the meta network, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks. Experimental results show that our model can improve the performances of several related tasks by exploring common features and outperforms the representational sharing scheme. The knowledge captured by the meta network can be transferred across other new tasks.", "In this paper, inspired by recent work on dynamic parameter generation BIBREF15 , BIBREF16 , BIBREF17 , we propose a function-level sharing scheme for multi-task learning, in which a shared meta-network is used to learn the meta-knowledge of semantic composition among the different tasks. The task-specific semantic composition function is generated by the meta-network. Then the task-specific composition function is used to obtain the task-specific representation of a text sequence. The difference between two sharing schemes is shown in Figure 1 . Specifically, we use two LSTMs as meta and basic (task-specific) network respectively. The meta LSTM is shared for all the tasks. The parameters of the basic LSTM are generated based on the current context by the meta LSTM, therefore the composition function is not only task-specific but also position-specific. The whole network is differentiable with respect to the model parameters and can be trained end-to-end."]}
{"question_id": "788f70a39c87abf534f4a9ee519f6e5dbf2543c2", "predicted_answer": "", "predicted_evidence": ["The parameters $\\mathbf {W}(\\mathbf {z}_t)$ and $\\mathbf {b}(\\mathbf {z}_t)$ of the basic LSTM are computed by", "For multi-task learning, we compare our Meta-LSTMs with the generic shared-private sharing scheme.", "$$\\mathcal {D}_k = \\lbrace (X_i^{(k)},Y_i^{(k)})\\rbrace _{i=1}^{N_k},$$   (Eq. 6)", "As we see it, the matrices change obviously facing the emotional vocabulary like \u201dfriendly\u201d, \u201drefund\u201d, and slowly change to a normal state. They can also capture words that affect sentiments like \u201dnot\u201d. For this case, SSP-MTL give a wrong answer, it captures the emotion word\u201drefund\u201d, but it makes an error on pattern \u201dnot user friendly\u201d, we consider that it's because fixed matrices don't have satisfactorily ability to capture long patterns' emotions and information. Dynamic matrices generated by Meta-LSTM will make the layer more flexible.", "PSP-MTL: Parallel shared-private sharing scheme, using a fully-shared LSTM to extract features for all tasks and concatenate with the outputs from task-specific LSTM."]}
{"question_id": "3d1ad8a4aaa2653d0095bafba74738bd20795acf", "predicted_answer": "", "predicted_evidence": ["The dropout was successfully applied to language modeling by BIBREF16 who applied it only on fully connected layers. The then state-of-the-art results were explained with the fact that by using the dropout, much deeper neural networks can be constructed without danger of overfitting. Gal and Ghahramani BIBREF17 implemented the variational inference based dropout which can also regularize recurrent layers. Additionally, they provide a solution for dropout within word embeddings. The method mimics Bayesian inference by combining probabilistic parameter interpretation and deep RNNs. Authors introduce the idea of augmenting probabilistic RNN models with the prediction uncertainty estimation. Recent works further investigate how to estimate prediction uncertainty within different data frameworks using RNNs BIBREF18. Some of the first investigation of probabilistic properties of SVM prediction is described in the work of Platt BIBREF19. Also, investigation how Bayes by Backprop (BBB) method can be applied to RNNs was done by BIBREF20.", "Projection of the vector of probability estimates into a two dimensional vector space.", "As the MCD neural network produces hundreds of probability samples for each target instance, it is not feasible to directly visualize such a multi-dimensional space. To solve this, we leverage the recently introduced UMAP algorithm BIBREF27, which projects the input $d$ dimensional data into a $s$-dimensional (in our case $s=2$) representation by using computational insights from the manifold theory. The result of this step is a two dimensional matrix, where each of the two dimensions represents a latent dimension into which the input samples were projected, and each row represents a text document.", "For example, the dropout in RNNs employed on a handwriting recognition task, disrupted the ability of recurrent layers to effectively model sequences BIBREF15. The dropout was successfully applied to language modeling by BIBREF16 who applied it only on fully connected layers. The then state-of-the-art results were explained with the fact that by using the dropout, much deeper neural networks can be constructed without danger of overfitting. Gal and Ghahramani BIBREF17 implemented the variational inference based dropout which can also regularize recurrent layers. Additionally, they provide a solution for dropout within word embeddings. The method mimics Bayesian inference by combining probabilistic parameter interpretation and deep RNNs. Authors introduce the idea of augmenting probabilistic RNN models with the prediction uncertainty estimation. Recent works further investigate how to estimate prediction uncertainty within different data frameworks using RNNs BIBREF18. Some of the first investigation of probabilistic properties of SVM prediction is described in the work of Platt BIBREF19.", "The obtained estimations are plotted alongside individual predictions and represent densities of the neural network's focus, which can be inspected from the point of view of correctness and reliability."]}
{"question_id": "ec54ae2f4811196fcaafa45e76130239e69995f9", "predicted_answer": "", "predicted_evidence": ["Our main contributions are:", "The areas which can significantly benefit from prediction uncertainty estimation are text classification tasks which trigger specific actions. Hate speech detection is an example of a task where reliable results are needed to remove harmful contents and possibly ban malicious users without preventing the freedom of speech. In order to assess the uncertainty of the predicted values, the neural networks require a Bayesian framework. On the other hand, Srivastava et al. BIBREF6 proposed a regularization approach, called dropout, which has a considerable impact on the generalization ability of neural networks. The approach drops some randomly selected nodes from the neural network during the training process. Dropout increases the robustness of networks and prevents overfitting. Different variants of dropout improved classification results in various areas BIBREF7. Gal and Ghahramani BIBREF8 exploited the interpretation of dropout as a Bayesian approximation and proposed a Monte Carlo dropout (MCD) approach to estimate the prediction uncertainty. In this paper, we analyze the applicability of Monte Carlo dropout in assessing the predictive uncertainty.", "The main reason is the ability of probabilistic neural networks to quantify trustworthiness of predicted results. This information can be important, especially in tasks were decision making plays an important role BIBREF5. The areas which can significantly benefit from prediction uncertainty estimation are text classification tasks which trigger specific actions. Hate speech detection is an example of a task where reliable results are needed to remove harmful contents and possibly ban malicious users without preventing the freedom of speech. In order to assess the uncertainty of the predicted values, the neural networks require a Bayesian framework. On the other hand, Srivastava et al. BIBREF6 proposed a regularization approach, called dropout, which has a considerable impact on the generalization ability of neural networks. The approach drops some randomly selected nodes from the neural network during the training process. Dropout increases the robustness of networks and prevents overfitting. Different variants of dropout improved classification results in various areas BIBREF7.", "implementation of hate speech detection with reliability output,", "The paper consists of six sections. In Section 2, we present related works on hate speech detection, prediction uncertainty assessment in text classification context, and visualization of uncertainty. In Section 3, we propose the methodology for uncertainty assessment using dropout within neural network models, as well as our novel visualization of prediction uncertainty. Section 4 presents the data sets and the experimental scenario. We discuss the obtained results in Section 5 and present conclusions and ideas for further work in Section 6."]}
{"question_id": "5102dc911913e9ca0311253e44fd31c73eed0a57", "predicted_answer": "", "predicted_evidence": ["data set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.", "For example, the dropout in RNNs employed on a handwriting recognition task, disrupted the ability of recurrent layers to effectively model sequences BIBREF15. The dropout was successfully applied to language modeling by BIBREF16 who applied it only on fully connected layers. The then state-of-the-art results were explained with the fact that by using the dropout, much deeper neural networks can be constructed without danger of overfitting. Gal and Ghahramani BIBREF17 implemented the variational inference based dropout which can also regularize recurrent layers. Additionally, they provide a solution for dropout within word embeddings. The method mimics Bayesian inference by combining probabilistic parameter interpretation and deep RNNs. Authors introduce the idea of augmenting probabilistic RNN models with the prediction uncertainty estimation. Recent works further investigate how to estimate prediction uncertainty within different data frameworks using RNNs BIBREF18. Some of the first investigation of probabilistic properties of SVM prediction is described in the work of Platt BIBREF19.", "The dropout technique was first introduced to RNNs in 2013 BIBREF14 but further research revealed negative impact of dropout in RNNs, especially within language modeling. For example, the dropout in RNNs employed on a handwriting recognition task, disrupted the ability of recurrent layers to effectively model sequences BIBREF15. The dropout was successfully applied to language modeling by BIBREF16 who applied it only on fully connected layers. The then state-of-the-art results were explained with the fact that by using the dropout, much deeper neural networks can be constructed without danger of overfitting. Gal and Ghahramani BIBREF17 implemented the variational inference based dropout which can also regularize recurrent layers. Additionally, they provide a solution for dropout within word embeddings. The method mimics Bayesian inference by combining probabilistic parameter interpretation and deep RNNs. Authors introduce the idea of augmenting probabilistic RNN models with the prediction uncertainty estimation.", "data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.", "Common typos are corrected and typical contractions and hash-tags are expanded."]}
{"question_id": "5752c8d333afc1e6c666b18d1477c8f669b7a602", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the predicted class distribution for INLINEFORM1 .", "Based on the LSTM implementation of BIBREF13 , we propose a generalized multi-task learning architecture for text classification with four types of recurrent neural layers to convey information inside and among tasks. Figure FIGREF21 illustrates the structure design and information flows of our model, where three tasks are jointly learned in parallel.", "Different tasks may differ in characteristics of the word sequences INLINEFORM0 or the labels INLINEFORM1 . We compare lots of benchmark tasks for text classification and conclude three different perspectives of multi-task learning.", "where INLINEFORM0 denotes the number of training samples and INLINEFORM1 is the class number.", "Multi-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 ."]}
{"question_id": "fcdafaea5b1c9edee305b81f6865efc8b8dc50d3", "predicted_answer": "", "predicted_evidence": ["In this section, we design three different scenarios of multi-task learning based on five benchmark datasets for text classification. we investigate the empirical performances of our model and compare it to existing state-of-the-art models.", "Similar to Coupling Layers, hidden states and memory cells of the Single Layers can selectively decide how much information to accept from the pair-wise Local Fusion Layers. We re-define Eqs.( EQREF29 ) by considering the interactions between the memory content INLINEFORM0 and outputs of the Local Fusion Layers as follows. DISPLAYFORM0", "We apply the optimal hyperparameter settings and compare our model against the following state-of-the-art models:", "Figure FIGREF45 shows the performances of datasets in Multi-Domain scenario with different INLINEFORM0 . Compared to INLINEFORM1 , our model can achieve considerable improvements when INLINEFORM2 as more samples combinations are available. However, there are no more salient gains as INLINEFORM3 gets larger and potential noises from other tasks may lead to performance degradations. For a trade-off between efficiency and effectiveness, we determine INLINEFORM4 as the optimal value for our experiments.", "We compare performances of our model with the implementation of BIBREF13 and the results are shown in Table TABREF43 . Our model obtains better performances in Multi-Domain scenario with an average improvement of 4.5%, where datasets are product reviews on different domains with similar sequence lengths and the same class number, thus producing stronger correlations. Multi-Cardinality scenario also achieves significant improvements of 2.77% on average, where datasets are movie reviews with different cardinalities."]}
{"question_id": "91d4fd5796c13005fe306bcd895caaed7fa77030", "predicted_answer": "", "predicted_evidence": ["BIBREF9 , BIBREF10 , BIBREF11 all belong to Type-II where samples from different tasks are learned sequentially. BIBREF9 applies bag-of-word representation and information of word orders are lost. BIBREF10 introduces an external memory for information sharing with a reading/writing mechanism for communicating, and BIBREF11 proposes three different models for multi-task learning with recurrent neural networks. However, models of these two papers only involve pair-wise interactions, which can be regarded as specific implementations of Coupling Layer and Fusion Layer in our model.", "As Table TABREF48 shows, our model obtains competitive or better performances on all tasks except for the QC dataset, as it contains poor correlations with other tasks. MT-RNN slightly outperforms our model on SST, as sentences from this dataset are much shorter than those from IMDB and MDSD, and another possible reason may be that our model are more complex and requires larger data for training. Our model proposes the designs of various interactions including coupling, local and global fusion, which can be further implemented by other state-of-the-art models and produce better performances.", "We apply the optimal hyperparameter settings and compare our model against the following state-of-the-art models:", "Most previous multi-task learning models BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 belongs to Type-I or Type-II. The total number of input samples is INLINEFORM0 , where INLINEFORM1 are the sample numbers of each task.", "Based on the LSTM implementation of BIBREF13 , we propose a generalized multi-task learning architecture for text classification with four types of recurrent neural layers to convey information inside and among tasks. Figure FIGREF21 illustrates the structure design and information flows of our model, where three tasks are jointly learned in parallel."]}
{"question_id": "27d7a30e42921e77cfffafac5cb0d16ce5a7df99", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 denotes the coupling term in Eqs.( EQREF29 ) and INLINEFORM1 represents the local fusion term. Again, we employ a gating mechanism INLINEFORM2 to control the portion of information flow from the Local Coupling Layers to INLINEFORM3 .", "One formidable constraint of deep neural networks (DNN) is their strong reliance on large amounts of annotated corpus due to substantial parameters to train. A DNN trained on limited data is prone to overfitting and incapable to generalize well. However, constructions of large-scale high-quality labeled datasets are extremely labor-intensive. To solve the problem, these models usually employ a pre-trained lookup table, also known as Word Embedding BIBREF6 , to map words into vectors with semantic implications. However, this method just introduces extra knowledge and does not directly optimize the targeted task. The problem of insufficient annotated resources is not solved either.", "Multi-task learning leverages potential correlations among related tasks to extract common features, increase corpus size implicitly and yield classification improvements. Inspired by BIBREF7 , there are a large literature dedicated for multi-task learning with neural network based models BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . These models basically share some lower layers to capture common features and further feed them to subsequent task-specific layers, which can be classified into three types:", "We calculate PPGs for every two tasks in Table TABREF35 and illustrate the results in Figure FIGREF47 , where darkness of colors indicate strength of correlation. It is intuitive that datasets of Multi-Domain scenario obtain relatively higher PPGs with each other as they share similar cardinalities and abundant low-level linguistic characteristics. Sentences of QC dataset are much shorter and convey unique characteristics from other tasks, thus resulting in quite lower PPGs.", "We further explore the influence of INLINEFORM0 in TOS on our model, which can be any positive integer. A higher value means larger and more various samples combinations, while requires higher computation costs."]}
{"question_id": "7561bd3b8ba7829b3a01ff07f9f3e93a7b8869cc", "predicted_answer": "", "predicted_evidence": ["Following wiki2018, a subset of the input has to be therefore first coarsely selected, using extractive summarization, before training an extractive or abstractive model that generates the Wikipedia gameplay text while conditioning on this extraction. Additionally, half of the summaries contain more than three hundred words (see Table TABREF11), which is larger than previous work.", "Removing tags: when a game has the same name than its franchise, its Wikipedia page has a title similar to Game (year video game) or Game (video game);", "In contrast, we propose a novel domain-specific dataset containing $14\\,652$ samples, based on professional video game reviews obtained via Metacritic and gameplay sections from Wikipedia. By using Metacritic reviews in addition to Wikipedia articles, we benefit from a number of factors. First, the set of aspects used to assess a game is limited and consequently, reviews share redundancy. Second, because they are written by professional journalists, reviews tend to be in-depth and of high-quality. Additionally, when a video game is released, journalists have an incentive to write a complete review and publish it online as soon as possible to draw the attention of potential customers and increase the revenue of their website BIBREF0. Therefore, several reviews for the same product become quickly available and the first version of the corresponding Wikipedia page is usually made available shortly after. Lastly, reviews and Wikipedia pages are available in multiple languages, which opens up the possibility for multilingual multi-document summarization.", "In this section, we introduce a new domain-specific corpus for the task of multi-document summarization, based on professional video game reviews and gameplay sections of Wikipedia.", "To our knowledge, wiki2018 is the only work that has proposed a large dataset for multi-document summarization. By considering Wikipedia entries as a collection of summaries on various topics given by their title (e.g., Machine Learning, Stephen King), they create a dataset of significant size, where the lead section of an article is defined as the reference summary and input documents are a mixture of pages obtained from the article's reference section and a search engine. While this approach benefits from the large number of Wikipedia articles, in many cases, articles contain only a few references that tend to be of the desired high quality, and most input documents end up being obtained via a search engine, which results in noisy data. Moreover, at testing time no references are provided, as they have to be provided by human contributors. wiki2018 showed that in this case, generated summaries based on search engine results alone are of poor quality and cannot be used."]}
{"question_id": "a3ba21341f0cb79d068d24de33b23c36fa646752", "predicted_answer": "", "predicted_evidence": ["Metacritic is a website aggregating music, game, TV series, and movie reviews. In our case, we only focus on the video game section and crawl different products with their associated links, pointing to professional reviews written by journalists. It is noteworthy that we consider reviews for the same game released on different platforms (e.g., Playstation, Xbox) separately. Indeed, the final product quality might differ due to hardware constraints and some websites are specialized toward a specific platform.", "To our knowledge, wiki2018 is the only work that has proposed a large dataset for multi-document summarization. By considering Wikipedia entries as a collection of summaries on various topics given by their title (e.g., Machine Learning, Stephen King), they create a dataset of significant size, where the lead section of an article is defined as the reference summary and input documents are a mixture of pages obtained from the article's reference section and a search engine. While this approach benefits from the large number of Wikipedia articles, in many cases, articles contain only a few references that tend to be of the desired high quality, and most input documents end up being obtained via a search engine, which results in noisy data. Moreover, at testing time no references are provided, as they have to be provided by human contributors. wiki2018 showed that in this case, generated summaries based on search engine results alone are of poor quality and cannot be used.", "With the growth of the internet in the last decades, users are faced with an increasing amount of information and have to find ways to summarize it. However, producing summaries in a multi-document setting is a challenging task; the language used to display the same information in a sentence can vary significantly, making it difficult for summarization models to capture. Thus large corpora are needed to develop efficient models. There exist two types of summarization: extractive and abstractive. Extractive summarization outputs summaries in two steps, namely via sentence ranking, where an importance score is assigned to each sentence, and via the subsequent sentence selection, where the most appropriate sentence is chosen. In abstractive summarization, summaries are generated word by word auto-regressively, using sequence-to-sequence or language models. Given the complexity of multi-document summarization and the lack of datasets, most researchers use extractive summarization and rely on hand-crafted features or additional annotated data, both needing human expertise.", "Journalists are paid to write complete reviews for various types of entertainment products, describing different aspects thoroughly. Reviewed aspects in video games include the gameplay, richness, and diversity of dialogues, or the soundtrack. Compared to usual reviews written by users, these are assumed to be of higher-quality and longer.", "Removing tags: when a game has the same name than its franchise, its Wikipedia page has a title similar to Game (year video game) or Game (video game);"]}
{"question_id": "96295e1fe8713417d2b4632438a95d23831fbbdc", "predicted_answer": "", "predicted_evidence": ["Metacritic is a website aggregating music, game, TV series, and movie reviews. In our case, we only focus on the video game section and crawl different products with their associated links, pointing to professional reviews written by journalists. It is noteworthy that we consider reviews for the same game released on different platforms (e.g., Playstation, Xbox) separately. Indeed, the final product quality might differ due to hardware constraints and some websites are specialized toward a specific platform.", "Figure FIGREF21 shows two samples with their gameplay sections from Wikipedia and summaries generated by the best baseline SemSentSum. In the first example, we notice that the model has selected sentences from the reviews that are also in the original Wikipedia page. Additionally, we observe, for both examples, that several text fragments describe the same content with different sentences. Consequently, this supports our hypothesis that professional reviews can be used in a multi-document summarization setting to produce summaries reflecting the gameplay section of Wikipedia pages.", "In this work, we introduce a new multi-document summarization dataset, GameWikiSum, based on professional video game reviews, which is one hundred times larger than commonly used datasets. We conclude that the size of GameWikiSum and its domain-specificity makes the training of abstractive and extractive models possible. In future work, we could increase the dataset with other languages and use it for multilingual multi-document summarization. We release GameWikiSum for further research: https://github.com/Diego999/GameWikiSum.", "Only the recent work of wiki2018 addresses the automatic creation of a large-scale multi-document summarization corpus, WikiSum. Summaries are lead sections of Wikipedia pages and input documents a mixture of 1) its citations from the reference section 2) results from search engines using the title of the Wikipedia page as the query. However, references (provided by contributors) are needed for their model to generate lead sections which are not garbaged texts, as shown in the experiments BIBREF3. Consequently, this dataset is unusable for real use-cases. Similarly, zopf-2018-auto propose a multilingual Multi-Document dataset of approximately $7\\,000$ examples based on English and German Wikipedia articles. We, however, are focused on the video game domain and provide twice more samples.", "In contrast, we propose a novel domain-specific dataset containing $14\\,652$ samples, based on professional video game reviews obtained via Metacritic and gameplay sections from Wikipedia. By using Metacritic reviews in addition to Wikipedia articles, we benefit from a number of factors. First, the set of aspects used to assess a game is limited and consequently, reviews share redundancy. Second, because they are written by professional journalists, reviews tend to be in-depth and of high-quality. Additionally, when a video game is released, journalists have an incentive to write a complete review and publish it online as soon as possible to draw the attention of potential customers and increase the revenue of their website BIBREF0. Therefore, several reviews for the same product become quickly available and the first version of the corresponding Wikipedia page is usually made available shortly after. Lastly, reviews and Wikipedia pages are available in multiple languages, which opens up the possibility for multilingual multi-document summarization."]}
{"question_id": "5bfbc9ca7fd41be9627f6ef587bb7e21c7983be0", "predicted_answer": "", "predicted_evidence": ["In order to match games with their respective Wikipedia pages, we use the game title as the query in the Wikipedia search engine and employ a set of heuristic rules.", "Journalists are paid to write complete reviews for various types of entertainment products, describing different aspects thoroughly. Reviewed aspects in video games include the gameplay, richness, and diversity of dialogues, or the soundtrack. Compared to usual reviews written by users, these are assumed to be of higher-quality and longer.", "In this section, we introduce a new domain-specific corpus for the task of multi-document summarization, based on professional video game reviews and gameplay sections of Wikipedia.", "Given a collection of professional reviews, manually creating a summary containing all key information is too costly at large scale as reviews tend to be long and thorough. To this end, we analyzed Wikipedia pages for various video games and observed that most contain a gameplay section, that is an important feature in video game reviews. Consequently, we opt for summaries describing only gameplay mechanics. Wikipedia pages are written following the Wikipedia Manual of Style and thus, guarantee summaries of a fairly uniform style. Additionally, we observed that the gameplay section often cites excerpts of professional reviews, which adds emphasis to the extractive nature of GameWikiSum.", "Table TABREF19 contains the results. LEAD-5 achieves less than 20 for ROUGE-L as well as ROUGE-1 and less than $3.5$ for ROUGE-2. Taking only 3 sentences leads to even worse results: below 13 and 3 respectively. Unlike in other datasets, these results are significantly outperformed by all other extractive models but surprisingly, abstractive models perform worse on average. This demonstrates the difficulty of the task in GameWikiSum compared to nallapati2016abstractive and graff2003."]}
{"question_id": "5181527e6a61a9a192db5f8064e56ec263c42661", "predicted_answer": "", "predicted_evidence": ["The Search QA system uses an internal knowledge base, which finely indexes data using Elasticsearch. It is powered by Wikidata and enriched by Wikipedia, especially to calculate a Page-Rank BIBREF10 on each entity. This QA system first determines the potential named entities in the question (i.e. subjects, predicates, and types of subjects). Second, it constructs a correlation matrix by looking for the triplets in Wikidata that link these entities. This matrix is filtered according the coverage of the question and the relevance of each entity in order to find the best answer.", "On the one hand, the system is able to answer complex out-of-context questions such as \u201cWhat are the capitals of the countries of the Iberian Peninsula?\", by correctly answering the list of capitals: \u201cAndorra la Vella, Gibraltar, Lisbon, Madrid\".", "The Speech Recognition component enables the translation of speech into text. Cobalt Speech Recognition for French is a Kaldi-based speech-to-text decoder using a TDNN BIBREF2 acoustic model; trained on more than 2 000 hours of clean and noisy speech, a 1.7-million-word lexicon, and a 5-gram language model trained on 3 billion words.", "The dependency tree and semantic frames provided by the linguistic module are used to solve ellipsis by taking into account the syntactic and semantic structure of the previous question. Once the question has been resolved, it calls the QA systems and passes their results to the generation module.", "Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French (Figure FIGREF20). The system reached a macro precision, recall and F-1 of $64.14\\%$, $64.33\\%$ and $63.46\\%$ respectively."]}
{"question_id": "334aa5540c207768931a0fe78aa4981a895ba37c", "predicted_answer": "", "predicted_evidence": ["In contrast to most conversational systems which support only speech, two input and output modalities are supported speech and text. Thus it is possible to let the user check the answers by either asking relevant Wikipedia excerpts or by navigating through the retrieved name entities or by exploring the answer details of the QA components: the confidence score as well as the set of explored triplets. Therefore, the user has the final word to consider the answer as correct or incorrect and to provide a reward, which can be used in the future for training reinforcement learning algorithms.", "The Speech Recognition component enables the translation of speech into text. Cobalt Speech Recognition for French is a Kaldi-based speech-to-text decoder using a TDNN BIBREF2 acoustic model; trained on more than 2 000 hours of clean and noisy speech, a 1.7-million-word lexicon, and a 5-gram language model trained on 3 billion words.", "The Search QA system uses an internal knowledge base, which finely indexes data using Elasticsearch. It is powered by Wikidata and enriched by Wikipedia, especially to calculate a Page-Rank BIBREF10 on each entity. This QA system first determines the potential named entities in the question (i.e. subjects, predicates, and types of subjects). Second, it constructs a correlation matrix by looking for the triplets in Wikidata that link these entities. This matrix is filtered according the coverage of the question and the relevance of each entity in order to find the best answer.", "The evaluation of the individual components of the proposed system was performed outside the scope of this work. We evaluated out-of-context questions, as well as the coreference resolution module.", "The transcribed utterance and the speaker information are passed to the dialogue system. This system contains an understanding component, a context manager, and a generation component (Figure FIGREF7)."]}
{"question_id": "b8bbdc3987bb456739544426c6037c78ede01b77", "predicted_answer": "", "predicted_evidence": ["We will soon integrate a state-of-the art reading comprehension approach, support English language and improve the coreference resolution module. We are also interested in exploring policy learning, thus the system will be able to find the best criterion to chose the answer or to ask for clarification in the case of ambiguity and uncertainty.", "On the other hand, consider the dialogue presented in Figure FIGREF23, in which the user asks several related questions about Michael Jackson. First she asks \u201cWho is Michael Jackson?\u201d and the system correctly answers \u201cMichael Jackson is an American author, composer, singer and dancer\u201d, note that this is the generated long answer.", "The generation component either returns the short answer provided by QA systems or relies on an external generation module that uses dependency grammar templates to generate more elaborated answers.", "On the one hand, the system is able to answer complex out-of-context questions such as \u201cWhat are the capitals of the countries of the Iberian Peninsula?\", by correctly answering the list of capitals: \u201cAndorra la Vella, Gibraltar, Lisbon, Madrid\".", "The dependency tree and semantic frames provided by the linguistic module are used to solve ellipsis by taking into account the syntactic and semantic structure of the previous question. Once the question has been resolved, it calls the QA systems and passes their results to the generation module."]}
{"question_id": "fea9b4d136156f23a88e5c7841874a467f2ba86d", "predicted_answer": "", "predicted_evidence": ["In section 5 of the paper, we study the importance of previous tokens by dropping the input tokens $w_{t-n}$ for the decoder, where $w_t$ is the current token in target sentence and $n \\in [1,t]$ is the distance to the token dropped. We drop the same number of tokens for each $n$. We also compare the autoregressive model with non-autoregressive counterpart by randomly dropping the tokens from the encoder input, encoder output and decoder input. The dropping operation is consistent with Section 4 of the paper.", "We vary the number of layers for the encoder and decoder respectively, to investigate which can achieve more gains with deeper layers. Deeper encoder (decoder) with more gains can indicate the task that encoder (decoder) handles needs more representation capability, which is harder to learn. We consider two different strategies when varying the number of layers: 1) increasing the number of layers for the encoder and decoder respectively based on a baseline model with a 2-layer encoder and a 2-layer decoder; 2) adjusting the number of layers for the encoder and decoder under the constraint that the total number of layers for encoder and decoder is fixed (totally 12 layers in our experiments).", "We further investigate the difficulty of the task for the encoder and decoder by comparing their convergence speed. Encoder (decoder) that needs more training time indicates the task that encoder (decoder) handles is more difficult.", "We evaluate the convergence speed for the encoder by initializing the model with the parameters of a well-trained decoder and fixing them during training, and initializing other components with random variables. We follow the opposite practice when evaluating the convergence speed for the decoder. We show the BLEU scores along the training process on all the four datasets in Figure FIGREF6, which demonstrates that the decoder converges faster than the encoder.", "BIBREF10 analyzed how NMT works based on the encoder-decoder framework and explained the translation errors with the proposed layer-wise relevance propagation. BIBREF14 focused on the representation of different layers of the encoder in NMT. BIBREF15 concluded that NMT systems are brittle. In the view of attention mechanism, BIBREF11 studied the encoder-decoder attention and provided detailed analysis of what is being learned by the attention mechanism in NMT model. BIBREF16 studied the effectiveness of hybrid attention in neural machine translation. BIBREF17 introduced dense connection in encoder-decoder attention. BIBREF13 analyzed the encoder-decoder attention mechanisms in the case of word sense disambiguation. For model architecture, BIBREF18 provided early study on RNN-based architectural hyperparameters. BIBREF19 proposed to pre-train the encoder-attention-decoder framework jointly. BIBREF12 conducted fine-grained analyses on various modules and made comparisons between RNN, CNN and self-attention."]}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "predicted_answer": "", "predicted_evidence": ["The decoder handles an easier task than the encoder. 1) We find that adding more layers to the encoder achieves larger improvements than adding more layers to the decoder. 2) We also compare the training time of the encoder and decoder by fixing the parameters of a well-trained decoder (encoder), and just update the parameters of the encoder (decoder). We found that the decoder converges faster than the encoder. These two results suggest that the decoder handles an easier task than the encoder in NMT.", "In this section, we mainly review the work on the analysis of encoder-decoder framework in the field of NMT. BIBREF10 analyzed how NMT works based on the encoder-decoder framework and explained the translation errors with the proposed layer-wise relevance propagation. BIBREF14 focused on the representation of different layers of the encoder in NMT. BIBREF15 concluded that NMT systems are brittle. In the view of attention mechanism, BIBREF11 studied the encoder-decoder attention and provided detailed analysis of what is being learned by the attention mechanism in NMT model. BIBREF16 studied the effectiveness of hybrid attention in neural machine translation. BIBREF17 introduced dense connection in encoder-decoder attention. BIBREF13 analyzed the encoder-decoder attention mechanisms in the case of word sense disambiguation. For model architecture, BIBREF18 provided early study on RNN-based architectural hyperparameters.", "We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. We find that the preceding tokens in the decoder provide strong conditional information, which partially explain the previous two observations on the decoder.", "We vary the number of layers for the encoder and decoder respectively, to investigate which can achieve more gains with deeper layers. Deeper encoder (decoder) with more gains can indicate the task that encoder (decoder) handles needs more representation capability, which is harder to learn. We consider two different strategies when varying the number of layers: 1) increasing the number of layers for the encoder and decoder respectively based on a baseline model with a 2-layer encoder and a 2-layer decoder; 2) adjusting the number of layers for the encoder and decoder under the constraint that the total number of layers for encoder and decoder is fixed (totally 12 layers in our experiments).", "In this section, we compare the characteristics between the encoder and decoder by analyzing their robustness according to the input noise in the inference phase. We simulate the input noise with three typical operations BIBREF20, BIBREF21: 1) random dropping: we randomly drop the input tokens of encoder and decoder respectively with different drop rates; 2) random noising: we randomly select tokens and replace its embedding with random noise; 3) random swapping: we randomly reverse the order for the adjacent tokens. The decoder in NMT model typically generates the current token one-by-one conditioned on the previous generated tokens, which suffers from error propagation BIBREF22: if a token is incorrectly predicted by the decoder, it will affect the prediction of the following tokens. Adding input noise to the decoder will further enhance the effect of error propagation, and thus influence our analysis. To eliminate the influence of error propagation, we apply teacher forcing BIBREF23 in the inference phase by feeding the previous ground-truth target tokens instead of the previously generated target tokens, following BIBREF24."]}
{"question_id": "7ef7a5867060f91eac8ad857c186e51b767c734b", "predicted_answer": "", "predicted_evidence": ["The consistent observations above suggest that the decoder is much more sensitive to the input noise than the encoder. Intuitively, the encoder aims at extracting abstract representations of the source sentence instead of depending on certain input tokens for prediction as the decoder does, demonstrating that the encoder is more robust than the decoder.", "Since an autoregressive decoding process highly depends on previous tokens, we further investigate the characteristics of non-autoregressive decoding in NMT BIBREF26, BIBREF27, where each target token is generated in parallel, without dependence on previous tokens. We compare the accuracy with different dropping rates for the encoder input, encoder output (the input of the encoder-decoder attention) and decoder input respectively, both for autoregressive NMT and non-autoregressive NMT, and illustrate the results in Figure FIGREF14 and FIGREF14. It can be observed that for autoregressive NMT, the model is more sensitive to the decoder input, and then the encoder input, and less sensitive to the encoder output.", "In Section 4, we do not train models from scratch and only evaluate well-trained models with different kind of perturbation to explore the robustness of the encoder and decoder.", "We train the Transformer BIBREF3 on IWSLT14 German$\\leftrightarrow $English (De$\\leftrightarrow $En) and Romanian$\\leftrightarrow $English (Ro$\\leftrightarrow $En) translation. The translation quality is measured by BLEU score. More details on datasets, model configurations and evaluation metrics can be found in the supplementary materials (Section 1.1).", "In this section, we give detailed descriptions on all the experimental settings in this work."]}
{"question_id": "0b10cfa61595b21bf3ff13b4df0fe1c17bbbf4e9", "predicted_answer": "", "predicted_evidence": ["As for STR-encoder, we only input different $MASK^{task}$ matrices, which calculate various representations of words required by different downstream task ($H_N^{task}$) with the same parameters:", "$N$ multi-head self-attention layers are applied to calculate the context-dependent representations of words (${H}_N$) based on the initial representations (${H}_0$). To solve the problems of gradient vanishing and exploding, ResNet architectureBIBREF33 is applied in the layer. In $N$ multi-head self-attention layers, every layer can produce the output (${H}_{m}$) given the previous output of $(m-1)$-th layer (${H}_{m-1}$):", "This structure has two advantages:", "In BERT, MASK matrix is originally used to mask the padding portion of the text. However, we found that by designing a specific MASK matrix, we can directly control the attention range of each word, thus obtaining specific context-sensitive representations. Specially, when calculating the attention (i.e., Equation (DISPLAY_FORM12)), the parameter matrix $MASK\\in {\\lbrace 0,1\\rbrace }^{T\\times T}$, where $T$ is the length of the sequence. If $MASK_{i,j} = 0$, then we have $(MASK_{i,j}-1)\\times \\infty = -\\infty $ and the Equation (DISPLAY_FORM15), which indicates that the $i$-th word ignores the $j$-th word when calculating attention.", "The remaining $K$ layers focus on words of downstream task by task-specific matrix $MASK^{task}$ based on dynamic range attention mechanism. Given the output ($H_{m-1}^{task}$) of previous $(m-1)$-th layer, the model calculate the current output ($H_m^{task}$) as:"]}
{"question_id": "67104a5111bf8ea626532581f20b33b851b5abc1", "predicted_answer": "", "predicted_evidence": ["However, this pipeline method usually fails to capture joint features between entity and relationship types. For example, for a valid relation \u201c\u5b58\u5728\u60c5\u51b5(presence)\u201d in Fig. FIGREF1, the types of its two relational entities must be \u201c\u75be\u75c5(disease)\u201d, \u201c\u75c7\u72b6(symptom)\u201d or \u201c\u5b58\u5728\u8bcd(presence word)\u201d. To capture these joint features, a large number of joint learning models have been proposed BIBREF3, BIBREF4, among which bidirectional long short term memory (Bi-LSTM) BIBREF5, BIBREF6 are commonly used as the shared parameter layer. However, compared with the language models that benefit from abundant knowledge from pre-training and strong feature extraction capability, Bi-LSTM model has relatively lower generalization performance.", "UTF8gkai With the widespread of electronic health records (EHRs) in recent years, a large number of EHRs can be integrated and shared in different medical environments, which further support the clinical decision making and government health policy formulationBIBREF0. However, most of the information in current medical records is stored in natural language texts, which makes data mining algorithms unable to process these data directly. To extract relational entity triples from the text, researchers generally use entity and relation extraction algorithm, and rely on the central word to convert the triples into key-value pairs, which can be processed by conventional data mining algorithms directly. Fig. FIGREF1 shows an example of entity and relation extraction in the text of EHRs.", "To further investigate the effectiveness of the proposed model on RC, we use two RC models in medical domain (i.e., RCN BIBREF27 and CNN BIBREF37) and one joint model in generic domain (i.e., Joint-Bi-LSTMBIBREF6) as baseline methods. Since RCN and CNN methods are only applied to RC tasks and cannot extract entities from the text, so we directly use the correct entities in the text to evaluate the RC models. Table TABREF45 illustrate that focused attention model achieves the best performance, and its precision, recall and F$_1$-score reach 96.06%, 96.83% and 96.44%, which beats the second model by 1.57%, 1.59% and 1.58%, respectively.", "The embedding layer is used to obtain the vector representations of all the words in the sequence, and it consists of three components: word embedding (${e}_{word}$), position embedding (${e}_{pos}$), and type embedding (${e}_{type}$). Specifically, word embeddings are obtained through the corresponding embedding matrices. Positional embedding is used to capture the order information of the sequence which is ignored during the self-attention process. Type embedding is used to distinguish two different sequences of the input. Given an input sequence (${S}$), the initial vector representations of all the words in the sequence (${H}_0$) are as follows:", "Given the aforementioned challenges and current researches, we propose a focused attention model based on widely known BERT language model BIBREF10 to jointly for NER and RC tasks. Specifically, through the dynamic range attention mechanism, we construct task-specific MASK matrix to control the attention range of the last $K$ layers in BERT language model, leading to the model focusing on the words of the task. This process helps obtain the corresponding task-specific context-dependent representations. In this way, the modified BERT language model can be used as the shared parameter layer in joint learning NER and RC task. We call the modified BERT language model shared task representation encoder (STR-encoder) in the following paper."]}
{"question_id": "1d40d177c5e410cef1142ec9a5fab9204db22ae1", "predicted_answer": "", "predicted_evidence": ["The existing methods of RC can be roughly divided into two categories, i.e., traditional methods and neural network approaches. The former are based on feature-basedBIBREF17, BIBREF18, BIBREF19 or kernel-basedBIBREF20 approaches. These models usually spend a lot of time on feature engineering. Neural network methods can extract the relation features without complicated feature engineering. e.g., convolutional neural network BIBREF21, BIBREF22, BIBREF23, recurrent neural network BIBREF24 and long short term memory BIBREF25, BIBREF26. In medical domain, there are recurrent capsule network BIBREF27 and domain invariant convolutional neural network BIBREF28. However, These methods cannot utilize the joint features between entity and relation, resulting in lower generalization performance when compared with joint learning methods.", "The embedding layer is used to obtain the vector representations of all the words in the sequence, and it consists of three components: word embedding (${e}_{word}$), position embedding (${e}_{pos}$), and type embedding (${e}_{type}$). Specifically, word embeddings are obtained through the corresponding embedding matrices. Positional embedding is used to capture the order information of the sequence which is ignored during the self-attention process. Type embedding is used to distinguish two different sequences of the input. Given an input sequence (${S}$), the initial vector representations of all the words in the sequence (${H}_0$) are as follows:", "In BERT, MASK matrix is originally used to mask the padding portion of the text. However, we found that by designing a specific MASK matrix, we can directly control the attention range of each word, thus obtaining specific context-sensitive representations. Specially, when calculating the attention (i.e., Equation (DISPLAY_FORM12)), the parameter matrix $MASK\\in {\\lbrace 0,1\\rbrace }^{T\\times T}$, where $T$ is the length of the sequence. If $MASK_{i,j} = 0$, then we have $(MASK_{i,j}-1)\\times \\infty = -\\infty $ and the Equation (DISPLAY_FORM15), which indicates that the $i$-th word ignores the $j$-th word when calculating attention.", "Due to the limitation of deep learning framework, we have to pad sequences to the same length. Therefore, all MASK matrices need to be expanded. The formula for expansion is as follows:", "$H_N^{ner}$ is the output of STR-encoder when given $MASK^{ner}$, $H_N^{ner}[1:T]$ denotes the representation of all words except $[CLS]$ token. $H_p^{ner}$ is the emission probability matrix of CRF layer, $Score(L|H_p^{ner})$ represents the score of the tag sequence $L$, $A_{L_{t-1},L_t}$ means the probability of the $(t-1)$-th tag transfering to the $t$-th tag, and ${H_p^{ner}}_{t,L_t}$ represents the probability that the $t$-th word is predicted as an $L_t$ tag. $p_{ner}(L|S,MASK^{ner},MASK^{all})$ indicates the probabilities of the tag sequence $L$ when given $S$, $MASK^{ner}$ and $MASK^{all}$, and $J$ is the possible tag sequence."]}
{"question_id": "344238de7208902f7b3a46819cc6d83cc37448a0", "predicted_answer": "", "predicted_evidence": ["W17-4209 incorporated user embeddings into Pavlopoulos' setup 1 pavlopoulos,pavlopoulos-emnlp described above. They divided all the users whose comments are included in data-gazzetta into 4 types based on proportion of abusive comments (e.g., red users if INLINEFORM0 comments and INLINEFORM1 abusive comments), yellow (users with INLINEFORM2 comments and INLINEFORM3 abusive comments), green (users with INLINEFORM4 comments and INLINEFORM5 abusive comments), and unknown (users with INLINEFORM6 comments). They then assigned unique randomly-initialized embeddings to users and added them as additional input to the lr layer, alongside representations of comments obtained from the gru, increasing auc from INLINEFORM7 to INLINEFORM8 . Qian et al. N18-2019 used lstms for modeling inter and intra-user relationships on data-twitter-wh, with sexist and racist tweets combined into one category. The authors applied a bi-lstm to users' recent tweets in order to generate intra-user representations that capture their historic behavior.", "This makes the presented techniques more difficult to compare. In addition, as abuse is a relatively infrequent phenomenon, the datasets are typically skewed towards non-abusive samples BIBREF6 . Metrics such as auroc may, therefore, be unsuitable since they may mask poor performance on the abusive samples as a side-effect of the large number of non-abusive samples BIBREF52 . Macro-averaged precision, recall, and F INLINEFORM1 , as well as precision, recall, and F INLINEFORM2 on specifically the abusive classes, may provide a more informative evaluation strategy; the primary advantage being that macro-averaged metrics provide a sense of effectiveness on the minority classes BIBREF73 . Additionally, area under the precision-recall curve (auprc) might be a better alternative to auroc in imbalanced scenarios BIBREF46 .", "First, the profiling approach should not compromise the privacy of the user. So a researcher might ask themselves such questions as: is the profiling based on identity traits of users (e.g., gender, race etc.) or solely on their online behavior? And is an appropriate generalization from (identifiable) user traits to population-level behavioural trends performed? Second, one needs to reflect on the possible bias in the training procedure: is it likely to induce a bias against users with certain traits? Third, the visibility aspect needs to be accounted for: is the profiling visible to the users, i.e., can users directly or indirectly observe how they (or others) have been profiled? And finally, one needs to carefully consider the purpose of such profiling: is it intended to take actions against users, or is it more benign (e.g. to better understand the content produced by them and make task-specific generalizations)? While we do not intend to provide answers to these questions within this survey, we hope that the above considerations can help to start a debate on these important issues.", "Deep learning in abuse detection. With the advent of deep learning, many researchers have explored its efficacy in abuse detection. Badjatiya et al. badjatiya evaluated several neural architectures on the data-twitter-wh dataset. Their best setup involved a two-step approach wherein they use a word-level long-short term memory (lstm) model, to tune glove or randomly-initialized word embeddings, and then train a gradient-boosted decision tree (gbdt) classifier on the average of the tuned embeddings in each tweet. They achieved the best results using randomly-initialized embeddings (weighted F INLINEFORM0 of INLINEFORM1 ). However, working with a similar setup, Mishra et al. mishra recently reported that glove initialization provided superior performance; a mismatch is attributed to the fact that Badjatiya et al. tuned the embeddings on the entire dataset (including the test set), hence allowing for the randomly-initialized ones to overfit.", "Pavlopoulos et al. pavlopoulos,pavlopoulos-emnlp applied deep learning to the data-wiki-att, data-wiki-tox, and data-gazzetta datasets. Their most effective setups were: (1) a word-level gru followed by an lr layer; (2) setup 1 extended with an attention mechanism on words. Both setups outperformed a simple word-list baseline and the character n-gram lr classifier (detox) of Wulczyn et al. wulczyn. Setup 1 achieved the best performance on data-wiki-att and data-wiki-tox (auc INLINEFORM0 and INLINEFORM1 respectively), while setup 2 performed the best on data-gazzetta (auc INLINEFORM2 ). The attention mechanism was additionally able to highlight abusive words and phrases within the comments, exhibiting a high level of agreement with annotators on the task. Lee et al."]}
{"question_id": "56bbca3fe24c2e9384cc57f55f35f7f5ad5c5716", "predicted_answer": "", "predicted_evidence": ["Mishra et al. mishra constructed a community graph of all users whose tweets are included in the data-twitter-wh dataset. Nodes in the graph were users while edges the follower-following relationship between them on Twitter. They then applied node2vec BIBREF21 to this graph to generate user embeddings. Inclusion of these embeddings into character n-gram based baselines yielded state of the art results on data-twitter-wh (F INLINEFORM0 increased from INLINEFORM1 and INLINEFORM2 to INLINEFORM3 and INLINEFORM4 on the racism and sexism classes respectively). The gains were attributed to the fact that user embeddings captured not only information about online communities, but also some elements of the wider conversation amongst connected users in the graph. Ribeiro et al. ribeiro and Mishra et al. mishragcn applied graph neural networks BIBREF22 , BIBREF23 to social graphs in order to generate user embeddings (i.e., profiles) that capture not only their surrounding community but also their linguistic behavior.", "User profiling with neural networks. More recently, researchers have employed neural networks to extract features for users instead of manually leveraging ones like gender, location, etc. as discussed before. Working with the data-gazzetta dataset, Pavlopoulos et al. W17-4209 incorporated user embeddings into Pavlopoulos' setup 1 pavlopoulos,pavlopoulos-emnlp described above. They divided all the users whose comments are included in data-gazzetta into 4 types based on proportion of abusive comments (e.g., red users if INLINEFORM0 comments and INLINEFORM1 abusive comments), yellow (users with INLINEFORM2 comments and INLINEFORM3 abusive comments), green (users with INLINEFORM4 comments and INLINEFORM5 abusive comments), and unknown (users with INLINEFORM6 comments). They then assigned unique randomly-initialized embeddings to users and added them as additional input to the lr layer, alongside representations of comments obtained from the gru, increasing auc from INLINEFORM7 to INLINEFORM8 .", "While the recent state of the art approaches rely on word-level cnns and rnns, they remain vulnerable to obfuscation of words BIBREF28 . Character n-gram, on the other hand, remain one of the most effective features for addressing obfuscation due to their robustness to spelling variations. Many researchers to date have exclusively relied on text based features for abuse detection. But recent works have shown that personal and community-based profiling features of users significantly enhance the state of the art.", "Several groups have investigated abusive language in Twitter. Waseem and Hovy waseemhovy created a corpus of INLINEFORM0 tweets, each annotated as one of racism ( INLINEFORM1 ), sexism, ( INLINEFORM2 ) or neither (data-twitter-wh). We note that although certain tweets in the dataset lack surface-level abusive traits (e.g., @Mich_McConnell Just \u201cher body\u201d right?), they have nevertheless been marked as racist or sexist as the annotators took the wider discourse into account; however, such discourse information or annotation is not preserved in the dataset. Inter-annotator agreement was reported at INLINEFORM3 , with a further insight that INLINEFORM4 of all the disagreements occurred on the sexism class alone. Waseem waseem later released a dataset of INLINEFORM5 tweets annotated as racism ( INLINEFORM6 ), sexism ( INLINEFORM7 ), both ( INLINEFORM8 ), or neither (data-twitter-w). data-twitter-w and data-twitter-wh have INLINEFORM9 tweets in common.", "Modeling wider conversation. Abuse is inherently contextual; it can only be interpreted as part of a wider conversation between users on the Internet. This means that individual comments can be difficult to classify without modeling their respective contexts. However, the vast majority of existing approaches have focused on modeling the lexical, semantic and syntactic properties of comments in isolation from other comments. Mishra et al. mishra have pointed out that some tweets in data-twitter-wh do not contain sufficient lexical or semantic information to detect abuse even in principle, e.g., @user: Logic in the world of Islam http://t.co/xxxxxxx, and techniques for modeling discourse and elements of pragmatics are needed."]}
{"question_id": "4c40fa01f626def0b69d1cb7bf9181b574ff6382", "predicted_answer": "", "predicted_evidence": ["These messages were marked as flame (containing insults or abuse; INLINEFORM1 ), maybe flame ( INLINEFORM2 ), or okay ( INLINEFORM3 ). We refer to this dataset as data-smokey. Yin et al. Yin09detectionof constructed three English datasets and annotated them for harassment, which they defined as \u201csystematic efforts by a user to belittle the contributions of other users\". The samples were taken from three social media platforms: Kongregate ( INLINEFORM4 posts; INLINEFORM5 harassment), Slashdot ( INLINEFORM6 posts; INLINEFORM7 harassment), and MySpace ( INLINEFORM8 posts; INLINEFORM9 harassment). We refer to the three datasets as data-harass. Several datasets have been compiled using samples taken from portals of Yahoo!, specifically the News and Finance portals. Djuric et al. djuric created a dataset of INLINEFORM10 user comments in English from the Yahoo! Finance website that were editorially labeled as either hate speech ( INLINEFORM11 ) or clean (data-yahoo-fin-dj).", "With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online. Undesirable psychological effects of abuse on individuals make it an important societal problem of our time. Munro munro2011 studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online. Pew Research Center, in its latest report on online harassment BIBREF0 , revealed that INLINEFORM0 of adults in the United States have experienced abusive behavior online, of which INLINEFORM1 have faced severe forms of harassment, e.g., that of sexual nature. The report goes on to say that harassment need not be experienced first-hand to have an impact: INLINEFORM2 of American Internet users admitted that they stopped using an online service after witnessing abusive and unruly behavior of their fellow users. These statistics stress the need for automated abuse detection and moderation systems.", "Online abuse stands as a significant challenge before society. Its nature and characteristics constantly evolve, making it a complex phenomenon to study and model. Automated abuse detection methods have seen a lot of development in recent years: from simple rule-based methods aimed at identifying directed, explicit abuse to sophisticated methods that can capture rich semantic information and even aspects of user behavior. By comprehensively reviewing the investigated methods to date, our survey aims to provide a platform for future research, facilitating progress in this important area. While we see an array of challenges that lie ahead, e.g., modeling extra-propositional aspects of language, user behavior and wider conversation, we believe that recent progress in the areas of semantics, dialogue modeling and social media analysis put the research community in a strong position to address them. Summaries of public datasets In table TABREF4 , we summarize the datasets described in this paper that are publicly available and provide links to them. A discussion of metrics The performance results we have reported highlight that, throughout work on abuse detection, different researchers have utilized different evaluation metrics for their experiments \u2013 from area under the receiver operating characteristic curve (auroc) BIBREF79 , BIBREF48 to micro and macro F INLINEFORM0 BIBREF28 \u2013 regardless of the properties of their datasets.", "Dataset descriptions. The earliest dataset published in this domain was compiled by Spertus smokey. It consisted of INLINEFORM0 private messages written in English from the web-masters of controversial web resources such as NewtWatch. These messages were marked as flame (containing insults or abuse; INLINEFORM1 ), maybe flame ( INLINEFORM2 ), or okay ( INLINEFORM3 ). We refer to this dataset as data-smokey. Yin et al. Yin09detectionof constructed three English datasets and annotated them for harassment, which they defined as \u201csystematic efforts by a user to belittle the contributions of other users\". The samples were taken from three social media platforms: Kongregate ( INLINEFORM4 posts; INLINEFORM5 harassment), Slashdot ( INLINEFORM6 posts; INLINEFORM7 harassment), and MySpace ( INLINEFORM8 posts; INLINEFORM9 harassment). We refer to the three datasets as data-harass. Several datasets have been compiled using samples taken from portals of Yahoo!, specifically the News and Finance portals.", "Davidson et al. davidson created a dataset of approximately INLINEFORM0 tweets, manually annotated as one of racist ( INLINEFORM1 ), offensive but not racist ( INLINEFORM2 ), or clean ( INLINEFORM3 ). We note, however, that their data sampling procedure relied on the presence of certain abusive words and, as a result, the distribution of classes does not follow a real-life distribution. Recently, Founta et al. founta crowd-sourced a dataset (data-twitter-f) of INLINEFORM4 tweets, of which INLINEFORM5 were annotated as normal, INLINEFORM6 as spam, INLINEFORM7 as hateful and INLINEFORM8 as abusive."]}
{"question_id": "71b29ab3ddcdd11dcc63b0bb55e75914c07a2217", "predicted_answer": "", "predicted_evidence": ["This makes the presented techniques more difficult to compare. In addition, as abuse is a relatively infrequent phenomenon, the datasets are typically skewed towards non-abusive samples BIBREF6 . Metrics such as auroc may, therefore, be unsuitable since they may mask poor performance on the abusive samples as a side-effect of the large number of non-abusive samples BIBREF52 . Macro-averaged precision, recall, and F INLINEFORM1 , as well as precision, recall, and F INLINEFORM2 on specifically the abusive classes, may provide a more informative evaluation strategy; the primary advantage being that macro-averaged metrics provide a sense of effectiveness on the minority classes BIBREF73 . Additionally, area under the precision-recall curve (auprc) might be a better alternative to auroc in imbalanced scenarios BIBREF46 .", "The GermEval shared task on identification of offensive language in German tweets BIBREF8 saw submission of both deep learning and feature engineering approaches. The winning system BIBREF15 (macro F INLINEFORM0 of INLINEFORM1 ) employed multiple character and token n-gram classifiers, as well as distributional semantic features obtained by averaging word embeddings. The second best approach BIBREF16 (macro F INLINEFORM2 INLINEFORM3 ), on the other hand, employed an ensemble of cnns, the outputs of which were fed to a meta classifier for final prediction. Most of the remaining submissions BIBREF17 , BIBREF18 used deep learning with cnns and rnns alongside techniques such as transfer learning (e.g., via machine translation or joint representation learning for words across languages) from abuse-annotated datasets in other languages (mainly English). Wiegand et al. wiegand2018overview noted that simple deep learning approaches themselves were quite effective, and the addition of other techniques did not necessarily provide substantial improvements.", "Figurative language. Figurative devices such as metaphor and sarcasm are common in natural language. They tend to be used to express emotions and sentiments that go beyond the literal meaning of words and phrases BIBREF39 . Nobata et al. nobata (among others, e.g., Aken et al. van2018challenges) noted that sarcastic comments are hard for abuse detection methods to deal with since surface features are not sufficient; typically the knowledge of the context or background of the user is also required. Mishra mishrathesis found that metaphors are more frequent in abusive samples as opposed to non-abusive ones. However, to fully understand the impact of figurative devices on abuse detection, datasets with more pronounced presence of these are required.", "We hereby propose three properties that an explainable abuse detection system should aim to exhibit. First, it needs to establish intent of abuse (or the lack of it) and provide evidence for it, hence convincingly segregating abuse from other phenomena such as sarcasm and humour. Second, it needs to capture abusive language, i.e., highlight instances of abuse if present, be they explicit (i.e., use of expletives) or implicit (e.g., dehumanizing comparisons). Third, it needs to identify the target(s) of abuse (or the absence thereof), be it an individual or a group. These properties align well with the categorizations of abuse we discussed in the introduction. They also aptly motivate the advances needed in the field: (1) developments in areas such as sarcasm detection and user profiling for precise segregation of abusive intent from humor, satire, etc.; (2) better identification of implicit abuse, which requires improvements in modeling of figurative language; (3) effective detection of generalized abuse and inference of target(s), which require advances in areas such as domain adaptation and conversation modeling.", ""]}
{"question_id": "22225ba18a6efe74b1315cc08405011d5431498e", "predicted_answer": "", "predicted_evidence": ["We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for access to headlines in the corpus of financial news articles collected from Factiva. This research was supported at Lancaster University by an EPSRC PhD studentship.", "The main difference between the two models is the use of drop out and when they stop training over the data (epoch). Both models architectures can be seen in figure FIGREF18 .", "We first present our findings on the best performing parameters and features for the SVRs. These were determined by cross validation (CV) scores on the provided training data set using cosine similarity as the evaluation metric. We found that using uni-grams and bi-grams performs best and using only bi-grams to be the worst. Using the Unitok tokeniser always performed better than simple whitespace tokenisation. The binary presence of tokens over frequency did not alter performance. The C parameter was tested for three values; 0.01, 0.1 and 1. We found very little difference between 0.1 and 1, but 0.01 produced much poorer results. The eplison parameter was tested for 0.001, 0.01 and 0.1 the performance did not differ much but the lower the higher the performance but the more likely to overfit.", "Even though we have outlined this task as an aspect based sentiment task, this is instantiated in only one of the features in the SVR. The following two subsections describe the two approaches, first SVR and then BLSTM. Key implementation details are exposed here in the paper, but we have released the source code and word embedding models to aid replicability and further experimentation.", "This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e."]}
{"question_id": "bd3562d2b3c162e9d27404d56b77e15f707d8b0f", "predicted_answer": "", "predicted_evidence": ["The output activation function is linear.", "The system was created using ScitKit learn BIBREF11 linear Support Vector Regression model BIBREF12 . We experimented with the following different features and parameter settings:", "The training data published by the organisers for this track was a set of headline sentences from financial news articles where each sentence was tagged with the company name (which we treat as the aspect) and the polarity of the sentence with respect to the company. There is the possibility that the same sentence occurs more than once if there is more than one company mentioned. The polarity was a real value between -1 (negative sentiment) and 1 (positive sentiment).", "Gradient clipping value of 5 - This was to help with the exploding gradients problem.", "BIBREF5 showed the importance of tuning sentiment analysis to the task of stock market prediction. However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data. In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 BIBREF1 . The winning system BIBREF6 used many different linguistic features and an ensemble model, and the runner up BIBREF7 used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. BIBREF8 created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineering or manual feature creation."]}
{"question_id": "9c529bd3f7565b2178a79aae01c98c90f9d372ad", "predicted_answer": "", "predicted_evidence": ["We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for access to headlines in the corpus of financial news articles collected from Factiva. This research was supported at Lancaster University by an EPSRC PhD studentship.", "The binary presence of tokens over frequency did not alter performance. The C parameter was tested for three values; 0.01, 0.1 and 1. We found very little difference between 0.1 and 1, but 0.01 produced much poorer results. The eplison parameter was tested for 0.001, 0.01 and 0.1 the performance did not differ much but the lower the higher the performance but the more likely to overfit. Using word replacements was effective for all three types (company, positive and negative) but using a value N=10 performed best for both positive and negative words. Using target aspects also improved results. Therefore, the best SVR model comprised of: Unitok tokenisation, uni- and bi- grams, word representation, C=0.1, eplison=0.01, company, positive, and negative word replacements and target aspects. DISPLAYFORM0", "As you can see from the results table TABREF28 , the difference between the metrics is quite substantial. This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect.", "This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e.", "Gradient clipping value of 5 - This was to help with the exploding gradients problem."]}
{"question_id": "cf82251a6a5a77e29627560eb7c05c3eddc20825", "predicted_answer": "", "predicted_evidence": ["While counterfactual data augmentation is relatively simple for sentences in English, the process for inflected languages is challenging, involving identifying and updating words that are co-referent with all gendered entities in a sentence. Gender-swapping MT training data additionally requires that the same entities are swapped in the corresponding parallel sentence. A robust scheme for gender-swapping multiple entities in inflected language sentences directly, together with corresponding parallel text, is beyond the scope of this paper. Instead we suggest a rough but straightforward approach for counterfactual data augmentation for NMT which to the best of our knowledge is the first application to parallel sentences.", "Forward-translated (FTrans) original: the source side of the original set with forward-translated target sentences.", "$\\mathbf {\\Delta G}$ \u2013 difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities.", "Balanced: the concatenation of the original and FTrans swapped parallel datasets. This is twice the size of the other counterfactual sets.", "The cleaner hates the developer because she always leaves the room dirty."]}
{"question_id": "b1fe6a39b474933038b44b6d45e5ca32af7c3e36", "predicted_answer": "", "predicted_evidence": ["We first perform simple gender-swapping on the subset of the English source sentences with gendered terms. We use the approach described in BIBREF5 which swaps a fixed list of gendered stopwords (e.g. man / woman, he / she).. We then greedily forward-translate the gender-swapped English sentences with a baseline NMT model trained on the the full source and target text, producing gender-swapped target language sentences.", "In lines 7-9 of Table TABREF40 we consider lattice-rescoring the baseline output, using three models debiased on the handcrafted data.", "For contrast, we fine-tune on an approximated counterfactual dataset. Counterfactual data augmentation is an intuitive solution to bias from data over-representation BIBREF23. It involves identifying the subset of sentences containing bias \u2013 in this case gendered terms \u2013 and, for each one, adding an equivalent sentence with the bias reversed \u2013 in this case a gender-swapped version.", "BIBREF13 treat gender as a domain for machine translation, training from scratch by augmenting Europarl data with a tag indicating the speaker's gender. This does not inherently remove gender bias from the system but allows control over the translation hypothesis gender. BIBREF14 similarly prepend a short phrase at inference time which acts as a gender domain label for the entire sentence. These approaches are not directly applicable to text which may have more than one gendered entity per sentence, as in coreference resolution tasks.", "Forward-translated (FTrans) swapped: the original source sentences are gender-swapped, then forward-translated to produce gender-swapped target sentences."]}
{"question_id": "919681faa9731057b3fae5052b7da598abd3e04b", "predicted_answer": "", "predicted_evidence": ["One interesting observation is that WinoMT accuracy after rescoring tends to fall in a fairly narrow range for each language relative to the performance range of the baseline systems. For example, a 25.5% range in baseline en-de accuracy becomes a 3.6% range after rescoring. This suggests that our rescoring approach is not limited as much by the bias level of the baseline system as by the gender-inflection transducer and the models used in rescoring. Indeed, we emphasise that the large improvements reported in Table TABREF41 do not require any knowledge of the commercial systems or the data they were trained on; we use only the translation hypotheses they produce and our own rescoring model and transducer.", "Overall, improvements from fine-tuning on counterfactual datasets (FTrans swapped and balanced) are present. However, they are not very different from the improvements when fine-tuning on equivalent non-counterfactual sets (original and FTrans original). Improvements are also inconsistent across language pairs.", "Our hypothesis is that the absence of gender bias can be treated as a small domain for the purposes of NMT model adaptation. In this case a well-formed small dataset may give better results than attempts at debiasing the entire original dataset.", "The purpose of EWC regularization is to avoid catastrophic forgetting of general translation ability. This does not occur in the counterfactual experiments, so we do not apply EWC. Moreover, WinoMT accuracy gains are small with standard fine-tuning, which allows maximum adaptation: we suspect EWC would prevent any improvements.", "Results for fine-tuning on the handcrafted set are given in lines 3-6 of Table TABREF40. These experiments take place in minutes on a single GPU, compared to several hours when fine-tuning on the counterfactual sets and far longer if training from scratch."]}
{"question_id": "2749fb1725a2c4bdba5848e2fc424a43e7c4be51", "predicted_answer": "", "predicted_evidence": ["The drop in BLEU and improvement on WinoMT can be explored by varying the training procedure. The model of line 5 simply adapts to handcrafted data for more iterations with no regularisation, to approximate loss convergence on the handcrafted set. This leads to a severe drop in BLEU, but even higher WinoMT scores.", "We wish to distinguish between a model which improves gender translation, and one which improves its WinoMT scores simply by learning the vocabulary for previously unseen or uncommon professions. We therefore create a handcrafted no-overlap set, removing source sentences with professions occurring in WinoMT to leave 216 sentences. We increase this set back to 388 examples with balanced adjective-based sentences in the same pattern, e.g. The tall $[$man$|$woman$]$ finished $[$his$|$her$]$ work.", "We suggest small-domain adaptation as a more effective and efficient approach to debiasing machine translation than counterfactual data augmentation. We do not claim to fix the bias problem in NMT, but demonstrate that bias can be reduced without degradation in overall translation quality.", "We note that $\\Delta S$ can be significantly skewed by very biased systems. A model that generates male forms for almost all test sentences, stereotypical roles or not, will have an extremely low $\\Delta S$, since its pro- and anti-stereotypical class accuracy will both be about 50%. Consequently we also report:", "Rescoring en-he maintains a much smaller proportion of WinoMT accuracy improvement than en-de and en-es. We believe this is because the en-he baseline is particularly weak, due to a small and non-diverse training set. The baseline must produce some inflection of the correct entity before lattice rescoring can have an effect on gender bias."]}
{"question_id": "7239c02a0dcc0c3c9d9cddb5e895bcf9cfcefee6", "predicted_answer": "", "predicted_evidence": ["The following models rely on (freely-available) data that has more structure than raw text.", "By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences. With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples include machine translation BIBREF8 , image captioning BIBREF9 and dialogue systems BIBREF10 . While it has been observed informally that the internal sentence representations of such models can reflect semantic intuitions BIBREF11 , it is not known which architectures or objectives yield the `best' or most useful representations. Resolving this question could ultimately have a significant impact on language processing systems. Indeed, it is phrases and sentences, rather than individual words, that encode the human-like general world knowledge (or `common sense') BIBREF12 that is a critical missing part of most current language understanding systems.", "Sequential (Denoising) Autoencoders The SkipThought objective requires training text with a coherent inter-sentence narrative, making it problematic to port to domains such as social media or artificial language generated from symbolic knowledge. To avoid this restriction, we experiment with a representation-learning objective based on denoising autoencoders (DAEs). In a DAE, high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. As a result of this process, DAEs learn to represent the data in terms of features that explain its important factors of variation BIBREF22 . Transforming data into DAE representations (as a `pre-training' or initialisation step) gives more robust (supervised) classification performance in deep feedforward networks BIBREF23 .", "The evaluations have limitations The internal consistency (Chronbach's INLINEFORM0 ) of all evaluations considered together is INLINEFORM1 (just above `acceptable'). Table TABREF25 shows that consistency is far higher (`excellent') when considering the supervised or unsupervised tasks as independent cohorts. This indicates that, with respect to common characteristics of sentence representations, the supervised and unsupervised benchmarks do indeed prioritise different properties. It is also interesting that, by this metric, the properties measured by MSRP and image-caption relatedness are the furthest removed from other evaluations in their respective cohorts.", "Performance of the models on the supervised evaluations (grouped according to the data required by their objective) is shown in Table TABREF20 . Overall, SkipThought vectors perform best on three of the six evaluations, the BOW DictRep model with pre-trained word embeddings performs best on two, and the SDAE on one. SDAEs perform notably well on the paraphrasing task, going beyond SkipThought by three percentage points and approaching state-of-the-art performance of models designed specifically for the task BIBREF38 . SDAE is also consistently better than SAE, which aligns with other findings that adding noise to AEs produces richer representations BIBREF22 ."]}
{"question_id": "9dcc10a4a325d4c9cb3bb8134831ee470be47e93", "predicted_answer": "", "predicted_evidence": ["Many additional conclusions can be drawn from the results in Tables TABREF20 and TABREF21 .", "We observe notable differences in approaches depending on the nature of the evaluation metric. In particular, deeper or more complex models (which require greater time and resources to train) generally perform best in the supervised setting, whereas shallow log-linear models work best on unsupervised benchmarks. Specifically, SkipThought Vectors BIBREF13 perform best on the majority of supervised evaluations, but SDAEs are the top performer on paraphrase identification. In contrast, on the (unsupervised) SICK sentence relatedness benchmark, FastSent, a simple, log-linear variant of the SkipThought objective, performs better than all other models. Interestingly, the method that exhibits strongest performance across both supervised and unsupervised benchmarks is a bag-of-words model trained to compose word embeddings using dictionary definitions BIBREF14 . Taken together, these findings constitute valuable guidelines for the application of phrasal or sentential representation-learning to language understanding systems.", "Different objectives yield different representations It may seem obvious, but the results confirm that different learning methods are preferable for different intended applications (and this variation appears greater than for word representations). For instance, it is perhaps unsurprising that SkipThought performs best on TREC because the labels in this dataset are determined by the language immediately following the represented question (i.e. the answer) BIBREF35 . Paraphrase detection, on the other hand, may be better served by a model that focused entirely on the content within a sentence, such as SDAEs. Similar variation can be observed in the unsupervised evaluations. For instance, the (multimodal) representations produced by the CaptionRep model do not perform particularly well apart from on the Image category of STS where they beat all other models, demonstrating a clear effect of the well-studied modality differences in representation learning BIBREF39 .", "We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.", "To constrain the analysis, we compare neural language models that compute sentence representations from unlabelled, naturally-ocurring data, as with the predominant methods for word representations. Likewise, we do not focus on `bottom up' models where phrase or sentence representations are built from fixed mathematical operations on word vectors (although we do consider a canonical case - see CBOW below); these were already compared by milajevs2014evaluating. Most space is devoted to our novel approaches, and we refer the reader to the original papers for more details of existing models."]}
{"question_id": "31236a876277c6e1c80891a3293c105a1b1be008", "predicted_answer": "", "predicted_evidence": ["We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.", "SkipThought Vectors For consecutive sentences INLINEFORM0 in some document, the SkipThought model BIBREF13 is trained to predict target sentences INLINEFORM1 and INLINEFORM2 given source sentence INLINEFORM3 . As with all sequence-to-sequence models, in training the source sentence is `encoded' by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits BIBREF11 ) and then `decoded' into the two target sentences in turn. Importantly, because RNNs employ a single set of update weights at each time-step, both the encoder and decoder are sensitive to the order of words in the source sentence.", "We observe notable differences in approaches depending on the nature of the evaluation metric. In particular, deeper or more complex models (which require greater time and resources to train) generally perform best in the supervised setting, whereas shallow log-linear models work best on unsupervised benchmarks. Specifically, SkipThought Vectors BIBREF13 perform best on the majority of supervised evaluations, but SDAEs are the top performer on paraphrase identification. In contrast, on the (unsupervised) SICK sentence relatedness benchmark, FastSent, a simple, log-linear variant of the SkipThought objective, performs better than all other models. Interestingly, the method that exhibits strongest performance across both supervised and unsupervised benchmarks is a bag-of-words model trained to compose word embeddings using dictionary definitions BIBREF14 . Taken together, these findings constitute valuable guidelines for the application of phrasal or sentential representation-learning to language understanding systems.", "By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences. With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples include machine translation BIBREF8 , image captioning BIBREF9 and dialogue systems BIBREF10 . While it has been observed informally that the internal sentence representations of such models can reflect semantic intuitions BIBREF11 , it is not known which architectures or objectives yield the `best' or most useful representations. Resolving this question could ultimately have a significant impact on language processing systems. Indeed, it is phrases and sentences, rather than individual words, that encode the human-like general world knowledge (or `common sense') BIBREF12 that is a critical missing part of most current language understanding systems.", "Non-Distributed Baseline We implement a TFIDF BOW model in which the representation of sentence INLINEFORM0 encodes the count in INLINEFORM1 of a set of feature-words weighted by their tfidf in INLINEFORM2 , the corpus. The feature-words are the 200,000 most common words in INLINEFORM3 ."]}
{"question_id": "19ebfba9aa5a9596b09a0cfb084ff8ebf24a3b91", "predicted_answer": "", "predicted_evidence": ["Unless stated above, all models were trained on the Toronto Books Corpus, which has the inter-sentential coherence required for SkipThought and FastSent. The corpus consists of 70m ordered sentences from over 7,000 books.", "We also experiment with a variant (+AE) in which the encoded (source) representation must predict its own words as target in addition to those of adjacent sentences. Thus in FastSent+AE, ( EQREF12 ) becomes DISPLAYFORM0", "SkipThought Vectors For consecutive sentences INLINEFORM0 in some document, the SkipThought model BIBREF13 is trained to predict target sentences INLINEFORM1 and INLINEFORM2 given source sentence INLINEFORM3 . As with all sequence-to-sequence models, in training the source sentence is `encoded' by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits BIBREF11 ) and then `decoded' into the two target sentences in turn. Importantly, because RNNs employ a single set of update weights at each time-step, both the encoder and decoder are sensitive to the order of words in the source sentence.", "We introduce two new approaches designed to address certain limitations with the existing models.", "NMT We consider the sentence representations learned by neural MT models. These models have identical architecture to SkipThought, but are trained on sentence-aligned translated texts. We used a standard architecture BIBREF11 on all available En-Fr and En-De data from the 2015 Workshop on Statistical MT (WMT)."]}
{"question_id": "2288f567d2f5cfbfc5097d8eddf9abd238ffbe25", "predicted_answer": "", "predicted_evidence": ["), coordinating conjunctions (e.g., and, but, etc.), and discourse adverbials (e.g., however, also, tec.). Different category has different discourse usage. Discourse connective word can be ambiguous between discourse or non-discourse usage. An apparent example is 'after' because it can be a VP (e.g., \"If you are after something, you are trying to get it\") or it can be a connective (e.g., \u201cIt wasn't until after Christmas that I met Paul\u201d). In the case of explicit relation, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument. But the span of the arguments of explicit relation can be clauses or sentences. In the case of implicit relation, Arg1 is before Arg2 BIBREF11 . For explicit, implicit and altLex relation, there are three-level hierarchy of relation senses. The first level consists of four major relation classes: Temporal, Contingency, Comparison, and Expansion.", "We design a complete discourse parser connecting subtasks together in pipeline. First let\u2019s have a quick view about the procedure of the parser. The first step is pre-processing, which takes the raw text as input and generates POS tag of token, the dependency tree, constituent tree and so on. Next the parser needs to distinguish the connective between discourse usage and non-discourse usage. Then, the two argu-ments of discourse connective need to be identified. Next to above steps, the parser labels the discourse relation right sense. Until now the explicit relations already have been found fully. The last step is indentifying the non-explicit relation. The parser will handle every pair of adjacent sentences in same paragraph. The text is pre-processed by the Stanford CoreNLP tools. Stanford CoreNLP provides a series of natural language analysis tools which can tokenize the text, label tokens with their part-of-speech (POS) tag, and provides full syntactic analysis, in-cluding both constituent and dependency representation.", "We design a complete discourse parser connecting subtasks together in pipeline. First let\u2019s have a quick view about the procedure of the parser. The first step is pre-processing, which takes the raw text as input and generates POS tag of token, the dependency tree, constituent tree and so on. Next the parser needs to distinguish the connective between discourse usage and non-discourse usage. Then, the two argu-ments of discourse connective need to be identified. Next to above steps, the parser labels the discourse relation right sense. Until now the explicit relations already have been found fully. The last step is indentifying the non-explicit relation. The parser will handle every pair of adjacent sentences in same paragraph. The text is pre-processed by the Stanford CoreNLP tools.", "After identifying the discourse connective, there is a need to find the arguments. There are some different methods to find the arguments. Ziheng Lin et al. LinNK14 first identify the locations of Arg1, and choose sentence from prior candidate sentence if the location is before the connective. Otherwise, label arguments span by choosing the high node in the parse tree. Wellner and Pustejovsky WellnerP07 focus on identifying rela-tions between the pairs of head words. Based on such thinking, Robert Elwell and Jason Baldridge ElwellB08 improve the performance using connective specific rankers, which differentiate between specific connectives and types of connectives. Ziheng Lin et al. LinNK14 present an implicit discourse relation classifier based the Penn Discourse Treebank. All of these efforts can be viewed as the part of the full parser. More and more researcher has been devoted to the subtask of the shallow discourse parsing, like dis-ambiguating discourse connective BIBREF8 , finding implicit relation BIBREF9 .", "On this step, we adopt the head-based thinking BIBREF12 , which turns the problem of identifying arguments of discourse connective into identifying the head and end of the arguments. First, we need to extract the candidates of arguments. To reduce the Arg1 candidates space, we only consider words with appropriate part-of-speech (all verbs, common nouns, adjectives) and within 10 \u201dsteps\u201d between word and connec-tive as candidates, where a step is either a sentence boundary or a dependency link. Only words in the same sentence with the connective are considered for Arg2 candi-dates. Second, we need to choose the best candidate as the head of Arg1 and Arg2. In the end, we need to obtain the arguments span according head and end of argu-ments on the constituent tree. The table 2 shows the feature we use. The table 3 shows the procedure of the arguments identifier."]}
{"question_id": "caebea05935cae1f5d88749a2fc748e62976eab7", "predicted_answer": "", "predicted_evidence": ["), coordinating conjunctions (e.g., and, but, etc.), and discourse adverbials (e.g., however, also, tec.). Different category has different discourse usage. Discourse connective word can be ambiguous between discourse or non-discourse usage. An apparent example is 'after' because it can be a VP (e.g., \"If you are after something, you are trying to get it\") or it can be a connective (e.g., \u201cIt wasn't until after Christmas that I met Paul\u201d). In the case of explicit relation, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument. But the span of the arguments of explicit relation can be clauses or sentences. In the case of implicit relation, Arg1 is before Arg2 BIBREF11 . For explicit, implicit and altLex relation, there are three-level hierarchy of relation senses.", "The non-explicit relation is the relation between adjacent sentences in same para-graph. So we just check adjacent sentences which don\u2019t form explicit relation and then label them with non-explicit relation or nothing. In the experiment, we find that the two arguments of non-explicit relation have association with each other and also have some common words. So we introduce feature words, which indicate appear-ance of relation, like \u201cit, them\u201d.", "Since the release of second version of the Penn Discourse Treebank (PDTB), which is over the 1 million word Wall Street Journal corpus, analyzing the PDTB-2.0 is very useful for further study on shallow discourse parsing. Prasad et al. PrasadDLMRJW08 describe lexically-grounded annotations of discourse relations in PDTB. Identifying the discourse connective from ordinary words accurately is not easy because discourse words can have discourse or non-discourse usage. Pitler and Nenkova PitlerN09 use syntax feature to disambiguate explicit discourse connective in text and prove that the syntactic features can improve performance in disambiguation task. After identifying the discourse connective, there is a need to find the arguments. There are some different methods to find the arguments. Ziheng Lin et al.", "The connective identifier finds the connective word, \u201cunless\u201d. The arguments identifier locates the two arguments of \u201cunless\u201d. The sense classifier labels the dis-course relation. The non-explicit identifier checks all the pair of adjacent sentences. If the non-explicit identifier indentifies the pair of sentences as non-explicit relation, it will label it the relation sense. Though many research work BIBREF2 , BIBREF3 , BIBREF4 are committed to the shallow discourse parsing field, all of them are focus on the subtask of parsing only rather than the whole parsing process. Given all that, a full shallow discourse parser framework is proposed in our paper to turn the free text into discourse relations set. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. In order to enhance the performance of the parser, the feature-based maximum entropy model approach is adopted in the experiment.", "We design a complete discourse parser connecting subtasks together in pipeline. First let\u2019s have a quick view about the procedure of the parser. The first step is pre-processing, which takes the raw text as input and generates POS tag of token, the dependency tree, constituent tree and so on. Next the parser needs to distinguish the connective between discourse usage and non-discourse usage. Then, the two argu-ments of discourse connective need to be identified. Next to above steps, the parser labels the discourse relation right sense. Until now the explicit relations already have been found fully. The last step is indentifying the non-explicit relation. The parser will handle every pair of adjacent sentences in same paragraph. The text is pre-processed by the Stanford CoreNLP tools."]}
{"question_id": "e381f1811774806be109f9b05896a2a3c5e1ef43", "predicted_answer": "", "predicted_evidence": ["Pitler and Nenkova PitlerN09 use syntax feature to disambiguate explicit discourse connective in text and prove that the syntactic features can improve performance in disambiguation task. After identifying the discourse connective, there is a need to find the arguments. There are some different methods to find the arguments. Ziheng Lin et al. LinNK14 first identify the locations of Arg1, and choose sentence from prior candidate sentence if the location is before the connective. Otherwise, label arguments span by choosing the high node in the parse tree. Wellner and Pustejovsky WellnerP07 focus on identifying rela-tions between the pairs of head words. Based on such thinking, Robert Elwell and Jason Baldridge ElwellB08 improve the performance using connective specific rankers, which differentiate between specific connectives and types of connectives. Ziheng Lin et al.", "The sense of discourse relation has three levels: class, type and subtype. There are four classes on the top level of the sense: Comparison, Temporal , Con-tingency, Expansion. Each class includes a set of different types, and some types may have different subtypes. The connective itself is a very good feature because discourse connective almost determine senses. So we train an explicit classifier using simple but effective features.", "Discourse connective is the signal of explicit relation. Discourse connective in the PTDB can be classified as three categories: subordinating conjunctions (e.g., because, if, etc.), coordinating conjunctions (e.g., and, but, etc.), and discourse adverbials (e.g., however, also, tec.). Different category has different discourse usage. Discourse connective word can be ambiguous between discourse or non-discourse usage. An apparent example is 'after' because it can be a VP (e.g., \"If you are after something, you are trying to get it\") or it can be a connective (e.g., \u201cIt wasn't until after Christmas that I met Paul\u201d). In the case of explicit relation, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument. But the span of the arguments of explicit relation can be clauses or sentences.", "It is not surprised to find that Baseline_1 shows the poorest performance, which it just considers the probability information, ignores the contextual link. The perfor-mance of Baseline_2 is better than that of \u201cBaseline_1\u201d. This can be mainly credited to the ability of abundant lexical and syntax features. Our parser shows better per-formance than Baselin_2 because the most of features we use are textual type fea-tures, which are convenient for the maximum entropy model. Though the textual type features can turn into numeric type according to hashcode of string, it is incon-venient for Support Vector Machine because the hashcode of string is not continu-ous. According the performance of the parser, we find that the connective identifying can achieve higher precision and recall rate.", "The non-explicit relation is the relation between adjacent sentences in same para-graph. So we just check adjacent sentences which don\u2019t form explicit relation and then label them with non-explicit relation or nothing. In the experiment, we find that the two arguments of non-explicit relation have association with each other and also have some common words. So we introduce feature words, which indicate appear-ance of relation, like \u201cit, them\u201d."]}
{"question_id": "9eec16e560f9ccafd7ba6f1e0db742b330b42ba9", "predicted_answer": "", "predicted_evidence": ["The main duty of this component is disambiguate the connective words which are in PDTB predefined set. Pitler and Nenkova citePitlerN09 show that syntactic features are very useful on disambiguate discourse connective, so we adopt these syntactic fea-tures as part of our features. Ziheng Lin et al. LinKN09 show that a connective\u2019s context and part-of-speech (POS) gives a very strong indication of discourse usage. The table 1 shows the feature we use.", "Discourse connective is the signal of explicit relation. Discourse connective in the PTDB can be classified as three categories: subordinating conjunctions (e.g., because, if, etc.), coordinating conjunctions (e.g., and, but, etc.), and discourse adverbials (e.g., however, also, tec.). Different category has different discourse usage. Discourse connective word can be ambiguous between discourse or non-discourse usage. An apparent example is 'after' because it can be a VP (e.g., \"If you are after something, you are trying to get it\") or it can be a connective (e.g., \u201cIt wasn't until after Christmas that I met Paul\u201d). In the case of explicit relation, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument.", "Ziheng Lin et al. LinNK14 first identify the locations of Arg1, and choose sentence from prior candidate sentence if the location is before the connective. Otherwise, label arguments span by choosing the high node in the parse tree. Wellner and Pustejovsky WellnerP07 focus on identifying rela-tions between the pairs of head words. Based on such thinking, Robert Elwell and Jason Baldridge ElwellB08 improve the performance using connective specific rankers, which differentiate between specific connectives and types of connectives. Ziheng Lin et al. LinNK14 present an implicit discourse relation classifier based the Penn Discourse Treebank. All of these efforts can be viewed as the part of the full parser. More and more researcher has been devoted to the subtask of the shallow discourse parsing, like dis-ambiguating discourse connective BIBREF8 , finding implicit relation BIBREF9 . There is a need to pull these subtasks together to achieve more efforts.", "The sense of discourse relation has three levels: class, type and subtype. There are four classes on the top level of the sense: Comparison, Temporal , Con-tingency, Expansion. Each class includes a set of different types, and some types may have different subtypes. The connective itself is a very good feature because discourse connective almost determine senses. So we train an explicit classifier using simple but effective features.", "It is not surprised to find that Baseline_1 shows the poorest performance, which it just considers the probability information, ignores the contextual link. The perfor-mance of Baseline_2 is better than that of \u201cBaseline_1\u201d. This can be mainly credited to the ability of abundant lexical and syntax features. Our parser shows better per-formance than Baselin_2 because the most of features we use are textual type fea-tures, which are convenient for the maximum entropy model. Though the textual type features can turn into numeric type according to hashcode of string, it is incon-venient for Support Vector Machine because the hashcode of string is not continu-ous. According the performance of the parser, we find that the connective identifying can achieve higher precision and recall rate."]}
{"question_id": "d788076c0d19781ff3f6525bd9c05b0ef0ecd0f1", "predicted_answer": "", "predicted_evidence": ["If the non-explicit identifier indentifies the pair of sentences as non-explicit relation, it will label it the relation sense. Though many research work BIBREF2 , BIBREF3 , BIBREF4 are committed to the shallow discourse parsing field, all of them are focus on the subtask of parsing only rather than the whole parsing process. Given all that, a full shallow discourse parser framework is proposed in our paper to turn the free text into discourse relations set. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. In order to enhance the performance of the parser, the feature-based maximum entropy model approach is adopted in the experiment. Maximum entropy model offers a clean way to combine diverse pieces of contextual evidence in order to estimate the probability of a certain linguistic class occurring with a certain linguistic context in a simple and accessible manner. The three main contributions of the paper are:", "After identifying the discourse connective, there is a need to find the arguments. There are some different methods to find the arguments. Ziheng Lin et al. LinNK14 first identify the locations of Arg1, and choose sentence from prior candidate sentence if the location is before the connective. Otherwise, label arguments span by choosing the high node in the parse tree. Wellner and Pustejovsky WellnerP07 focus on identifying rela-tions between the pairs of head words. Based on such thinking, Robert Elwell and Jason Baldridge ElwellB08 improve the performance using connective specific rankers, which differentiate between specific connectives and types of connectives. Ziheng Lin et al. LinNK14 present an implicit discourse relation classifier based the Penn Discourse Treebank. All of these efforts can be viewed as the part of the full parser. More and more researcher has been devoted to the subtask of the shallow discourse parsing, like dis-ambiguating discourse connective BIBREF8 , finding implicit relation BIBREF9 .", "The connective identifier finds the connective word, \u201cunless\u201d. The arguments identifier locates the two arguments of \u201cunless\u201d. The sense classifier labels the dis-course relation. The non-explicit identifier checks all the pair of adjacent sentences. If the non-explicit identifier indentifies the pair of sentences as non-explicit relation, it will label it the relation sense. Though many research work BIBREF2 , BIBREF3 , BIBREF4 are committed to the shallow discourse parsing field, all of them are focus on the subtask of parsing only rather than the whole parsing process. Given all that, a full shallow discourse parser framework is proposed in our paper to turn the free text into discourse relations set. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. In order to enhance the performance of the parser, the feature-based maximum entropy model approach is adopted in the experiment.", "The arguments identifier locates the two arguments of \u201cunless\u201d. The sense classifier labels the dis-course relation. The non-explicit identifier checks all the pair of adjacent sentences. If the non-explicit identifier indentifies the pair of sentences as non-explicit relation, it will label it the relation sense. Though many research work BIBREF2 , BIBREF3 , BIBREF4 are committed to the shallow discourse parsing field, all of them are focus on the subtask of parsing only rather than the whole parsing process. Given all that, a full shallow discourse parser framework is proposed in our paper to turn the free text into discourse relations set. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. In order to enhance the performance of the parser, the feature-based maximum entropy model approach is adopted in the experiment. Maximum entropy model offers a clean way to combine diverse pieces of contextual evidence in order to estimate the probability of a certain linguistic class occurring with a certain linguistic context in a simple and accessible manner.", "After identifying the discourse connective, there is a need to find the arguments. There are some different methods to find the arguments. Ziheng Lin et al. LinNK14 first identify the locations of Arg1, and choose sentence from prior candidate sentence if the location is before the connective. Otherwise, label arguments span by choosing the high node in the parse tree. Wellner and Pustejovsky WellnerP07 focus on identifying rela-tions between the pairs of head words. Based on such thinking, Robert Elwell and Jason Baldridge ElwellB08 improve the performance using connective specific rankers, which differentiate between specific connectives and types of connectives. Ziheng Lin et al. LinNK14 present an implicit discourse relation classifier based the Penn Discourse Treebank. All of these efforts can be viewed as the part of the full parser."]}
{"question_id": "ec70c7c560e08cff2820bad93f5216bc0a469f5a", "predicted_answer": "", "predicted_evidence": ["Our objective is to produce maximal performance on sample $(S^{(B)},Y^{(B)})$:", "In this paper, we focus on the extractive summarization and demonstrate that news publications can cause data distribution differences, which means that they can also be defined as domains. Based on this, we re-purpose a multi-domain summarization dataset MULTI-SUM and further explore the issue of domain shift.", "Here, the introduced communicating protocol claims that each domain should tell others what its updating details (gradients) are. Through its different updating behaviors of different domains can be more consistent.", "Text summarization has been an important research topic due to its widespread applications. Existing research works for summarization mainly revolve around the exploration of neural architectures BIBREF0, BIBREF1 and design of training constraints BIBREF2, BIBREF3. Apart from these, several works try to integrate document characteristics (e.g. domain) to enhance the model performance BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9 or make interpretable analysis towards existing neural summarization models BIBREF10.", "We focus on the testbed that requires both training and evaluating performance on a set of domains. Therefore, we care about two questions: 1) how to learn a model when the training set contains multiple domains \u2013 involving MDL. 2) how to adapt the multi-domain model to new domains \u2013 involving DA. Beyond the investigation of some effective approaches like existing works, we have first verified how domain shift influences the summarization tasks."]}
{"question_id": "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "predicted_answer": "", "predicted_evidence": ["-12pt", "Analysis: This model instructs the processing of multi-domain learning by utilizing external pre-trained knowledge. Another perspective is to address this problem algorithmically.", "We compare our models by ROUGE-1 scores in Table TABREF29. Note that we select two sentences for MULTI-SUM domains and three sentences for CNN/Daily Mail due to the different average lengths of reference summaries.", "We focus on the testbed that requires both training and evaluating performance on a set of domains. Therefore, we care about two questions: 1) how to learn a model when the training set contains multiple domains \u2013 involving MDL. 2) how to adapt the multi-domain model to new domains \u2013 involving DA. Beyond the investigation of some effective approaches like existing works, we have first verified how domain shift influences the summarization tasks.", "In this paper, we extend the concept into the article sources, which can be easily obtained and clearly defined."]}
{"question_id": "0b1cc6c0de286eb724b1fd18dbc93e67ab89a236", "predicted_answer": "", "predicted_evidence": ["The recently proposed dataset Newsroom BIBREF16 is used, which was scraped from 38 major news publications. We select top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time and Mashable) and process them in the way of BIBREF22. To obtain the ground truth labels for extractive summarization task, we follow the greedy approach introduced by BIBREF1. Finally, we randomly divide ten domains into two groups, one for training and the other for test. We call this re-purposed subset of Newsroom MULTI-SUM to indicate it is specially designed for multi-domain learning in summarization tasks.", "", "ROUGE-2 and ROUGE-L show similar trends and their results are attached in Appendix.", "", "Although a domain is often defined by the content category of a text BIBREF17, BIBREF18 or image BIBREF19, the initial motivation for a domain is a metadata attribute which is used in order to divide the data into parts with different distributions BIBREF20."]}
{"question_id": "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "predicted_answer": "", "predicted_evidence": ["This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.", "Formally, given a main domain $A$ and an auxiliary domain $B$, the model will first compute the gradients of A $\\nabla _{\\theta } \\mathcal {L}^{A}$ with regard to the model parameters $\\theta $. Then the model will be updated with the gradients and calculate the gradients of B.", "", "Therefore, the loss function of each domain can be written as:", "Specifically, each domain tag $C^{(k)}$ will be embedded into a low dimensional real-valued vector and then be concatenated with sentence embedding $\\mathbf {s^{(k)}_i}$. The loss function can be formulated as:"]}
{"question_id": "654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "predicted_answer": "", "predicted_evidence": ["Dependency parsing involves the prediction of a directed labeled graph over tokens. Finnish dependency parsing has a long history and several established resources are available for the task.", "We have demonstrated that it is possible to create a language-specific BERT model for a lower-resourced language, Finnish, that clearly outperforms the multilingual BERT at a range of tasks and advances the state of the art in many NLP tasks. These findings raise the question whether it would be possible to realize similar advantages for other languages that currently lack dedicated models of this type. It is likely that the feasibility of training high quality deep transfer learning models hinges on the availability of pretraining data.", "Table TABREF25 summarizes the results for POS tagging. We find that neither M-BERT model improves on the previous state of the art for any of the three resources, with results ranging 0.1-0.8% points below the best previously published results. By contrast, both language-specific models outperform the previous state of the art, with absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points. While these improvements over the already very high reference results are modest in absolute terms, the relative reductions in errors are notable: in particular, the FinBERT cased error rate on FTB is less than half of the best CoNLL'18 result BIBREF22. We also note that the uncased models are surprisingly competitive with their cased equivalents for a task where capitalization has long been an important feature: for example, FinBERT uncased performance is within approx.", "The current transfer learning methods have evolved from word embedding techniques, such as word2vec BIBREF5, GLoVe BIBREF6 and fastText BIBREF7, to take into account the textual context of words. Crucially, incorporating the context avoids the obvious limitations stemming from the one-vector-per-unique-word assumption inherent to the previous word embedding methods. The current successful wave of work proposing and applying different contextualized word embeddings was launched with ELMo BIBREF0, a context embedding method based on bidirectional LSTM networks. Another notable example is the ULMFit model BIBREF8, which specifically focuses on techniques for domain adaptation of LSTM-based language models. Following the introduction of the attention-based (as opposed to recurrent) Transformer architecture BIBREF9, BERT was proposed by BIBREF2, demonstrating superior performance on a broad array of tasks.", "FiNER, a manually annotated NER corpus for Finnish, was recently introduced by ruokolainen2019finnish. The corpus annotations cover five types of named entities \u2013 person, organization, location, product and event \u2013 as well as dates. The primary corpus texts are drawn from a Finnish technology news publication, and it additionally contains an out-of-domain test set of documents drawn from the Finnish Wikipedia. In addition to conventional CoNLL-style named entity annotation, the corpus includes a small number of nested annotations (under 5% of the total). As ruokolainen2019finnish report results also for top-level (non-nested) annotations and the recognition of nested entity mentions would complicate evaluation, we here consider only the top-level annotations of the corpus. Table TABREF26 summarizes the statistics of these annotations."]}
{"question_id": "5a7d1ae6796e09299522ebda7bfcfad312d6d128", "predicted_answer": "", "predicted_evidence": ["The CoNLL 2018 shared task addressed end-to-end parsing from raw text into dependency structures on 82 different corpora representing 57 languages BIBREF28. We evaluate the pre-trained BERT models on the dependency parsing task using the three Finnish UD corpora introduced in Section SECREF27: the Turku Dependency Treebank (TDT), FinnTreeBank (FTB) and the Parallel UD treebank (PUD). To allow direct comparison with CoNLL 2018 results, we use the same versions of the corpora as used in the shared task (UD version 2.2) and evaluate performance using the official script provided by the task organizers. These corpora are the same used in the part-of-speech tagging experiments, and their key statistics were summarized above in Table TABREF17.", "Finally, we explored the ability of the models to capture linguistic properties using the probing tasks proposed by BIBREF46. We use the implementation and Finnish data introduced for these tasks by BIBREF47, which omit the TopConst task defined in the original paper. We also left out the Semantic odd-man-out (SOMO) task, as we found the data to have errors making the task impossible to perform correctly. All of the tasks involve freezing the BERT layers and training a dense layer on top of it to function as a diagnostic classifier. The only information passed from BERT to the classifier is the state represented by the [CLS] token.", "FiNER, a manually annotated NER corpus for Finnish, was recently introduced by ruokolainen2019finnish. The corpus annotations cover five types of named entities \u2013 person, organization, location, product and event \u2013 as well as dates. The primary corpus texts are drawn from a Finnish technology news publication, and it additionally contains an out-of-domain test set of documents drawn from the Finnish Wikipedia. In addition to conventional CoNLL-style named entity annotation, the corpus includes a small number of nested annotations (under 5% of the total). As ruokolainen2019finnish report results also for top-level (non-nested) annotations and the recognition of nested entity mentions would complicate evaluation, we here consider only the top-level annotations of the corpus. Table TABREF26 summarizes the statistics of these annotations.", "The FinBERT models and all of the tools and resources introduced in this paper are available under open licenses from https://turkunlp.org/finbert.", "Finnish lacks the annotated language resources to construct a comprehensive collection of classification tasks such as those available for English BIBREF42, BIBREF43, BIBREF44. To assess model performance at text classification, we create two datasets based on Finnish document collections with topic information, one representing formal language (news) and the other informal (online discussion)."]}
{"question_id": "bd191d95806cee4cf80295e9ce1cd227aba100ab", "predicted_answer": "", "predicted_evidence": ["We compare our results to the best-performing system in the CoNLL 2018 shared task for the LAS metric, HIT-SCIR BIBREF22. In addition to having the highest average score over all treebanks for this metric, the system also achieved the highest LAS among 26 participants for each of the three Finnish treebanks. The dependency parser used in the HIT-SCIR system is the biaffine graph-based parser of BIBREF35 with deep contextualized word embeddings (ELMo) BIBREF36 trained monolingually on web crawl and Wikipedia data provided by BIBREF37. The final HIT-SCIR model is an ensemble over three parser models trained with different parameter initializations, where the final prediction is calculated by averaging the softmaxed output scores.", "We implement the BERT POS tagger straightforwardly by attaching a time-distributed dense output layer over the top layer of BERT and using the first piece of each wordpiece-tokenized input word to represent the word. The implementation and data processing tools are openly available. We compare POS tagging results to the best-performing methods for each corpus in the CoNLL 2018 shared task, namely that of che2018towards for TDT and FTB and lim2018sex for PUD. We report performance for the UPOS metric as implemented by the official CoNLL 2018 evaluation script.", "We have demonstrated that it is possible to create a language-specific BERT model for a lower-resourced language, Finnish, that clearly outperforms the multilingual BERT at a range of tasks and advances the state of the art in many NLP tasks. These findings raise the question whether it would be possible to realize similar advantages for other languages that currently lack dedicated models of this type. It is likely that the feasibility of training high quality deep transfer learning models hinges on the availability of pretraining data.", "The methods we applied to collect and filter texts for training FinBERT have only few language dependencies, such as the use of UD parsing results for filtering. As UD resources are already available for over 70 languages, the specific approach and tools introduced in this work could be readily applied to a large number of languages. To facilitate such efforts, we also make all of the supporting tools developed in this work available under open licenses.", "The cased and uncased variants of FinBERT perform very similarly for both datasets and all training set sizes, while for M-BERT the uncased model consistently outperforms the cased \u2013 as was also found for parsing \u2013 with a marked advantage for small dataset sizes."]}
{"question_id": "a9cae57f494deb0245b40217d699e9a22db0ea6e", "predicted_answer": "", "predicted_evidence": ["which can be solved easily with a Quadratic Programming solver. With learned INLINEFORM0 and INLINEFORM1 , decision is made by determining whether DISPLAYFORM0", "What we want to minimize becomes DISPLAYFORM0", "As our final goal is to learn a generic classifier, which is agnostic to TV series but can predict review's category reasonably, we did experiments following our procedures of building the classifier as discussed in section 1.", "On the other hand, discriminant learning algorithms will estimate INLINEFORM0 directly, or learn some \u201cdiscriminant\u201d function INLINEFORM1 . Then by comparing INLINEFORM2 with some threshold, we can make the final decision. Here we applied two common classifiers logistic regression and support vector machine to classify movie reviews. Logistic regression squeezes the input feature into some interval between 0 and 1 by the sigmoid function, which can be treated as the probability INLINEFORM3 . DISPLAYFORM0", "Based on INLINEFORM0 and DRC discussed in section 3.4, we can sort the importance of each word term. With different feature size, we can train the eight generic classifiers and get their performances on both training and testing set. Here we use SVM as the classifier to compare feature size's influence. Our results suggest that it performs best among the three. The results are shown in Figure FIGREF32 . The red squares represent the training accuracy, while the blue triangles are testing accuracies."]}
{"question_id": "0a736e0e3305a50d771dfc059c7d94b8bd27032e", "predicted_answer": "", "predicted_evidence": ["Besides flourishing of movie/TV series, there are differences of aspect focuses between product and TV series reviews. When a reviewer writes a movie/TV series review, he or she not only care about the TV elements like actor/actress, visual effect, dialogues and music, but also related teams consisted of director, screenwriter, producer, etc. However, with product reviews, few reviewers care about the corresponding backstage teams. What they do care and will comment about are only product related issues like drawbacks of the product functions, or which aspect of the merchandise they like or dislike. Moreover, most of recent researchers' work has been focused on English texts due to its simpler grammatical structure and less vocabulary, as compared with Chinese. Therefore, Chinese movie reviews not only provide more content based information, but also raise more technical challenges. With bloom of Chinese movies, automatic classification of Chinese movie reviews is really essential and meaningful.", "Recall that in classical statistics, INLINEFORM0 is a method designed to measure the independence between two variables or events, which in our case is the word INLINEFORM1 and its relevance to the class INLINEFORM2 . Higher INLINEFORM3 value means higher correlations between them. Therefore, based on the definition of INLINEFORM4 in BIBREF6 and the above Table TABREF13 , we can represent the INLINEFORM5 value as below: DISPLAYFORM0", "These reviews are helpful for both readers and product manufacturers. For example, for TV or movie producers, online reviews indicates the aspects that viewers like and/or dislike. This information facilitates producers' production process. When producing future films TV series, they can tailer their shows to better accommodate consumers' tastes. For manufacturers, reviews may reveal customers' preference and feedback on product functions, which help manufacturers to improve their products in future development. On the other hand, consumers can evaluate the quality of product or TV series based on online reviews, which help them make final decisions of whether to buy or watch it. However, there are thousands of reviews emerging every day. Given the limited time and attention consumers have, it is impossible for them to allocate equal amount of attention to all the reviews. Moreover, some readers may be only interested in certain aspects of a product or TV series. It's been a waste of time to look at other irrelevant ones. As a result, automatic classification of reviews is essential for the review platforms to provide a better perception of the review contents to the users.", "The Maximum A Posteriori of logistic regression with Gaussian priors on parameter INLINEFORM0 is defined as below INLINEFORM1", "To prove the generalization of our classifiers, we use two of the TV series as training data and the rest as testing set. We compare them with classifiers trained without the replacement of generic tags like role_i or actor_j. So 3 sets of experiments are performed, and each are trained on top of Bayes, Logistic Regression and SVM. Average accuracies among them are reported as the performance measure for the sake of space limit. The results are shown in Table TABREF42 . \u201c1\u201d, \u201c2\u201d and \u201c3\u201d represent the TV series \u201cThe Journey of Flower\u201d, \u201cNirvana in Fire\u201d and \u201cGood Time\u201d respectively. In each cell, the left value represents accuracy of classifier without replacement of generic tags and winners are bolded."]}
{"question_id": "283d358606341c399e369f2ba7952cd955326f73", "predicted_answer": "", "predicted_evidence": ["in which, INLINEFORM0 is a smoothing parameter in case there is no training sample for INLINEFORM1 and INLINEFORM2 outputs the number of a set. With all these probabilities computed, we can make decisions by whether DISPLAYFORM0", "What we need is to classify each review into several generic categories that might be attractive to the readers, so classifier selection is also quite important in our problem. Supervised learning takes labeled training pairs and tries to learn an inferred function, which can be used to predict new samples. In this paper, our selection is based on two kinds of learning, i.e., discriminative and generative learning algorithms. And we choose three typical algorithms to compare. Bayes BIBREF7 , which is the representative of generative learning, will output the class with the highest probability that is generated through the bayes' rule. While for the discriminative classifiers like logistic regression BIBREF8 or Support Vector Machine BIBREF9 , final decisions are based on the classifier's output score, which is compared with some threshold to distinguish between different classes.", "On the other hand, discriminant learning algorithms will estimate INLINEFORM0 directly, or learn some \u201cdiscriminant\u201d function INLINEFORM1 . Then by comparing INLINEFORM2 with some threshold, we can make the final decision. Here we applied two common classifiers logistic regression and support vector machine to classify movie reviews. Logistic regression squeezes the input feature into some interval between 0 and 1 by the sigmoid function, which can be treated as the probability INLINEFORM3 . DISPLAYFORM0", "Let INLINEFORM0 be a set of Chinese movie reviews with no categorical information. The ultimate task of movie review classification is to label them into different predefined categories as INLINEFORM1 . Starting from scratch, we need to collect such review set INLINEFORM2 from an online review website and then manually label them into generic categories INLINEFORM3 . Based on the collected dataset, we can apply natural language processing techniques to get raw text features and further learn the classifiers. In the following subsections, we will go through and elaborate all the subtasks shown in Figure FIGREF5 .", "What we want to minimize becomes DISPLAYFORM0"]}
{"question_id": "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "predicted_answer": "", "predicted_evidence": ["Based on the collected reviews, we are ready to build a rough classifier. Before feeding the reviews into a classifier, we applied two common procedures: tokenization and stop words removal for all the reviews. We also applied a common text processing technique to make our reviews more generic. We replaced the roles' and actors/actresses' names in the reviews with some common tokens like role_i, actor_j, where i and j are determined by their importance in this TV series. Therefore, we have the following inference DISPLAYFORM0", "Based on the results from LDA, we carefully defined eight generic categories of movie reviews which are most representative in the dataset as shown in Table TABREF11 .", "where INLINEFORM0 is called the Relevance Correlation Value for word INLINEFORM1 . Because INLINEFORM2 is either 1 or 0, with the notation in the contingency table, RCV can be simplified as DISPLAYFORM0", "We can see that most of the reviews are focused on discussing the roles and analyzing the plots in the movie, i.e., 6th and 7th topics in Figure FIGREF30 , while quite a few are just following the posts, like the 4th and 5th topic in the figure. Based on the findings, we generate the category definition shown in Table TABREF11 . Then 5000 out of each TV series reviews, with no label bias between readers, are selected to make up our final data set.", "Then on top of RCV, they incorporate the probability of the presence of word INLINEFORM0 if we are given that the document is relevant. In this way, our final formula for computing DRC becomes DISPLAYFORM0"]}
{"question_id": "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c", "predicted_answer": "", "predicted_evidence": ["BIBREF30 analyzed the TED Talks for humor detection. BIBREF31 analyzed the transcripts of the TED talks to predict audience engagement in the form of applause. BIBREF32 predicted user interest (engaging vs. non-engaging) from high-level visual features (e.g., camera angles) and audience applause. BIBREF33 proposed a sentiment-aware nearest neighbor model for a multimedia recommendation over the TED talks. BIBREF34 predicted the TED talk ratings from the linguistic features of the transcripts. This work is most similar to ours. However, we are proposing a new prediction framework using the Neural Networks.", "We use two neural network architectures in the prediction task. In the first architecture, we use LSTM BIBREF7 for a sequential input of the words within the sentences of the transcripts. In the second architecture, we use TreeLSTM BIBREF8 to represent the input sentences in the form of a dependency tree. Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings. We compare the performances of these two models with a baseline of classical machine learning techniques using hand-engineered features. We find that the neural networks largely outperform the classical methods.", "For our analysis, we curate an observational dataset of public speech transcripts and other meta-data collected from the ted.com website. This website contains a large collection of high-quality public speeches that are freely available to watch, share, rate, and comment on. Every day, numerous people watch and annotate their perceptions about the talks. Our dataset contains 2231 public speech transcripts and over 5 million ratings from the spontaneous viewers of the talks. The viewers annotate each talk by 14 different labels\u2014Beautiful, Confusing, Courageous, Fascinating, Funny, Informative, Ingenious, Inspiring, Jaw-Dropping, Long-winded, Obnoxious, OK, Persuasive, and Unconvincing.", "Predicting human behavior, however, is challenging due to its huge variability and the way the variables interact with each other. Running Randomized Control Trials (RCT) to decouple each variable is not always feasible and also expensive. It is possible to collect a large amount of observational data due to the advent of content sharing platforms such as YouTube, Massive Open Online Courses (MOOC), or ted.com. However, the uncontrolled variables in the observational dataset always keep a possibility of incorporating the effects of the \u201cdata bias\u201d into the prediction model. Recently, the problems of using biased datasets are becoming apparent. BIBREF3 showed that the error rates in the commercial face-detectors for the dark-skinned females are 43 times higher than the light-skinned males due to the bias in the training dataset. The unfortunate incident of Google's photo app tagging African-American people as \u201cGorilla\u201d BIBREF4 also highlights the severity of this issue.", "We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc."]}
{"question_id": "bba70f3cf4ca1e0bb8c4821e3339c655cdf515d6", "predicted_answer": "", "predicted_evidence": ["BIBREF30 analyzed the TED Talks for humor detection. BIBREF31 analyzed the transcripts of the TED talks to predict audience engagement in the form of applause. BIBREF32 predicted user interest (engaging vs. non-engaging) from high-level visual features (e.g., camera angles) and audience applause. BIBREF33 proposed a sentiment-aware nearest neighbor model for a multimedia recommendation over the TED talks. BIBREF34 predicted the TED talk ratings from the linguistic features of the transcripts. This work is most similar to ours. However, we are proposing a new prediction framework using the Neural Networks.", "An example of human behavioral prediction research is to automatically grade essays, which has a long history BIBREF9 . Recently, the use of deep neural network based solutions BIBREF10 , BIBREF11 are becoming popular in this field. BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques.", "An example of human behavioral prediction research is to automatically grade essays, which has a long history BIBREF9 . Recently, the use of deep neural network based solutions BIBREF10 , BIBREF11 are becoming popular in this field. BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc.", "BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc. However, the practice of explicitly modeling the data generating process is relatively uncommon.", "While the demand for physical and manual labor is gradually declining, there is a growing need for a workforce with soft skills. Which soft skill do you think would be the most valuable in your daily life? According to an article in Forbes BIBREF0 , 70% of employed Americans agree that public speaking skills are critical to their success at work. Yet, it is one of the most dreaded acts. Many people rate the fear of public speaking even higher than the fear of death BIBREF1 . To alleviate the situation, several automated systems are now available that can quantify behavioral data for participants to reflect on BIBREF2 . Predicting the viewers' ratings from the speech transcripts would enable these systems to generate feedback on the potential audience behavior."]}
{"question_id": "c5f9894397b1a0bf6479f5fd9ee7ef3e38cfd607", "predicted_answer": "", "predicted_evidence": ["In this section, we describe a few relevant prior arts on behavioral prediction.", "Predicting human behavior, however, is challenging due to its huge variability and the way the variables interact with each other. Running Randomized Control Trials (RCT) to decouple each variable is not always feasible and also expensive. It is possible to collect a large amount of observational data due to the advent of content sharing platforms such as YouTube, Massive Open Online Courses (MOOC), or ted.com. However, the uncontrolled variables in the observational dataset always keep a possibility of incorporating the effects of the \u201cdata bias\u201d into the prediction model. Recently, the problems of using biased datasets are becoming apparent. BIBREF3 showed that the error rates in the commercial face-detectors for the dark-skinned females are 43 times higher than the light-skinned males due to the bias in the training dataset. The unfortunate incident of Google's photo app tagging African-American people as \u201cGorilla\u201d BIBREF4 also highlights the severity of this issue.", "An example of human behavioral prediction research is to automatically grade essays, which has a long history BIBREF9 . Recently, the use of deep neural network based solutions BIBREF10 , BIBREF11 are becoming popular in this field. BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques.", "There is a limited amount of work on predicting the TED talk ratings. In most cases, TED talk performances are analyzed through introspection BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 .", "We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc."]}
{"question_id": "9f8c0e02a7a8e9ee69f4c1757817cde85c7944bd", "predicted_answer": "", "predicted_evidence": ["We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc.", "The data for this study was gathered from the ted.com website on November 15, 2017. We removed the talks published six months before the crawling date to make sure each talk has enough ratings for a robust analysis. More specifically, we filtered any talk that\u2014", "An example of human behavioral prediction research is to automatically grade essays, which has a long history BIBREF9 . Recently, the use of deep neural network based solutions BIBREF10 , BIBREF11 are becoming popular in this field. BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques.", "An example of human behavioral prediction research is to automatically grade essays, which has a long history BIBREF9 . Recently, the use of deep neural network based solutions BIBREF10 , BIBREF11 are becoming popular in this field. BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc.", "Predicting human behavior, however, is challenging due to its huge variability and the way the variables interact with each other. Running Randomized Control Trials (RCT) to decouple each variable is not always feasible and also expensive. It is possible to collect a large amount of observational data due to the advent of content sharing platforms such as YouTube, Massive Open Online Courses (MOOC), or ted.com. However, the uncontrolled variables in the observational dataset always keep a possibility of incorporating the effects of the \u201cdata bias\u201d into the prediction model. Recently, the problems of using biased datasets are becoming apparent. BIBREF3 showed that the error rates in the commercial face-detectors for the dark-skinned females are 43 times higher than the light-skinned males due to the bias in the training dataset. The unfortunate incident of Google's photo app tagging African-American people as \u201cGorilla\u201d BIBREF4 also highlights the severity of this issue."]}
{"question_id": "6cbbedb34da50286f44a0f3f6312346e876e2be5", "predicted_answer": "", "predicted_evidence": ["BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc. However, the practice of explicitly modeling the data generating process is relatively uncommon.", "For our analysis, we curate an observational dataset of public speech transcripts and other meta-data collected from the ted.com website. This website contains a large collection of high-quality public speeches that are freely available to watch, share, rate, and comment on. Every day, numerous people watch and annotate their perceptions about the talks. Our dataset contains 2231 public speech transcripts and over 5 million ratings from the spontaneous viewers of the talks. The viewers annotate each talk by 14 different labels\u2014Beautiful, Confusing, Courageous, Fascinating, Funny, Informative, Ingenious, Inspiring, Jaw-Dropping, Long-winded, Obnoxious, OK, Persuasive, and Unconvincing.", "The data for this study was gathered from the ted.com website on November 15, 2017. We removed the talks published six months before the crawling date to make sure each talk has enough ratings for a robust analysis. More specifically, we filtered any talk that\u2014", "BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc. However, the practice of explicitly modeling the data generating process is relatively uncommon. In this paper, we expand the prior work by explicitly modeling the data generating process in order to remove the data bias.", "We use two neural network architectures in the prediction task. In the first architecture, we use LSTM BIBREF7 for a sequential input of the words within the sentences of the transcripts. In the second architecture, we use TreeLSTM BIBREF8 to represent the input sentences in the form of a dependency tree. Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings. We compare the performances of these two models with a baseline of classical machine learning techniques using hand-engineered features. We find that the neural networks largely outperform the classical methods. We believe this gain in performance is achieved by the networks' ability to capture better the natural relationship of the words (as compared to the hand engineered feature selection approach in the baseline methods) and the correlations among different rating labels."]}
{"question_id": "173060673cb15910cc310058bbb9750614abda52", "predicted_answer": "", "predicted_evidence": ["BIBREF12 proposed an adversarial approach for their task. BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc. However, the practice of explicitly modeling the data generating process is relatively uncommon.", "There is a limited amount of work on predicting the TED talk ratings. In most cases, TED talk performances are analyzed through introspection BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 .", "The data for this study was gathered from the ted.com website on November 15, 2017. We removed the talks published six months before the crawling date to make sure each talk has enough ratings for a robust analysis. More specifically, we filtered any talk that\u2014", "BIBREF30 analyzed the TED Talks for humor detection. BIBREF31 analyzed the transcripts of the TED talks to predict audience engagement in the form of applause. BIBREF32 predicted user interest (engaging vs. non-engaging) from high-level visual features (e.g., camera angles) and audience applause. BIBREF33 proposed a sentiment-aware nearest neighbor model for a multimedia recommendation over the TED talks. BIBREF34 predicted the TED talk ratings from the linguistic features of the transcripts. This work is most similar to ours. However, we are proposing a new prediction framework using the Neural Networks.", "While the demand for physical and manual labor is gradually declining, there is a growing need for a workforce with soft skills. Which soft skill do you think would be the most valuable in your daily life? According to an article in Forbes BIBREF0 , 70% of employed Americans agree that public speaking skills are critical to their success at work. Yet, it is one of the most dreaded acts. Many people rate the fear of public speaking even higher than the fear of death BIBREF1 . To alleviate the situation, several automated systems are now available that can quantify behavioral data for participants to reflect on BIBREF2 . Predicting the viewers' ratings from the speech transcripts would enable these systems to generate feedback on the potential audience behavior."]}
{"question_id": "98c8ed9019e43839ffb53a714bc37fbb1c28fe2c", "predicted_answer": "", "predicted_evidence": ["BIBREF13 proposed a two-stage deep neural network based solution. Predicting helpfulness BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 in the online reviews is another example of predicting human behavior. BIBREF18 proposed a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based framework to predict humor in the dialogues. Their method achieved an 8% improvement over a Conditional Random Field baseline. BIBREF19 analyzed the performance of phonological pun detection using various natural language processing techniques. In general, behavioral prediction encompasses numerous areas such as predicting outcomes in job interviews BIBREF20 , hirability BIBREF21 , presentation performance BIBREF22 , BIBREF23 , BIBREF24 etc. However, the practice of explicitly modeling the data generating process is relatively uncommon. In this paper, we expand the prior work by explicitly modeling the data generating process in order to remove the data bias.", "There is a limited amount of work on predicting the TED talk ratings. In most cases, TED talk performances are analyzed through introspection BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 .", "The data for this study was gathered from the ted.com website on November 15, 2017. We removed the talks published six months before the crawling date to make sure each talk has enough ratings for a robust analysis. More specifically, we filtered any talk that\u2014", "BIBREF30 analyzed the TED Talks for humor detection. BIBREF31 analyzed the transcripts of the TED talks to predict audience engagement in the form of applause. BIBREF32 predicted user interest (engaging vs. non-engaging) from high-level visual features (e.g., camera angles) and audience applause. BIBREF33 proposed a sentiment-aware nearest neighbor model for a multimedia recommendation over the TED talks. BIBREF34 predicted the TED talk ratings from the linguistic features of the transcripts. This work is most similar to ours. However, we are proposing a new prediction framework using the Neural Networks.", "While the demand for physical and manual labor is gradually declining, there is a growing need for a workforce with soft skills. Which soft skill do you think would be the most valuable in your daily life? According to an article in Forbes BIBREF0 , 70% of employed Americans agree that public speaking skills are critical to their success at work. Yet, it is one of the most dreaded acts. Many people rate the fear of public speaking even higher than the fear of death BIBREF1 . To alleviate the situation, several automated systems are now available that can quantify behavioral data for participants to reflect on BIBREF2 . Predicting the viewers' ratings from the speech transcripts would enable these systems to generate feedback on the potential audience behavior."]}
{"question_id": "50c441a9cc7345a0fa408d1ce2e13f194c1e82a8", "predicted_answer": "", "predicted_evidence": ["Several other works have focused on creating more engaging responses by producing affective responses. One of the earlier works to incorporate affect through language modeling is the work done by Ghosh et al. BIBREF8. This work leverages the LIWC BIBREF33 text analysis platform for affective features. Alternative approaches of inducing emotion in generated responses from a seq2seq framework include the work done by Zhou et alBIBREF6 that uses internal and external memory, Asghar et al. BIBREF5 that models emotion through affective embeddings and Huang et al BIBREF7 that induce emotion through concatenation with input sequence. More recently, introduction of transformer based approaches have helped advance the state of art across several natural language understanding tasks BIBREF26.", "Researchers have used methods such as BLEU , METEOR BIBREF17, ROUGE BIBREF18 from machine translation and text summarization BIBREF19 tasks. BLEU and METEOR are based on word overlap between the proposed and ground truth responses; they do not adequately account for the diversity of responses that are possible for a given input utterance and show little to no correlation with human judgments BIBREF19. We report on the BLEU BIBREF20 and Perplexity (PPL) metric to provide a comparison with the current state-of-the-art methods. We also report our performance using other metrics such as length of responses produced by the model. Following, Mei et al BIBREF21, we also report the diversity metric that helps us measure the ability of the model to promote diversity in responses BIBREF22. Diversity is calculated as the as the number of distinct unigrams in the generation scaled by the total number of generated tokens BIBREF21, BIBREF1.", "Table TABREF15 shows the results obtained from the human evaluation comparing the performance of our fine-tuned, emotion pre-pend model to the ground-truth response. We find that our fine-tuned model outperforms the emo-prepend on all three metrics from the ratings provided by the human ratings.", "We report on the BLEU BIBREF20 and Perplexity (PPL) metric to provide a comparison with the current state-of-the-art methods. We also report our performance using other metrics such as length of responses produced by the model. Following, Mei et al BIBREF21, we also report the diversity metric that helps us measure the ability of the model to promote diversity in responses BIBREF22. Diversity is calculated as the as the number of distinct unigrams in the generation scaled by the total number of generated tokens BIBREF21, BIBREF1. We report on two additional automated metrics of readability and coherence. Readability quantifies the linguistic quality of text and the difficulty of the reader in understanding the text BIBREF23. We measure readability through the Flesch Reading Ease (FRE) BIBREF24 which computes the number of words, syllables and sentences in the text. Higher readability scores indicate that utterance is easier to read and comprehend.", "Readability - Is the response easy to understand, fluent and grammatical and does not have any consecutive repeating words."]}
{"question_id": "2895a3fc63f6f403445c11043460584e949fb16c", "predicted_answer": "", "predicted_evidence": ["Thus, we have DISPLAYFORM0", "Many typical verb representations, including FrameNet BIBREF3 , PropBank BIBREF4 , and VerbNet BIBREF5 , describe verbs' semantic roles (e.g. ingestor and ingestibles for \u201ceat\u201d). However, semantic roles in general are too coarse to differentiate a verb's fine-grained semantics. A verb in different phrases can have different semantics but similar roles. In Example SECREF1 , both \u201ceat\u201ds in \u201ceat breakfast\u201d and \u201ceat apple\u201d have ingestor. But they have different semantics.", "Thus, our problem is how to generate conceptualized patterns and idiom patterns for verbs. We use two public data sets for this purpose: Google Syntactic N-Grams (http://commondatastorage.googleapis.com/books/syntactic -ngrams/index.html) and Probase BIBREF7 . Google Syntactic N-grams contains millions of verb phrases, which allows us to mine rich patterns for verbs. Probase contains rich concepts for instances, which enables the conceptualization for objects. Thus, our problem is given a verb INLINEFORM0 and a set of its phrases, generating a set of patterns (either conceptualized patterns or idiom patterns) for INLINEFORM1 . However, the pattern generation for verbs is non-trivial. In general, the most critical challenge we face is the trade-off between generality and specificity of the generated patterns, as explained below.", "Example 2 In Fig FIGREF9 , (eat $ INLINEFORM0 meal) is obviously better than the three patterns (eat $ INLINEFORM1 breakfast + eat $ INLINEFORM2 lunch+ eat $ INLINEFORM3 dinner). The former case provides a more general representation.", "The unawareness of verbs' polysemy makes traditional verb representations unable to fully understand the verb in some applications. In sentence I like eating pitaya, people directly know \u201cpitaya\u201d is probably one kind of food since eating a food is the most fundamental semantic of \u201ceat\u201d. This enables context-aware conceptualization of pitaya to food concept. But by only knowing pitaya's role is the \u201cingestibles\u201d, traditional representations cannot tell if pitaya is a food or a meal."]}
{"question_id": "1e7e3f0f760cd628f698b73d82c0f946707855ca", "predicted_answer": "", "predicted_evidence": ["Idiomatic Baseline (IB) We treat each verb phrase as a idiom.", "In this section, we propose an algorithm based on simulated annealing to solve Problem SECREF21 . We also show how we use external knowledge to optimize the idiom patterns.", "Now we give an overview of our extracted verb patterns. For all 22,230 verbs, we report the statistics for the top 100 verbs of the highest frequency. After filtering noisy phrases with INLINEFORM0 , each verb has 171 distinct phrases and 97.2 distinct patterns on average. 53% phrases have conceptualized patterns. 47% phrases have idiom patterns. In Table TABREF48 , we list 5 typical verbs and their top patterns. The case study verified that (1) our definition of verb pattern reflects verb's polysemy; (2) most verb patterns we found are meaningful.", "Verb Phrase Data The pattern assignment uses the phrase distribution INLINEFORM0 . To do this, we use the \u201cEnglish All\u201d dataset in Google Syntactic N-Grams. The dataset contains counted syntactic ngrams extracted from the English portion of the Google Books corpus. It contains 22,230 different verbs (without stemming), and 147,056 verb phrases. For a fixed verb, we compute the probability of phrase INLINEFORM1 by: DISPLAYFORM0", ",where INLINEFORM0 is the number of phrases in the test data for which our solution finds corresponding patterns, INLINEFORM1 is the total number of phrases, INLINEFORM2 is the number of phrases whose corresponding patterns are correct. To evaluate INLINEFORM3 , we randomly selected 100 verb phrases from the test data and ask volunteers to label the correctness of their assigned patterns. We regard a phrase-pattern matching is incorrect if it's either too specific or too general (see examples in Fig FIGREF9 ). For comparison, we also tested two baselines for pattern summarization:"]}
{"question_id": "64632981279c7aa16ffc1a44ffc31f4520f5559e", "predicted_answer": "", "predicted_evidence": ["Verb is crucial in sentence understanding BIBREF0 , BIBREF1 . A major issue of verb understanding is polysemy BIBREF2 , which means that a verb has different semantics or senses when collocating with different objects. In this paper, we only focus on verbs that collocate with objects. As illustrated in Example SECREF1 , most verbs are polysemous. Hence, a good semantic representation of verbs should be aware of their polysemy.", "Now we give an overview of our extracted verb patterns. For all 22,230 verbs, we report the statistics for the top 100 verbs of the highest frequency. After filtering noisy phrases with INLINEFORM0 , each verb has 171 distinct phrases and 97.2 distinct patterns on average. 53% phrases have conceptualized patterns. 47% phrases have idiom patterns. In Table TABREF48 , we list 5 typical verbs and their top patterns. The case study verified that (1) our definition of verb pattern reflects verb's polysemy; (2) most verb patterns we found are meaningful.", "We adopted a simulated annealing (SA) algorithm to compute the best pattern assignment INLINEFORM0 . The algorithm proceeds as follows. We first pick a random assignment as the initialization (initial temperature). Then, we generate a new assignment and evaluate it. If it is a better assignment, we replace the previous assignment with it; otherwise we accept it with a certain probability (temperature reduction). The generation and replacement step are repeated until no change occurs in the last INLINEFORM1 iterations (termination condition).", "Many typical verb representations, including FrameNet BIBREF3 , PropBank BIBREF4 , and VerbNet BIBREF5 , describe verbs' semantic roles (e.g. ingestor and ingestibles for \u201ceat\u201d). However, semantic roles in general are too coarse to differentiate a verb's fine-grained semantics. A verb in different phrases can have different semantics but similar roles. In Example SECREF1 , both \u201ceat\u201ds in \u201ceat breakfast\u201d and \u201ceat apple\u201d have ingestor. But they have different semantics.", "We conducted extensive experiments. The results verify the effectiveness of our model and algorithm. We presented the applications of verb patterns in context-aware conceptualization. The application justifies the effectiveness of verb patterns to represent verb semantics."]}
{"question_id": "deed225dfa94120fafcc522d4bfd9ea57085ef8d", "predicted_answer": "", "predicted_evidence": ["We collected millions of English-language tweets from different times, dates, authors and US states. We used a total of six emoticons, three mapping to positive and three mapping to negative sentiment (table TABREF7 ). We identified more than 120 positive and negative ASCII emoticons and unicode emojis, but we decided to only use the six most common emoticons in order to avoid possible selection biases. For example, people who use obscure emoticons and emojis might have a different base sentiment from those who do not. Using the six most commonly used emoticons limits this bias. Since there are no \"neutral\" emoticons, our dataset is limited to tweets with positive or negative sentiments. Accordingly, in this work we are only concerned with analysing and classifying the polarity of tweets (negative vs. positive) and not their subjectivity (neutral vs. non-neutral). Below we will explain our data collection and corpus in greater detail.", "We considered five contextual categories: one spatial, three temporal and one authorial. Here is the list of the five categories:", "This is our purely linguistic baseline model.", "As it is not feasible to show the prior average sentiment of all INLINEFORM0 users, we created 20 even sentiment bins, from INLINEFORM1 to INLINEFORM2 . We then plotted the number of users whose average sentiment falls into these bins (Figure FIGREF34 ). Similar to other variables, the positive end of the graph is much heavier than the negative end.", "There has also been previous work on measuring the happiness of people in different contexts (location, time, etc). This has been done mostly through traditional land-line polling BIBREF5 , BIBREF4 , with Gallup's annual happiness index being a prime example BIBREF4 . More recently, some have utilized Twitter to measure people's mood and happiness and have found Twitter to be a generally good measure of the public's overall happiness, well-being and mood. For example, Bollen et al. BIBREF15 used Twitter to measure the daily mood of the public and compare that to the record of social, political, cultural and economic events in the real world. They found that these events have a significant effect on the public mood as measured through Twitter. Another example would be the work of Mitchell et al. BIBREF16 , in which they estimated the happiness levels of different states and cities in the USA using Twitter and found statistically significant correlations between happiness level and the demographic characteristics (such as obesity rates and education levels) of those regions."]}
{"question_id": "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd", "predicted_answer": "", "predicted_evidence": ["It is interesting to note that even with the noisy dataset, our ranking of US states based on their Twitter sentiment correlates with the ranking of US states based on the well-being index calculated by Oswald and Wu BIBREF25 in their work on measuring well-being and life satisfaction across America. Their data is from the behavioral risk factor survey score (BRFSS), which is a survey of life satisfaction across the United States from INLINEFORM0 million citizens. Figure FIGREF27 shows this correlation ( INLINEFORM1 , INLINEFORM2 ).", "As our baseline model, we built purely linguistic bigram models in Python, utilizing some components from NLTK BIBREF22 . These models used a vocabulary that was filtered to remove words occurring 5 or fewer times. Probability distributions were calculated using Kneser-Ney smoothing BIBREF23 . In addition to Kneser-Ney smoothing, the bigram models also used \u201cbackoff\u201d smoothing BIBREF24 , in which an n-gram model falls back on an INLINEFORM0 -gram model for words that were unobserved in the n-gram context.", "There have been several works that do sentiment classification on Twitter using standard sentiment classification techniques, with variations of n-gram and bag of words being the most common. There have been attempts at using more advanced syntactic features as is done in sentiment classification for other domains BIBREF1 , BIBREF2 , however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form BIBREF3 .", "Because of the great increase in the volume of data, distant supervised sentiment classifiers for Twitter tend to generally outperform more standard classifiers using human-labelled datasets. Therefore, it makes sense to compare the performance of our classifier to other distant supervised classifiers. Though not directly comparable, our contextual classifier outperforms the distant supervised Twitter sentiment classifier by Go et al BIBREF6 by more than INLINEFORM0 (absolute).", "We considered five contextual categories: one spatial, three temporal and one authorial. Here is the list of the five categories:"]}
{"question_id": "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "predicted_answer": "", "predicted_evidence": ["There are different methods of obtaining labelled data using distant supervision BIBREF1 , BIBREF6 , BIBREF19 , BIBREF12 . We used emoticons to label tweets as positive or negative, an approach that was introduced by Read BIBREF1 and used in multiple works BIBREF6 , BIBREF12 . We collected millions of English-language tweets from different times, dates, authors and US states. We used a total of six emoticons, three mapping to positive and three mapping to negative sentiment (table TABREF7 ). We identified more than 120 positive and negative ASCII emoticons and unicode emojis, but we decided to only use the six most common emoticons in order to avoid possible selection biases. For example, people who use obscure emoticons and emojis might have a different base sentiment from those who do not. Using the six most commonly used emoticons limits this bias.", "We considered five contextual categories: one spatial, three temporal and one authorial. Here is the list of the five categories:", "We then measured the agreement between the human labels and emoticon-based labels, using only tweets that were labelled the same by at least two of the three human annotators (the majority label was used as the label for the tweet). Table TABREF13 shows the confusion matrix between human and emoticon-based annotations. As you can see, INLINEFORM0 of all labels matched ( INLINEFORM1 ).", "We looked at three temporal variables: time of day, day of the week and month. All tweets are tagged with timestamp data, which we used to extract these three variables. Since all timestamps in the Twitter historical archives (and public API) are in the UTC time zone, we first converted the timestamp to the local time of the location where the tweet was sent from. We then calculated the sentiment for each day of week (figure FIGREF29 ), hour (figure FIGREF30 ) and month (figure FIGREF31 ), averaged across all 18 million tweets over three years. The 18 million tweets were divided evenly between each month, with INLINEFORM0 million tweets per month. The tweets were also more or less evenly divided between each day of week, with each day having somewhere between INLINEFORM1 and INLINEFORM2 of the tweets. Similarly, the tweets were almost evenly divided between each hour, with each having somewhere between INLINEFORM3 and INLINEFORM4 of the tweets.", "We would like to thank all the annotators for their efforts. We would also like to thank Brandon Roy for sharing his insights on Bayesian modelling. This work was supported by a generous grant from Twitter."]}
{"question_id": "242c626e89bca648b65af135caaa7ceae74e9720", "predicted_answer": "", "predicted_evidence": ["We would like to thank all the annotators for their efforts. We would also like to thank Brandon Roy for sharing his insights on Bayesian modelling. This work was supported by a generous grant from Twitter.", "Note that the prior probabilities that we calculated need to be recalculated and updated every once in a while to account for changes in the world. For example, a state might become more affluent, causing its citizens to become on average happier. This change could potentially have an effect on the average sentiment expressed by the citizens of that state on Twitter, which would make our priors obsolete.", "The Bayesian approach allows us to easily integrate the contextual information into our models. INLINEFORM0 in Equation EQREF16 is the prior probability of a tweet having the sentiment INLINEFORM1 . The prior probability ( INLINEFORM2 ) can be calculated using the contextual information of the tweets. Therefore, INLINEFORM3 in equation EQREF16 is replaced by INLINEFORM4 , which is the probability of the hypothesis given the contextual information. INLINEFORM5 is the posterior probability of the following Bayesian equation: DISPLAYFORM0", "There has also been previous work on measuring the happiness of people in different contexts (location, time, etc). This has been done mostly through traditional land-line polling BIBREF5 , BIBREF4 , with Gallup's annual happiness index being a prime example BIBREF4 . More recently, some have utilized Twitter to measure people's mood and happiness and have found Twitter to be a generally good measure of the public's overall happiness, well-being and mood. For example, Bollen et al. BIBREF15 used Twitter to measure the daily mood of the public and compare that to the record of social, political, cultural and economic events in the real world. They found that these events have a significant effect on the public mood as measured through Twitter. Another example would be the work of Mitchell et al.", "Here INLINEFORM0 can be INLINEFORM1 or INLINEFORM2 , corresponding to the hypothesis that the sentiment of the tweet is positive or negative respectively. INLINEFORM3 is the sequence of INLINEFORM4 words, written as INLINEFORM5 , that make up the tweet. INLINEFORM6 is not dependent on the hypothesis, and can thus be ignored. Since we are using a bigram model, Equation EQREF15 can be written as: DISPLAYFORM0"]}
{"question_id": "bba677d1a1fe38a41f61274648b386bdb44f1851", "predicted_answer": "", "predicted_evidence": ["Where INLINEFORM0 is the set of contextual variables: INLINEFORM1 . INLINEFORM2 captures the probability that a tweet is positive or negative, given the state, hour of day, day of the week, month and author of the tweet. Here INLINEFORM3 is not dependent on the hypothesis, and thus can be ignored. Equation EQREF16 can therefore be rewritten to include the contextual information: DISPLAYFORM0", "We would like to thank all the annotators for their efforts. We would also like to thank Brandon Roy for sharing his insights on Bayesian modelling. This work was supported by a generous grant from Twitter.", "We considered five contextual categories: one spatial, three temporal and one authorial. Here is the list of the five categories:", "We collected a total of 18 million, geo-tagged, English-language tweets over three years, from January 1st, 2012 to January 1st, 2015, evenly divided across all 36 months, using Historical PowerTrack for Twitter provided by GNIP. We created geolocation bounding boxes for each of the 50 states which were used to collect our dataset. All 18 million tweets originated from one of the 50 states and are tagged as such. Moreover, all tweets contained one of the six emoticons in Table TABREF7 and were labelled as either positive or negative based on the emoticon. Out of the 18 million tweets, INLINEFORM0 million ( INLINEFORM1 ) were labelled as positive and INLINEFORM2 million ( INLINEFORM3 ) were labelled as negative. The 18 million tweets came from INLINEFORM4 distinct users.", "Note that the prior probabilities that we calculated need to be recalculated and updated every once in a while to account for changes in the world. For example, a state might become more affluent, causing its citizens to become on average happier. This change could potentially have an effect on the average sentiment expressed by the citizens of that state on Twitter, which would make our priors obsolete."]}
{"question_id": "b6c2a391c4a94eaa768150f151040bb67872c0bf", "predicted_answer": "", "predicted_evidence": ["We would like to thank all the annotators for their efforts. We would also like to thank Brandon Roy for sharing his insights on Bayesian modelling. This work was supported by a generous grant from Twitter.", "Note that the prior probabilities that we calculated need to be recalculated and updated every once in a while to account for changes in the world. For example, a state might become more affluent, causing its citizens to become on average happier. This change could potentially have an effect on the average sentiment expressed by the citizens of that state on Twitter, which would make our priors obsolete.", "There have been several works that do sentiment classification on Twitter using standard sentiment classification techniques, with variations of n-gram and bag of words being the most common. There have been attempts at using more advanced syntactic features as is done in sentiment classification for other domains BIBREF1 , BIBREF2 , however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form BIBREF3 .", "We used emoticons to label tweets as positive or negative, an approach that was introduced by Read BIBREF1 and used in multiple works BIBREF6 , BIBREF12 . We collected millions of English-language tweets from different times, dates, authors and US states. We used a total of six emoticons, three mapping to positive and three mapping to negative sentiment (table TABREF7 ). We identified more than 120 positive and negative ASCII emoticons and unicode emojis, but we decided to only use the six most common emoticons in order to avoid possible selection biases. For example, people who use obscure emoticons and emojis might have a different base sentiment from those who do not. Using the six most commonly used emoticons limits this bias. Since there are no \"neutral\" emoticons, our dataset is limited to tweets with positive or negative sentiments. Accordingly, in this work we are only concerned with analysing and classifying the polarity of tweets (negative vs.", "Even though our contextual classifier was able to outperform the previous state-of-the-art, distant supervised sentiment classifier, it should be noted that our contextual classifier's performance is boosted significantly by spatial information extracted through geo-tags. However, only about one to two percent of tweets in the wild are geo-tagged. Therefore, we trained and evaluated our contextual model using all the variables except for state. The accuracy of this model was INLINEFORM0 , which is still significantly better than the performance of the purely linguistic classifier. Fortunately, all tweets are tagged with timestamps and author information, so all the other four contextual variables used in our model can be used for classifying the sentiment of any tweet."]}
{"question_id": "06d5de706348dbe8c29bfacb68ce65a2c55d0391", "predicted_answer": "", "predicted_evidence": ["Bigrams: {(I like), (I kitties), (I and), (like kitties), (like and), (like doggies), (kitties and), (kitties doggies), (and doggies)}.", "Text: \u201cI like kitties and doggies\u201d", "We propose an alternative. As before, store the frequencies of words and the frequencies of bigrams, but this time store two additional maps called too_far_left and too_far_right, of the form {word : list of offending indices of word}. The offending indices are those that are either too far to the left or too far to the right for approximation ( 1 ) to hold. All four of these structures are built during the construction of a bigram finder, and do not cripple performance when computing statistical measures since maps are queried in $O(1)$ time.", "Text: \u201cI like kitties and doggies\u201d", "Bigrams: {(I like), (like kitties), (kitties and), (and doggies)} and this one:"]}
{"question_id": "6014c2219d29bae17279625716e7c2a1f8a2bd05", "predicted_answer": "", "predicted_evidence": ["This short note is the result of a brief conversation between the authors and Joel Nothman. We came across a potential problem, he gave a sketch of a fix, and we worked out the details of a solution.", "Window: 4", "An efficient method for computing the contingency matrix for a bigram (word1, word2) is suggested by the approximation. Store $freq(w1, w2)$ for all bigrams $(w1, w2)$ and the frequencies of all words. Then,", "where $w$ is the window size being searched for bigrams, $wfd$ is a frequency distribution of all words in the corpus, $tfl$ is the map too_far_left and $N$ is the number of occurrences of the $word$ in a position too far left.The computation of $freq(word, *)$ can now be performed in the same way by simply substituting $tfl$ with $tfr$ thanks to transformation $g$ , which reverses the indexing.", "Bigrams: {(I like), (like kitties), (kitties and), (and doggies)} and this one:"]}
{"question_id": "9be9354eeb2bb1827eeb1e23a20cfdca59fb349a", "predicted_answer": "", "predicted_evidence": ["To achieve that, let $Relevance$ $(NewP, f_j)$ be the relevance of $f_j$ for $NewP$ based upon a policy, which can be estimated in different ways including following:", "For each $App_i\\in Bind\\left(f_j\\right)$: Add $x$ to $Ch[App_i]$, where $x$ is estimated as follows: If user provides explicit relevance scores for $App_i$,", "Rank feature specifications in $\\mathit {\\Theta }_F$ decreasing order based upon $Relevance(NewP,.)$, which are suggested to the NLP Data Scientist together with the supporting evidence.", "Figure FIGREF23 depicts overall process flow for enabling automated feature recommendations.", "After representing different fields of an application into embedding space (except AU), estimate field level similarity between two applications as follows: Let $[X_i^{en}$, $X^{act}_i$, $X_i^{r}]$ and $[X_j^{en}$, $X^{act}_j$, $X_j^{r}]$ be the representations for field $f$ for two applications $App_i$, $App_j$ $\\in APPS$."]}
{"question_id": "5d5c25d68988fa5effe546507c66997785070573", "predicted_answer": "", "predicted_evidence": ["In relation to $NewP$, let $Rank:{\\mathrm {\\Theta }}_F\\times \\left\\lbrace system,user\\right\\rbrace \\rightarrow \\lbrace 1,\\dots ,k\\rbrace ~$ return rank of a feature and $Rel:{\\mathrm {\\Theta }}_F\\times \\left\\lbrace system,user\\right\\rbrace \\rightarrow [0,1]$ return relevance score of a feature based upon type \u2013 `system' or `user'.", "For each feature specification $f\\in \\Theta _F$, determine whether `user' given rank is different from `system' given rank, i.e., $Rank(f,`system^{\\prime }) \\ne Rank(f,`user^{\\prime })$. If so, execute steps next.", "Towards that, we analysed published works on three different types of events in different domains as described next:", "Figure FIGREF1 depicts typical design life cycle of a (traditional) ML based solution for the TA applications, which involves steps to manually define relevant features and implement code components to extract those feature from input text corpus during training, validation, testing and actual usage of the application. In traditional ML based solutions, feature interactions also need to be explicitly specified, though this step is largely automated when using deep neural network based solutions BIBREF9.", "After representing different fields of an application into embedding space (except AU), estimate field level similarity between two applications as follows: Let $[X_i^{en}$, $X^{act}_i$, $X_i^{r}]$ and $[X_j^{en}$, $X^{act}_j$, $X_j^{r}]$ be the representations for field $f$ for two applications $App_i$, $App_j$ $\\in APPS$. In terms of these, field level similarity is estimated as $\\Delta _{f}(App_i,App_j)$ = $[\\Delta _{en}({f_{i}, f_j})$, $\\Delta _{act}({f_{i}, f_j})$, $\\Delta _{r}({f_{i}, f_j})]$, where $\\Delta _{en}({f_{i}, f_j})$ = 0 if field level details of either of the applications is unavailable else $\\Delta _{en}({f_{i}, f_j})$ = $cosine(X_i^{en}$, $X_j^{en})$; etc."]}
{"question_id": "ca595151735444b5b30a003ee7f3a7eb36917208", "predicted_answer": "", "predicted_evidence": ["For each $App_i\\in Bind\\left(f_j\\right)$: $NewSim_{FE}\\left[App_i\\right]\\leftarrow Average(Ch[App_i])$. If $|NewSim_{FE}[App_i]$-${\\alpha }_i|$ $\\ge \\epsilon {\\alpha }_i$ i.e., when the difference between old and new similarity scores is more than $\\epsilon $ fraction of original similarity, add $(\\Delta (NewP,App_i),NewSim_{FE}[App_i])$ to training set $Tr_{rpls}$ so that it used to train a regression model for $\\Delta _{new}(.,.)$ by applying partial recursive PLS BIBREF28 with $\\Delta (NewP,App_i)$ as set of predictor or independent variables and $NewSim_{FE}[App_i]$ as response variable. Existing proximity scores between applications in $APPS$ (ref. Section SECREF31) are also added to training set $Tr_{rpls}$ before generating the regression model.", "Analyze the corpus of all unique words generated from the text based details across all applications in the knowledge base. Generally corpus of such textual details would be relatively small, therefore, one can potentially apply pre-trained word embeddings (e.g., word2vec BIBREF19 or Glove BIBREF20). Let $v(w)$ be the neural embedding of word $w$ in the corpus. We need to follow additional steps to generate term-level embeddings (alternate solutions also exist BIBREF26): Represent corpus into Salton's vector space model BIBREF16 and estimate information theoretic weighing for each word using BM25 BIBREF27 scheme: Let $BM25(w)$ be the weight for word $w$. Next update word embedding as $v(w)\\leftarrow BM25(w)\\times v(w)$.", "To take this work forward, it is essential to have it integrated to a ML platform, which is being used by large user base for building TA applications so that to be able to populate a repository of statistically significant number of TA applications with details as specified in Section SECREF5 and thereafter refine the proposed approach so that eventually it rightly enables reuse of features across related applications.", "In this paper, we have presented high level overview of a feature specification language for ML based TA applications and an approach to enable reuse of feature specifications across semantically related applications. Currently, there is no generic method or approach, which can be applied during TA applications' design process to define and extract features for any arbitrary application in an automated or semi-automated manner primarily because there is no standard way to specify wide range of features which can be extracted and used. We considered different classes of features including linguistic, semantic, and statistical for various levels of analysis including words, phrases, sentences, paragraphs, documents, and corpus. As a next step, we presented an approach for building a recommendation system for enabling automated reuse of features for new application scenarios which improves its underlying similarity model based upon user feedback.", "To illustrate that semantically different yet related applications may have significant potential for reuse of features, let us consider the problem of event extraction, which involves identifying occurrences of specific type of events or activities from raw text."]}
{"question_id": "a2edd0454026811223b8f31512bdae91159677be", "predicted_answer": "", "predicted_evidence": ["Note that $\\epsilon $ is a small fraction $>$ 0 which controls when should similarity model be retrained. For example, $\\epsilon = 0.05$ would imply that if change in similarity is more than 5% only then it underlying model should use this feedback for retraining.", "In terms of these embeddings of terms, for each text based field of each application in the knowledge base, generate field level embedding as a triplet as follows: Let $f$ be a field of an application in $APPS$. Let the lists of entity-terms and action-terms in $f$ be $en(f)$ and $act(f)$ respectively. Let remaining words in $f$ be: $r(f)$. Estimate embedding for $f$ as: $v(f)$=$[v(en(f))$, $v(act(f))$, $v(r(f))]$, where $v(en(f))$=$\\Sigma _{z\\in en(f)} v(z)$, $v(act(f))$=$\\Sigma _{z\\in act(f)}v(z)$, and $v(r(f))$=$\\Sigma _{z\\in r(f)}v(z)$.", "For each $App_i\\in Bind\\left(f_j\\right)$: $NewSim_{FE}\\left[App_i\\right]\\leftarrow Average(Ch[App_i])$. If $|NewSim_{FE}[App_i]$-${\\alpha }_i|$ $\\ge \\epsilon {\\alpha }_i$ i.e., when the difference between old and new similarity scores is more than $\\epsilon $ fraction of original similarity, add $(\\Delta (NewP,App_i),NewSim_{FE}[App_i])$ to training set $Tr_{rpls}$ so that it used to train a regression model for $\\Delta _{new}(.,.)$ by applying partial recursive PLS BIBREF28 with $\\Delta (NewP,App_i)$ as set of predictor or independent variables and $NewSim_{FE}[App_i]$ as response variable. Existing proximity scores between applications in $APPS$ (ref.", "In this paper, we have presented high level overview of a feature specification language for ML based TA applications and an approach to enable reuse of feature specifications across semantically related applications. Currently, there is no generic method or approach, which can be applied during TA applications' design process to define and extract features for any arbitrary application in an automated or semi-automated manner primarily because there is no standard way to specify wide range of features which can be extracted and used. We considered different classes of features including linguistic, semantic, and statistical for various levels of analysis including words, phrases, sentences, paragraphs, documents, and corpus. As a next step, we presented an approach for building a recommendation system for enabling automated reuse of features for new application scenarios which improves its underlying similarity model based upon user feedback.", "Next, for each $App\\in APPS$, let ${Ch}\\left[App\\right]$ $\\leftarrow $ $\\emptyset $ be a hash table with keys as application ids and values as list of numbers estimated next. Also let $NewSim_{FE}\\left[.\\right]\\leftarrow 0$ contain updated similarity scores between $NewP$ and existing applications in $APPS$."]}
{"question_id": "3b4077776f4e828f0d1687d0ce8018c9bce4fdc6", "predicted_answer": "", "predicted_evidence": ["* Equal contribution. Listing order is random.", "The results showing comparisons with all other methods are summarized in fig:results. Each bar represents the average accuracy across 20 languages. Our method achieves an average accuracy of INLINEFORM0 and the strongest baseline, N18-1126, achieves an average accuracy of INLINEFORM1 . The difference in performance ( INLINEFORM2 ) is statistically significant with INLINEFORM3 under a paired permutation test. We outperform the strongest baseline in 11 out of 20 languages and underperform in only 3 languages with INLINEFORM4 . The difference between our method and all other baselines is statistical significant with INLINEFORM5 in all cases. We highlight two additional features of the data. First, decoding using gold morphological tags gives an accuracy of INLINEFORM6 for a difference in performance of INLINEFORM7 .", "The results showing comparisons with all other methods are summarized in fig:results. Each bar represents the average accuracy across 20 languages. Our method achieves an average accuracy of INLINEFORM0 and the strongest baseline, N18-1126, achieves an average accuracy of INLINEFORM1 . The difference in performance ( INLINEFORM2 ) is statistically significant with INLINEFORM3 under a paired permutation test. We outperform the strongest baseline in 11 out of 20 languages and underperform in only 3 languages with INLINEFORM4 . The difference between our method and all other baselines is statistical significant with INLINEFORM5 in all cases. We highlight two additional features of the data. First, decoding using gold morphological tags gives an accuracy of INLINEFORM6 for a difference in performance of INLINEFORM7 . We take the large difference between the upper bound and the current performance of our model to indicate that improved morphological tagging is likely to significantly help lemmatization.", "Our system and pre-trained models on all languages in the latest version of the UD corpora are released at https://sigmorphon.github.io/sharedtasks/2019/task2/.", "Experimentally, our contributions are threefold. First, we show that our joint model achieves state-of-the-art results, outperforming (on average) all competing approaches on a 20-language subset of the Universal Dependencies (UD) corpora BIBREF8 . Second, by providing the joint model with gold morphological tags, we demonstrate that we are far from achieving the upper bound on performance\u2014improvements on morphological tagging could lead to substantially better lemmatization. Finally, we provide a detailed error analysis indicating when and why morphological analysis helps lemmatization. We offer two tangible recommendations: one is better off using a joint model (i) for languages with fewer training data available and (ii) languages that have richer morphology."]}
{"question_id": "d1a88fe6655c742421da93cf88b5c541c09866d6", "predicted_answer": "", "predicted_evidence": ["However, the task is quite nuanced as the proper choice of the lemma is context dependent. For instance, in the sentence A running of the bulls took place in Pamplona, the word running is its own lemma, since, here, running is a noun rather than an inflected verb. Several counter-examples exist to this trend, as discussed in depth in haspelmath2013understanding. Thus, a good lemmatizer must make use of some representation of each word's sentential context. The research question in this work is, then, how do we design a lemmatization model that best extracts the morpho-syntax from the sentential context?", "For the morphological tagger, we use the baseline implementation from P18-1247. This implementation uses an input layer and linear layer dimension of 128 and a 2-layer LSTM with a hidden layer dimension of 256. The Adam BIBREF27 optimizer is used for training and a dropout rate BIBREF28 of 0.3 is enforced during training. The tagger was trained for 10 epochs.", "* Equal contribution. Listing order is random.", "Lemmatization is a core NLP task that involves a string-to-string transduction from an inflected word form to its citation form, known as the lemma. More concretely, consider the English sentence: The bulls are running in Pamplona. A lemmatizer will seek to map each word to a form you may find in a dictionary\u2014for instance, mapping running to run. This linguistic normalization is important in several downstream NLP applications, especially for highly inflected languages. Lemmatization has previously been shown to improve recall for information retrieval BIBREF0 , BIBREF1 , to aid machine translation BIBREF2 , BIBREF3 and is a core part of modern parsing systems BIBREF4 , BIBREF5 .", "Much like D15-1272, Morfette relies on the concept of edit trees. However, a simple perceptron is used for classification with hand-crafted features. A full description of the model is given in grzegorz2008learning."]}
{"question_id": "184382af8f58031c6e357dbee32c90ec95288cb3", "predicted_answer": "", "predicted_evidence": ["As stated in Section SECREF1, x-vectors are deep neural network-based speaker embeddings that were originally proposed by BIBREF8 as an alternative to i-vectors for speaker and language recognition. In contrast with i-vectors, that represent the total speaker and channel variability, x-vectors aim to model characteristics that discriminate between speakers. When compared to i-vectors, x-vectors require shorter temporal segments to achieve good results, and have been shown to be more robust to data variability and domain mismatches BIBREF8.", "Proposed by Snyder et al., x-vectors are discriminative deep neural network-based speaker embeddings, that have outperformed i-vectors in tasks such as speaker and language recognition BIBREF7, BIBREF8, BIBREF9. Even though it may not be evident that discriminative data representations are suitable for disease detection when trained with general datasets (that do not necessarily include diseased patients), recent works have shown otherwise. X-vectors have been successfully applied to paralinguistic tasks such as emotion recognition BIBREF10, age and gender classification BIBREF11, the detection of obstructive sleep apnea BIBREF12 and as a complement to the detection of Alzheimer's Disease BIBREF0. Following this line of research, in this work we study the hypothesis that speaker characteristics embedded in x-vectors extracted from a single network, trained for speaker identification using general data, contain sufficient information to allow the detection of multiple diseases.", "Proposed by Snyder et al., x-vectors are discriminative deep neural network-based speaker embeddings, that have outperformed i-vectors in tasks such as speaker and language recognition BIBREF7, BIBREF8, BIBREF9. Even though it may not be evident that discriminative data representations are suitable for disease detection when trained with general datasets (that do not necessarily include diseased patients), recent works have shown otherwise. X-vectors have been successfully applied to paralinguistic tasks such as emotion recognition BIBREF10, age and gender classification BIBREF11, the detection of obstructive sleep apnea BIBREF12 and as a complement to the detection of Alzheimer's Disease BIBREF0. Following this line of research, in this work we study the hypothesis that speaker characteristics embedded in x-vectors extracted from a single network, trained for speaker identification using general data, contain sufficient information to allow the detection of multiple diseases. Moreover, we aim to assess if this information is kept even when language mismatch is present, as has already been shown to be true for speaker recognition BIBREF8.", "Even though it may not be evident that discriminative data representations are suitable for disease detection when trained with general datasets (that do not necessarily include diseased patients), recent works have shown otherwise. X-vectors have been successfully applied to paralinguistic tasks such as emotion recognition BIBREF10, age and gender classification BIBREF11, the detection of obstructive sleep apnea BIBREF12 and as a complement to the detection of Alzheimer's Disease BIBREF0. Following this line of research, in this work we study the hypothesis that speaker characteristics embedded in x-vectors extracted from a single network, trained for speaker identification using general data, contain sufficient information to allow the detection of multiple diseases. Moreover, we aim to assess if this information is kept even when language mismatch is present, as has already been shown to be true for speaker recognition BIBREF8. In particular, we use the x-vector model as a feature extractor, to train Support Vector Machines for the detection of two speech-affecting diseases: Parkinson's disease (PD) and obstructive sleep apnea (OSA).", "Among other data types, speech has been proposed as a valuable biomarker for the detection of a myriad of diseases, including: neurological conditions, such as Alzheimer\u2019s BIBREF0, Parkinson\u2019s disease (PD) BIBREF1 and Amyotrophic Lateral Sclerosis BIBREF2; mood disorders, such as depression, anxiety BIBREF3 and bipolar disorder BIBREF4; respiratory diseases, such as obstructive sleep apnea (OSA) BIBREF5. However, temporal and financial constraints, lack of awareness in the medical community, ethical issues and patient-privacy laws make the acquisition of medical data one of the greatest obstacles to the development of health-related speech-based classifiers, particularly for deep learning models. For this reason, most systems rely on knowledge-based (KB) features, carefully designed and selected to model disease symptoms, in combination with simple machine learning models (e.g. Linear classifiers, Support Vector Machines). KB features may not encompass subtler symptoms of the disease, nor be general enough to cover varying levels of severity of the disease."]}
{"question_id": "97abc2e7b39869f660986b91fc68be4ba196805c", "predicted_answer": "", "predicted_evidence": ["All utterances were split into 4 second-long segments using overlapping windows, with a shift of 2 seconds. Further details about each of these datasets can be found in Table TABREF8.", "Speaker embeddings are fixed-length representations of a variable length speech signal, which capture relevant information about the speaker. Traditional speaker representations include Gaussian Supervectors BIBREF23 obtained from MAP adapted GMM-UBM BIBREF24 and i-vectors BIBREF25.", "OSA is a sleep-concerned breathing disorder characterized by a complete stop or decrease of the airflow, despite continued or increased inspiratory efforts BIBREF16. This disorder has a prevalence that ranges from 9% to 38% through different populations BIBREF17, with higher incidence in male and elderly groups. OSA causes mood and personality changes, depression, cognitive impairment, excessive daytime sleepiness, thus reducing the patients' quality of life BIBREF18, BIBREF19. It is also associated with diabetes, hypertension and cardiovascular diseases BIBREF16, BIBREF20. Moreover, undiagnosed sleep apnea can have a serious economic impact, having had an estimated cost of $\\$150$ billion in the U.S, in 2015 BIBREF21.", "This corpus is an extended version of the Portuguese Sleep Disorders (PSD) corpus (a detailed description of which can be found in BIBREF30). It includes three tasks spoken in European Portuguese: reading a phonetically rich text; read sentences recorded during a task for cognitive load assessment; and a spontaneous description of an image.", "Proposed by Pompili et al. BIBREF13, the KB feature set used for PD classification contains 36 features common to eGeMAPS BIBREF31 alongside with the mean and standard deviation (std.) of 12 Mel frequency cepstral coefficients (MFCCs) + log-energy, and their corresponding first and second derivatives, resulting in a 114-dimensional feature vector."]}
{"question_id": "9ec0527bda2c302f4e82949cc0ae7f7769b7bfb8", "predicted_answer": "", "predicted_evidence": ["Among other data types, speech has been proposed as a valuable biomarker for the detection of a myriad of diseases, including: neurological conditions, such as Alzheimer\u2019s BIBREF0, Parkinson\u2019s disease (PD) BIBREF1 and Amyotrophic Lateral Sclerosis BIBREF2; mood disorders, such as depression, anxiety BIBREF3 and bipolar disorder BIBREF4; respiratory diseases, such as obstructive sleep apnea (OSA) BIBREF5. However, temporal and financial constraints, lack of awareness in the medical community, ethical issues and patient-privacy laws make the acquisition of medical data one of the greatest obstacles to the development of health-related speech-based classifiers, particularly for deep learning models. For this reason, most systems rely on knowledge-based (KB) features, carefully designed and selected to model disease symptoms, in combination with simple machine learning models (e.g. Linear classifiers, Support Vector Machines). KB features may not encompass subtler symptoms of the disease, nor be general enough to cover varying levels of severity of the disease.", "Recent advances in Machine Learning (ML) and, in particular, in Deep Neural Networks (DNN) have allowed the development of highly accurate predictive systems for numerous applications. Among others, health has received significant attention due to the potential of ML-based diagnostic, monitoring and therapeutic systems, which are fast (when compared to traditional diagnostic processes), easily distributed and cheap to implement (many such systems can be executed in mobile devices). Furthermore, these systems can incorporate biometric data to perform non-invasive diagnostics.", "Four corpora were used in our experiments: three to determine the presence or absence of PD and OSA, which include a European Portuguese PD corpus (PPD), a European Portuguese OSA corpus (POSA) and a Spanish PD corpus (SPD); one task-agnostic European Portuguese corpus to train the i-vector and x-vector extractors. For each of the disease-related datasets, we compared three distinct data representations: knowledge-based features, i-vectors and x-vectors. All disease classifications were performed with an SVM classifier. Further details on the corpora, data representations and classification method follow bellow.", "Among other data types, speech has been proposed as a valuable biomarker for the detection of a myriad of diseases, including: neurological conditions, such as Alzheimer\u2019s BIBREF0, Parkinson\u2019s disease (PD) BIBREF1 and Amyotrophic Lateral Sclerosis BIBREF2; mood disorders, such as depression, anxiety BIBREF3 and bipolar disorder BIBREF4; respiratory diseases, such as obstructive sleep apnea (OSA) BIBREF5. However, temporal and financial constraints, lack of awareness in the medical community, ethical issues and patient-privacy laws make the acquisition of medical data one of the greatest obstacles to the development of health-related speech-based classifiers, particularly for deep learning models. For this reason, most systems rely on knowledge-based (KB) features, carefully designed and selected to model disease symptoms, in combination with simple machine learning models (e.g. Linear classifiers, Support Vector Machines).", "The x-vector system, described in detail in BIBREF7, has three main blocks. The first block is a set of five time-delay layers which operate at frame level, with a small temporal context. These layers work as a 1-dimensional convolution, with a kernel size corresponding to the temporal context. The second block, a statistical pooling layer, aggregates the information across the time dimension and outputs a summary for the entire speech segment. In this work, we implemented the attentive statistical pooling layer, proposed by Okabe et al. BIBREF27. The attention mechanism is used to weigh frames according to their importance when computing segment level statistics. The third and final block is a set of fully connected layers, from which x-vector embeddings can be extracted."]}
{"question_id": "330fe3815f74037a9be93a4c16610c736a2a27b3", "predicted_answer": "", "predicted_evidence": ["Among other data types, speech has been proposed as a valuable biomarker for the detection of a myriad of diseases, including: neurological conditions, such as Alzheimer\u2019s BIBREF0, Parkinson\u2019s disease (PD) BIBREF1 and Amyotrophic Lateral Sclerosis BIBREF2; mood disorders, such as depression, anxiety BIBREF3 and bipolar disorder BIBREF4; respiratory diseases, such as obstructive sleep apnea (OSA) BIBREF5. However, temporal and financial constraints, lack of awareness in the medical community, ethical issues and patient-privacy laws make the acquisition of medical data one of the greatest obstacles to the development of health-related speech-based classifiers, particularly for deep learning models. For this reason, most systems rely on knowledge-based (KB) features, carefully designed and selected to model disease symptoms, in combination with simple machine learning models (e.g. Linear classifiers, Support Vector Machines).", "As stated in Section SECREF1, x-vectors are deep neural network-based speaker embeddings that were originally proposed by BIBREF8 as an alternative to i-vectors for speaker and language recognition. In contrast with i-vectors, that represent the total speaker and channel variability, x-vectors aim to model characteristics that discriminate between speakers. When compared to i-vectors, x-vectors require shorter temporal segments to achieve good results, and have been shown to be more robust to data variability and domain mismatches BIBREF8.", "OSA is a sleep-concerned breathing disorder characterized by a complete stop or decrease of the airflow, despite continued or increased inspiratory efforts BIBREF16. This disorder has a prevalence that ranges from 9% to 38% through different populations BIBREF17, with higher incidence in male and elderly groups. OSA causes mood and personality changes, depression, cognitive impairment, excessive daytime sleepiness, thus reducing the patients' quality of life BIBREF18, BIBREF19. It is also associated with diabetes, hypertension and cardiovascular diseases BIBREF16, BIBREF20. Moreover, undiagnosed sleep apnea can have a serious economic impact, having had an estimated cost of $\\$150$ billion in the U.S, in 2015 BIBREF21.", "Even though it may not be evident that discriminative data representations are suitable for disease detection when trained with general datasets (that do not necessarily include diseased patients), recent works have shown otherwise. X-vectors have been successfully applied to paralinguistic tasks such as emotion recognition BIBREF10, age and gender classification BIBREF11, the detection of obstructive sleep apnea BIBREF12 and as a complement to the detection of Alzheimer's Disease BIBREF0. Following this line of research, in this work we study the hypothesis that speaker characteristics embedded in x-vectors extracted from a single network, trained for speaker identification using general data, contain sufficient information to allow the detection of multiple diseases. Moreover, we aim to assess if this information is kept even when language mismatch is present, as has already been shown to be true for speaker recognition BIBREF8. In particular, we use the x-vector model as a feature extractor, to train Support Vector Machines for the detection of two speech-affecting diseases: Parkinson's disease (PD) and obstructive sleep apnea (OSA).", "PD is the second most common neurodegenerative disorder of mid-to-late life after Alzheimer\u2019s disease BIBREF13, affecting 1% of people over the age of 65. Common symptoms include bradykinesia (slowness or difficulty to perform movements), muscular rigidity, rest tremor, as well as postural and gait impairment. 89% of PD patients develop also speech disorders, typically hypokinetic dysarthria, which translates into symptoms such as reduced loudness, monoloudness, monopitch, hypotonicity, breathy and hoarse voice quality, and imprecise articulation BIBREF14BIBREF15."]}
{"question_id": "7546125f43eec5b09a3368c95019cb2bf1478255", "predicted_answer": "", "predicted_evidence": ["The EWT training regime, which uses out of domain texts written in standard English, provides the lowest performance on all the evaluation metrics. An additional factor which negatively affects performance in this regime are systematic differences in the EWT annotation of possessive pronouns, expletives and names compared to the UD guidelines, which are utilized in the TLE. In particular, the EWT annotates possessive pronoun UPOS as PRON rather than DET, which leads the UPOS results in this setup to be lower than the PTB POS results. Improved results are obtained using the TLE training data, which, despite its smaller size, is closer in genre and syntactic characteristics to the TLE test set. The strongest PTB POS tagging and parsing results are obtained by combining the EWT with the TLE training data, yielding 95.77 POS accuracy and a UAS of 90.3 on the original version of the TLE test set.", "#SENT=...they do not <ns type=\"DV\"><i>sale</i> <c>sell</c></ns> them...", "While the above mentioned studies focus on annotation guidelines, attention has also been drawn to the topic of parsing in the learner language domain. However, due to the shortage of syntactic resources for ESL, much of the work in this area resorted to using surrogates for learner data. For example, in Foster foster2007treebanks and Foster et al. foster2008 parsing experiments are carried out on synthetic learner-like data, that was created by automatic insertion of grammatical errors to well formed English text. In Cahill et al. cahill2014 a treebank of secondary level native students texts was used to approximate learner text in order to evaluate a parser that utilizes unlabeled learner data.", "Figure 1 presents the average sentential performance as a function of the percentage of tokens in the original sentence marked with grammatical errors. In this experiment, we train the parser on the EWT training set and test on the entire TLE corpus. Performance curves are presented for POS, UAS and LAS on the original and error corrected versions of the annotations. We observe that while the performance on the corrected sentences is close to constant, original sentence performance is decreasing as the percentage of the erroneous tokens in the sentence grows.", "4 we PRON PRP 6 nsubjpass"]}
{"question_id": "e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2", "predicted_answer": "", "predicted_evidence": ["14 not PART RB 15 neg", "Taken together, our ESL conventions cover many of the annotation challenges related to grammatical errors present in the TLE. In addition to the presented overview, the complete manual of ESL guidelines used by the annotators is publicly available. The manual contains further details on our annotation scheme, additional annotation guidelines and a list of common annotation errors. We plan to extend and refine these guidelines in future releases of the treebank.", "Table TABREF16 presents tagging and parsing results on a test set of 500 TLE sentences (9,591 original tokens, 9,700 corrected tokens). Results are provided for three different training regimes. The first regime uses the training portion of version 1.3 of the EWT, the UD English treebank, containing 12,543 sentences (204,586 tokens). The second training mode uses 4,124 training sentences (78,541 original tokens, 79,581 corrected tokens) from the TLE corpus. In the third setup we combine these two training corpora. The remaining 500 TLE sentences (9,549 original tokens, 9,695 corrected tokens) are allocated to a development set, not used in this experiment. Parsing of the test sentences was performed on predicted POS tags.", "Cases of erroneous tense usage are annotated according to the morphological tense of the verb. For example, below we annotate \u201cshopping\u201d with present participle VBG, while the correction \u201cshop\u201d is annotated in the corrected version of the sentence as VBP.", "15 sale VERB VB 0 root"]}
{"question_id": "576a3ed6e4faa4c3893db632e97a52ac6e864aac", "predicted_answer": "", "predicted_evidence": ["#SENT=...we <ns type=\"SX\"><i>where</i> <c>were</c></ns> invited to visit...", "Previous studies on learner language proposed several annotation schemes for both POS tags and syntax BIBREF14 , BIBREF5 , BIBREF6 , BIBREF15 . The unifying theme in these proposals is a multi-layered analysis aiming to decouple the observed language usage from conventional structures in the foreign language.", "13 do AUX VBP 15 aux", "Erroneous word formations that are contextually plausible and can be assigned with a PTB tag are annotated literally. In the following example, \u201cstuffs\u201d is handled as a plural count noun.", "Syntactic annotations for ESL were previously developed by Nagata et al. nagata2011, who annotate an English learner corpus with POS tags and shallow syntactic parses. Our work departs from shallow syntax to full syntactic analysis, and provides annotations on a significantly larger scale. Furthermore, differently from this annotation effort, our treebank covers a wide range of learner native languages. An additional syntactic dataset for ESL, currently not available publicly, are 1,000 sentences from the EFCamDat dataset BIBREF8 , annotated with Stanford dependencies BIBREF19 . This dataset was used to measure the impact of grammatical errors on parsing by comparing performance on sentences with grammatical errors to error free sentences. The TLE enables a more direct way of estimating the magnitude of this performance gap by comparing performance on the same sentences in their original and error corrected versions. Our comparison suggests that the effect of grammatical errors on parsing is smaller that the one reported in this study."]}
{"question_id": "73c535a7b46f0c2408ea2b1da0a878b376a2bca5", "predicted_answer": "", "predicted_evidence": ["In particular, ill formed adjectives that have a plural suffix receive a standard adjectival POS tag. When applicable, such cases also receive an additional marking for unnecessary agreement in the error annotation using the attribute \u201cua\u201d.", "We utilize our two step review process to estimate agreement rates between annotators. We measure agreement as the fraction of annotation tokens approved by the editor. Table TABREF15 presents the agreement between annotators and reviewers, as well as the agreement between reviewers and the judges. Agreement measurements are provided for both the original the corrected versions of the dataset.", "The dual annotation of sentences in their original and error corrected forms enables estimating the impact of grammatical errors on tagging and parsing by examining the performance gaps between the two sentence versions. Averaged across the three training conditions, the POS tagging accuracy on the original sentences is lower than the accuracy on the sentence corrections by 1.0 UPOS and 0.61 POS. Parsing performance degrades by 1.9 UAS, 1.59 LA and 2.21 LAS.", "Overall, our results suggest a negative, albeit limited effect of grammatical errors on parsing. This outcome contrasts a study by Geertzen et al. geertzen2013 which reported a larger performance gap of 7.6 UAS and 8.8 LAS between sentences with and without grammatical errors. We believe that our analysis provides a more accurate estimate of this impact, as it controls for both sentence content and sentence length. The latter factor is crucial, since it correlates positively with the number of grammatical errors in the sentence, and negatively with parsing accuracy.", "The EWT training regime, which uses out of domain texts written in standard English, provides the lowest performance on all the evaluation metrics. An additional factor which negatively affects performance in this regime are systematic differences in the EWT annotation of possessive pronouns, expletives and names compared to the UD guidelines, which are utilized in the TLE. In particular, the EWT annotates possessive pronoun UPOS as PRON rather than DET, which leads the UPOS results in this setup to be lower than the PTB POS results. Improved results are obtained using the TLE training data, which, despite its smaller size, is closer in genre and syntactic characteristics to the TLE test set. The strongest PTB POS tagging and parsing results are obtained by combining the EWT with the TLE training data, yielding 95.77 POS accuracy and a UAS of 90.3 on the original version of the TLE test set."]}
{"question_id": "620b6c410a055295d137511d3c99207a47c03b5e", "predicted_answer": "", "predicted_evidence": ["Following BIBREF12, we ensured that both users and products are twenty-core, split them into train, dev, and test sets with an 8:1:1 ratio, and tokenized and sentence-split the text using Stanford CoreNLP BIBREF32. The final dataset contains 77,028 data points, with 1,728 users and 1,890 products. This is used as the sentiment classification dataset. To create the task-specific datasets, we split the dataset again such that no users and no products are seen in at least two different splits. That is, if user $u$ is found in the train set, then it should not be found in the dev and the test sets. We remove the user-product pairs that do not satistfy this condition. We then append the corresponding product category and review headline for each user-product pair.", "Theoretically, this should perform better than bias-based representations since direct relationship between text and attributes are modeled. For example, following the example above, $W^{\\prime }x$ is a user-biased logits vector based on the document encoding $d$ (e.g., $u$ tends to classify texts as two-star positive when the text mentions that the dessert was sweet).", "Unless otherwise stated, our models are implemented with the following settings. We set the dimensions of the word, user, and product vectors to 300. We use pre-trained GloVe embeddings BIBREF28 to initialize the word vectors. We also set the dimensions of the hidden state of BiLSTM to 300 (i.e., 150 dimensions for each of the forward/backward hidden state). The chunk size factors $C_1$ and $C_2$ are both set to 15. We use dropout BIBREF29 on all non-linear connections with a dropout rate of 0.1. We set the batch size to 32. Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule BIBREF30 and with $l_2$ constraint BIBREF31 of 3. We perform early stopping using the development set. Training and experiments are done using an NVIDIA GeForce GTX 1080 Ti graphics card.", "We collected a new dataset from Amazon, which includes the product category and the review headline, aside from the review text, the sentiment score, and the user and product attributes. Following BIBREF12, we ensured that both users and products are twenty-core, split them into train, dev, and test sets with an 8:1:1 ratio, and tokenized and sentence-split the text using Stanford CoreNLP BIBREF32. The final dataset contains 77,028 data points, with 1,728 users and 1,890 products. This is used as the sentiment classification dataset. To create the task-specific datasets, we split the dataset again such that no users and no products are seen in at least two different splits. That is, if user $u$ is found in the train set, then it should not be found in the dev and the test sets. We remove the user-product pairs that do not satistfy this condition.", "However, the model is burdened by a large number of parameters; matrix-based attribute representation increases the number of parameters by $|U|*|P|*D_1*D_2$, where $|U|$ and $|P|$ correspond to the number of users and products, respectively. This subsequently makes the weights difficult to optimize during training. Thus, directly incorporating attributes into the weight matrix may cause harm in the performance of the model."]}
{"question_id": "e459760879f662b2205cbdc0f5396dbfe41323ae", "predicted_answer": "", "predicted_evidence": ["UPDMN BIBREF16 uses an LSTM classifier as base model and incorporates attributes as a separate deep memory network that uses other related documents as memory.", "Among our four models, CHIM-embedding performs the best in terms of accuracy, with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively. CHIM-classifier performs the best in terms of RMSE, outperforming all other models on both Yelp 2013 and 2014 datasets. Among our models, CHIM-attention mechanism performs the worst, which shows similar results to our previous experiment (see Figure FIGREF25). We emphasize that our models use a simple BiLSTM as base model, and extensions to the base model (e.g., using multiple hierarchical LSTMs as in BIBREF21), as well as to other aspects (e.g., consideration of cold-start entities as in BIBREF9), are orthogonal to our proposed attribute representation and injection method. Thus, we expect a further increase in performance when these extensions are done.", "The use of categorical attributes (e.g., user, topic, aspects) in the sentiment analysis community BIBREF0, BIBREF1, BIBREF2 is widespread. Prior to the deep learning era, these information were used as effective categorical features BIBREF3, BIBREF4, BIBREF5, BIBREF6 for the machine learning model. Recent work has used them to improve the overall performance BIBREF7, BIBREF8, interpretability BIBREF9, BIBREF10, and personalization BIBREF11 of neural network models in different tasks such as sentiment classification BIBREF12, review summarization BIBREF13, and text generation BIBREF8.", "In particular, user and product information have been widely incorporated in sentiment classification models, especially since they are important metadata attributes found in review websites. BIBREF12 first showed significant accuracy increase of neural models when these information are used. Currently, the accepted standard method is to use them as additional biases when computing the weights $a$ in the attention mechanism, as introduced by BIBREF7 as:", "Injecting attributes to the word embedding means that we bias the sentiment intensity of a word independent from its neighboring context. For example, if a user normally uses the words tasty and delicious with a less and more positive intensity, respectively, the corresponding attribute-injected word embeddings would come out less similar, despite both words being synonymous."]}
{"question_id": "1c3a20dceec2a86fb61e70fab97a9fb549b5c54c", "predicted_answer": "", "predicted_evidence": ["Using the approaches described above, we can inject attribute representation into four different parts of the model. This section describes what it means to inject attributes to a certain location and why previous work have been injecting them in the worst location (i.e., in the attention mechanism).", "Following BIBREF12, we ensured that both users and products are twenty-core, split them into train, dev, and test sets with an 8:1:1 ratio, and tokenized and sentence-split the text using Stanford CoreNLP BIBREF32. The final dataset contains 77,028 data points, with 1,728 users and 1,890 products. This is used as the sentiment classification dataset. To create the task-specific datasets, we split the dataset again such that no users and no products are seen in at least two different splits. That is, if user $u$ is found in the train set, then it should not be found in the dev and the test sets. We remove the user-product pairs that do not satistfy this condition. We then append the corresponding product category and review headline for each user-product pair. The final split contains 46,151 training, 711 development, and 840 test instances.", "The results of our experiments can be summarized in three statements. First, our preliminary experiments show that doing bias-based attribute representation and attention-based injection is not an effective method to incorporate user and product information in sentiment classification models. Second, despite using only a simple BiLSTM with attention classifier, we significantly outperform previous state-of-the-art models that use more complicated architectures (e.g., models that use hierarchical models, external memory networks, etc.). Finally, we show that these attribute representations transfer well to other tasks such as product category classification and review headline generation.", "To summarize, chunking helps reduce the number of parameters while retaining the model performance, and importance matrix makes optimization easier during training, resulting to a performance improvement. We also tried alternative methods for importance matrix such as residual addition (i.e., $\\tanh (W^{\\prime }) + W$) introduced in BIBREF25, and low-rank adaptation methods BIBREF26, BIBREF27, but these did not improve the model performance.", "Injecting attributes to the classifier means that we bias the probability distribution of sentiment based on the final document encoding. If a user tends to classify the sentiment of reviews about sweet cakes as highly positive, then the model would give a high probability to highly positive sentiment classes for texts such as \u201cthe cake was sweet\u201d."]}
{"question_id": "9686f3ff011bc6e3913c329c6a5671932c27e63e", "predicted_answer": "", "predicted_evidence": ["Zero-shot translation is of considerable concern among the multilingual translation community. By sharing network parameters across languages, ZS was proven feasible for universal multilingual MT BIBREF4 , BIBREF3 . There are many variations of multilingual models geared towards zero-shot translation. BIBREF20 proposed to explicitly define a recurrent layer with a fixed number of states as \u201cInterlingua\u201d which resembles our attention-pooling models. However, they compromise the model compactness by having separate encoder-decoder per language, which linearly increases the model size across languages. On the other hand, BIBREF21 shares all parameters, but utilized a parameter generator to generate specific parameters for the LSTMs in each language pair using language embeddings. The closest to our work is probably BIBREF9 .", "This work provides a through investigation of zero-shot translation in multilingual NMT. We conduct an analysis of neural architectures for zero-shot through two three different modifications showing that a beneficial shared representation can be learned for zero-shot translation. Furthermore, we provide a regularization scheme to encourage the model to capture language-independent features for the Transformer model which increases zero-shot performance by INLINEFORM0 BLEU points, achieving the state-of-the-art zero-shot performance in the standard benchmark IWSLT2017 dataset. We also proposed an alternative setting with more than one language as a bridge. In this challenging setup for zero-shot translation, we confirmed the consistent effects of our method by showing that the benefit is still significant when languages are far from each other in the pivot path. This result also motivates future works to apply the same strategy for other end-to-end tasks such as speech translation where there may be more variability in domains and modalities.", "This observation leads us to the most important contribution in this work, which is to propose several techniques to learn a joint semantic space for different languages in multilingual models without any architectural modification. The key idea is to prefer a source language-independent representation in the decoder using an additional loss function. As a result, the NMT architecture remains untouched and the technique is scalable to the number of languages in the training data. The success of this method is shown by significant gains on zero-shot translation quality in the standard IWSLT 2017 multilingual benchmark BIBREF11 . Finally, we introduce a more challenging scenario that involves more than one bridge language between source and target languages. This challenging setup confirms the consistency of our zero-shot techniques while clarifying the disadvantages of pivot-based translation.", "On the other hand, the current neural architecture and learning mechanisms of multilingual NMT is not geared towards having a common representation. Different languages are likely to convey the same semantic content with sentences of different lengths BIBREF8 , which makes the desiderata difficult to achieve. Moreover, the loss function of the neural translation model does not favour having sentences encoded in the same representation space regardless of the source language. As a result, if the network capacity is large enough, it may partition itself into different sub-spaces for different language pairs BIBREF9 .", "On the other hand, BIBREF21 shares all parameters, but utilized a parameter generator to generate specific parameters for the LSTMs in each language pair using language embeddings. The closest to our work is probably BIBREF9 . The authors aimed to regularize the model into a common encoding space by taking the mean-pooling of the encoder states and minimize the cosine similarity between the source and the target sentence encodings. In comparison, our approach is more generalized because the decoder is also taken into account during regularization, which is shown by our results on the IWSLT benchmark. Also, we proposed stronger representation-forcing since the cosine similarity minimizes the angle between two representational vectors, while the MSE forces them to be exactly equal. In addition, zero-resource techniques which generate artificial data for the missing directions have been proposed as an alternative to zero-shot translation BIBREF22 , BIBREF23 , BIBREF24 . The main disadvantage, however, is the requirement of computationally expensive sampling during training which makes the algorithm less scalable to the number of languages."]}
{"question_id": "1f053f338df6d238cb163af1a0b1b073e749ed8a", "predicted_answer": "", "predicted_evidence": ["In this paper, we will propose a neural approach to parallel sentence extraction and compare the BLEU scores of machine translation systems with and without the use of the extracted sentence pairs to justify the effectiveness of this method. Compared to previous approaches which require specialized meta-data from document structure or significant amount of hand-engineered features, the neural model for extracting parallel sentences is learned end-to-end using only a small bootstrap set of parallel sentence pairs.", "The experiments are shown for English-Tamil and English-Hindi language pairs. Our model achieved a marked percentage increase in the BLEU score for both en\u2013ta and en\u2013hi language pairs. We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.", "For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en\u2013ta and en\u2013hi pairs, as can be seen in Table TABREF23 .", "The binary classifier described in the next section, assigns a translation probability score to a given sentence pair, after learning from examples of translations and negative examples of non-translation pairs. For, this we make a simplistic assumption that the parallel sentence pairs found in the bootstrap dataset are unique combinations, which fail being translations of each other, when we randomly pick a sentence from both the sets. Thus, there might be cases of false negatives due to the reliance on unsupervised random sampling for generation of negative labels.", "We experimented with two language pairs: English \u2013 Hindi (en\u2013hi) and English \u2013 Tamil (en\u2013ta). The parallel sentence extraction systems for both en\u2013ta and en\u2013hi were trained using the architecture described in SECREF7 on the following bootstrap set of parallel corpora:"]}
{"question_id": "fb06ed5cf9f04ff2039298af33384ca71ddbb461", "predicted_answer": "", "predicted_evidence": ["As a follow-up to this work, we would be comparing our framework against other sentence alignment methods described in BIBREF20 , BIBREF21 , BIBREF22 and BIBREF23 . It has also been interesting to note that the 2018 edition of the Workshop on Machine Translation (WMT) has released a new shared task called Parallel Corpus Filtering where participants develop methods to filter a given noisy parallel corpus (crawled from the web), to a smaller size of high quality sentence pairs. This would be the perfect avenue to test the efficacy of our neural network based approach of extracting parallel sentences from unaligned corpora.", "As illustrated in Figure FIGREF5 (d), the architecture uses a siamese network BIBREF7 , consisting of a bidirectional RNN BIBREF8 sentence encoder with recurrent units such as long short-term memory units, or LSTMs BIBREF9 and gated recurrent units, or GRUs BIBREF10 learning a vector representation for the source and target sentences and the probability of any given pair of sentences being translations of each other. For seq2seq architectures, especially in translation, we have found the that the recommended recurrent unit is GRU, and all our experiments use this over LSTM.", "The experiments are shown for English-Tamil and English-Hindi language pairs. Our model achieved a marked percentage increase in the BLEU score for both en\u2013ta and en\u2013hi language pairs. We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.", "Similar to our proposed approach, BIBREF5 ( BIBREF5 ) showed how using parallel documents from Wikipedia for domain specific alignment would improve translation quality of SMT systems on in-domain data. In this method, similarity between all pairs of cross-language sentences with different text similarity measures are estimated. The issue of domain definition is overcome by the use of IR techniques which use the characteristic vocabulary of the domain to query a Lucene search engine over the entire corpus. The candidate sentences are defined based on word overlap and the decision whether a sentence pair is parallel or not using the maximum entropy classifier. The difference in the BLEU scores between out of domain and domain-specific translation is proved clearly using the word embeddings from characteristic vocabulary extracted using the extracted additional bitexts.", "A lot of work has been done on the problem of automatic sentence alignment from comparable corpora, but a majority of them BIBREF2 , BIBREF1 , BIBREF3 use a pre-existing translation system as a precursor to ranking the candidate sentence pairs, which the low resource language pairs are not at the luxury of having; or use statistical machine learning approaches, where a Maximum Entropy classifier is used that relies on surface level features such as word overlap in order to obtain parallel sentence pairs BIBREF4 . However, the deep neural network model used in our paper is probably the first of its kind, which does not need any feature engineering and also does not need a pre-existing translation system."]}
{"question_id": "754d7475b8bf50499ed77328b4b0eeedf9cb2623", "predicted_answer": "", "predicted_evidence": ["", "For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en\u2013ta and en\u2013hi pairs, as can be seen in Table TABREF23 .", "As the dataset for training the machine translation systems, we used high precision sentences extracted with greedy decoding, by ranking the sentence-pairs on their translation probabilities. Phrase-Based SMT systems were trained using Moses BIBREF14 . We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA BIBREF15 for tuning (the default parameters on Moses). We trained 5-gram language models with Kneser-Ney smoothing using KenLM BIBREF16 . With these parameters, we trained SMT systems for en\u2013ta and en\u2013hi language pairs, with and without the use of extracted parallel sentence pairs.", "An English-Hindi parallel corpus BIBREF12 containing a total of INLINEFORM0 sentence pairs, from which a set of INLINEFORM1 sentence pairs were picked randomly.", "For prediction, a sentence pair is classified as parallel if the probability score is greater than or equal to a decision threshold INLINEFORM0 that we need to fix. We found that to get high precision sentence pairs, we had to use INLINEFORM1 , and if we were able to sacrifice some precision for recall, a lower INLINEFORM2 of 0.80 would work in the favor of reducing OOV rates. DISPLAYFORM0"]}
{"question_id": "1d10e069b4304fabfbed69acf409f0a311bdc441", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the sigmoid function, INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 and INLINEFORM5 are model parameters. The model is trained by minimizing the cross entropy of our labeled sentence pairs: DISPLAYFORM0", "", "Figures FIGREF16 shows the number of high precision sentences that were extracted at INLINEFORM0 without greedy decoding. Greedy decoding could be thought of as sampling without replacement, where a sentence that's already been extracted on one side of the extraction system, is precluded from being considered again. Hence, the number of sentences without greedy decoding, are of an order of magnitude higher than with decoding, as can be seen in Figure FIGREF16 .", "The forward RNN reads the variable-length sentence and updates its recurrent state from the first token until the last one to create a fixed-size continuous vector representation of the sentence. The backward RNN processes the sentence in reverse. In our experiments, we use the concatenation of the last recurrent state in both directions as a final representation INLINEFORM0 DISPLAYFORM0", "In this paper, we evaluated the benefits of using a neural network procedure to extract parallel sentences. Unlike traditional translation systems which make use of multi-step classification procedures, this method requires just a parallel corpus to extract parallel sentence pairs using a Siamese BiRNN encoder using GRU as the activation function."]}
{"question_id": "718c0232b1f15ddb73d40c3afbd6c5c0d0354566", "predicted_answer": "", "predicted_evidence": ["Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.", "We evaluated the quality of the extracted parallel sentence pairs, by performing machine translation experiments on the augmented parallel corpus.", "For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en\u2013ta and en\u2013hi pairs, as can be seen in Table TABREF23 .", "This method is extremely beneficial for translating language pairs with very little parallel corpora. These parallel sentences facilitate significant improvement in machine translation quality when compared to a generic system as has been shown in our results.", "We experimented with two language pairs: English \u2013 Hindi (en\u2013hi) and English \u2013 Tamil (en\u2013ta). The parallel sentence extraction systems for both en\u2013ta and en\u2013hi were trained using the architecture described in SECREF7 on the following bootstrap set of parallel corpora:"]}
{"question_id": "7cf44877dae8873139aede381fb9908dd0c546c4", "predicted_answer": "", "predicted_evidence": ["By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta BIBREF14 , RmsProp BIBREF15 , Adam BIBREF16 ).", "Nematus implements an attentional encoder\u2013decoder architecture similar to the one described by DBLP:journals/corr/BahdanauCB14, but with several implementation differences. The main differences are as follows:", "Nematus is implemented in Python, and based on the Theano framework BIBREF4 . It implements an attentional encoder\u2013decoder architecture similar to DBLP:journals/corr/BahdanauCB14. Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus.", "Nematus has its roots in the dl4mt-tutorial. We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year's shared translation tasks at WMT BIBREF2 and IWSLT BIBREF3 .", "Nematus provides support for applying single models, as well as using multiple models in an ensemble \u2013 the latter is possible even if the model architectures differ, as long as the output vocabulary is the same. At each time step, the probability distribution of the ensemble is the geometric average of the individual models' probability distributions. The toolkit includes scripts for beam search decoding, parallel corpus scoring and n-best-list rescoring."]}
{"question_id": "86de8de906e30bb2224a2f70f6e5cf5e5ad4be72", "predicted_answer": "", "predicted_evidence": ["We hope that researchers will find Nematus an accessible and well documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production BIBREF21 .", "By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta BIBREF14 , RmsProp BIBREF15 , Adam BIBREF16 ).", "Nematus is implemented in Python, and based on the Theano framework BIBREF4 . It implements an attentional encoder\u2013decoder architecture similar to DBLP:journals/corr/BahdanauCB14. Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus.", "We have presented Nematus, a toolkit for Neural Machine Translation. We have described implementation differences to the architecture by DBLP:journals/corr/BahdanauCB14; due to the empirically strong performance of Nematus, we consider these to be of wider interest.", "Given a source sequence INLINEFORM0 of length INLINEFORM1 and a target sequence INLINEFORM2 of length INLINEFORM3 , let INLINEFORM4 be the annotation of the source symbol at position INLINEFORM5 , obtained by concatenating the forward and backward encoder RNN hidden states, INLINEFORM6 , and INLINEFORM7 be the decoder hidden state at position INLINEFORM8 ."]}
{"question_id": "361f330d3232681f1a13c6d59abb6c18246e7b35", "predicted_answer": "", "predicted_evidence": ["Here the summary $C$ consists of not only the dependencies between words, but also the relations between sentences. Following Voita:2018:ACL, we share the parameters of word-level encoder $\\textsc {Encoder}_{word}$ with the encoder component in standard NMT model. Note that, $\\textsc {Encoder}_{word}$ and $\\textsc {Encoder}_{sentence}$ can be implemented as arbitrary networks, such as recurrent networks BIBREF15 , convolutional networks BIBREF16 , or self-attention networks BIBREF17 . In this study, we used recurrent networks to implement our $\\textsc {Encoder}$ .", "where ${\\bf h}^{rec}_t$ is the hidden state in the reconstructor:", "In addition, we implemented two comparative models (Row 2-3), which differ with respect to the training data used. The \u201c+ ZP-Annotated Data\u201d model was still trained on standard NMT model but using new training instances ( $\\bf \\hat{x}$ , $\\bf y$ ) whose source-side sentences are auto-annotated with ZPs. The \u201c+ Reconstruction\u201d is the best model reported in Wang:2018:AAAI, which employs two reconstructors to reconstruct the $\\bf \\hat{x}$ from hidden representations of encoder and decoder. At decoding time, ZPs can not be annotated by alignment method since target sentences are not available. Thus, source sentences are annotated by an external ZP prediction model, which is trained on monolingual training instances $\\bf \\hat{x}$ . Finally, we evaluated two proposed models (Row 4-5) which are introduced in Section \"Joint ZP Prediction and Translation\" and \"Discourse-Aware ZP Prediction\" , respectively.", "In \u201cFixed Error\u201d case, the dropped word \u201c\u5b83 (it)\u201d is an anaphoric ZP whose antecedent is the noun \u201c\u7535\u89c6 (television)\u201d in previous sentence while the dropped word \u201c\u4f60 (you)\u201d is a non-anaphoric ZP that depends upon speaker or listener. As seen, our \u201cJoin.\u201d model performs better than the \u201cExte.\u201d model because two ZP positions are syntactically recalled in the target side, showing that the joint approach have better capability of utilizing intra-sentential information for identifying ZPs. Besides, our \u201c+Dis.\u201d model can semantically fix the error by predicting correct ZP words, demonstrating that inter-sentential context can aid to recovering such complex ZPs. However, as shown in \u201cNon-Fixed Error\u201d case, there are still some ZPs can not be precisely predicted due to the misunderstanding of intentions of utterances. Thus, exploiting dialogue focus for ZP translation is our future work BIBREF26 .", "Here $g_r(\\cdot )$ and $f_r(\\cdot )$ are respective softmax and activation functions for the reconstructor. The context vectors $\\hat{\\bf c}^{enc}_t$ and $\\hat{\\bf c}^{dec}_t$ are the weighted sum of ${\\bf h}^{enc}$ and ${\\bf h}^{dec}$ , and the weights are calculated by two interactive attention models:"]}
{"question_id": "f7d61648ae4bd46c603a271185c3adfac5fc5114", "predicted_answer": "", "predicted_evidence": ["In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourse-level information for better ZP prediction. Experimental results on both Chinese $\\Rightarrow $ English and Japanese $\\Rightarrow $ English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translation models in previous work, and achieve a new state-of-the-art on the widely-used subtitle corpus. Manual evaluation confirms that the performance improvement comes from the alleviation of translation errors, which are mainly caused by subjective, objective as well as discourse-aware ZPs.", "We cast ZP prediction as a sequence labelling task, where each word is labelled if there is a pronoun missing before it. Given the input ${\\bf x}=\\lbrace {x}_1, {x}_2, \\dots , {x}_T\\rbrace $ with the last word $x_T$ being the end-of-sentence tag \u201c $\\langle $ eos $\\rangle $ \u201d, the output to be labelled is a sequence of labels ${\\bf zp} = \\lbrace {zp}_1, {zp}_2, \\dots , {zp}_T\\rbrace $ with ${zp}_t \\in \\lbrace N\\rbrace  \\cup \\mathbb {V}_{zp}$ . Among the label set, \u201c $N$ \u201d denotes no ZP, and $\\mathbb {V}_{zp}$ is the vocabulary of pronouns.", "There are two potential extensions to our work. First, we will evaluate our method on other implication phenomena (or called unaligned words BIBREF33 ) such as tenses and article words for NMT. Second, we will investigate the impact of different context-aware models on ZP translation, including multi-attention BIBREF23 and context-aware Transformer BIBREF25 .", "In \u201cFixed Error\u201d case, the dropped word \u201c\u5b83 (it)\u201d is an anaphoric ZP whose antecedent is the noun \u201c\u7535\u89c6 (television)\u201d in previous sentence while the dropped word \u201c\u4f60 (you)\u201d is a non-anaphoric ZP that depends upon speaker or listener. As seen, our \u201cJoin.\u201d model performs better than the \u201cExte.\u201d model because two ZP positions are syntactically recalled in the target side, showing that the joint approach have better capability of utilizing intra-sentential information for identifying ZPs. Besides, our \u201c+Dis.\u201d model can semantically fix the error by predicting correct ZP words, demonstrating that inter-sentential context can aid to recovering such complex ZPs. However, as shown in \u201cNon-Fixed Error\u201d case, there are still some ZPs can not be precisely predicted due to the misunderstanding of intentions of utterances. Thus, exploiting dialogue focus for ZP translation is our future work BIBREF26 .", "In \u201cFixed Error\u201d case, the dropped word \u201c\u5b83 (it)\u201d is an anaphoric ZP whose antecedent is the noun \u201c\u7535\u89c6 (television)\u201d in previous sentence while the dropped word \u201c\u4f60 (you)\u201d is a non-anaphoric ZP that depends upon speaker or listener. As seen, our \u201cJoin.\u201d model performs better than the \u201cExte.\u201d model because two ZP positions are syntactically recalled in the target side, showing that the joint approach have better capability of utilizing intra-sentential information for identifying ZPs. Besides, our \u201c+Dis.\u201d model can semantically fix the error by predicting correct ZP words, demonstrating that inter-sentential context can aid to recovering such complex ZPs. However, as shown in \u201cNon-Fixed Error\u201d case, there are still some ZPs can not be precisely predicted due to the misunderstanding of intentions of utterances."]}
{"question_id": "c9a323c152c5d9bc2d244e0ed10afbdb0f93062a", "predicted_answer": "", "predicted_evidence": ["Table 3 lists the results. We compare our models and the best external ZP prediction approach. As seen, our models also significantly improve translation performance, demonstrating the effectiveness and universality of the proposed approach.", "Here the summary $C$ consists of not only the dependencies between words, but also the relations between sentences. Following Voita:2018:ACL, we share the parameters of word-level encoder $\\textsc {Encoder}_{word}$ with the encoder component in standard NMT model. Note that, $\\textsc {Encoder}_{word}$ and $\\textsc {Encoder}_{sentence}$ can be implemented as arbitrary networks, such as recurrent networks BIBREF15 , convolutional networks BIBREF16 , or self-attention networks BIBREF17 . In this study, we used recurrent networks to implement our $\\textsc {Encoder}$ .", "$$\\begin{split}\nP({\\bf zp}|{\\bf h}^{rec}) = \\prod _{t=1}^{T}P({zp}_{t}|{\\bf h}^{rec}_{t}) \\\\ = \\prod _{t=1}^{T} g_l(zp_t, {\\bf h}^{rec}_t)\n\\end{split}$$   (Eq. 19)", "After we can obtain all sentence-level representations ${\\bf H}^{X}=\\lbrace {\\bf h}^{-K}, \\dots , {\\bf h}^{-1}\\rbrace $ , we feed them into a sentence-level encoder to produce a vector that represents the discourse-level context:", "In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourse-level information for better ZP prediction. Experimental results on both Chinese $\\Rightarrow $ English and Japanese $\\Rightarrow $ English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translation models in previous work, and achieve a new state-of-the-art on the widely-used subtitle corpus. Manual evaluation confirms that the performance improvement comes from the alleviation of translation errors, which are mainly caused by subjective, objective as well as discourse-aware ZPs."]}
{"question_id": "d6a815d24c46557827d8aca65d3ffd008ac1bc07", "predicted_answer": "", "predicted_evidence": ["Besides the automatic evaluation of answer generation over the DBpedia Infobox properties, we performed a detailed manual evaluation of the answers produced by the neural networks.", "The results of the automatic evaluation using the BLEU, METEOR and chrF metric are shown in Table TABREF17 . We observer BLEU and Meteor scores, which evaluate the generated answers on based on a n-gram overlap, of 16.48 and 13.97, respectively. BLEU-1 and the chrF score, which perform on a single word or character level, are slightly higher, i.e. 56.0 and 26.84.", "The training approach to the sequence-to-sequence neural model requires an aligned dataset of questions and answers, which are aligned on a sentence level. To train our QA system on generic knowledge, we extracted this required information from the DBpedia Infobox Property repository.", "Their RNN-based system, called FastQA, demonstrates good performance for extractive question answering due to the awareness of question words while processing the context. Additionally they introduce a composition function that goes beyond simple bag-of-words modelling. BIBREF7 demonstrate an approach to non-factoid answer generation with a separate component, which bases on BiLSTM to determine the importance of segments in the input. In contrast to other attention-based models, they determine the importance while assuming the independence of questions and candidate answers. BIBREF8 present their Gated-Attention reader for answering cloze-style questions over documents. The reader features a novel multiplicative gating mechanism in combination with a multi-hop architecture, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection.", "The automatic evaluation of the proposed QA system is based on the correspondence between the generated answer and gold standard. For the automatic evaluation we used the BLEU BIBREF19 , METEOR BIBREF20 and chrF BIBREF21 metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual generated segments (n-grams) by comparing them with a dataset of of the gold standard. Considering the shortness of the questions, we report besides the four-gram overlap (BLEU) also scores based on the unigram overlap (BLEU-1). Those scores, between 0 and 100 (perfect overlap), are then averaged over the whole evaluation dataset to reach an estimate of the overall quality of the generated answers."]}
{"question_id": "23252644c04a043f630a855b563666dd57179d98", "predicted_answer": "", "predicted_evidence": ["Visible \u201cthing\u201d objects are the only one to be described.", "In comparison with MS-COCO BIBREF16 data collection guidelines in terms of annotation, UIT-ViIC\u2019s guidelines has similar rules (1, 2, 8, 9, 10) . We extend from MS-COCO\u2019s guidelines with five new rules to our own and have modifications in the original ones.", "", "Generating descriptions for multimedia contents such as images and videos, so called Image Captioning, is helpful for e-commerce companies or news agencies. For instance, in e-commerce field, people will no longer need to put much effort into understanding and describing products' images on their websites because image contents can be recognized and descriptions are automatically generated. Inspired by Horus BIBREF0 , Image Captioning system can also be integrated into a wearable device, which is able to capture surrounding images and generate descriptions as sound in real time to guide people with visually impaired.", "Image Captioning has attracted attentions from researchers in recent years BIBREF1, BIBREF2, BIBREF3, and there has been promising attempts dealing with language barrier in this task by extending existed dataset captions into different languages BIBREF3, BIBREF4."]}
{"question_id": "2f75b0498cf6a1fc35f1fb1cac44fc2fbd3d7878", "predicted_answer": "", "predicted_evidence": ["Sportball Image Captioning can be used in certain sport applications, such as supportting journalists describing great amount of images for their articles.", "Personal opinion and emotion must be excluded while annotating.", "Only describe visible activities and objects included in image.", "Image Captioning has attracted attentions from researchers in recent years BIBREF1, BIBREF2, BIBREF3, and there has been promising attempts dealing with language barrier in this task by extending existed dataset captions into different languages BIBREF3, BIBREF4.", "Generating descriptions for multimedia contents such as images and videos, so called Image Captioning, is helpful for e-commerce companies or news agencies. For instance, in e-commerce field, people will no longer need to put much effort into understanding and describing products' images on their websites because image contents can be recognized and descriptions are automatically generated. Inspired by Horus BIBREF0 , Image Captioning system can also be integrated into a wearable device, which is able to capture surrounding images and generate descriptions as sound in real time to guide people with visually impaired."]}
{"question_id": "0d3193d17c0a4edc8fa9854f279c2a1b878e8b29", "predicted_answer": "", "predicted_evidence": ["Image Captioning has attracted attentions from researchers in recent years BIBREF1, BIBREF2, BIBREF3, and there has been promising attempts dealing with language barrier in this task by extending existed dataset captions into different languages BIBREF3, BIBREF4.", "", "In this study, generating image captions in Vietnamese language is put into consideration. One straightforward approach for this task is to translate English captions into Vietnamese by human or by using machine translation tool, Google translation. With the method of translating directly from English to Vietnamese, we found that the descriptions are sometimes confusing and unnatural to native people. Moreover, image understandings are cultural dependent, as in Western, people usually have different ways to grasp images and different vocabulary choices for describing contexts. For instance, in Fig. FIGREF2, one MS-COCO English caption introduce about \"a baseball player in motion of pitching\", which makes sense and capture accurately the main activity in the image. Though it sounds sensible in English, the sentence becomes less meaningful when we try to translate it into Vietnamese. One attempt of translating the sentence is performed by Google Translation, and the result is not as expected.", "Our main goal in this section is to see if Image Captioning models could learn well with Vietnamese language. To accomplish this task, we train and evaluate our dataset with two published Image Captioning models applying encoder-decoder architecture. The models we propose are Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey.", "Finally, we conduct experiments to evaluate state-of-the-art models (evaluated on English dataset) on UIT-ViIC dataset, then we analyze the performance results to have insights into our corpus."]}
{"question_id": "b424ad7f9214076b963a0077d7345d7bb5a7a205", "predicted_answer": "", "predicted_evidence": ["Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.", "", "In both datasets, we would like to control sentence length and focus on describing important subjects only in order to make sure that essential information is mainly included in captions. The MS-COCO threshold for sentence\u2019s length is 8, and we raise the number to 10 for our dataset. One reason for this change is that an object in image is usually expressed in many Vietnamese words. For example, a \u201cbaseball player\u201d in English can be translated into \u201cv\u1eadn \u0111\u1ed9ng vi\u00ean b\u00f3ng ch\u00e0y\u201d or \u201cc\u1ea7u th\u1ee7 b\u00f3ng ch\u00e0y\u201d, which already accounted for a significant length of the Vietnamese sentence. In addition, captions must be single sentences with continuous tense as we expect our model\u2019s output to capture what we are seeing in the image in a consise way.", "Google-translated Vietnamese (GT-sportball): The translated MS-COCO English dataset into Vietnamese using Google Translation API, categorized into sportball.", "Table TABREF24 summarizes top three most occuring words for each part-of-speech. Our dataset vocabulary size is 1,472 word classes, including 723 nouns, 567 verbs, and 182 adjectives. It is no surprise that as our dataset is about sports with balls, the noun \u201cb\u00f3ng\u201d (meaning \u201cball\") occurs most, followed by \u201cs\u00e2n\u201d and \"c\u1ea7u th\u1ee7\" (\u201cpitch\u201d and \u201cathlete\u201d respectively). We also found that the frequency of word \u201ctennis\u201d stands out among other adjectives, which specifies that the set covers the majority of tennis sport, followed by \u201cb\u00f3ng ch\u00e0y\u201d (meaning \u201cbaseball\u201d). Therefore, we expect our model to generate the best results for tennis images."]}
{"question_id": "0dfe43985dea45d93ae2504cccca15ae1e207ccf", "predicted_answer": "", "predicted_evidence": ["To enhance annotation efficiency, we present a web-based application for caption annotation. Fig. FIGREF10 is the annotation screen of the application.", "Image Captioning has attracted attentions from researchers in recent years BIBREF1, BIBREF2, BIBREF3, and there has been promising attempts dealing with language barrier in this task by extending existed dataset captions into different languages BIBREF3, BIBREF4.", "Familiar English words such as laptop, TV, tennis, etc. are allowed.", "Therefore, we come up with the approach of constructing a Vietnamese Image Captioning dataset with descriptions written manually by human. Composed by Vietnamese people, the sentences would be more natural and friendlier to Vietnamese users. The main resources we used from MS-COCO for our dataset are images. Besides, we consider having our dataset focus on sportball category due to several reasons:", "In this section, we describes procedures of building our sportball Vietnamese dataset, called UIT-ViIC."]}
{"question_id": "8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833", "predicted_answer": "", "predicted_evidence": ["Our dataset UIT-ViIC is constructed using images from Microsoft COCO (MS-COCO). MS-COCO dataset includes more than 150,000 images, divided into three distributions: train, vailidate, test. For each image, five captions are provided independently by Amazon\u2019s Mechanical Turk. MS-COCO is the most popular dataset for Image Captioning thanks to the MS-COCO challenge (2015) and it has a powerful evaluation server for candidates.", "Only describe visible activities and objects included in image.", "Original English (English-sportball): The original MS-COCO English dataset with 3,850 sportball images. This dataset is first evaluated in order to have base results for following comparisons.", "Each caption must be a single sentence with continuous tense.", "Image Captioning has attracted attentions from researchers in recent years BIBREF1, BIBREF2, BIBREF3, and there has been promising attempts dealing with language barrier in this task by extending existed dataset captions into different languages BIBREF3, BIBREF4."]}
{"question_id": "79885526713cc16eb734c88ff1169ae802cad589", "predicted_answer": "", "predicted_evidence": ["After finishing constructing UIT-ViIC dataset, we have a look in statistical analysis on our corpus in this section. UIT-ViIC covers 3,850 images described by 19,250 Vietnamese captions. Sticking strictly to our annotation guidelines, the majority of our captions are at the length of 10-15 tokens. We are using the term \u201ctokens\u201d here as a Vietnamese word can consist of one, two or even three tokens. Therefore, to apply Vietnamese properly to Image Captioning, we present a tokenization tool - PyVI BIBREF17, which is specialized for Vietnamese language tokenization, at words level. The sentence length using token-level tokenizer and word-level tokenizer are compared and illustrated in Fig. FIGREF23, we can see there are variances there. So that, we can suggest that the tokenizer performs well enough, and we can expect our Image Captioning models to perform better with Vietnamese sentences that are tokenized, as most models perform more efficiently with captions having fewer words.", "Therefore, we come up with the approach of constructing a Vietnamese Image Captioning dataset with descriptions written manually by human. Composed by Vietnamese people, the sentences would be more natural and friendlier to Vietnamese users. The main resources we used from MS-COCO for our dataset are images. Besides, we consider having our dataset focus on sportball category due to several reasons:", "Exclude name of places, streets (Chinatown, New York, etc.) and number (apartment numbers, specific time on TV, etc.)", "Personal opinion and emotion must be excluded while annotating.", "To evaluate our dataset, we use metrics proposed by most authors in related works of extending Image Captioning dataset, which are BLEU BIBREF11, ROUGE BIBREF12 and CIDEr BIBREF13. BLEU and ROUGE are often used mainly for text summarization and machine translation, whereas CIDEr was designed especially for evaluating Image Captioning models."]}
{"question_id": "0871827cfeceed4ee78ce7407aaf6e85dd1f9c25", "predicted_answer": "", "predicted_evidence": ["We evaluate our model on RACE dataset BIBREF6 , which consists of two subsets: RACE-M and RACE-H. RACE-M comes from middle school examinations while RACE-H comes from high school examinations. RACE is the combination of the two.", "For each candidate answer choice $i$ , its matching representation with the passage and question can be represented as $\\textbf {C}_i$ . Then our loss function is computed as follows:", "We compare our model with the following baselines: MRU(Multi-range Reasoning) BIBREF12 , DFN(Dynamic Fusion Networks) BIBREF11 , HCM(Hierarchical Co-Matching) BIBREF8 , OFT(OpenAI Finetuned Transformer LM) BIBREF13 , RSM(Reading Strategies Model) BIBREF14 . We also compare our model with the BERT baseline and implement the method described in the original paper BIBREF7 , which uses the final hidden vector corresponding to the first input token ([CLS]) as the aggregate representation followed by a classification layer and finally a standard classification loss is computed.", "To get the final representation for each candidate answer, a row-wise max pooling operation is used to $\\textbf {S}^{p}$ and $\\textbf {S}^{a}$ . Then we get $\\textbf {C}^{p} \\in R^l$ and $\\textbf {C}^{a} \\in R^l$ respectively. In the question side, $\\textbf {C}^{p^{\\prime }} \\in R^l$ and $\\textbf {C}^{q} \\in R^l$ are calculated. Finally, we concatenate all of them as the final output $\\textbf {C} \\in R^{4l}$ for each {P, Q, A} triplet.", "Machine reading comprehension and question answering has becomes a crucial application problem in evaluating the progress of AI system in the realm of natural language processing and understanding BIBREF0 . The computational linguistics communities have devoted significant attention to the general problem of machine reading comprehension and question answering."]}
{"question_id": "240058371e91c6b9509c0398cbe900855b46c328", "predicted_answer": "", "predicted_evidence": ["$$\\begin{split}\n\\textbf {C}^{p} = &Pooling(\\textbf {S}^{p}),\n\\textbf {C}^{a} = Pooling(\\textbf {S}^{a}),\\\\\n\\textbf {C}^{p^{\\prime }} = &Pooling(\\textbf {S}^{p^{\\prime }}),\n\\textbf {C}^{q} = Pooling(\\textbf {S}^{q}),\\\\\n\\textbf {C} &= [\\textbf {C}^{p}; \\textbf {C}^{a};\\textbf {C}^{p^{\\prime }};\\textbf {C}^{q}]\n\\end{split}$$   (Eq. 9)", "This layer encodes each token in passage and question into a fixed-length vector including both word embedding and contextualized embedding. We utilize the latest result from BERT BIBREF7 as our encoder and the final hidden state of BERT is used as our final embedding. In the origin BERT BIBREF7 , the procedure of processing multi-choice problem is that the final hidden vector corresponding to first input token ([CLS]) is used as the aggregation representation of the passage, the question and the candidate answer, which we think is too simple and too rough. So we encode the passage, the question and the candidate answer respectively as follows:", "For each candidate answer choice $i$ , its matching representation with the passage and question can be represented as $\\textbf {C}_i$ . Then our loss function is computed as follows:", "To integrate the original contextual representation, we follow the idea from BIBREF8 to fuse $\\textbf {M}^{a}$ with original $\\textbf {H}^p$ and so is $\\textbf {M}^{p}$ . The final representation of passage and the candidate answer is calculated as follows:", "where $W_1, W_2 \\in R^{2l \\times l}$ and $b_1 \\in R^{P \\times l}, b_2 \\in R^{(A) \\times l}$ are the parameters to learn. $[ ; ]$ is the column-wise concatenation and $-, \\cdot $ are the element-wise subtraction and multiplication between two matrices. Previous work in BIBREF9 , BIBREF10 shows this method can build better matching representation. $F$ is the activation function and we choose $ReLU$ activation function there. $\\textbf {S}^{p} \\in R^{P \\times l}$ and $\\textbf {S}^{a} \\in R^{A \\times l}$ are the final representations of the passage and candidate answer."]}
{"question_id": "c7d3bccee59ab683e6bf047579bc6eab9de9d973", "predicted_answer": "", "predicted_evidence": ["These results show that models trained on the news data have a significant advantage over the tweets model, and that bigram models performed slightly better than trigrams. We submitted trigram models trained on news and tweets to the official evaluation of SemEval-2017 Task 6. The trigram language models trained on the news data placed fourth in Subtask A and first in Subtask B.", "We began this research by participating in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor BIBREF7 . This included two subtasks : Pairwise Comparison (Subtask A) and Semi-ranking (Subtask B). Pairwise comparison asks a system to choose the funnier of two tweets. Semi-ranking requires that each of the tweets associated with a particular hashtag be assigned to one of the following categories : top most funny tweet, next nine most funny tweets, and all remaining tweets.", "Our language models performed better in the pairwise comparison, but it is clear that more investigation is needed to improve the semi-ranking results. We believe that Deep Learning may overcome some of the limits of Ngram language models, and so will explore those next.", "Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.", "Table 1 shows our results for both data sets when trained on bigrams and trigrams. The accuracy and distance measures are defined by the task organizers BIBREF7 . We seek high accuracy in picking the funnier tweet (Subtask A) and low distance (from the gold standard) in organizing the tweets into categories (Subtask B)."]}
{"question_id": "376c6c74f008bb79a0dd9f073ac7de38870e80ad", "predicted_answer": "", "predicted_evidence": ["We believe that Deep Learning techniques potentially offer improved handling of unknown words, long distance dependencies in text, and non\u2013linear relationships among words and concepts. Moving forward we intend to explore a variety of these ideas and describe those briefly below.", "Our language models performed better in the pairwise comparison, but it is clear that more investigation is needed to improve the semi-ranking results. We believe that Deep Learning may overcome some of the limits of Ngram language models, and so will explore those next.", "We used traditional Ngram language models as our first approach for two reasons : First, Ngram language models can learn a certain style of humor by using examples of that as the training data for the model. Second, they assign a probability to each input they are given, making it possible to rank statements relative to each other. Thus, Ngram language models make relative rankings of humorous statements based on a particular style of humor, thereby accounting for the continuous and subjective nature of humor.", "BIBREF6 finds that external knowledge is necessary to detect humor in tweet based data. This might include information about book and movie titles, song lyrics, biographies of celebrities etc. and is necessary given the reliance on current events and popular culture in making certain kinds of jokes.", "These results show that models trained on the news data have a significant advantage over the tweets model, and that bigram models performed slightly better than trigrams. We submitted trigram models trained on news and tweets to the official evaluation of SemEval-2017 Task 6. The trigram language models trained on the news data placed fourth in Subtask A and first in Subtask B."]}
{"question_id": "c59d67930edd3d369bd51a619849facdd0770644", "predicted_answer": "", "predicted_evidence": ["Our current language model approach is effective but does not account for out of vocabulary words nor long distance dependencies. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.", "We believe that Deep Learning techniques potentially offer improved handling of unknown words, long distance dependencies in text, and non\u2013linear relationships among words and concepts. Moving forward we intend to explore a variety of these ideas and describe those briefly below.", "After evaluating CNNs and LSTMs we will explore how to include domain knowledge in these models. One possibility is to create word embeddings from domain specific materials and provide those to the CNNs along with more general text. Another is to investigate the use of Tree\u2013Structured LSTMs BIBREF13 . These have the potential advantage of preserving non-linear structure in text, which may be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor.", "We used traditional Ngram language models as our first approach for two reasons : First, Ngram language models can learn a certain style of humor by using examples of that as the training data for the model. Second, they assign a probability to each input they are given, making it possible to rank statements relative to each other. Thus, Ngram language models make relative rankings of humorous statements based on a particular style of humor, thereby accounting for the continuous and subjective nature of humor.", "Computational humor is an emerging area of research that ties together ideas from psychology, linguistics, and cognitive science. Humor generation is the problem of automatically creating humorous statements (e.g., BIBREF0 , BIBREF1 ). Humor detection seeks to identify humor in text, and is sometimes cast as a binary classification problem that decides if some input is humorous or not (e.g., BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 ). However, our focus is on the continuous and subjective aspects of humor."]}
{"question_id": "9d6b2672b11d49c37a6bfb06172d39742d48aef4", "predicted_answer": "", "predicted_evidence": ["Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.", "These results show that models trained on the news data have a significant advantage over the tweets model, and that bigram models performed slightly better than trigrams. We submitted trigram models trained on news and tweets to the official evaluation of SemEval-2017 Task 6. The trigram language models trained on the news data placed fourth in Subtask A and first in Subtask B.", "BIBREF6 finds that external knowledge is necessary to detect humor in tweet based data. This might include information about book and movie titles, song lyrics, biographies of celebrities etc. and is necessary given the reliance on current events and popular culture in making certain kinds of jokes.", "Our language models performed better in the pairwise comparison, but it is clear that more investigation is needed to improve the semi-ranking results. We believe that Deep Learning may overcome some of the limits of Ngram language models, and so will explore those next.", "Computational humor is an emerging area of research that ties together ideas from psychology, linguistics, and cognitive science. Humor generation is the problem of automatically creating humorous statements (e.g., BIBREF0 , BIBREF1 ). Humor detection seeks to identify humor in text, and is sometimes cast as a binary classification problem that decides if some input is humorous or not (e.g., BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 ). However, our focus is on the continuous and subjective aspects of humor."]}
{"question_id": "b0e894536857cb249bd75188c3ca5a04e49ff0b6", "predicted_answer": "", "predicted_evidence": ["thm:attentioninfinitevalues applies if the sequence INLINEFORM0 is computed as the output of INLINEFORM1 . A similar result holds if it is computed as the output of an unsquashed linear transformation.", "Next, the pooling layer ( SECREF5 ) collapses the filter values at each time step. A pooled filter will be 1 if the invalid sequence it detects was matched somewhere and INLINEFORM0 otherwise.", "[Attention state complexity lower bound] The attention summary vector has state complexity INLINEFORM0", "Consider the case where keys and values have dimension 1. Further, let the input strings come from a binary alphabet INLINEFORM0 . We pick parameters INLINEFORM1 in the encoder such that, for all INLINEFORM2 , DISPLAYFORM0", "We can simulate a finite-state machine using the INLINEFORM0 construction from thm:srnreduction. We compute values for the following predicate at each time step: DISPLAYFORM0"]}
{"question_id": "94c22f72665dfac3e6e72e40f2ffbc8c99bf849c", "predicted_answer": "", "predicted_evidence": ["This equation is still linearly separable and guarantees that the initial step will be computed correctly.", "Thus, the output of the attention mechanism reduces to the sum DISPLAYFORM0", "p = (Wa h+ + ba) .", "By construction, there is a finite set INLINEFORM0 containing all possible configurations of every INLINEFORM1 . We bound the number of configurations for each INLINEFORM2 by INLINEFORM3 to get DISPLAYFORM0", "thm:lstmupperbound constitutes a very tight upper bound on the expressiveness of LSTM computation. Asymptotically, LSTMs are not powerful enough to model even the deterministic context-free language INLINEFORM0 ."]}
{"question_id": "ce8d8de78a21a3ba280b658ac898f73d0b52bf1b", "predicted_answer": "", "predicted_evidence": ["zhou2017mojitalk introduce emotion into generated utterances by creating a large-scale fine-grained emotion dialogue dataset that uses tagged emojis to classify utterance sentiment. Then they train a conditional variational autoencoder (CVAE) to generate responses given an input emotion. Along this line of research, li2016persona use Reddit users as a source of persona, and learn individual persona embeddings per user. The system then conditions on these embeddings to generate a response while maintaining coherence specific to the given user. pandey2018exemplar expand the context of an existing dialogue model by extracting input responses from the training set that are most similar to the current input. These \"exemplar\" responses are then conditioned on to use as reference for final response generation. In another attempt to add context, young2018augmenting utilize a relational database to extract specific entity relations that are relevant for the current input. These relations provide more context for the dialogue model and allows it to respond to the user with information it did not observe in the training set.", "In this section we introduce a somewhat simpler, but more effective memory module architecture. In contrast to the previous D-NTMS architecture, we combine the encoder-decoder architecture of the sequence to sequence GRU into a single language model. This combination entails the model predicting all tokens in the dialogue history in sequence. This change in setup exploits the property that the response is in essence drawn from the same distribution as all previous utterances, and so should not be treated any differently. This language model variant learns to predict all utterances in the dialogue history, and thus treats the response as just another utterance to predict. This setup may also help the model learn the flow of conversation from beginning to end.", "In this architecture, we only use one NTM. This change is motivated by the possibility that the speaker NTMs from the previous architecture may have difficulty exchanging information, and thus cannot adequately represent each utterance in the context of the previous one. We follow an identical setup as before and split the dialogue history into segments. A GRU processes each segment in sequence. Between each segment, the output GRU state is used to query and write to the NTM module to store and retrieve relevant information about the context history so far. This information is conditioned on for all subsequent tokens in the next segment, in order to exploit this information to make more informed predictions. Lastly, the GRU NTM has an internal LSTM controller which guides the read and writes to and from the memory section. Reads are facilitated via content-based addressing, where a cosine similarity mechanism selects entries that most resemble the query. The Neural Turing Machine utilized can be found as an existing Github implementation.", "This model was able to answer philosophical questions and performed well with common sense reasoning. Similarly, serban2016building train a hierarchical LSTM architecture (HRED) on the MovieTriples dataset, which contains examples of the form (utterance #1, utterance #2, utterance #3). However, this dataset is small and does not have conversations of larger length. They show that using a context recurrent neural network (RNN) to read representations at the utterance-level allows for a more top-down perspective on the dialogue history. Finally, serban2017hierarchical build a dialogue system which injects diversity into output responses (VHRED) through the use of a latent variable for variational inference BIBREF3. They argue that the injection of information from the latent variables during inference increases response coherence without degrading response quality. They train the full system on the Twitter Dialogue corpus, which contains generic multi-turn conversations from public Twitter accounts. They also train on the Ubuntu Dialogue Corpus, a collection of multi-turn vocabulary-rich conversations extracted from Ubuntu chat logs.", "$z_t$ denotes the update gate. The encoder GRU in this memory architecture reads a token at each time step, and encodes a context representation $c$ at the end of the input sequence. In addition to that, the memory enhanced model implements two Neural Turing Machines (NTMs). Each of them is for one speaker in the conversation, since the Ubuntu dataset has two speakers in every conversation. Every turn in a dialogue is divided in 4 \u201csegments\". If a turn has 20 tokens, for example, a segment contains 5 tokens. The output of the GRU is written to the NTM at the end of every segment. It does not output anything useful here, but the internal memory is being updated each time. When the dialogue switches to next turn, the current NTM pauses and the other NTM starts to work in the same way."]}
{"question_id": "e069fa1eecd711a573c0d5c83a3493f5f04b1d8a", "predicted_answer": "", "predicted_evidence": ["bowman2015generating observe that latent variables can sometimes degrade, where the system chooses not to store information in the variable and does not condition on it when producing the output. bowman2015generating introduce a process called KL-annealing which slowly increases the KL divergence loss component over the course of training. However, BIBREF8 claim that KL annealing is not enough, and introduce utterance dropout to force the model to rely on information stored in the latent variable during response generation. They apply this system to conversational modelling.", "The decoder GRU works the same way as the baseline model. Each time it reads a token, from either the true response or the generated response, depending on whether teacher force training is used. This token and the context representation $c$ generated by the encoder GRU are both used as input to the decoder GRU.", "zhou2017mojitalk introduce emotion into generated utterances by creating a large-scale fine-grained emotion dialogue dataset that uses tagged emojis to classify utterance sentiment. Then they train a conditional variational autoencoder (CVAE) to generate responses given an input emotion. Along this line of research, li2016persona use Reddit users as a source of persona, and learn individual persona embeddings per user. The system then conditions on these embeddings to generate a response while maintaining coherence specific to the given user. pandey2018exemplar expand the context of an existing dialogue model by extracting input responses from the training set that are most similar to the current input. These \"exemplar\" responses are then conditioned on to use as reference for final response generation. In another attempt to add context, young2018augmenting utilize a relational database to extract specific entity relations that are relevant for the current input. These relations provide more context for the dialogue model and allows it to respond to the user with information it did not observe in the training set.", "However, this dataset is small and does not have conversations of larger length. They show that using a context recurrent neural network (RNN) to read representations at the utterance-level allows for a more top-down perspective on the dialogue history. Finally, serban2017hierarchical build a dialogue system which injects diversity into output responses (VHRED) through the use of a latent variable for variational inference BIBREF3. They argue that the injection of information from the latent variables during inference increases response coherence without degrading response quality. They train the full system on the Twitter Dialogue corpus, which contains generic multi-turn conversations from public Twitter accounts. They also train on the Ubuntu Dialogue Corpus, a collection of multi-turn vocabulary-rich conversations extracted from Ubuntu chat logs. du2018variational adapt from the VHRED architecture by increasing the influence of the latent variables on the output utterance.", "With a neural language model predicting tokens, it is then necessary to insert reads and writes from a Neural Turing Machine. In this architecture, we only use one NTM. This change is motivated by the possibility that the speaker NTMs from the previous architecture may have difficulty exchanging information, and thus cannot adequately represent each utterance in the context of the previous one. We follow an identical setup as before and split the dialogue history into segments. A GRU processes each segment in sequence. Between each segment, the output GRU state is used to query and write to the NTM module to store and retrieve relevant information about the context history so far. This information is conditioned on for all subsequent tokens in the next segment, in order to exploit this information to make more informed predictions. Lastly, the GRU NTM has an internal LSTM controller which guides the read and writes to and from the memory section. Reads are facilitated via content-based addressing, where a cosine similarity mechanism selects entries that most resemble the query."]}
{"question_id": "8db11d9166474a0e98b99ac7f81d1f14539d79ec", "predicted_answer": "", "predicted_evidence": ["vinyals2015neural train a sequence-to-sequence LSTM-based dialogue model on messages from an IT help-desk chat service, as well as the OpenSubtitles corpus, which contains subtitles from popular movies. This model was able to answer philosophical questions and performed well with common sense reasoning. Similarly, serban2016building train a hierarchical LSTM architecture (HRED) on the MovieTriples dataset, which contains examples of the form (utterance #1, utterance #2, utterance #3). However, this dataset is small and does not have conversations of larger length. They show that using a context recurrent neural network (RNN) to read representations at the utterance-level allows for a more top-down perspective on the dialogue history. Finally, serban2017hierarchical build a dialogue system which injects diversity into output responses (VHRED) through the use of a latent variable for variational inference BIBREF3. They argue that the injection of information from the latent variables during inference increases response coherence without degrading response quality.", "While coherence and diversity remain the primary focus of model dialogue architectures, many have tried to incorporate additional capabilities. zhou2017mojitalk introduce emotion into generated utterances by creating a large-scale fine-grained emotion dialogue dataset that uses tagged emojis to classify utterance sentiment. Then they train a conditional variational autoencoder (CVAE) to generate responses given an input emotion. Along this line of research, li2016persona use Reddit users as a source of persona, and learn individual persona embeddings per user. The system then conditions on these embeddings to generate a response while maintaining coherence specific to the given user. pandey2018exemplar expand the context of an existing dialogue model by extracting input responses from the training set that are most similar to the current input. These \"exemplar\" responses are then conditioned on to use as reference for final response generation. In another attempt to add context, young2018augmenting utilize a relational database to extract specific entity relations that are relevant for the current input.", "Other attempts to increase diversity focus on selecting diverse responses after the model is trained. li2015diversity introduce a modification of beam search. Beam search attempts to find the highest probability response to a given input by producing a tree of possible responses and \"pruning\" branches that have the lowest probability. The top K highest probability responses are returned, of which the highest is selected as the output response. li2015diversity observe that beam search tends to select certain families of responses that temporarily have higher probability. To combat this, a discount factor of probabilities is added to responses that come from the same parent response candidate. This encourages selecting responses that are different from one another when searching for the highest probability target.", "bowman2015generating observe that latent variables can sometimes degrade, where the system chooses not to store information in the variable and does not condition on it when producing the output. bowman2015generating introduce a process called KL-annealing which slowly increases the KL divergence loss component over the course of training. However, BIBREF8 claim that KL annealing is not enough, and introduce utterance dropout to force the model to rely on information stored in the latent variable during response generation. They apply this system to conversational modelling.", "As in the baseline model, the encoder and decoder each has an Gated Recurrent Unit (GRU) inside. A GRU is a type of recurrent neural networks that coordinates forgetting and write of information, to make sure they don't both occur simultaneously. This is accomplished via an \"update gate.\" A GRU architecture processes a list of inputs in sequence, and is described by the following equations:"]}
{"question_id": "fa5f5f58f6277a1e433f80c9a92a5629d6d9a271", "predicted_answer": "", "predicted_evidence": ["The arbitrator module is fundamentally a text classifier. However, in this task, we make the module maximally utilize both dialogue history and ground truth's semantic information. So we turned the problem of maximizing $Y$ from $X$ in equation (DISPLAY_FORM13) to:", "The performances on different arbitrators with the same LSTM-attention imaginators are shown in Table TABREF30. From those results, we can directly compared with the corresponding baseline models. The imaginators with BERT based arbitrator make the best results in both datasets while all ITA models beat the baseline models.", "Now, given a dialogue history $X$ and tags $T$, the goal of the model is to predict a label $Y \\in \\lbrace 0,1\\rbrace $, the action the agent would take, where $Y = 0$ means the agent will wait the user for next message, and $Y = 1$ means the agent will reply immediately. Formally we are going to maximize following probability:", "Finally, we have the modified datasets which imitate the real life human chatting behaviors as shown in Figure FIGREF1. Our datasets and code will be released to public for further researches in both academic and industry.", "The hyper-parameter settings adopted in baselines and our model are the best practice settings for each training set. All models are tested with various hyper-parameter settings to get their best performance. Baseline models are Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14."]}
{"question_id": "3b9da1af1550e01d2e6ba2b9edf55a289f5fa8e2", "predicted_answer": "", "predicted_evidence": ["Our problem is formulated as follows. There is a conversation history represented as a sequence of utterances: $X = \\lbrace x_1, x_2, ..., x_m\\rbrace $, where each utterance $x_i$ itself is a sequence of words $x_{i_1}, x_{i_2}, x_{i_3}...x_{i_n}$. Besides, each utterance has some additional tags:", "Though end-to-end methods play a more and more important role in dialogue system, the text classification modules BIBREF8, BIBREF9 remains very useful in many problems like emotion recognition BIBREF10, gender recognition BIBREF11, verbal intelligence, etc. There have been several widely used text classification methods proposed, e.g. Recurrent Neural Networks (RNNs) and CNNs. Typically RNN is trained to recognize patterns across time, while CNN learns to recognize patterns across space. BIBREF12 proposed TextCNNs trained on top of pre-trained word vectors for sentence-level classification tasks, and achieved excellent results on multiple benchmarks.", "Finally, we have the modified datasets which imitate the real life human chatting behaviors as shown in Figure FIGREF1. Our datasets and code will be released to public for further researches in both academic and industry.", "As shown in Figure FIGREF7 (a), imaginator itself is a sequence generation model. We use one-hot embedding to convert all words and relative tags, e.g. turn tags and place holders, to one-hot vectors $w_n \\in \\textbf {R}^V$, where $V$ is the length of vocabulary list. Then we extend each word $x_{i_j}$ in utterance $x_i$ by concatenating the token itself with turn tag, identity tag and subturn tag. We adopt SEQ2SEQ as the basic architecture and LSTMs as the encoder and decoder networks. LSTMs will encode each extended word $w_t$ as a continuous vector $h_t$ at each time step $t$. The process can be formulated as following:", ", and thus we get the final feature map of the input sequence."]}
{"question_id": "f88f45ef563ea9e40c5767ab2eaa77f4700f95f8", "predicted_answer": "", "predicted_evidence": ["Finally, the trained agent imaginator and user imaginator are obtained.", "The arbitrator module is fundamentally a text classifier. However, in this task, we make the module maximally utilize both dialogue history and ground truth's semantic information. So we turned the problem of maximizing $Y$ from $X$ in equation (DISPLAY_FORM13) to:", "All species are unique, but languages make humans uniquest BIBREF0. Dialogues, especially spoken and written dialogues, are fundamental communication mechanisms for human beings. In real life, tons of businesses and entertainments are done via dialogues. This makes it significant and valuable to build an intelligent dialogue product. So far there are quite a few business applications of dialogue techniques, e.g. personal assistant, intelligent customer service and chitchat companion.", "We adopt several architectures like Bi-GRUs, TextCNNs and BERT as the basis of arbitrator module. We will show how to build an arbitrator by taking TextCNNs as an example.", "In summary, this paper makes the following contributions:"]}
{"question_id": "99e99f2c25706085cd4de4d55afe0ac43213d7c8", "predicted_answer": "", "predicted_evidence": ["There are two issues when applying existing dialogue agents to real life conversation. Firstly, when user sends a short utterance as the start of a conversation, the agent has to make a decision to avoid generating bad responses based on semantically incomplete utterance. Secondly, dialogue agent cutting in the conversation at an unreasonable time could confuse user and mess up the pace of conversation, leading to nonsense interactions.", "Creating a perfect artificial human-computer dialogue system is always a ultimate goal of natural language processing. In recent years, deep learning has become a basic technique in dialogue system. Lots of work has investigated on applying neural networks to dialogue system's components or end-to-end dialogue frameworks BIBREF4, BIBREF5. The advantage of deep learning is its ability to leverage large amount of data from internet, sensors, etc. The big conversation data and deep learning techniques like SEQ2SEQ BIBREF6 and attention mechanism BIBREF7 help the model understand the utterances, retrieve background knowledge and generate responses.", "speakers' identification tags $agent$ or $user$ to show who sends this utterance.", "Besides RNNs and CNNs, BIBREF13 proposed a new network architecture called Transformer, based solely on attention mechanism and obtained promising performance on many NLP tasks. To make the best use of unlabeled data, BIBREF14 introduced a new language representation model called BERT based on transformer and obtained state-of-the-art results.", "Because the task we concentrate on is different from traditional ones, to make the datasets fit our problems and real life, we modify the datasets with the following steps:"]}
{"question_id": "da10e3cefbbd7ec73eabc6c93d338239ce84709e", "predicted_answer": "", "predicted_evidence": ["To make all the various pieces communicate, a comprehensive pipeline is necessary in order to successfully coordinate the various tasks. Specifically, there are three main pieces of software/technology that must communicate with each other: PETRARCH, Stanford's CoreNLP software, and the MongoDB instance. For the realtime data component, the web scraper must also fit into this system. The overall flow of this pipeline is demonstrated in the figure below.", "As has been noted, events are coded on two primary dimensions: event codes and actors. Most political event datasets are dominated by low-level political events that lack a strong valence. These are usually routine events such as statements that occur often. Figures 4 and 5 show the breakdown of event types within the current Phoenix data, both of which confirm this existing pattern. The addition of the 0 quad class category was designed to capture these types of events so that they can be easily removed to allow end users to easily focus on more substantive political events. Following these lower-level event types, the event codes 19 and 17, \u201cFight\u201d and \u201cCoerce\u201d respectively, are the next most common. The prevalence of 19 codes is unsurprising given that the underlying dictionaries were structured in such a way that many events defaulted to this category.", "The second major change between PETRARCH and PETRARCH2 is the internal category coding logic within PETRARCH2. In short, PETRARCH2 allows for interactions of verbs to create a different category classification than either verb on its own would produce. For PETRARCH, such things would have to be defined explicitly within the dictionaries. In PETRARCH2, however, there is a coding scheme that allows verbs like \u201cintend\u201d and \u201caid\u201d to interact in order to create a different coding than either verb on its own would create. Additionally, PETRARCH2 brought about a refactoring and speedup of the code base and a reformatting of the underlying verb dictionaries. This reformatting of the dictionaries also included a \u201ccleaning up\u201d of various verb patterns within the dictionaries. This was largely due to changes internal to the coding engine such as the tight coupling to the constituency parse tree and the verb interactions mentioned above.", "There are several ways that the scraping of news content from the web can occur. A system can sit on top of an aggregator such as Google News, use a true spidering system that follows links from a seed list, or can pull from a designated list of trusted resources. Each system has its benefits and challenges. The use of an aggregator means that a project is subject to another layer of complexity that is out of the user's control; those making use of Google News have no say over how, and what, content is aggregated. Implementing a full-scale web spider to obtain news content is a labor and maintenance intensive process that calls for a dedicated team of software engineers. This type of undertaking is beyond the scope of the current event data projects. The final option is to use a list of predefined resources, in this case RSS feeds of news websites, and pull content from these resources. For the purposes of the realtime event data discussed herein, I have settled on the final option.", "In order to better understand how the dataset is performing it is helpful to pull out a specific case and examine a similar set of attributes as seen in the previous section. One of the major, ongoing events in the international arena during the time currently covered by the Phoenix dataset is the conflict in Syria. Given this, I extract any events that contain the Syria country code, SYR, as the SourceActorEntity or TargetActorEntity. Figure 9 shows the plot of the daily aggregated event counts. In this plot it is possible to see actions such as the beginning of United State intervention against ISIL, along with other significant events within the country. As with any event data, it is important to note that the event counts shown do not represent the on-the-ground truth of events in Syria, but instead reflect the media coverage of said events. Thus, some of the peaks and troughs are the result of media coverage instead of any actual shift in reality."]}
{"question_id": "00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd", "predicted_answer": "", "predicted_evidence": ["Given this, I extract any events that contain the Syria country code, SYR, as the SourceActorEntity or TargetActorEntity. Figure 9 shows the plot of the daily aggregated event counts. In this plot it is possible to see actions such as the beginning of United State intervention against ISIL, along with other significant events within the country. As with any event data, it is important to note that the event counts shown do not represent the on-the-ground truth of events in Syria, but instead reflect the media coverage of said events. Thus, some of the peaks and troughs are the result of media coverage instead of any actual shift in reality. In order to provide more context to the time series, Figure 10 shows the breakout of the QuadClass variable for this data subset. The dominant event types are the low-level events described in the previous section, but the \u201cMaterial Conflict\u201d class is higher than in the broader dataset. This is, of course, as expected given the ongoing conflict within Syria.", "While it is not currently possible to make definitive judgements as to which dataset most closely captures \u201ctruth\u201d, another point more deeply discussed in Chapter 5, it is interesting to note that the statistical signal contained within the two datasets, as evidenced by the correlations and broad trends, is not largely different.", "Figures 11 and 12 show the top actor and entity codes for the Syria subset. Various Syrian actors appear most often, with other Middle East countries also accounting for a fairly high portion of events. Also seen within this group of top actors is ISIL and the United States. Additionally, Russia appears high in the rankings of actors within Syria, capturing the recent activity by Russian forces in support of the Assad regime.", "Political event data has existed in various forms since the 1970s. Two of the most common political event datasets were the World Event Interaction Survey (WEIS) and the Conflict and Peace Data Bank (COPDAB) BIBREF0 , BIBREF1 . These two datasets were eventually replaced by the projects created by Philip Schrodt and various collaborators. In general, these projects were marked by the use of the Conflict and Mediation Event Observations (CAMEO) coding ontology and automated, machine-coding rather than human coding BIBREF2 , BIBREF3 . The CAMEO ontology is made up of 20 \u201ctop-level\u201d categories that encompass actions such as \u201cMake Statement\u201d or \u201cProtest\u201d, and contains over 200 total event classifications.", "Figure FIGREF58 shows a pairwise comparison of each of the four QuadClass, excluding the \u201cNeutral\u201d category, as shown in Table TABREF32 . The main takeaway is that the broad trends appear largely the same, though it is important to note the two categories that differ in a significant manner: \u201cVerbal Cooperation\u201d and \u201cMaterial Conflict.\u201d These differences largely come down to implantation details that differ between the BBN ACCENT coder and the PETRARCH coder. In short, the two coders implement slightly different definitions of the various CAMEO categories based on a perception on the part of the designers or end-users as to what constitutes an interesting and/or valid event within CAMEO. This point leads to a deeper discussion as to what, exactly, constitutes the CAMEO coding ontology; Chapter 5 contains a deeper discussion of these issues."]}
{"question_id": "6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a", "predicted_answer": "", "predicted_evidence": ["Discussion 2.14. A critical idea in this section was to use the fact that INLINEFORM0 near INLINEFORM1 , and in fact this idea can be used for any activation function with a well-behaved Taylor series expansion around INLINEFORM2 .", "Discussion 2.15. We \u201ccheated\" a little bit by allowing INLINEFORM0 edge weights and by having INLINEFORM1 where INLINEFORM2 wasn't quite linear. However, INLINEFORM3 edge weights make sense in the context of allowing infinite precision, and simple nonlinear functions over the hidden nodes are often used in practice, like the common softmax activation function.", "We are now ready to combine these lemmas to prove an important result, the analogue of Theorem 1.10 for GRUs:", "Because to the our knowledge there is no analogue of the Chomsky-Sch INLINEFORM0 tzenberger Theorem for Turing recognizable languages, it seems difficult to directly extend our methods to prove that recurrent architectures are as computationally powerful as Turing machines. However, just as PDAs can lazily be described as a DFA with an associated stack, it is well-known that Turing machines are equally as powerful as DFAs with associated queues, which can be simulated with two stacks. Such an approach using two counters was used in proofs in [6], [8] to establish that RNNs with arbitrary precision can emulate Turing machines. We believe that an approach related to this fact could ultimately prove successful, but it would be more useful if set up as in the proofs above in a way that is faithful to the architecture of the neural networks. Counter automata of this sort are also quite unlike the usual implementations found for context-free languages or their extensions for natural languages. Work described in [10] demonstrates that in practice, LSTMs cannot really generalize to recognize the Dyck language INLINEFORM1 .", "For this example we obtain INLINEFORM0"]}
{"question_id": "0b5a505c1fca92258b9e83f53bb8cfeb81cb655a", "predicted_answer": "", "predicted_evidence": ["Thus the state INLINEFORM0 allows us to keep track of the old state INLINEFORM1 without having to multiply by any constant greater than INLINEFORM2 . Furthermore, for large INLINEFORM3 , INLINEFORM4 will be extremely small, allowing us to abuse the fact that INLINEFORM5 for small values of INLINEFORM6 . In terms of the stack of digits interpretation of INLINEFORM7 , INLINEFORM8 is the same except between every pop or push we add INLINEFORM9 zeros to the top of the stack.", "Consider the GRU described in the proof of Theorem 2.12 for INLINEFORM0 and INLINEFORM1 . We will show the evolution of its hidden state as it reads various inputs:", "where INLINEFORM0 is the inverse of the sigmoid function. For sufficiently large INLINEFORM1 , clearly our use of INLINEFORM2 is well-defined. We will show the following invariant:", "Otherwise, there exists some minimal INLINEFORM0 such that INLINEFORM1 . Then INLINEFORM2 for some INLINEFORM3 . Consider the unique INLINEFORM4 such that INLINEFORM5 . If INLINEFORM6 then from the proof of Lemma 2.8 we have that INLINEFORM7 and so INLINEFORM8 . Since INLINEFORM9 this means that INLINEFORM10 . If INLINEFORM11 then from the analogue of the proof of Lemma 2.8 for INLINEFORM12 , we obtain INLINEFORM13 . This completes the proof. INLINEFORM14", "For this example we obtain INLINEFORM0"]}
{"question_id": "2b32cf05c5e736f764ceecc08477e20ab9f2f5d7", "predicted_answer": "", "predicted_evidence": ["The label cardinality of the training dataset was about 1.070 (train: 1.069, dev: 1.072) in the root nodes, pointing to a clearly low multi-label problem, although there were samples with up to 4 root nodes assigned. This means that the traditional machine learning systems would promote single label predictions. Subtask B has a label cardinality of 3.107 (train: 3.106, dev: 3.114), with 1 up to 14 labels assigned per sample. Table TABREF4 shows a short dataset summary by task.", "We used two different approaches for each subtask. In subtask A, we used a heavier feature extraction method and a linear Support-Vector-Machine (SVM) whereas for subtask B we used a more light-weighted feature extraction with same SVM but in a local hierarchical classification fashion, i.e. for each parent node such a base classifier was used. We describe in the following the approaches in detail. They were designed to be light and fast, to work almost out of the box, and to easily generalise.", "Scikit Learn Hierarchical (Hsklearn) was forked and improved to deal with multi-labels for the task, especially, allowing each node to perform its own preprocessing. This guaranteed that the performance of our own implementation was surpassed and that a contribution for the community was made. This ensured as well that the results are easily reproducible.", "We divide this Section in two parts, in first we conduct experiments on the development set and in the second on the test set, discussing there the competition results.", "Our approach was a traditional NLP one, since we employed them successfully in several projects BIBREF1, BIBREF2, BIBREF3, with even more samples and larger hierarchies. We compared also new libraries and our own implementation, but focused on the post-processing of the multi-labels, since this aspect seemed to be the most promising improvement to our matured toolkit for this task. This means but also, to push recall up and hope to not overshot much over precision."]}
{"question_id": "014a3aa07686ee18a86c977bf0701db082e8480b", "predicted_answer": "", "predicted_evidence": ["For the HMC subtask B, we used a simple threshold based on the results obtained for LCA. Especially, using multiple models per node could cause a different scaling.", "We divide this Section in two parts, in first we conduct experiments on the development set and in the second on the test set, discussing there the competition results.", "We use a local parent node strategy, which means the parent node decides which of its children will be assigned to the sample. This creates also the necessity of a virtual root node. For each node the same base classifier is trained independently of the other nodes. We also adapt each feature extraction with the classifier in each single node much like BIBREF11. As base classifier, a similar one to Fig. FIGREF8 was used, where only one 1-7 word n-gram, one 1-3 word n-gram with German stopwords removal and one char 2-3 n-gram feature extraction were employed, all with maximum 70k features. We used two implementations achieving very similar results. In the following, we give a description of both approaches.", "Our approach was a traditional NLP one, since we employed them successfully in several projects BIBREF1, BIBREF2, BIBREF3, with even more samples and larger hierarchies. We compared also new libraries and our own implementation, but focused on the post-processing of the multi-labels, since this aspect seemed to be the most promising improvement to our matured toolkit for this task. This means but also, to push recall up and hope to not overshot much over precision.", "Despite the very high the scores, it will be difficult to achieve even higher scores with simple NLP scores. Especially, the n-gram TF-IDF with SVM could not resolve descriptions which are science fiction, but are written as non-fiction book, where context over multiple sentences and word groups are important for the prediction."]}
{"question_id": "6e6d64e2cb7734599890fff3f10c18479756d540", "predicted_answer": "", "predicted_evidence": ["Hierarchical Multi-label Classification (HMC) is an important task in Natural Language Processing (NLP). Several NLP problems can be formulated in this way, such as patent, news articles, books and movie genres classification (as well as many other classification tasks like diseases, gene function prediction). Also, many tasks can be formulated as hierarchical problem in order to cope with a large amount of labels to assign to the sample, in a divide and conquer manner (with pseudo meta-labels). A theoretical survey exists BIBREF0 discussing on how the task can be engaged, several approaches and the prediction quality measures. Basically, the task in HMC is to assign a sample to one or many nodes of a Directed Acyclic Graph (DAG) (in special cases a tree) based on features extracted from the sample.", "Our implementation is light-weighted and optimized for a short pipeline, however for large amount of data, saving each local parent node model to the disk. However, it does not conforms the way scikit-learn is designed. Further, in contrast to the Scikit Learn Hierarchical, we give the possibility to optimize with a grid search each feature extraction and classifier per node. This can be quite time consuming, but can also be heavily parallelized. In the final phase of the competition, we did not employ it because of time constrains and the amount of experiments performed in the Experiments Section was only possible with a light-weighted implementation.", "The GermEval 2019 Task 1 - Shared Task on hierarchical classification of German blurbs focus on the concrete challenge of classifying short descriptive texts of books into the root nodes (subtask A) or into the entire hierarchy (subtask B). The hierarchy can be described as a tree and consisted of 343 nodes, in which there are 8 root nodes. With about 21k samples it was not clear if deep learning methods or traditional NLP methods would perform better. Especially, in the subtask A, since for subtask B some classes had only a few examples. Although an ensemble of traditional and deep learning methods could profit in this area, it is difficult to design good heterogeneous ensembles.", "Still, a less considered problem in HMC is the number of predicted labels, especially regarding the post-processing of the predictions. We discussed this thoroughly in BIBREF1. The main two promising approaches were proposed by BIBREF9 and BIBREF10. The former focuses on column and row based methods for estimating the appropriate threshold to convert a prediction confidence into a label prediction. BIBREF10 used the label cardinality (BIBREF5), which is the mean average label per sample, of the training set and change the threshold globally so that the test set achieved similar label cardinality.", "For subtask A, we use the one depicted in Fig. FIGREF8, for subtask B, a similar more light-weight approach was employed as base classifier (described later). As can be seen, several vectorizer based on different n-grams (word and character) with a maximum of 100k features and preprocessing, such as using or not stopwords, were applied to the blurbs. The term frequency obtained were then weighted with inverse document frequency (TF-IDF). The results of five different feature extraction and weighting modules were given as input for a vanilla SVM classifier (parameter C=1.5) which was trained in a one-versus-all fashion."]}
{"question_id": "8675d39f1647958faab7fa40cdaab207d4fe5a29", "predicted_answer": "", "predicted_evidence": ["In Table TABREF25, the results of various steps towards the final model can be seen. An SVM-TF-IDF model with word unigram already performed very well. Adding more n-grams did not improve, on the contrary using n-grams 1-7 decreased the performance. Only when removing stopwords it improved again, but then substantially. Nonetheless, a character 2-3 n-gram performed best between these simple models. This is interesting, since this points much more to not which words were used, but more on the phonetics.", "The label cardinality of the training dataset was about 1.070 (train: 1.069, dev: 1.072) in the root nodes, pointing to a clearly low multi-label problem, although there were samples with up to 4 root nodes assigned. This means that the traditional machine learning systems would promote single label predictions. Subtask B has a label cardinality of 3.107 (train: 3.106, dev: 3.114), with 1 up to 14 labels assigned per sample. Table TABREF4 shows a short dataset summary by task.", "An overview of hierarchical classification was given in BIBREF0 covering many aspects of the challenge. Especially, there are local approaches which focus on only part of the hierarchy when classifying in contrast to the global (big bang) approaches.", "Our implementation is light-weighted and optimized for a short pipeline, however for large amount of data, saving each local parent node model to the disk. However, it does not conforms the way scikit-learn is designed. Further, in contrast to the Scikit Learn Hierarchical, we give the possibility to optimize with a grid search each feature extraction and classifier per node. This can be quite time consuming, but can also be heavily parallelized. In the final phase of the competition, we did not employ it because of time constrains and the amount of experiments performed in the Experiments Section was only possible with a light-weighted implementation.", "For the HMC subtask B, we used a simple threshold based on the results obtained for LCA. Especially, using multiple models per node could cause a different scaling."]}
{"question_id": "14fdc8087f2a62baea9d50c4aa3a3f8310b38d17", "predicted_answer": "", "predicted_evidence": ["The plots show that as the amount of speaker overlap increases, the accuracy gain (relative to the unprocessed case) of the weaker signal enhancement (BFIt) drops. This is an expected result since BFIt is not a source separation algorithm. Conversely, as the amount of speaker overlap increases, the accuracy gain (relative to None) of the stronger GSS enhancement improves quite significantly. A rather small decrease in accuracy is observed in the mismatched case (Fig. FIGREF25) for GSS1 in the lower overlap regions. As already mentioned in Section SECREF3, this is due to the masking stage. It has previously been observed that using masking for speech enhancement without a cross talker decreases ASR recognition performance. We have also included in Fig. FIGREF25 the GSS1 version without masking (GSS w/o Mask), which indeed yields significant accuracy gains on segments with little overlap. However, since the overall accuracy of GSS1 with masking is higher than the overall gain of GSS1 without masking, GSS w/o mask was not included in the previous experiments.", "FIGREF22 and FIGREF25. The plots show that as the amount of speaker overlap increases, the accuracy gain (relative to the unprocessed case) of the weaker signal enhancement (BFIt) drops. This is an expected result since BFIt is not a source separation algorithm. Conversely, as the amount of speaker overlap increases, the accuracy gain (relative to None) of the stronger GSS enhancement improves quite significantly. A rather small decrease in accuracy is observed in the mismatched case (Fig. FIGREF25) for GSS1 in the lower overlap regions. As already mentioned in Section SECREF3, this is due to the masking stage. It has previously been observed that using masking for speech enhancement without a cross talker decreases ASR recognition performance. We have also included in Fig. FIGREF25 the GSS1 version without masking (GSS w/o Mask), which indeed yields significant accuracy gains on segments with little overlap.", "To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported inBIBREF12.", "For the single array track, the proposed system without RNN LM rescoring achieves 16% (11%) relative WER reduction on the DEV (EVAL) set when compared with System8 in BIBREF12 (row one in Table TABREF14). RNN LM rescoring further helps improve the proposed system performance.", "Temporal context also plays an important role in the EM initialization. Simulations have shown that a large context of 15 seconds left and right of the considered segment improves the mixture model estimation performance significantly for CHiME-5 BIBREF13. However, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement BIBREF12. Alternatively, one can run the EM first with a larger temporal context until convergence, then drop the context and re-run it for some more iterations. As shown later in the paper, this approach did not improve ASR performance. Therefore, the temporal context was only used for dereverberation and the mixture model parameter estimation, while for the estimation of covariance matrices for beamforming the context was dropped and only the original segment length was considered BIBREF12."]}
{"question_id": "3d2b5359259cd3518f361d760bacc49d84c40d82", "predicted_answer": "", "predicted_evidence": ["The first row corresponds to the GSS configuration in BIBREF13 while the second one corresponds to the GSS configuration in BIBREF12. First two rows show that dropping the temporal context for estimating statistics for beamforming improves ASR accuracy. For the last row, the EM algorithm was run 20 iterations with temporal context, followed by another 10 without context. Since the performance decreased, we concluded that the best configuration for the GSS enhancement in CHiME-5 scenario is using full temporal context for the EM stage and dropping it for the beamforming stage. Consequently, we have chosen system BIBREF12 as baseline in this study since is using the stronger GSS configuration.", "To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported inBIBREF12.", "An overview block diagram of this enhancement by source separation is depicted in fig:enhancementblock. It follows the approach presented in BIBREF12, which was shown to outperform the baseline version. The system operates in the STFT domain and consists of two stages: (1) a dereverberation stage, and (2) a guided source separation stage. For the sake of simplicity, the overall system is referred to as GSS for the rest of the paper. Regarding the first stage, the multiple input multiple output version of the WPE method was used for dereverberation ($M$ inputs and $M$ outputs) BIBREF14, BIBREF15 and, regarding the second stage, it consists of a spatial MM BIBREF16 and a source extraction (SE) component. The model has five mixture components, one representing each speaker, and an additional component representing the noise class.", "Multi-channel speech enhancement, such as acoustic BF or source separation, would not only reduce the acoustic variability, it would also result in a reduction of the amount of training data by a factor of $M$, where $M$ is the number of microphones BIBREF4. Previous studies have shown the benefit of training an ASR on matching enhanced speech BIBREF5, BIBREF6 or on jointly training the enhancement and the acoustic model BIBREF7. Alternatively, the training data is often artificially increased by adding even more degraded speech to it. For instance, Ko et al. BIBREF8 found that adding simulated reverberated speech improves accuracy significantly on several large vocabulary tasks. Similarly, Manohar et al. BIBREF9 improved the WER of the baseline CHiME-5 system by relative 5.5% by augmenting the training data with approx. 160hrs of simulated reverberated speech. However, not only can the generation of new training data be costly and time consuming, the training process itself is also prolonged if the amount of data is increased.", "We also performed a test using GSS with the oracle alignments (GSS w/ oracle) to assess the potential of time annotation refinement (gray shade lines in Table TABREF14). It can be seen that there is some, however not much room for improvement."]}
{"question_id": "26a321e242e58ea5f2ceaf37f26566dd0d0a0da1", "predicted_answer": "", "predicted_evidence": ["We perform experiments using data from the CHiME-5 challenge which focuses on distant multi-microphone conversational ASR in real home environments BIBREF10. The CHiME-5 data is heavily degraded by reverberation and overlapped speech. As much as 23% of the time more than one speaker is active at the same time BIBREF11. The challenge's baseline system poor performance (about 80% WER) is an indication that ASR training did not work well. Recently, GSS enhancement on the test data was shown to significantly improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data BIBREF12. GSS is a spatial mixture model based blind source separation approach which exploits the annotation given in the CHiME-5 database for initialization and, in this way, avoids the frequency permutation problem BIBREF13.", "We conjectured that cleaning up the training data would enable a more effective acoustic model training for the CHiME-5 scenario. We have therefore experimented with enhancement algorithms of various strengths, from relatively simple beamforming over single-array GSS to a quite sophisticated multi-array GSS approach, and tested all combinations of training and test data enhancement methods. Furthermore, compared to the initial GSS approach in BIBREF13, we describe here some modifications, which led to improved performance. We also propose an improved neural acoustic modeling structure compared to the CHiME-5 baseline system described in BIBREF9. It consists of initial CNN layers followed by TDNN-F layers, instead of a homogeneous TDNN-F architecture.", "Using a single acoustic model trained with 308hrs of training data, which resulted after applying multi-array GSS data cleaning and a three-fold speed perturbation, we achieved a WER of 41.6% on the development (DEV) and 43.2% on the evaluation (EVAL) test set of CHiME-5, if the test data is also enhanced with multi-array GSS. This compares very favorably with the recently published top-line in BIBREF12, where the single-system best result, i.e., the WER without system combination, was 45.1% and 47.3% on DEV and EVAL, respectively, using an augmented training data set of 4500hrs total.", "Depending on the number of available arrays for CHiME-5, two flavours of GSS enhancement were used in this work. In the single array track, all 4 channels of the array are used as input ($M = 4$), and the system is referred to as GSS1. In the multi array track, all six arrays are stacked to form a 24 channels super-array ($M = 24$), and this system is denoted as GSS6. The baseline time synchronization provided by the challenge organizers was sufficient to align the data for GSS6.", "GSS exploits the baseline CHiME-5 speaker diarization information available from the transcripts (annotations) to determine when multiple speakers talk simultaneously (see fig:activity). This crosstalk information is then used to guide the parameter estimation of the MM both during EM initialization (posterior masks set to one divided by the number of active speakers for active speakers' frames, and zero for the non-active speakers) and after each E-step (posterior masks are clamped to zero for non-active speakers)."]}
{"question_id": "6920fd470e6a99c859971828e20276a1b9912280", "predicted_answer": "", "predicted_evidence": ["We also performed a test using GSS with the oracle alignments (GSS w/ oracle) to assess the potential of time annotation refinement (gray shade lines in Table TABREF14). It can be seen that there is some, however not much room for improvement.", "For the multi array track, the proposed system without RNN LM rescoring achieved 6% (7%) relative WER reduction on the DEV (EVAL) set when compared with System16 in BIBREF12 (row six in Table TABREF14).", "However, there has been a long debate whether it is advisable to apply speech enhancement on data used for ASR training, because it is generally agreed upon that the recognizer should be exposed to as much acoustic variability as possible during training, as long as this variability matches the test scenario BIBREF1, BIBREF2, BIBREF3. Multi-channel speech enhancement, such as acoustic BF or source separation, would not only reduce the acoustic variability, it would also result in a reduction of the amount of training data by a factor of $M$, where $M$ is the number of microphones BIBREF4. Previous studies have shown the benefit of training an ASR on matching enhanced speech BIBREF5, BIBREF6 or on jointly training the enhancement and the acoustic model BIBREF7. Alternatively, the training data is often artificially increased by adding even more degraded speech to it. For instance, Ko et al.", "The reference channel for the beamformer is estimated based on an SNR criterionBIBREF18. The beamformer is followed by a postfilter to reduce the remaining speech distortions BIBREF19, which in turn is followed by an additional (optional) masking stage to improve crosstalk suppression. Those masks are also given by the mentioned class affiliations. For the single array (CHiME-5) track, simulations have shown that multiplying the beamformer output with the target speaker mask improves the performance on the U data, but the same approach degrades the performance in the multiple array track BIBREF13. This is because the spatial selectivity of a single array is very limited in CHiME-5: the speakers' signals arrive at the array, which is mounted on the wall at some distance, at very similar impinging angles, rendering single array beamforming rather ineffective. Consequently, additional masking has the potential to improve the beamformer performance.", "The results presented so far were overall accuracies on the test set of CHiME-5. However, since speaker overlap is a major issue for these data, it is of interest to investigate the methods' performance as a function of the amount of overlapped speech. Employing the original CHiME-5 annotations, the word distribution of overlapped speech was computed for DEV and EVAL sets (silence portions were not filtered out). The five-bin normalized histogram of the data is plotted in Fig. FIGREF19. Interestingly, the percentage of segments with low overlapped speech is significantly higher for the EVAL than for the DEV set, and, conversely, the number of words with high overlapped speech is considerably lower for the EVAL than for the DEV set. This distribution may explain the difference in performance observed between the DEV and EVAL sets."]}
{"question_id": "f741d32b92630328df30f674af16fbbefcad3f93", "predicted_answer": "", "predicted_evidence": ["An analysis of these evaluation corpora has shown that the class prior distributions vary significantly between the classes (see Table 2 ). For German and English, more than 80% of the examples in the test set belong to the classes GCAT and MCAT and at most 2% to the class CCAT. These class prior distributions are very different for French and Spanish: the class CCAT is quite frequent with 21% and 15% of the French and Spanish test set respectively. One may of course argue that variability in the class prior distribution is typical for real-world problems, but this shifts the focus from a high quality cross-lingual transfer to \u201ctricks\u201d for how to best handle the class imbalance. Indeed, in previous research the transfer between English and German achieves accuracies higher than 90%, while the performance is below 80% for EN/FR or even 70% EN/ES. We have seen experimental evidence that these important differences are likely to be caused by the discrepancy in the class priors of the test sets.", "In this work, we propose a new evaluation framework for highly multilingual document classification which significantly extends the current state. We continue to use Reuters Corpus Volume 2, but based on the above mentioned limitations of the current subset of RCV2, we propose new tasks for cross-lingual document classification. The design choices are as follow:", "In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations. Details on each approach are given in section \"Multilingual word representations\" and \"Multilingual sentence representations\" respectively. In contrast to previous works on cross-lingual document classification with RVC2, we explore training the classifier on all languages and transfer it to all others, ie. we do not limit our study to the transfer between English and a foreign language.", "Each news story was manually classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). Topic codes were assigned to capture the major subject of the news story. The entire corpus covers thirteen languages, i.e. Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish, written by local reporters in each language. The news stories are not parallel. Single-label stories, i.e. those labeled with only one topic out of the four top categories, are often used for evaluations. However, the class distributions vary significantly across all the thirteen languages (see Table 1 ). Therefore, using random samples to extract evaluation corpora may lead to very imbalanced test sets, i.e. undesired and misleading variability among the languages when the main focus is to evaluate cross-lingual transfer.", "The Reuters Corpus Volume 2 BIBREF2 , in short RCV2, is a multilingual corpus with a collection of 487,000 news stories. Each news story was manually classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). Topic codes were assigned to capture the major subject of the news story. The entire corpus covers thirteen languages, i.e. Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish, written by local reporters in each language. The news stories are not parallel. Single-label stories, i.e. those labeled with only one topic out of the four top categories, are often used for evaluations. However, the class distributions vary significantly across all the thirteen languages (see Table 1 ). Therefore, using random samples to extract evaluation corpora may lead to very imbalanced test sets, i.e."]}
{"question_id": "fe7f7bcf37ca964b4dc9e9c7ebf35286e1ee042b", "predicted_answer": "", "predicted_evidence": ["There are many tasks in natural language processing which require the classification of sentences or longer paragraphs into a set of predefined categories. Typical applications are for instance topic identification (e.g. sports, news, $\\ldots $ ) or product reviews (positive or negative). There is a large body of research on approaches for document classification. An important aspect to compare these different approaches is the availability of high quality corpora to train and evaluate them. Unfortunately, most of these evaluation tasks focus on the English language only, while there is an ever increasing need to perform document classification in many other languages. One could of course collect and label training data for other languages, but this would be costly and time consuming. An interesting alternative is \u201ccross-lingual document classification\u201d. The underlying idea is to use a representation of the words or whole documents which is independent of the language. By these means, a classifier trained on one language can be transferred to a different one, without the need of resources in that transfer language.", "Each news story was manually classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). Topic codes were assigned to capture the major subject of the news story. The entire corpus covers thirteen languages, i.e. Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish, written by local reporters in each language. The news stories are not parallel. Single-label stories, i.e. those labeled with only one topic out of the four top categories, are often used for evaluations. However, the class distributions vary significantly across all the thirteen languages (see Table 1 ). Therefore, using random samples to extract evaluation corpora may lead to very imbalanced test sets, i.e. undesired and misleading variability among the languages when the main focus is to evaluate cross-lingual transfer.", "Most works in the literature use only 1 000 examples to train the document classifier. To invest the impact of more training data, we also provide training corpora of 2 000, 5 000 and 10 000 documents. The development corpus for each language is composed of 1 000 and the test set of 4 000 documents respectively. All have uniform class distributions. An important aspect of this work is to provide a framework to study and evaluate cross-lingual document classification for many language pairs. In that spirit, we will name this corpus \u201cMultilingual Document Classification Corpus\u201d, abbreviated as MLDoc. The full Reuters Corpus Volume 2 has a special license and we can not distribute it ourselves. Instead, we provide tools to extract all the subsets of MLDoc at https://github.com/facebookresearch/MLDoc.", "Typical applications are for instance topic identification (e.g. sports, news, $\\ldots $ ) or product reviews (positive or negative). There is a large body of research on approaches for document classification. An important aspect to compare these different approaches is the availability of high quality corpora to train and evaluate them. Unfortunately, most of these evaluation tasks focus on the English language only, while there is an ever increasing need to perform document classification in many other languages. One could of course collect and label training data for other languages, but this would be costly and time consuming. An interesting alternative is \u201ccross-lingual document classification\u201d. The underlying idea is to use a representation of the words or whole documents which is independent of the language. By these means, a classifier trained on one language can be transferred to a different one, without the need of resources in that transfer language. Ideally, the performance obtained by cross-lingual transfer should be as close as possible to training the entire system on language specific resources.", "The Reuters Corpus Volume 2 BIBREF2 , in short RCV2, is a multilingual corpus with a collection of 487,000 news stories. Each news story was manually classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). Topic codes were assigned to capture the major subject of the news story. The entire corpus covers thirteen languages, i.e. Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish, written by local reporters in each language. The news stories are not parallel. Single-label stories, i.e. those labeled with only one topic out of the four top categories, are often used for evaluations. However, the class distributions vary significantly across all the thirteen languages (see Table 1 )."]}
{"question_id": "d9354c0bb32ec037ff2aacfed58d57887a713163", "predicted_answer": "", "predicted_evidence": ["- opponent ( INLINEFORM0 ). For every event, along with the current target entity, we also keep track of other contenders for the same event. If a named entity in the tweet matches with one of other contenders, it is labeled as opponent.", "Table TABREF33 shows some examples which TwiVer incorrectly classifies. These errors indicate that even though shallow features and dependency paths do a decent job at predicting veridicality, deeper text understanding is needed for some cases. The opposition between \u201cthe heart ...the mind\" in the first example is not trivial to capture. Paying attention to matrix clauses might be important too (as shown in the last tweet \u201cThere is no doubt ...\").", "Pair context. For the election type of events, in which two target entities are present (contender and state. e.g., Clinton, Ohio), we extract words between these two entities: e.g., INLINEFORM0 will win INLINEFORM1 .", "We obtained veridicality annotations on a sample of the data using Amazon Mechanical Turk. For each tweet, we asked Turkers to judge veridicality toward a candidate winning as expressed in the tweet as well as the author's desire toward the event. For veridicality, we asked Turkers to rate whether the author believes the event will happen on a 1-5 scale (\u201cDefinitely Yes\", \u201cProbably Yes\", \u201cUncertain about the outcome\", \u201cProbably No\", \u201cDefinitely No\"). We also added a question about the author's desire toward the event to make clear the difference between veridicality and desire. For example, \u201cI really want Leonardo to win at the Oscars!\" asserts the author's desire toward Leonardo winning, but remains agnostic about the likelihood of this outcome, whereas \u201cLeonardo DiCaprio will win the Oscars\" is predicting with confidence that the event will happen.", "We use a re-implementation of BIBREF23 's system to estimate sentiment for tweets in our corpus. We run the tweets obtained for every contender through the sentiment analysis system to obtain a count of positive labels. Sentiment scores are computed analogously to veridicality using Equation ( EQREF43 ). For each contest, the contender with the highest sentiment prediction score is predicted as the winner."]}
{"question_id": "c035a011b737b0a10deeafc3abe6a282b389d48b", "predicted_answer": "", "predicted_evidence": ["Keyword context. For target and opponent entities, we also extract words between the entity and our specified keyword ( INLINEFORM0 ) (win in our case): INLINEFORM1 predicted to INLINEFORM2 , INLINEFORM3 might INLINEFORM4 .", "We randomly divided the annotated tweets into a training set of 2,480 tweets, a development set of 354 tweets and a test set of 709 tweets. MAP parameters were fit using LBFGS-B BIBREF22 . Table TABREF29 provides examples of high-weight features for positive and negative veridicality.", "We retrieve dependency paths between the two target entities and between the target and keyword (win) using the TweeboParser BIBREF21 after applying rules to normalize paths in the tree (e.g., \u201cdoesn't\" INLINEFORM0 \u201cdoes not\").", "We use a re-implementation of BIBREF23 's system to estimate sentiment for tweets in our corpus. We run the tweets obtained for every contender through the sentiment analysis system to obtain a count of positive labels. Sentiment scores are computed analogously to veridicality using Equation ( EQREF43 ). For each contest, the contender with the highest sentiment prediction score is predicted as the winner.", "We now have access to a classifier that can automatically detect positive veridicality predictions about a candidate winning a contest. This enables us to evaluate the accuracy of the crowd's wisdom by retrospectively comparing popular beliefs (as extracted and aggregated by TwiVer) against known outcomes of contests."]}
{"question_id": "d3fb0d84d763cb38f400b7de3daaa59ed2a1b0ab", "predicted_answer": "", "predicted_evidence": ["IIS-1464128 to Alan Ritter and IIS-1464252 to Marie-Catherine de Marneffe. Alan Ritter is supported by the Department of Defense under Contract No. FA8702-15-D-0002 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center in addition to the Office of the Director of National Intelligence (ODNI) and the Intelligence Advanced Research Projects Activity (IARPA) via the Air Force Research Laboratory (AFRL) contract number FA8750-16-C-0114. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, AFRL, NSF, or the U.S. Government.", "Table TABREF39 shows the 10 top predictions made by the veridicality and sentiment-based systems on two of the events we considered - the Oscars and the presidential primaries, highlighting correct predictions.", "Similarly, for the primaries, the two incorrect predictions made by the veridicality-based approach were surprise losses. News articles , , indeed reported the loss of Maine for Trump and the loss of Indiana for Clinton as unexpected.", "Accounts like \u201cgoal_ghana\", \u201cbreakingnewsnig\" and \u201c1Mrfutball\", which are automatically identified by our analysis, are known to post tweets predominantly about soccer.", "We use a re-implementation of BIBREF23 's system to estimate sentiment for tweets in our corpus. We run the tweets obtained for every contender through the sentiment analysis system to obtain a count of positive labels. Sentiment scores are computed analogously to veridicality using Equation ( EQREF43 ). For each contest, the contender with the highest sentiment prediction score is predicted as the winner."]}
{"question_id": "6da1320fa25b2b6768358d3233a5ecf99cc73db5", "predicted_answer": "", "predicted_evidence": ["We now outline a simple classification model that uses summaries of TSMs. Given a labeled training set of documents, we would like to find the prototypical TSM corresponding to each label. This can be done by identifying the matrix that minimizes the cumulative deviation from those corresponding to the documents with the label. DISPLAYFORM0", "Given a document INLINEFORM0 and a topic INLINEFORM1 , our method relies on identifying the sentiment as expressed by content in INLINEFORM2 towards the topic INLINEFORM3 . The sentiment could be estimated in the form of a categorical label such as one of positive, negative and neutral BIBREF7 . Within our modelling, however, we adopt a more fine-grained sentiment labelling, whereby the sentiment for a topic-document pair is a probability distribution over a plurality of ordinal polarity classes ranging from strongly positive to strongly negative. Let INLINEFORM4 represent the topic-sentiment polarity vector of INLINEFORM5 towards INLINEFORM6 such that INLINEFORM7 represents the probability of the polarity class INLINEFORM8 . Combining the topic-sentiment vectors for all topics yields a document-specific topic-sentiment matrix (TSM) as follows: DISPLAYFORM0", "GloVe-d2v+TSM: A logistic regression classifier trained on the GloVe features as well as TSM features.", "Each row in the matrix corresponds to a topic within INLINEFORM0 , with each element quantifying the probability associated with the sentiment polarity class INLINEFORM1 for the topic INLINEFORM2 within document INLINEFORM3 . The topic-sentiment matrix above may be regarded as a sentiment signature for the document over the topic set INLINEFORM4 .", "Sentiment analysis has proved to be a useful tool in detecting controversial topics as it can help identify topics that evoke different feelings among people on opposite side of the arguments. Mejova et al. controversy-news2 analyzed language use in controversial news articles and found that a writer may choose to highlight the negative aspects of the opposing view rather than emphasizing the positive aspects of one\u2019s view. Lourentzou et al. controversy-news3 utilize the sentiments expressed in social media comments to identify controversial portions of news articles. Given a news article and its associated comments on social media, the paper links comments with each sentence of the article (by using a sentence as a query and retrieving comments using BM25 score). For all the comments associated with a sentence, a sentiment score is then computed, and sentences with large variations in positive and negative comments are identified as controversial sentences. Choi et al. controversy-news go one step further and identify controversial topics and their sub-topics in news articles."]}
{"question_id": "351f7b254e80348221e0654478663a5e53d3fe65", "predicted_answer": "", "predicted_evidence": ["Sentiment analysis has proved to be a useful tool in detecting controversial topics as it can help identify topics that evoke different feelings among people on opposite side of the arguments. Mejova et al. controversy-news2 analyzed language use in controversial news articles and found that a writer may choose to highlight the negative aspects of the opposing view rather than emphasizing the positive aspects of one\u2019s view. Lourentzou et al. controversy-news3 utilize the sentiments expressed in social media comments to identify controversial portions of news articles. Given a news article and its associated comments on social media, the paper links comments with each sentence of the article (by using a sentence as a query and retrieving comments using BM25 score). For all the comments associated with a sentence, a sentiment score is then computed, and sentences with large variations in positive and negative comments are identified as controversial sentences. Choi et al. controversy-news go one step further and identify controversial topics and their sub-topics in news articles.", "Table TABREF20 reports the classification results for different methods described above. TSM-NC, the method that uses the INLINEFORM0 vectors and performs simple nearest class classification achieves an overall accuracy of INLINEFORM1 . Next, training a logistic regression classifier trained on INLINEFORM2 vectors as features, TSM-LR, achieves significant improvement with an overall accuracy of INLINEFORM3 . The word embedding based baseline, the GloVe-d2v method, achieves slightly lower performance with an overall accuracy of INLINEFORM4 . However, we do note that the per-class performance of GloVe-d2v method is more balanced with about INLINEFORM5 accuracy for both classes. The TSM-LR method on the other hand achieves about INLINEFORM6 for INLINEFORM7 class and only INLINEFORM8 for the INLINEFORM9 class.", "Given a document INLINEFORM0 and a topic INLINEFORM1 , our method relies on identifying the sentiment as expressed by content in INLINEFORM2 towards the topic INLINEFORM3 . The sentiment could be estimated in the form of a categorical label such as one of positive, negative and neutral BIBREF7 . Within our modelling, however, we adopt a more fine-grained sentiment labelling, whereby the sentiment for a topic-document pair is a probability distribution over a plurality of ordinal polarity classes ranging from strongly positive to strongly negative. Let INLINEFORM4 represent the topic-sentiment polarity vector of INLINEFORM5 towards INLINEFORM6 such that INLINEFORM7 represents the probability of the polarity class INLINEFORM8 . Combining the topic-sentiment vectors for all topics yields a document-specific topic-sentiment matrix (TSM) as follows: DISPLAYFORM0", "In order to validate our hypothesis, we consider exploiting the sentiment information towards topics from archives of political debates to build a model for identifying political orientation of speakers as one of right or left leaning, which corresponds to republicans and democrats respectively, within the context of US politics. This is inspired by our observation that the political leanings of debators are often expressed in debates by way of speakers' sentiments towards particular topics. Parliamentary or Senate debates often bring the ideological differences to the centre stage, though somewhat indirectly. Heated debates in such forums tend to focus on the choices proposed by the executive that are in sharp conflict with the preference structure of the opposition members. Due to this inherent tendency of parliamentary debates to focus on topics of disagreement, the sentiments exposited in debates hold valuable cues to identify the political orientation of the participants.", "Towards analyzing the significance of the results, we would like to start with drawing attention to the format of the data used in the TSM methods. The document-specific TSM matrices do not contain any information about the topics themselves, but only about the sentiment in the document towards each topic; one may recollect that INLINEFORM0 is a quantification of the strength of the sentiment in INLINEFORM1 towards topic INLINEFORM2 . Thus, in contrast to distributional embeddings such as doc2vec, TSMs contain only the information that directly relates to sentiment towards specific topics that are learnt from across the corpus. The results indicate that TSM methods are able to achieve comparable performance to doc2vec-based methods despite usage of only a small slice of informatiom. This points to the importance of sentiment information in determining the political leanings from text."]}
{"question_id": "d323f0d65b57b30ae85fb9f24298927a3d1216e9", "predicted_answer": "", "predicted_evidence": ["Towards analyzing the significance of the results, we would like to start with drawing attention to the format of the data used in the TSM methods. The document-specific TSM matrices do not contain any information about the topics themselves, but only about the sentiment in the document towards each topic; one may recollect that INLINEFORM0 is a quantification of the strength of the sentiment in INLINEFORM1 towards topic INLINEFORM2 . Thus, in contrast to distributional embeddings such as doc2vec, TSMs contain only the information that directly relates to sentiment towards specific topics that are learnt from across the corpus. The results indicate that TSM methods are able to achieve comparable performance to doc2vec-based methods despite usage of only a small slice of informatiom. This points to the importance of sentiment information in determining the political leanings from text.", "The ideological leanings of a person within the left-right political spectrum are often reflected by how one feels about different topics and by means of preferences among various choices on particular issues. For example, a left-leaning person would prefer nationalization and state control of public services (such as healthcare) where privatization would be often preferred by people that lean towards the right. Likewise, a left-leaning person would often be supportive of immigration and will often talk about immigration in a positive manner citing examples of benefits of immigration on a country's economy. A right-leaning person, on the other hand, will often have a negative opinion about immigration.", "We now outline a simple classification model that uses summaries of TSMs. Given a labeled training set of documents, we would like to find the prototypical TSM corresponding to each label. This can be done by identifying the matrix that minimizes the cumulative deviation from those corresponding to the documents with the label. DISPLAYFORM0", "Table TABREF20 reports the classification results for different methods described above. TSM-NC, the method that uses the INLINEFORM0 vectors and performs simple nearest class classification achieves an overall accuracy of INLINEFORM1 . Next, training a logistic regression classifier trained on INLINEFORM2 vectors as features, TSM-LR, achieves significant improvement with an overall accuracy of INLINEFORM3 . The word embedding based baseline, the GloVe-d2v method, achieves slightly lower performance with an overall accuracy of INLINEFORM4 . However, we do note that the per-class performance of GloVe-d2v method is more balanced with about INLINEFORM5 accuracy for both classes. The TSM-LR method on the other hand achieves about INLINEFORM6 for INLINEFORM7 class and only INLINEFORM8 for the INLINEFORM9 class.", "Table TABREF21 lists the top five topics with most distance, i.e., most polarizing topics (top) and five topics with least distance, i.e.,least polarizing topics (bottom) as computed by equation EQREF23 . Note that the topics are represented using the top keywords that they contain according to the probability distribution of the topic. We observe that the most polarizing topics include topics related to healthcare (H3, H4), military programs (H5), and topics related to administration processes (H1 and H2). The least polarizing topics include topics related to worker safety (L3) and energy projects (L2). One counter-intuitive observation is topic related to gun control (L4) that is amongst the least polarizing topics. This anomaly could be attributed to only a few speeches related to this issue in the training set (only 23 out of 1175 speeches mention gun) that prevents a reliable estimate of the probability distributions."]}
{"question_id": "05118578b46e9d93052e8a760019ca735d6513ab", "predicted_answer": "", "predicted_evidence": ["2pt", "Our model mostly relates to BIBREF4 which combines CNNs of different filter lengths and either static or fine-tuned word vectors, and BIBREF5 which stacks CNN and LSTM in a unified architecture with static word vectors. It is known that in computer vision, the deeper network architecture usually possess the better performance. We consider NLP also has this property. In order to make our model deeper, we apply the idea of asymmetric convolution introduced in BIBREF8 , which can reduce the number of the parameters, and increase the representation ability of the model by adding more nonlinearity. Then we stack the multi-layer BLSTM, which is cable of analysing the future as well as the past of every position in the sequence, on top of the ACNN. The experiment results also demonstrate the effectiveness of our model.", "In this section, we will introduce our AC-BLSTM architecture in detail. We first describe the ACNN which takes the word vector represented matrix of the sentence as input and produces higher-level presentation of word features. Then we introduce the BLSTM which can incorporate context on both sides of every position in the input sequence. Finally, we introduce the techniques to avoid overfitting in our model. An overall illustration of our architecture is shown in Figure FIGREF1 .", "We also benchmark our system on question type classification task (TREC) BIBREF36 , where sentences are questions in the following 6 classes: abbreviation, human, entity, description, location, numeric. The entire dataset consists of 5,452 training examples and 500 testing examples.", "We implement our model based on Mxnet BIBREF37 - a C++ library, which is a deep learning framework designed for both efficiency and flexibility. In order to benefit from the efficiency of parallel computation of the tensors, we train our model on a Nvidia GTX 1070 GPU. Training is done through stochastic gradient descent over shuffled mini-batches with the optimizer RMSprop BIBREF38 . For all experiments, we simultaneously apply three asymmetric convolution operation with the second filter length INLINEFORM0 of 2, 3, 4 to the input, set the dropout rate to 0.5 before feeding the feature into BLSTM, and set the initial learning rate to 0.0001. But there are some hyper-parameters that are not the same for all datasets, which are listed in table TABREF14 . We conduct experiments on 3 datasets (MR, SST and SUBJ) to verify the effectiveness our semi-supervised framework. And the setting of INLINEFORM1 and INLINEFORM2 for different datasets are listed in table TABREF15 ."]}
{"question_id": "31b9337fdfbbc33fc456552ad8c355d836d690ff", "predicted_answer": "", "predicted_evidence": ["For model regularization, we employ two commonly used techniques to prevent overfitting during training: dropout BIBREF27 and batch normalization BIBREF9 . In our model, we apply dropout to the input feature of the BLSTM, and the output of BLSTM before the softmax layer. And we apply batch normalization to outputs of each convolution operation just before the relu activation. During training, after we get the gradients of the AC-BLSTM network, we first calculate the INLINEFORM0 INLINEFORM1 of all gradients and sum together to get INLINEFORM2 . Then we compare the INLINEFORM3 to 0.5. If the INLINEFORM4 is greater than 0.5, we let all the gradients multiply with INLINEFORM5 , else just use the original gradients to update the weights.", "We assume the original classifier classify a sample into one of INLINEFORM0 possible classes. So we can do semi-supervised learning by simply adding samples from a generative network G to our dataset and labeling them to an extra class INLINEFORM1 . And correspondingly the dimension of our classifier output increases from INLINEFORM2 to INLINEFORM3 . The configuration of our generator network G is inspired by the architecture proposed in BIBREF16 . And we modify the architecture to make it suitable to the text classification tasks. Table TABREF13 shows the configuration of each layer in the generator G. Lets assume the training batch size is INLINEFORM4 and the percentage of the generated samples among a batch training samples is INLINEFORM5 . At each iteration of the training process, we first generate INLINEFORM6 samples from the generator G then we draw INLINEFORM7 samples from the real dataset. We then perform gradient descent on the AC-BLSTM and generative net G and finally update the parameters of both nets.", "Our semi-supervised text classification framewrok is inspired by works BIBREF18 , BIBREF19 . We assume the original classifier classify a sample into one of INLINEFORM0 possible classes. So we can do semi-supervised learning by simply adding samples from a generative network G to our dataset and labeling them to an extra class INLINEFORM1 . And correspondingly the dimension of our classifier output increases from INLINEFORM2 to INLINEFORM3 . The configuration of our generator network G is inspired by the architecture proposed in BIBREF16 . And we modify the architecture to make it suitable to the text classification tasks. Table TABREF13 shows the configuration of each layer in the generator G. Lets assume the training batch size is INLINEFORM4 and the percentage of the generated samples among a batch training samples is INLINEFORM5 . At each iteration of the training process, we first generate INLINEFORM6 samples from the generator G then we draw INLINEFORM7 samples from the real dataset.", "with c INLINEFORM0 . Where INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are the same as described above.", "For those sentences that are shorter than INLINEFORM0 , we simply pad them with space."]}
{"question_id": "389ff1927ba9fc8bac50959fc09f30c2143cc14e", "predicted_answer": "", "predicted_evidence": ["Part-of-speech (POS) tagging is a substantially more complicated task than word similarity. We use a bidirectional LSTM implemented using DyNet BIBREF30 . We train nine sets of 128-dimensional word embeddings with word2vec using different random seeds. The LSTM has a single layer and 50-dimensional hidden vectors. Outputs are passed through a tanh layer before classification. To train, we use SGD with a learning rate of 0.1, an input noise rate of 0.1, and recurrent dropout of 0.4.", "", "There has been much recent interest in the applications of word embeddings, as well as a small, but growing, amount of work analyzing the properties of word embeddings.", "In the following experiments, we explore which factors affect stability, as well as how this stability affects downstream tasks that word embeddings are commonly used for. To our knowledge, this is the first study comprehensively examining the factors behind instability.", "The idea of evaluating ten best options is also found in other tasks, like lexical substitution BIBREF16 and word association BIBREF17 , where the top ten results are considered in the final evaluation metric. To give some intuition for how changing the number of nearest neighbors affects our stability metric, consider Figure FIGREF5 . This graph shows how the stability of GloVe changes with the frequency of the word and the number of neighbors used to calculate stability; please see the figure caption for a more detailed explanation of how this graph is structured. Within each frequency bucket, the stability is consistent across varying numbers of neighbors. Ten nearest neighbors performs approximately as well as a higher number of nearest neighbors (e.g., 100). We see this pattern for low frequency words as well as for high frequency words. Because the performance does not change substantially by increasing the number of nearest neighbors, it is computationally less intensive to use a small number of nearest neighbors."]}
{"question_id": "b968bd264995cd03d7aaad1baba1838c585ec909", "predicted_answer": "", "predicted_evidence": ["Part-of-speech (POS) tagging is a substantially more complicated task than word similarity. We use a bidirectional LSTM implemented using DyNet BIBREF30 . We train nine sets of 128-dimensional word embeddings with word2vec using different random seeds. The LSTM has a single layer and 50-dimensional hidden vectors. Outputs are passed through a tanh layer before classification. To train, we use SGD with a learning rate of 0.1, an input noise rate of 0.1, and recurrent dropout of 0.4.", "Observation 5. Frequency is not a major factor in stability. To better understand the role that frequency plays in stability, we run separate ablation experiments comparing regression models with frequency features to regression models without frequency features. Our current model (using raw frequency) achieves an INLINEFORM0 score of 0.301. Comparably, a model using the same features, but with normalized instead of raw frequency, achieves a score of 0.303. Removing frequency from either regression model gives a score of 0.301. This indicates that frequency is not a major factor in stability, though normalized frequency is a larger factor than raw frequency.", "To model word similarity, we use 300-dimensional word2vec embedding spaces trained on the PTB. For each pair of words, we take the cosine similarity between those words averaged over ten randomly initialized embedding spaces. We consider three datasets for evaluating word similarity: WS353 (353 pairs) BIBREF27 , MTurk287 (287 pairs) BIBREF28 , and MTurk771 (771 pairs) BIBREF29 . For each dataset, we normalize the similarity to be in the range INLINEFORM0 , and we take the absolute difference between our predicted value and the ground-truth value. Figure FIGREF22 shows the results broken down by stability of the two words (we always consider Word 1 to be the more stable word in the pair). Word similarity pairs where one of the words is not present in the PTB are omitted.", "", ""]}
{"question_id": "afcd1806b931a97c0679f873a71b825e668f2b75", "predicted_answer": "", "predicted_evidence": ["We would like to thank Ben King and David Jurgens for helpful discussions about this paper, as well as our anonymous reviewers for useful feedback. This material is based in part upon work supported by the National Science Foundation (NSF #1344257) and the Michigan Institute for Data Science (MIDAS). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or MIDAS.", "Understanding how the LSTM is changing the input embeddings is useful information for tasks with limited data, and it could allow us to improve embeddings and LSTM training for these low-resource tasks.", "To model word similarity, we use 300-dimensional word2vec embedding spaces trained on the PTB. For each pair of words, we take the cosine similarity between those words averaged over ten randomly initialized embedding spaces. We consider three datasets for evaluating word similarity: WS353 (353 pairs) BIBREF27 , MTurk287 (287 pairs) BIBREF28 , and MTurk771 (771 pairs) BIBREF29 . For each dataset, we normalize the similarity to be in the range INLINEFORM0 , and we take the absolute difference between our predicted value and the ground-truth value. Figure FIGREF22 shows the results broken down by stability of the two words (we always consider Word 1 to be the more stable word in the pair). Word similarity pairs where one of the words is not present in the PTB are omitted.", "Part-of-speech (POS) tagging is a substantially more complicated task than word similarity. We use a bidirectional LSTM implemented using DyNet BIBREF30 . We train nine sets of 128-dimensional word embeddings with word2vec using different random seeds. The LSTM has a single layer and 50-dimensional hidden vectors. Outputs are passed through a tanh layer before classification. To train, we use SGD with a learning rate of 0.1, an input noise rate of 0.1, and recurrent dropout of 0.4.", "The code used in the experiments described in this paper is publicly available from http://lit.eecs.umich.edu/downloads.html."]}
{"question_id": "01c8c3836467a4399cc37e86244b5bdc5dda2401", "predicted_answer": "", "predicted_evidence": ["", "We find that these word similarity datasets do not contain a balanced distribution of words with respect to stability; there are substantially more unstable words than there are stable words. However, we still see a slight trend: As the combined stability of the two words increases, the average absolute error decreases, as reflected by the lighter color of the cells in Figure FIGREF22 while moving away from the (0,0) data point.", "This simple model is not state-of-the-art, scoring 95.5% on the development set, but the word vectors are a central part of the model, providing a clear signal of their impact. For each word, we group tokens based on stability and frequency. Figure FIGREF24 shows the results. Fixing the word vectors provides a clearer pattern in the results, but also leads to much worse performance: 85.0% on the development set. Based on these results, it seems that training appears to compensate for stability. This hypothesis is supported by Figure FIGREF24 , which shows the similarity between the original word vectors and the shifted word vectors produced by the training. In general, lower stability words are shifted more during training.", "To model word similarity, we use 300-dimensional word2vec embedding spaces trained on the PTB. For each pair of words, we take the cosine similarity between those words averaged over ten randomly initialized embedding spaces. We consider three datasets for evaluating word similarity: WS353 (353 pairs) BIBREF27 , MTurk287 (287 pairs) BIBREF28 , and MTurk771 (771 pairs) BIBREF29 . For each dataset, we normalize the similarity to be in the range INLINEFORM0 , and we take the absolute difference between our predicted value and the ground-truth value. Figure FIGREF22 shows the results broken down by stability of the two words (we always consider Word 1 to be the more stable word in the pair). Word similarity pairs where one of the words is not present in the PTB are omitted.", "PPMI creates embeddings by first building a positive pointwise mutual information word-context matrix, and then reducing the dimensionality of this matrix using SVD BIBREF6 . A more recent word embedding algorithm, word2vec (skip-gram model) BIBREF7 uses a shallow neural network to learn word embeddings by predicting context words. Another recent method for creating word embeddings, GloVe, is based on factoring a matrix of ratios of co-occurrence probabilities BIBREF8 ."]}
{"question_id": "568466c62dd73a025bfd9643417cdb7a611f23a1", "predicted_answer": "", "predicted_evidence": ["We have presented three methods of creating such hybrid data: (i) by allowing the TA decide whether to select authentic or synthetic data (hybr); (ii) by performing independent executions of the TA on authentic and synthetic sets (batch); and (iii) using an MT-generated seed to select monolingual sentences so only the extracted subset is back-translated (online).", "This research has been supported by the ADAPT Centre for Digital Content Technology which is funded under the SFI Research Centres Programme (Grant 13/RC/2106).", "In addition, the amount of duplicated target-side sentences is very low (between 10% and 13%). This indicates that the MT-generated sentences contain n-grams that are different from the authentic counterpart, which increases the variety of the candidates that are useful for the TA to select.", "NEWS test set: The test set provided in WMT 2015 News Translation Task.", "In this work, we explore whether TAs are more inclined to select authentic or artificial sentences. In addition, we propose three different methods of how they can be combined into a single hybrid set. Finally, we investigate whether the hybrid sets retrieved by TAs can be more useful than the authentic set of sentences to fine-tune NMT models."]}
{"question_id": "3a19dc6999aeb936d8a1c4509ebd5bfcda50f0f1", "predicted_answer": "", "predicted_evidence": ["In rows 1 and 2 we present sentences in which the artificial sentence (German (synth) column) is a better translation than the authentic counterpart. In addition to the example described previously (the example of the first row), we also see in row 2 that the authentic candidate pair is (die Veranstalter haben viele Konzerte und Recitale geplant. Es wird f\u00fcr uns eine vorz\u00fcgliche Gelegenheit sein Ihre Freizeit angenehm zu gestalten und Sie f\u00fcr die ernste Musik zu gewinnen.,every participant will play at least one programme.) whereas the synthetic counterpart is the pair (jeder Teilnehmer wird mindestens ein Programm spielen.,every participant will play at least one programme.). In this case, it is preferable to use the synthetic sentence for training instead of the authentic as it is a more accurate translation (observe that the authentic German side consists of two sentences and it is longer than the English-side).", "In the selected data, the only sentence containing the n-gram nach Krankenhausangaben is the authentic sentence presented in the first row of Table TABREF24 (selected by every execution of TA). As we see, this is a noisy sentence as the target-side does not correspond to an accurate translation (observe that in the source sentence we cannot find names such as La Passione or Carlo Mazzacurati that are present in the English side). Accordingly, using this sentence in the training of the NMT is harmful.", "In these tables, we see that the performance of the models following the batch and online approaches is similar. These results are also in accord with those obtained following the hybr approach, as the improvements depend more on the domain (most evaluation scores in the NEWS test set indicate statistically significant improvements whereas for the BIO test set most of them are not) than the TA used, or the value of $\\gamma $. Although the best scores tend to be when $\\gamma =0.50$ this is not always the case, and moreover we can find experiments in which using high amounts of synthetic sentences (i.e. $\\gamma =0.25$) achieve better results than using a higher proportion of authentic sentences. For instance, in BIO subtable of Table TABREF22, using 100K sentences with the online $\\gamma =0.25$ approach, the improvements are statistically significant for two evaluation metrics whereas in the other experiments in that row they are not.", "The NMT models are built using the attentional encoder-decoder framework with OpenNMT-py BIBREF22. We use the default values in the parameters: 2-layer LSTM BIBREF23 with 500 hidden units. The size of the vocabulary is 50,000 words for each language.", "Surprisingly, this correction of inaccurate translations can also be seen on the set of sentences that have been used for training the BT model. As this model does not overfit, when it is provided with the same target sentence used for training, it is capable of generating different valid translations. For example, in row 4 of Table TABREF24 we see the pair (die Preise liegen zwischen 32.000 und 110.000 Won.,the first evening starts with a big parade of all participants through the city towards the beach.) which is one of the sentence pairs used for training the BT model. This is a noisy sentence (see, for instance, that the English-side does not include the numbers). However, the sentence generated by the BT model is der erste Abend beginnt mit einer gro\u00dfen Parade aller Teilnehmer durch die Stadt zum Strand. which is a more accurate translation of than the sentence used for training the model that generates it."]}
{"question_id": "338a3758dccfa438a52d173fbe23a165ef74a0f0", "predicted_answer": "", "predicted_evidence": ["BIO test set: the Cochrane dataset from the WMT 2017 biomedical translation shared task BIBREF20.", "In the hybrid models, we see that the same sentence has been translated as according to hospital information, a policeman was injured. (in this case the models fine-tuned with hybrid data have produced the same translations). The models tuned with hybrid data are capable of producing the n-gram according to which is the same as the reference.", "Training data: German-English parallel sentences provided in WMT 2015 BIBREF19 (4.5M sentence pairs).", "We have presented three methods of creating such hybrid data: (i) by allowing the TA decide whether to select authentic or synthetic data (hybr); (ii) by performing independent executions of the TA on authentic and synthetic sets (batch); and (iii) using an MT-generated seed to select monolingual sentences so only the extracted subset is back-translated (online).", "In order to generate artificial sentences, we use an NMT model (we refer to it as BT model) to back-translate sentences from the target language into the source language. This model is built by training a model with 1M sentences sampled from the training data and using the same configuration described above (but in the reverse language direction, English-to-German)."]}
{"question_id": "2686e8d51caff9a19684e0c9984bcb5a1937d08d", "predicted_answer": "", "predicted_evidence": ["We generalize this to an arbitrary number of features INLINEFORM0 : DISPLAYFORM0", "where INLINEFORM0 is a word embedding matrix, INLINEFORM1 , INLINEFORM2 are weight matrices, with INLINEFORM3 and INLINEFORM4 being the word embedding size and number of hidden units, respectively, and INLINEFORM5 being the vocabulary size of the source language.", "For German INLINEFORM0 English, the parser annotates the German input with morphological features. Different word types have different sets of features \u2013 for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect \u2013 and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value.", "The decoder is a recurrent neural network that predicts a target sequence INLINEFORM0 . Each word INLINEFORM1 is predicted based on a recurrent hidden state INLINEFORM2 , the previously predicted word INLINEFORM3 , and a context vector INLINEFORM4 . INLINEFORM5 is computed as a weighted sum of the annotations INLINEFORM6 . The weight of each annotation INLINEFORM7 is computed through an alignment model INLINEFORM8 , which models the probability that INLINEFORM9 is aligned to INLINEFORM10 . The alignment model is a single-layer feedforward neural network that is learned jointly with the rest of the network through backpropagation.", "We here show the equation for the forward states of the encoder (for the simple RNN case; consider BIBREF0 for GRU): DISPLAYFORM0"]}
{"question_id": "df623717255ea2c9e0f846859d8a9ef51dc1102b", "predicted_answer": "", "predicted_evidence": ["We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word's feature value to all its subword units. An example is shown in Figure FIGREF26 .", "Our main innovation over the standard encoder-decoder architecture is that we represent the encoder input as a combination of features BIBREF4 .", "The encoder is a bidirectional neural network with gated recurrent units BIBREF3 that reads an input sequence INLINEFORM0 and calculates a forward sequence of hidden states INLINEFORM1 , and a backward sequence INLINEFORM2 . The hidden states INLINEFORM3 and INLINEFORM4 are concatenated to obtain the annotation vector INLINEFORM5 .", "We also evaluated adding linguistic features to a stronger baseline, which includes synthetic parallel training data. In addition, we compare our neural systems against phrase-based (PBSMT) and syntax-based (SBSMT) systems by BIBREF18 , all of which make use of linguistic annotation on the source and/or target side. Results are shown in Table TABREF34 . For German INLINEFORM0 English, we observe similar improvements in the best development perplexity (45.2 INLINEFORM1 44.1), test set Bleu (37.5 INLINEFORM2 38.5) and chrF3 (62.2 INLINEFORM3 62.8). Our test set Bleu is on par to the best submitted system to this year's WMT 16 shared translation task, which is similar to our baseline MT system, but which also uses a right-to-left decoder for reranking BIBREF19 .", "Factored translation models are often used in phrase-based SMT BIBREF21 as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combinations, the individual models either treat each feature combination as an atomic unit, resulting in data sparsity, or assume independence between features, for instance by having separate language models for words and POS tags. In contrast, we exploit the strong generalization ability of neural networks, and expect that even new feature combinations, e.g. a word that appears in a novel syntactic function, are handled gracefully."]}
{"question_id": "ac482ab8a5c113db7c1e5f106a5070db66e7ba37", "predicted_answer": "", "predicted_evidence": ["Recently, there has been an increasing interest in the development of models which are trained to learn what to (and what not to) share between a set of tasks, with the general aim of preventing negative transfer when the tasks are not closely related BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 . Our Learning What to Share setting is based on this idea and closely related to BIBREF15 's shared layer architecture.", "Multi-task learning (MTL) is a recently resurgent approach to machine learning in which multiple tasks are simultaneously learned. By optimising the multiple loss functions of related tasks at once, multi-task learning models can achieve superior results compared to models trained on a single task. The key principle is summarized by BIBREF0 as \u201cMTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks\". Neural MTL has become an increasingly successful approach by exploiting similarities between Natural Language Processing (NLP) tasks BIBREF1 , BIBREF2 , BIBREF3 . Our work builds upon BIBREF4 , who demonstrate that employing semantic tagging as an auxiliary task for Universal Dependency BIBREF5 part-of-speech tagging can lead to improved performance.", "fig:nli shows the three MTL models for NLI. All hyperparameters were tuned with respect to loss on the SNLI and SICK-E validation datasets (separately). For the SNLI experiments, we trained for 37 epochs with a batch size of 128. For the SICK-E experiments, we trained for 20 epochs with a batch size of 8. Note that the ESIM model was designed for the SNLI dataset, therefore performance is non-optimal for SICK-E. For both sets of experiments: we optimized using Adam with a learning rate of $0.00005$ ; we weight the auxiliary semantic tagging loss with $\\lambda $ = $0.1$ ; the pre-trained word embeddings we use are GloVe embeddings of dimension 300 trained on 840 billion tokens of Common Crawl; and we applied dropout and recurrent dropout with a probability of $0.3$ to all bi-LSTM, and non-output dense layers.", "tab:examples shows demonstrative examples from the SNLI test set on which the Learning What to Share (LWS) model outperforms the single-task (ST) model. The examples cover all possible combinations of entailment classes. tab:semtags explains the relevant part of the semantic tagset. tab:fscore shows the per-label precision and recall scores.", "fig:upos shows the three MTL models used for UPOS. All hyperparameters were tuned with respect to loss on the English UD 2.0 UPOS validation set. We trained for 20 epochs with a batch size of 128 and optimized using Adam BIBREF27 with a learning rate of $0.0001$ . We weight the auxiliary semantic tagging loss with $\\lambda $ = $0.1$ . The pre-trained word embeddings we used are GloVe embeddings BIBREF28 of dimension 100 trained on 6 billion tokens of Wikipedia 2014 and Gigaword 5. We applied dropout and recurrent dropout with a probability of $0.3$ to all bi-LSTMs."]}
{"question_id": "24897f57e3b0550be1212c0d9ebfcf83bad4164e", "predicted_answer": "", "predicted_evidence": ["In addition to evaluating performance directly, we attempt to qualify how semtags affect performance with respect to each of the SNLI MTL settings.", "The fact that NLI is a sentence-level task, while semantic tags are word-level annotations presents a difficulty in measuring the effect of semantic tags on the systems' performance, as there is no one-to-one correspondence between a correct label and a particular semantic tag. We therefore employ the following method in order to assess the contribution of semantic tags. Given the performance ranking of all our systems \u2014 $FSN < ST < PSN < LWS$ \u2014 we make a pairwise comparison between the output of a superior system $S_{sup}$ and an inferior system $S_{inf}$ . This involves taking the pairs of sentences that every $S_{sup}$ classifies correctly, but some $S_{inf}$ does not. Given that FSN is the worst performing system and, as such, has no `worse' system for comparison, we are left with six sets of sentences: ST-FSN, PSN-FSN, PSN-ST, LWS-PSN, LWS-ST, and LWS-FSN.", "The fact that NLI is a sentence-level task, while semantic tags are word-level annotations presents a difficulty in measuring the effect of semantic tags on the systems' performance, as there is no one-to-one correspondence between a correct label and a particular semantic tag. We therefore employ the following method in order to assess the contribution of semantic tags. Given the performance ranking of all our systems \u2014 $FSN < ST < PSN < LWS$ \u2014 we make a pairwise comparison between the output of a superior system $S_{sup}$ and an inferior system $S_{inf}$ . This involves taking the pairs of sentences that every $S_{sup}$ classifies correctly, but some $S_{inf}$ does not.", "Unlike BIBREF15 's Shared-Layer Architecture, in our setup each task has its own shared subspace rather than one common shared layer. This enables the sharing of different parameters in each direction (i.e., from main to auxiliary task and from auxiliary to main task), allowing each task to choose what to learn from the other, rather than having \u201cone shared layer to capture the shared information for all the tasks\u201d as in BIBREF15 .", "We implement three neural MTL settings, shown in Figure 1 . They differ in the way the network's parameters are shared between the tasks:"]}
{"question_id": "d576af4321fe71ced9e521df1f3fe1eb90d2df2d", "predicted_answer": "", "predicted_evidence": ["We call it the back-translation objective. Therefore, our final objective consists of content fidelity objective, style preservation objective and back-translation objective.", "In the last, a bidirectional LSTM is used for fusing the relatedness to the interactive features. The output $F = [f_1,..., f_{L_X}] \\in \\mathbb {R}^{ 2d \\times L_x} $, which provides a foundation for selecting which record may be the best suitable content, as fusion feature bank.", "As shown in the middle-right dashed box of Figure 3, we first construct the Record Bank as $R= [rc_1,...,rc_o,..., rc_{L_x},] \\in \\mathbb {R}^{2d \\times L_x}$, where $L_x = M \\times N$ is the number of records in Table $x$ and each $rc_o$ is the final representation of record $r_{ij}$, $r_{ij} = [\\overrightarrow{hc_{ij}}, \\overleftarrow{hc_{ij}}]$, as well as the Reference Bank $W$, which is $W = [{w.h_1, ..., w.h_K}] $. Then, we calculate the affinity matrix, which contains affinity scores corresponding to all pairs of structured records and reference words: $L = R^TW \\in \\mathbb {R}^{ L_x \\times K} $.", "In this subsection, we construct a large document-scale text content manipulation dataset as a testbed of our task. The dataset is derived from an NBA game report corpus ROTOWIRE BIBREF10, which consists of 4,821 human written NBA basketball game summaries aligned with their corresponding game tables. In our work, each of the original table-summary pair is treated as a pair of $(x, y_{aux})$, as described in previous subsection. To this end, we design a type-based method for obtaining a suitable reference summary $y^{\\prime }$ via retrieving another table-summary from the training data using $x$ and $y_{aux}$. The retrieved $y^{\\prime }$ contains record types as same as possible with record types contained in $y$. We use an existing information extraction tool BIBREF10 to extract record types from the reference text. Table TABREF3 shows the statistics of constructed document-level dataset and a sentence-level benchmark dataset BIBREF1.", "Figure 4 shows the generated examples by different models given content records $x$ and reference summary $y^{\\prime }$. We can see that our full model can manipulate the reference style words more accurately to express the new records. Whereas four generations seem to be fluent, the summary of Rule-SF includes logical erroneous sentences colored in orange. It shows a common sense error that Davis was injured again when he had left the stadium with an injury. This is because although the rule-based method has the most style words, they cannot be modified, which makes these style expressions illogical. An important discovery is that the sentence-level text content manipulation model TMTE fails to generate the style words similar to the reference summary. The reason is that TMTE has no interactive attention module unlike our model, which models the semantic relationship between records and reference words and therefore accurately select the suitable information from bi-aspect inputs."]}
{"question_id": "fd651d19046966ca65d4bcf6f6ae9c66cdf13777", "predicted_answer": "", "predicted_evidence": ["In this section, we present an overview of our model for document-scale text content manipulation, as illustrated in Figure 2. Since there are unaligned training pairs, the model is trained with three competing objectives of reconstructing the auxiliary document $y_{aux}$ based on $x$ and $y^{\\prime }$ (for content fidelity), the reference document $y^{\\prime }$ based on $x^{\\prime }$ and $y^{\\prime }$ (for style preservation), and the reference document $y^{\\prime }$ based on $x^{\\prime }$ and pseudo $z$ (for pseudo training pair). Formally, let $p_{\\theta }=(z|x,y^{\\prime })$ denotes the model that takes in records $x$ and a reference summary $y^{\\prime }$, and generates a summary $z$. Here $\\theta $ is the model parameters.", "We use two-layers LSTMs in all encoders and decoders, and employ attention mechanism BIBREF19. Trainable model parameters are randomly initialized under a Gaussian distribution. We set the hyperparameters empirically based on multiple tries with different settings. We find the following setting to be the best. The dimension of word/feature embedding, encoder hidden state, and decoder hidden state are all set to be 600. We apply dropout at a rate of 0.3. Our training process consists of three parts. In the first, we set $\\lambda _1=0$ and $\\lambda _2=1$ in Eq. 7 and pre-train the model to convergence. We then set $\\lambda _1=0.5$ and $\\lambda _2=0.5$ for the next stage training.", "In this section, we present an overview of our model for document-scale text content manipulation, as illustrated in Figure 2. Since there are unaligned training pairs, the model is trained with three competing objectives of reconstructing the auxiliary document $y_{aux}$ based on $x$ and $y^{\\prime }$ (for content fidelity), the reference document $y^{\\prime }$ based on $x^{\\prime }$ and $y^{\\prime }$ (for style preservation), and the reference document $y^{\\prime }$ based on $x^{\\prime }$ and pseudo $z$ (for pseudo training pair). Formally, let $p_{\\theta }=(z|x,y^{\\prime })$ denotes the model that takes in records $x$ and a reference summary $y^{\\prime }$, and generates a summary $z$. Here $\\theta $ is the model parameters. In detail, the model consists of a reference encoder, a record encoder, an interactive attention and a decoder.", "In addition, for sentence-level task, we adopt the same baseline methods as the paper BIBREF1, including an attention-based Seq2Seq method with copy mechanism BIBREF23, a rule-based method, two style transfer methods, MAST BIBREF24 and AdvST BIBREF25, as well as their state-of-the-art method, abbreviate as S-SOTA.", "We similarly compute the summaries $WA^R$ of the reference in light of each record of the table. Similar to BIBREF14, we also place reference-level attention over the record-level attention by compute the record summaries $C^WA^R$ of the previous attention weights in light of each record of the table. These two operations can be done in parallel, as is shown in Eq. 6."]}
{"question_id": "08b77c52676167af72581079adf1ca2b994ce251", "predicted_answer": "", "predicted_evidence": ["(8) Ours w/o Back-translation (-BackT) is also a variation of our model by omitting back-translation loss.", "In this task, the definition of the text content (e.g., statistical records of a basketball game) is clear, but the text style is vague BIBREF3. It is difficult to construct paired sentences or documents for the task of text content manipulation. Therefore, the majority of existing text editing studies develop controlled generator with unsupervised generation models, such as Variational Auto-Encoders (VAEs) BIBREF4, Generative Adversarial Networks (GANs) BIBREF5 and auto-regressive networks BIBREF6 with additional pre-trained discriminators.", "Bing Qin is the corresponding author of this work. This work was supported by the National Key R&D Program of China (No. 2018YFB1005103), National Natural Science Foundation of China (No. 61906053) and Natural Science Foundation of Heilongjiang Province of China (No. YQ2019F008).", "Following BIBREF1, BIBREF26, we presented to annotators five generated summaries, one from our model and four others from comparison methods, such as Rule-SF, Copy-SF, HEDT, TMTE. These students were asked to rank the five summaries by considering \u201cContent Fidelity\u201d, \u201cStyle Preservation\u201d and \u201cFluency\u201d separately. The rank of each aspect ranged from 1 to 5 with the higher score the better and the ranking scores are averaged as the final score. For each study, we evaluated on 50 test instances. From Table 3, we can see that the Content Fidelity and Style Preservation results are highly consistent with the results of the objective evaluation. An exception is that the Fluency of our model is much higher than other methods. One possible reason is that the reference-based generation method is more flexible than template-based methods, and more stable than pure language models on document-level long text generation tasks.", "In detail, we present a flexible copying mechanism which is able to copy contents from table records. The basic idea of the copying mechanism is to copy a word from the table contents as a trade-off of generating a word from target vocabulary via softmax operation. On one hand, we define the probability of copying a word $\\tilde{z}$ from table records at time step $t$ as $g_t(\\tilde{z}) \\odot \\alpha _{(t, id(\\tilde{z}))}$, where $g_t(\\tilde{z})$ is the probability of copying a record from the table, $id(\\tilde{z})$ indicates the record number of $\\tilde{z}$, and $\\alpha _{(t, id(\\tilde{z}))}$ is the attention probability on the $id(\\tilde{z})$-th record."]}
{"question_id": "89fa14a04008c93907fa13375f9e70b655d96209", "predicted_answer": "", "predicted_evidence": ["We similarly compute the summaries $WA^R$ of the reference in light of each record of the table. Similar to BIBREF14, we also place reference-level attention over the record-level attention by compute the record summaries $C^WA^R$ of the previous attention weights in light of each record of the table. These two operations can be done in parallel, as is shown in Eq. 6.", "In the last, a bidirectional LSTM is used for fusing the relatedness to the interactive features. The output $F = [f_1,..., f_{L_X}] \\in \\mathbb {R}^{ 2d \\times L_x} $, which provides a foundation for selecting which record may be the best suitable content, as fusion feature bank.", "As shown in the middle-right dashed box of Figure 3, we first construct the Record Bank as $R= [rc_1,...,rc_o,..., rc_{L_x},] \\in \\mathbb {R}^{2d \\times L_x}$, where $L_x = M \\times N$ is the number of records in Table $x$ and each $rc_o$ is the final representation of record $r_{ij}$, $r_{ij} = [\\overrightarrow{hc_{ij}}, \\overleftarrow{hc_{ij}}]$, as well as the Reference Bank $W$, which is $W = [{w.h_1, ..., w.h_K}] $. Then, we calculate the affinity matrix, which contains affinity scores corresponding to all pairs of structured records and reference words: $L = R^TW \\in \\mathbb {R}^{ L_x \\times K} $. The affinity matrix is normalized row-wise to produce the attention weights $A^W$ across the structured table for each word in the reference text, and column-wise to produce the attention weights $A^R$ across the reference for each record in the Table:", "Following BIBREF1, BIBREF26, we presented to annotators five generated summaries, one from our model and four others from comparison methods, such as Rule-SF, Copy-SF, HEDT, TMTE. These students were asked to rank the five summaries by considering \u201cContent Fidelity\u201d, \u201cStyle Preservation\u201d and \u201cFluency\u201d separately. The rank of each aspect ranged from 1 to 5 with the higher score the better and the ranking scores are averaged as the final score. For each study, we evaluated on 50 test instances. From Table 3, we can see that the Content Fidelity and Style Preservation results are highly consistent with the results of the objective evaluation. An exception is that the Fluency of our model is much higher than other methods. One possible reason is that the reference-based generation method is more flexible than template-based methods, and more stable than pure language models on document-level long text generation tasks.", "We find the following setting to be the best. The dimension of word/feature embedding, encoder hidden state, and decoder hidden state are all set to be 600. We apply dropout at a rate of 0.3. Our training process consists of three parts. In the first, we set $\\lambda _1=0$ and $\\lambda _2=1$ in Eq. 7 and pre-train the model to convergence. We then set $\\lambda _1=0.5$ and $\\lambda _2=0.5$ for the next stage training. Finally, we set $\\lambda _1=0.4$ and $\\lambda _2=0.5$ for full training. Adam is used for parameter optimization with an initial learning rate of 0.001 and decaying rate of 0.97. During testing, we use beam search with beam size of 5. The minimum decoding length is set to be 150 and maximum decoding length is set to be 850."]}
{"question_id": "ff36168caf48161db7039e3bd4732cef31d4de99", "predicted_answer": "", "predicted_evidence": ["Although, it is indeed a helpful phenomena, we would like to minimize the number of subclasses created by the community detection algorithm simply because we want to avoid having too many subclasses that would add more complexity in designing any applications using the community data. On the other hand, the Class-merge happens when multiple human labeled classes are merged into one giant community. This Class-merge phenomena also helps improve the original data set by detecting either misslabeled or ambiguous data entries. We will discuss more details in the following subsection. Nonetheless, we also want to minimize the number of classes merged into the one giant community, because when too many classes are merged into one class, it simply implies that the sentence network is not correctly clustered. For example, as shown in Figure.FIGREF15 red lines, 12 different human labeled classes that do not share any similar intents are merged into COMMUNITY_7. If we trained a text classification model on this data, we would have lost the specifically designed purposes of the 12 different classes, expecting COMMUNITY_7 to deal with all the 12 different types of sentences.", "On the other hand, the Class-merge happens when multiple human labeled classes are merged into one giant community. This Class-merge phenomena also helps improve the original data set by detecting either misslabeled or ambiguous data entries. We will discuss more details in the following subsection. Nonetheless, we also want to minimize the number of classes merged into the one giant community, because when too many classes are merged into one class, it simply implies that the sentence network is not correctly clustered. For example, as shown in Figure.FIGREF15 red lines, 12 different human labeled classes that do not share any similar intents are merged into COMMUNITY_7. If we trained a text classification model on this data, we would have lost the specifically designed purposes of the 12 different classes, expecting COMMUNITY_7 to deal with all the 12 different types of sentences. This would dramatically degrade the performance of the text classification models.", "We checked the community detection results with the original human labeled data by comparing the sentences in each community with the sentences in each human labeled class to confirm how well the algorithm worked. We built class maps to facilitate this process (see Figure.FIGREF15) that show mapping between communities in the sentence networks and classes in the original data set. Using the class maps, we found two notable cases where; 1. the sentences from multiple communities are consist of the sentences of one class of the human labeled data, meaning the original class is splitted into multiple communities and 2. the sentences from one community consist of the sentences of multiple classes in human labeled data, meaning multiple classes in the original data are merged into one community. For example, in the earlier case (see blue lines in Figure.FIGREF15) which we call Class-split, the sentences in COMMUNITY_1, COMMUNITY_2, COMMUNITY_5, COMMUNITY_8, COMMUNITY_10, COMMUNITY_14 and COMMUNITY_17 are the same as the sentences in CHAT_AGENT class.", "Specifically, let $D = \\lbrace d_1, \\dots , d_n\\rbrace $ be a set of documents and $T = \\lbrace t_1, \\dots , t_m\\rbrace $ the set of unique terms in the entire documents where $n$ is the number of documents in the data set and $m$ the number of unique words in the documents. In this study, the documents are the preprocessed sentences and the terms are the unique words in the preprocessed sentences. The importance of a word is captured with its frequency as $tf(d,t)$ denoting the frequency of the word $t \\in T$ in the document $d \\in D$. Then a document $d$ is represented as an $m$-dimensional vector ${{t_d}}=(tf(d,t_1),\\dots ,tf(d,t_m))$. However, In order to compute more concise and meaningful importance of a word, TFIDF not only takes the frequency of a particular word in a particular document into account, but also considers the number of documents that the word appears in the entire data set.", "With the TFIDF vector representations, we formed sentence networks to investigate the usefulness of the network community detection. In total, 10 sentence networks (see Figure.FIGREF13 and Figure.FIGREF16) were constructed with 2,212 nodes representing sentences and edge weights representing the pairwise similarities between sentences with 10 different network connectivity threshold values. The networks we formed were all undirected and weighted graphs. Particularly, as for the network edge weights, the cosine similarity BIBREF14, BIBREF15 is used to compute the similarities between sentences. The cosine similarity is a similarity measure that is in a floating number between 0 and 1, and computed as the angle difference between two vectors. A cosine similarity of 0 means that the two vectors are perpendicular to each other implying no similarity, on the other hand a cosine similarity of 1 means that the two vectors are identical."]}
{"question_id": "556782bb96f8fc07d14865f122362ebcc79134ec", "predicted_answer": "", "predicted_evidence": ["We then applied the Louvain method to detect communities in the network, and to automatically label the data set. The network with threshold of 0.5477 has 399 communities with 20,856 edges. Class_split and Class_merge scores of the network was 22.3158 and 1.0627 respectively. We finally trained and tested machine learning based text classification models on the data set labeled by the community detection outcome to see how well our approach worked. Following a general machine learning train and test practice, we split the data set into train set(80% of the data) and test set(20% of the data). The particular models we trained and tested were standard Support vector machine BIBREF16 and Randome forest BIBREF17 models that are popularly used in natural language processing such as spam e-mail and news article categorizations. More details about the two famous machine learning models are well discussed in the cited papers.", "For example, for a question of \"how do I get there by subway?\", in the chatbot, there is a designed message of \"You can take line M or B to 35th street.\" to respond to that particular query. Using these output messages in the chatbot, we were able to compute the ground truth accuracy of our classification models by comprehending the input sentences in the test sets, the detected classes from the models and linked messages. In our test, the Support vector machine trained on human labeled data performed 0.9572 while the same model trained on the data labeled by our method resulted 0.9931. Also, the Random forest model trained on human labeled data performed 0.9504 while the same model trained on the data labeled by our method did 0.9759.", "Tokenizing and cleaning the sentences by removing punctuations, special characters and English stopwords that appear frequently without holding much important meaning. For example, [\"how can I get there if I'm taking a subway?\"] becomes ['get', 'taking', 'subway']", "Once we got the optimal connectivity threshold using the Class_split and Class_merge scores as shown above sections, we built the sentence network with the optimal threshold of 0.5477. We then applied the Louvain method to detect communities in the network, and to automatically label the data set. The network with threshold of 0.5477 has 399 communities with 20,856 edges. Class_split and Class_merge scores of the network was 22.3158 and 1.0627 respectively. We finally trained and tested machine learning based text classification models on the data set labeled by the community detection outcome to see how well our approach worked. Following a general machine learning train and test practice, we split the data set into train set(80% of the data) and test set(20% of the data). The particular models we trained and tested were standard Support vector machine BIBREF16 and Randome forest BIBREF17 models that are popularly used in natural language processing such as spam e-mail and news article categorizations.", "Specifically, let $D = \\lbrace d_1, \\dots , d_n\\rbrace $ be a set of documents and $T = \\lbrace t_1, \\dots , t_m\\rbrace $ the set of unique terms in the entire documents where $n$ is the number of documents in the data set and $m$ the number of unique words in the documents. In this study, the documents are the preprocessed sentences and the terms are the unique words in the preprocessed sentences. The importance of a word is captured with its frequency as $tf(d,t)$ denoting the frequency of the word $t \\in T$ in the document $d \\in D$. Then a document $d$ is represented as an $m$-dimensional vector ${{t_d}}=(tf(d,t_1),\\dots ,tf(d,t_m))$. However, In order to compute more concise and meaningful importance of a word, TFIDF not only takes the frequency of a particular word in a particular document into account, but also considers the number of documents that the word appears in the entire data set."]}
{"question_id": "cb58605a7c230043bd0d6e8d5b068f8b533f45fe", "predicted_answer": "", "predicted_evidence": ["With the TFIDF vector representations, we formed sentence networks to investigate the usefulness of the network community detection. In total, 10 sentence networks (see Figure.FIGREF13 and Figure.FIGREF16) were constructed with 2,212 nodes representing sentences and edge weights representing the pairwise similarities between sentences with 10 different network connectivity threshold values. The networks we formed were all undirected and weighted graphs. Particularly, as for the network edge weights, the cosine similarity BIBREF14, BIBREF15 is used to compute the similarities between sentences. The cosine similarity is a similarity measure that is in a floating number between 0 and 1, and computed as the angle difference between two vectors. A cosine similarity of 0 means that the two vectors are perpendicular to each other implying no similarity, on the other hand a cosine similarity of 1 means that the two vectors are identical. It is popularly used in text mining and information retrieval techniques.", "From each sentence, we removed punctuations, special characters and English stopwords to keep only those meaningful words that serve the main purpose of the sentence, and to avoid any redundant computing. We then tokenized each sentence into words to process the data further in word level. For words in each sentence, we added synonyms of the words to handle more variations of the sentence as a typical method of increasing the resulting classification models' capability of understanding more unseen expressions with different words that describe similar meanings. Although we used the predefined synonyms from the Python NLTK package, one might develop it's own synonym data to use in accordance with the context of the particular data to achieve a better accuracy. We also added bigrams of the words to deal with those cases where the tokenization breaks the meaning of the word that consist of two words.", "Stemmizing the words, and adding synonyms and bigrams of the sequence of the words left in each sentence to enable the model to learn more kinds of similar expressions and the sequences of the words. For example, ['get', 'taking', 'subway'] becomes ['get', 'take', 'subway', 'tube', 'underground', 'metro', 'take metro', 'get take', 'take subway', 'take underground', ...]", "For example, for a question of \"how do I get there by subway?\", in the chatbot, there is a designed message of \"You can take line M or B to 35th street.\" to respond to that particular query. Using these output messages in the chatbot, we were able to compute the ground truth accuracy of our classification models by comprehending the input sentences in the test sets, the detected classes from the models and linked messages. In our test, the Support vector machine trained on human labeled data performed 0.9572 while the same model trained on the data labeled by our method resulted 0.9931. Also, the Random forest model trained on human labeled data performed 0.9504 while the same model trained on the data labeled by our method did 0.9759.", "TFIDF, as known as Term frequency inversed document frequency, is a document representation that takes account of the importance of each word by its frequency in the whole set of documents and its frequency in particular sets of documents. Specifically, let $D = \\lbrace d_1, \\dots , d_n\\rbrace $ be a set of documents and $T = \\lbrace t_1, \\dots , t_m\\rbrace $ the set of unique terms in the entire documents where $n$ is the number of documents in the data set and $m$ the number of unique words in the documents. In this study, the documents are the preprocessed sentences and the terms are the unique words in the preprocessed sentences. The importance of a word is captured with its frequency as $tf(d,t)$ denoting the frequency of the word $t \\in T$ in the document $d \\in D$. Then a document $d$ is represented as an $m$-dimensional vector ${{t_d}}=(tf(d,t_1),\\dots ,tf(d,t_m))$."]}
{"question_id": "7969b8d80e12aa3ebb89b5622bc564f44e98329f", "predicted_answer": "", "predicted_evidence": ["Once we got the optimal connectivity threshold using the Class_split and Class_merge scores as shown above sections, we built the sentence network with the optimal threshold of 0.5477. We then applied the Louvain method to detect communities in the network, and to automatically label the data set. The network with threshold of 0.5477 has 399 communities with 20,856 edges. Class_split and Class_merge scores of the network was 22.3158 and 1.0627 respectively. We finally trained and tested machine learning based text classification models on the data set labeled by the community detection outcome to see how well our approach worked. Following a general machine learning train and test practice, we split the data set into train set(80% of the data) and test set(20% of the data).", "From each sentence, we removed punctuations, special characters and English stopwords to keep only those meaningful words that serve the main purpose of the sentence, and to avoid any redundant computing. We then tokenized each sentence into words to process the data further in word level. For words in each sentence, we added synonyms of the words to handle more variations of the sentence as a typical method of increasing the resulting classification models' capability of understanding more unseen expressions with different words that describe similar meanings. Although we used the predefined synonyms from the Python NLTK package, one might develop it's own synonym data to use in accordance with the context of the particular data to achieve a better accuracy. We also added bigrams of the words to deal with those cases where the tokenization breaks the meaning of the word that consist of two words.", "Transforming the preprocessed text data into a vector form by computing TFIDF of each preprocessed sentence with regard to the entire data set, and computing pair-wise cosine similiarity of the TFIDF vectors to form the adjacency matrix of the sentence network", "With the preprocessed text data, we built vector representations of the sentences by performing weighted document representation using TFIDF weighting scheme BIBREF13, BIBREF14. TFIDF, as known as Term frequency inversed document frequency, is a document representation that takes account of the importance of each word by its frequency in the whole set of documents and its frequency in particular sets of documents. Specifically, let $D = \\lbrace d_1, \\dots , d_n\\rbrace $ be a set of documents and $T = \\lbrace t_1, \\dots , t_m\\rbrace $ the set of unique terms in the entire documents where $n$ is the number of documents in the data set and $m$ the number of unique words in the documents. In this study, the documents are the preprocessed sentences and the terms are the unique words in the preprocessed sentences.", "Applying a network community detection algorithm on the sentence network to detect the communities where each preprocessed sentence belong, and build a labeled data set with detected communities for training and testing machine learning classification models."]}
{"question_id": "a554cd1ba2a8d1348a898e0cb4b4c16cc8998257", "predicted_answer": "", "predicted_evidence": ["The main contributions of our work are the following:", "then writing the triplet $(x, y, g)$ to $M$ consist of:", "If the bias score during testing is greater than the one during training,", "To enable a fair comparison, we limit the number of articles for each dataset to 20,000 and the size of the vocabulary to the 18,000 most common words. Datasets are split into 60%, 20%, and 20% for training, validation, and testing. We want to see if there are correlations showing stereotypes across different nations. Does the biased correlations learned by an encoder transfer to the decoder considering word sequences from different countries?", "The Fair Region of a memory network consists of a subset of the memory keys which are responsible for computing error signals and generating gradients that will flow through the entire architecture with backpropagation. We do not want to attend over all the memory entries but explicitly induce a uniform gender distribution within this region. The result is a training process in which gender-related embeddings equally contribute in number to the update of the entire architecture. This embedding-level constraint prevents the unconstrained learning of correlations between a latent vector $h$ and similar memory entries in $M$ directly in the latent space considering explicit gender indicators."]}
{"question_id": "3cc9a820c4a2cd2ff61da920c41ed09f3c0135be", "predicted_answer": "", "predicted_evidence": ["Our main finding is that our approach (Seq2Seq+FairRegion) shows similar perplexity values ($10.79$) than the Seq2Seq+Attention baseline model ($10.73$) when generating word sequences despite using the Fair Region strategy. These results encourage the use of a controlled region as an automatic technique that maintains the efficacy of generating text. We observe a larger perplexity for country-based datasets, likely because of their smaller training datasets.", "Neural Networks have proven to be useful for automating tasks such as question answering, system response, and language generation considering large textual datasets. In learning systems, bias can be defined as the negative consequences derived by the implicit association of patterns that occur in a high-dimensional space. In dialogue systems, these patterns represent associations between word embeddings that can be measured by a Cosine distance to observe male- and female-related analogies that resemble the gender stereotypes of the real world. We propose an automatic technique to mitigate bias in language generation models based on the use of an external memory in which word embeddings are associated to gender information, and they can be sparsely updated based on content-based lookup.", "Definition 2.1 (Fair Region) Let $h$ be an latent representation of the input and $M$ be an external memory. The male-neighborhood of $h$ is represented by the indices of the $n$-nearest keys to $h$ in decreasing order that share the same gender type male as $\\lbrace i^m_1, ..., i^m_k\\rbrace = KNN(h, n, male)$. Running this process for each gender type estimates the indices $i^m$, $i^f$, and $i^{ng}$ which correspond to the male, female, and non-gender neighborhoods. Then, the FairRegion of $M$ given $h$ consists of $K[i^m; i^f; i^{ng}]$.", "Instead of using the decoder output $h_i^{deco}$ to directly predict the next word as a prediction over the vocabulary $O$, as in BIBREF6. We combine this vector with a query to the memory module to compute the embedding vector $h^{fair}_{i}$. We do this by computing an attention score BIBREF5 with each key of a Fair Region. The attention logits become the unormalized probabilities of including their associated values for predicting the $i^{th}$ token of the response $\\hat{y}$. We then argmax the most likely entry in the output vocabulary $O$ to obtain the $i^{th}$ predicted token of the response $\\hat{y}$. More formally,", "We experimentally show that this architecture leads to mitigate gender bias amplification in the automatic generation of text when extending the Sequence2Sequence model."]}
{"question_id": "95ef89dc29ff291bdbe48cb956329a6a06d36db8", "predicted_answer": "", "predicted_evidence": ["If the bias score during testing is greater than the one during training,", "For all the experiments, the size of the word embeddings is 256. The encoders and decoders are bidirectional LSTMs of 2-layers with state size of 256 for each direction. For the Seq2Seq+FairRegion model, the number of memory entries is 1,000. We train all models with Adam optimizer BIBREF7 with a learning rate of $0.001$ and initialized all weights from a uniform distribution in $[-0.01, 0.01]$. We also applied dropout BIBREF8 with keep probability of $95.0\\%$ for the inputs and outputs of recurrent neural networks.", "Naturally, the objective function is to minimize the cross entropy of actual and generated content:", "However, the number of word embeddings does not provide an equal representation across gender types because context-sensitive embeddings are severely biased in natural language, BIBREF1. For example, it has been shown in that man is closer to programmer than woman, BIBREF2. Similar problems have been recently observed in popular work embedding algorithms such as Word2Vec, Glove, and BERT, BIBREF3.", "We introduce a novel architecture that considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network."]}
{"question_id": "79258cea30cd6c0662df4bb712bf667589498a1f", "predicted_answer": "", "predicted_evidence": ["Complementary to the collection of tweets using the Twitter API, we used 886 tweets provided by the \u201cSoci\u00e9t\u00e9 Nationale des Chemins de fer Fran\u00e7ais\u201d (SNCF), that is the French National Railway Corporation. The latter subset is biased towards information in the interest of the corporation such as train lines or names of train stations. To account for the different distribution of entities in the tweets collected by SNCF we incorporated them in the data as follows:", "Submission 4 BIBREF5 The system also relies on a CRF classifier operating on features extracted for each word of the tweet such as POS tags etc. In addition, they employ an existing pattertn mining NER system (mXS) which is not trained for tweets. The addition of the system's results in improving the recall at the expense of precision.", "Abbreviations must be annotated. For example, LMP is the abbreviation of \u201cLe Meilleur Patissier\" which is a tvshow.", "Submission 2 BIBREF3 The system follows a state-of-the-art approach by using a CRF for to tag sentences with NER tags. The authors develop a set of features divided into six families (orthographic, morphosyntactic, lexical, syntactic, polysemic traits, and language-modeling traits).", "Submission 1 BIBREF2 The system relies on a recurrent neural network (RNN). More precisely, a bi-directional GRU network is used and a CRF layer is adde on top of the network to improve label prediction given information from the context of a word, that is the previous and next tags."]}
{"question_id": "8e5ce0d2635e7bdec4ba1b8d695cd06790c8cdaa", "predicted_answer": "", "predicted_evidence": ["Abbreviations must be annotated. For example, LMP is the abbreviation of \u201cLe Meilleur Patissier\" which is a tvshow.", "To collect the tweets that were used to construct the dataset we relied on the Twitter streaming API. The API makes available a part of Twitter flow and one may use particular keywords to filter the results. In order to collect tweets written in French and obtain a sample that would be unbiased towards particular types of entities we used common French words like articles, pronouns, and prepositions: \u201cle\u201d,\u201cla\u201d,\u201cde\u201d,\u201cil\u201d,\u201celle\u201d, etc.. In total, we collected 10,000 unique tweets from September 1st until September the 15th of 2016.", "Submission 4 BIBREF5 The system also relies on a CRF classifier operating on features extracted for each word of the tweet such as POS tags etc. In addition, they employ an existing pattertn mining NER system (mXS) which is not trained for tweets. The addition of the system's results in improving the recall at the expense of precision.", "ducray) [text=magenta] Ducray; & text=black] et; & a) [text=magenta] A;& text=magenta] -; & derma) [text=magenta] Derma;", "Submission 1 BIBREF2 The system relies on a recurrent neural network (RNN). More precisely, a bi-directional GRU network is used and a CRF layer is adde on top of the network to improve label prediction given information from the context of a word, that is the previous and next tags."]}
{"question_id": "4e568134c896c4616bc7ab4924686d8d59b57ea1", "predicted_answer": "", "predicted_evidence": ["A given entity must be annotated with one label. The annotator must therefore choose the most relevant category according to the semantics of the message. We can therefore find in the dataset an entity annotated with different labels. For instance, Facebook can be categorized as a media (\u201cnotre page Facebook\") as well as an organization (\u201cFacebook acquires acquiert Nascent Objects\").", "We measure the inter-annotator agreement between the annotators based on the Cohen's Kappa (cf. Table TABREF15 ) calculated on the first 200 tweets of the training set. According to BIBREF1 our score for Cohen's Kappa (0,70) indicates a strong agreement.", "Given, for instance, a tweet \u201cLes Parisiens supportent PSG ;-)\u201d one needs to identify that the abbreviation \u201cPSG\u201d refers to an entity, namely the football team \u201cParis Saint-Germain\u201d. Therefore, there two main challenges in the problem. First one needs to identify the boundaries of an entity (in the example PSG is a single word entity), and then to predict the type of the entity. In the CAp 2017 challenge one needs to identify among 13 types of entities: person, musicartist, organisation, geoloc, product, transportLine, media, sportsteam, event, tvshow, movie, facility, other in a given tweets. Importantly, we do not allow the entities to be hierarchical, that is contiguous words belong to an entity as a whole and a single entity type is associated per word. It is also to be noted that some of the tweets may not contain entities and therefore systems should not be biased towards predicting one or more entities for each tweet.", "ducray) [text=magenta] Ducray; & text=black] et; & a) [text=magenta] A;& text=magenta] -; & derma) [text=magenta] Derma;", "Complementary to the collection of tweets using the Twitter API, we used 886 tweets provided by the \u201cSoci\u00e9t\u00e9 Nationale des Chemins de fer Fran\u00e7ais\u201d (SNCF), that is the French National Railway Corporation. The latter subset is biased towards information in the interest of the corporation such as train lines or names of train stations. To account for the different distribution of entities in the tweets collected by SNCF we incorporated them in the data as follows:"]}
{"question_id": "55612e92791296baf18013d2c8dd0474f35af770", "predicted_answer": "", "predicted_evidence": ["For the training set, which comprises 3,000 tweets, we used 2,557 tweets collected using the API and 443 tweets of those provided by SNCF.", "Submission 7 Lastly, BIBREF8 uses a rule based system which performs several linguistic analysis like morphological and syntactic as well as the extraction of relations. The dictionaries used by the system was augmented with new entities from the Web. Finally, linguistics rules were applied in order to tag the detected entities.", "As shown in Figure 1, the training and the test set have a similar distribution in terms of named entity types. The training set contains 2,902 entities among 1,656 unique entities (i.e. 57,1%). The test set contains 3,660 entities among 2,264 unique entities (i.e. 61,8%). Only 15,7% of named entities are in both datasets (i.e. 307 named entities). Finally we notice that less than 2% of seen entities are ambiguous on the testset.", "In the example given in Figure FIGREF20 :", "To collect the tweets that were used to construct the dataset we relied on the Twitter streaming API. The API makes available a part of Twitter flow and one may use particular keywords to filter the results. In order to collect tweets written in French and obtain a sample that would be unbiased towards particular types of entities we used common French words like articles, pronouns, and prepositions: \u201cle\u201d,\u201cla\u201d,\u201cde\u201d,\u201cil\u201d,\u201celle\u201d, etc.. In total, we collected 10,000 unique tweets from September 1st until September the 15th of 2016."]}
{"question_id": "2f23bd86a9e27dcd88007c9058ddfce78a1a377b", "predicted_answer": "", "predicted_evidence": ["In this paper we presented the challenge on French Twitter Named Entity Recognition. A large corpus of around 6,000 tweets were manyally annotated for the purposes of training and evaluation. To the best of our knowledge this is the first corpus in French for NER in short and noisy texts. A total of 8 teams participated in the competition, employing a variety of state-of-the-art approaches. The evaluation of the systems helped us to reveal the strong points and the weaknesses of these approaches and to suggest potential future directions.", "Submission 3 BIBREF4 , ranked first, employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations.", "Overall, the results of 8 systems were submitted for evaluation. Among them, 7 submitted a paper discussing their implementation details. The participants proposed a variety of approaches principally using Deep Neural Networks (DNN) and Conditional Random Fields (CRF). In the rest of the section we provide a short overview for the approaches used by each system and discuss the achieved scores.", "As shown in Figure 1, the training and the test set have a similar distribution in terms of named entity types. The training set contains 2,902 entities among 1,656 unique entities (i.e. 57,1%). The test set contains 3,660 entities among 2,264 unique entities (i.e. 61,8%). Only 15,7% of named entities are in both datasets (i.e. 307 named entities). Finally we notice that less than 2% of seen entities are ambiguous on the testset.", "Event-named entities must include the type of the event. For example, colloque (colloquium) must be annotated in \u201cle colloque du R\u00e9veil fran\u00e7ais est rejoint par\"."]}
{"question_id": "e0b8a2649e384bbdb17472f8da2c3df4134b1e57", "predicted_answer": "", "predicted_evidence": ["In this paper we presented the challenge on French Twitter Named Entity Recognition. A large corpus of around 6,000 tweets were manyally annotated for the purposes of training and evaluation. To the best of our knowledge this is the first corpus in French for NER in short and noisy texts. A total of 8 teams participated in the competition, employing a variety of state-of-the-art approaches. The evaluation of the systems helped us to reveal the strong points and the weaknesses of these approaches and to suggest potential future directions.", "We also observe that the majority of the systems obtained good scores in terms of F1-score while having important differences in precision and recall. For example, the Lattice team achieved the highest precision score.", "Mentions (strings starting with @) and hashtags (strings starting with #) have a particular function in tweets. The former is used to refer to persons while the latter to indicate keywords. Therefore, in the annotation process we treated them using the following protocol: A hashtag or a mention should be annotated as an entity if:", "The paper is organized in two parts. In the first, we discuss the data preparation steps (collection, annotation) and we describe the proposed dataset. The dataset was first released in the framework of the CAp 2017 challenge, where 8 systems participated. Following, the second part of the paper presents an overview of baseline systems and the approaches employed by the systems that participated. We conclude with a discussion of the performance of Twitter NER systems and remarks for future work.", "We measure the inter-annotator agreement between the annotators based on the Cohen's Kappa (cf. Table TABREF15 ) calculated on the first 200 tweets of the training set. According to BIBREF1 our score for Cohen's Kappa (0,70) indicates a strong agreement."]}
{"question_id": "3f8a42eb0e904ce84c3fded2103f674e9cbc893d", "predicted_answer": "", "predicted_evidence": ["In this paper, we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data. We experiment with a number of auxiliary penalty terms to regularise the fine-tuning process for three modern RC models: QANet BIBREF10, decaNLP BIBREF11 and BERT BIBREF12. We observe that combining different auxiliary penalty terms results in the best performance, outperforming benchmark methods that require source data.", "where $\\mathcal {L}_{ce}$ is the cross-entropy loss.", "In spite of these successes, it is difficult to train these modern comprehension systems on narrow domain data (e.g. biomedical), as these models often have a large number of parameters. A better approach is to transfer knowledge via fine-tuning, i.e. by first pre-training the model using data from a large source domain and continue training it with examples from the small target domain. It is an effective strategy, although a fine-tuned model often performs poorly when it is re-applied to the source domain, a phenomenon known as catastrophic forgetting BIBREF6, BIBREF7, BIBREF8, BIBREF9. This is generally not an issue if the goal is to optimise purely for the target domain, but in real-word applications where model robustness is an important quality, over-optimising for a development set often leads to unexpected poor performance when applied to test cases in the wild.", "We explore 3 modern RC models in our experiments: QANet BIBREF10; decaNLP BIBREF11; and BERT BIBREF12. QANet is a Transformer-based BIBREF26 comprehension model, where the encoder consists of stacked convolution and self-attention layers. The objective of the model is to predict the position of the starting and ending indices of the answer words in the context. decaNLP is a recurrent network-based comprehension model trained on ten NLP tasks simultaneously, all casted as a question-answer problem. Much of decaNLP's flexibility is due to its pointer-generator network, which allows it to generate words by extracting them from the question or context passages, or by drawing them from a vocabulary. BERT is a deep bi-directional encoder model based on Transformers.", "Note that BERT and QANet RC models are extractive models (goal is to predict 2 indices), while decaNLP is a generative model (goal is to generate the correct word sequence). Also, unlike QANet and decaNLP, BERT is not designed specifically for RC. It represents a growing trend in the literature where large models are pre-trained on big corpora and further adapted to downstream tasks."]}
{"question_id": "521a3e7300567f6e8e4c531f223dbc9fc306c393", "predicted_answer": "", "predicted_evidence": ["For ms -cp (Figure FIGREF11), we first notice that there is considerably less forgetting overall (ms -cp performance ranges from 65\u201375 F1, while squad performance in Figure FIGREF11 ranges from 45\u201375 F1). This is perhaps unsurprising, as the model is already generally well-tuned (e.g. it takes less iterations to reach optimal performance for ms -cp compared to ms -bm and squad). Most models perform similarly here. +all produces stronger recovery when fine-tuning on ms -fm (129K\u2013149K) and ms -lw (150K\u2013170K). At the end of the continuous learning, the gap between all models is around 2 F1 points.", "where $\\mathcal {L}_{ce}$ is the cross-entropy loss.", "BIBREF7 explore supervised domain adaptation for reading comprehension, by pre-training their model first on large open-domain comprehension data and fine-tuning it further on biomedical data. This approach improves the biomedical domain's performance substantially compared to training the model from scratch. At the same time, its performance on source domain decreases dramatically due to catastrophic forgetting BIBREF6, BIBREF15, BIBREF16.", "where $\\lbrace F\\rbrace ^{v_i}$ denotes the set of parameters for variable $v$ where parameter $i$ belongs.", "Most large comprehension data sets are open-domain because non-experts can be readily recruited via crowdsourcing platforms to collect annotations. Development of domain-specific RC data sets, on the other hand, is costly due to the need of subject matter experts and as such the size of these data sets is typically limited. Examples include bioasq BIBREF14 in the biomedical domain, which has less than 3k QA pairs \u2014 orders of magnitude smaller compared to most large-scale open-domain data sets BIBREF1, BIBREF2, BIBREF3, BIBREF5."]}
{"question_id": "863b3f29f8c59f224b4cbdb5f1097b45a25f1d88", "predicted_answer": "", "predicted_evidence": ["where $c$ is the total number of user\u2013item pairs in the training data, and $y_{i}$ is the true rating of the $i$-th user\u2013item pair. The $\\ell $ in Eq. (DISPLAY_FORM28) serves as our loss function for model training.", "We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews. Table TABREF30 summarizes the domains and statistics for these datasets. Across all datasets, we follow the existing work BIBREF3, BIBREF17 to perform preprocessing to ensure they are in a $t$-core fashion, i.e., the datasets only include users and items that have at least $t$ reviews. In our experiments, we evaluate the two cases of $t=5$ and $t=10$. For the Yelp dataset, we follow BIBREF3 to focus on restaurants in the AZ metropolitan area. For each dataset, we randomly split the user\u2013item pairs into $80\\%$ training set, $10\\%$ validation set, and $10\\%$ testing set. When learning the representations for users and items, we only use their reviews from the training set, and none from the validation and testing sets.", "Suppose a review has $k$ sentences. We can then represent this review by a sequence $[{\\bf s}_{1}, ..., {\\bf s}_{k}]$, where ${\\bf s}_{i}$ is the embedding of the $i$-th sentence in the review, as inferred by Eq. (DISPLAY_FORM6). However, using Eq. (DISPLAY_FORM6), each ${\\bf s}_{i}$ only encodes its own semantic meaning, but remains oblivious of any contextual cues from its surrounding sentences in the same review. To further refine the sentence embedding, we introduce a context-encoding layer by employing another bi-directional LSTM on top of the previous layer to model the temporal interactions between sentences, i.e.,", "Then, attention weights for the reviews of the user", "In contrast, from $v$'s reviews, we wish to account for the sentiment of other users with regard to relevant aspects of $v$. If $u$ pays special attention to certain aspects of items similar to $v$, while other users wrote highly about $v$ with regard to these particular aspects, then it is much more likely that $v$ will be of interest to $u$. For example, in Fig. FIGREF1, reviews 1 and 2 of $u$ are about non-prescription medicines that are similar to $v$. In reviews 1 and 2, $u$ mentioned aspects such as \u201cnot sourced from genetically modified corn\u201d, \u201ceasier to swallow\u201d, \u201cgreat price\u201d, and \u201cno after taste\u201d, indicating that $u$ considers the source and price and prefers easily swallowed products without after-taste. Meanwhile, reviews 1-3 of $v$ mention that $v$ \u201chave no taste\u201d, is \u201ceasy to swallow\u201d, \u201cgmo-free\u201d, and \u201cprices low\u201d, which are opinions expressed by others that match $u$'s preferences."]}
{"question_id": "e4cbfabf4509ae0f476f950c1079714a9cd3814e", "predicted_answer": "", "predicted_evidence": ["We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews. Table TABREF30 summarizes the domains and statistics for these datasets. Across all datasets, we follow the existing work BIBREF3, BIBREF17 to perform preprocessing to ensure they are in a $t$-core fashion, i.e., the datasets only include users and items that have at least $t$ reviews. In our experiments, we evaluate the two cases of $t=5$ and $t=10$. For the Yelp dataset, we follow BIBREF3 to focus on restaurants in the AZ metropolitan area. For each dataset, we randomly split the user\u2013item pairs into $80\\%$ training set, $10\\%$ validation set, and $10\\%$ testing set.", "where $\\sum _{i=1}^{m}\\beta _{i}^{v} = 1$, and $\\beta _{i}^{v}$ is the attention weight assigned to review ${\\bf r}_{i}^{v}$. It quantifies the informativeness of the review ${\\bf r}_{i}^{v}$ with respect to $v$'s overall rating. $\\beta _{i}^{v}$ is produced by an attentive module with gating mechanism as follows:", "In this work, we highlight the asymmetric attention problem for review-based recommendation, which has been ignored by existing approaches. To address it, we propose a flexible neural architecture, AHN, which is characterized by its asymmetric attentive modules for distinguishing the learning of user embeddings and item embeddings from reviews, as well as by its hierarchical paradigm to extract fine-grained signals from sentences and reviews. Extensive experimental results on datasets from different domains demonstrate the effectiveness and interpretability of our method.", "is computed, where ${\\bf M}_{r} \\in \\mathbb {R}^{d_{r} \\times d_{r}}$ is a learnable parameter. Here, the $(p, q)$-th entry of ${\\bf G}$ represents the affinity between the $p$-th review of the user and the $q$-th review of the item.", "Fig. FIGREF43(c) and (d) zoom into the attention weights of AHN on the top three sentences of the first review of the user and item, respectively. The highlighted words indicate the reason of why the sentences are ranked highly. Apparently, the user cares about the taste of the medicine and prefers easily-swallowed softgels, while the item indeed appears to taste good and is easy to swallow. Although the first two sentences in Fig. FIGREF43(d) are short, they convey more useful information than the lowest-weighted sentence. Thus, the sentence-level attention weights are also meaningful. This explains why AHN predicts a 4.4 rating score on this user\u2013item pair, close to the true rating 5.0 given by the user."]}
{"question_id": "7a84fed904acc1e0380deb6e5a2e1daacfb5907a", "predicted_answer": "", "predicted_evidence": ["Exploiting reviews has proven considerably useful in recent work on recommendation. Many methods primarily focus on topic modeling based on the review texts. For example, HFT BIBREF6 employs LDA to discover the latent aspects of users and items from reviews. RMR BIBREF7 extracts topics from reviews to enhance the user and item embeddings obtained by factorizing the rating matrix. TopicMF BIBREF8 jointly factorizes a rating matrix and bag-of-words representations of reviews to infer user and item embeddings. Despite the improvements achieved, these methods only focus on topical cues in reviews, but neglect the rich semantic contents. Moreover, they typically represent reviews as bag-of-words, and thus remain oblivious of the order and contexts of words and sentences in reviews, which are essential for modeling the characteristics of users and items BIBREF1.", "is computed, where ${\\bf M}_{r} \\in \\mathbb {R}^{d_{r} \\times d_{r}}$ is a learnable parameter. Here, the $(p, q)$-th entry of ${\\bf G}$ represents the affinity between the $p$-th review of the user and the $q$-th review of the item.", "Specifically, an affinity matrix at the review level", "The intuition is that, if a user's sentence (i.e., a row of ${\\bf G}_{i}$) has a large affinity to at least one sentence of the target item (i.e., a column of ${\\bf G}_{i}$) \u2013 in other words, the maximal affinity of this row is large \u2013 then this user's sentence is relevant to the target item. However, not all sentences of the target item are useful for searching relevant sentences from the user. For instance, in Fig. FIGREF1, the first sentence of the item's review 2, \u201cI received it three days ago.\u201d, conveys little information about the target item, and hence cannot aid in identifying relevant sentences from the user, and indeed may introduce noise into the affinity matrix. To solve this problem, recall that $\\alpha _{i}^{v}$ in Eq. (DISPLAY_FORM13) represents how informative an item's sentence is. Thus, we concatenate $\\alpha _{i}^{v}$'s of all sentences of the target item to form $\\alpha ^{v} \\in \\mathbb {R}^{1 \\times mk}$.", "Table TABREF30 summarizes the domains and statistics for these datasets. Across all datasets, we follow the existing work BIBREF3, BIBREF17 to perform preprocessing to ensure they are in a $t$-core fashion, i.e., the datasets only include users and items that have at least $t$ reviews. In our experiments, we evaluate the two cases of $t=5$ and $t=10$. For the Yelp dataset, we follow BIBREF3 to focus on restaurants in the AZ metropolitan area. For each dataset, we randomly split the user\u2013item pairs into $80\\%$ training set, $10\\%$ validation set, and $10\\%$ testing set. When learning the representations for users and items, we only use their reviews from the training set, and none from the validation and testing sets. This ensures a practical scenario where we cannot include any future reviews into a user's (item's) history for model training."]}
{"question_id": "16b816925567deb734049416c149747118e13963", "predicted_answer": "", "predicted_evidence": ["$- \\log p(y|x + r_{adv};\\theta )$", "Our model is depicted in Figure FIGREF1. As can be seen, we create adversarial examples from BERT embeddings using the gradient of the loss. Then, we feed the perturbed examples to the BERT encoder to calculate the adversarial loss. In the end, the backpropagation algorithm is applied to the sum of both losses.", "where $r$ denotes the perturbations on the input and $\\hat{\\theta }$ is a constant copy of $\\theta $ in order not to allow the gradients to propagate in the process of constructing the artificial examples. Solving the above minimization problem means that we are searching for the worst perturbations while trying to minimize the loss of the model. An approximate solution for Equation DISPLAY_FORM3 is found by linearizing $\\log p(y|x;\\theta )$ around $x$ BIBREF0. Therefore, the following perturbations are added to the input embeddings to create new adversarial sentences in the embedding space.", "BIBREF25 design a network to transfer aspect knowledge learned from a coarse-grained network which performs aspect category sentiment classification to a fine-grained one performing aspect term sentiment classification. This is carried out using an attention mechanism (Coarse2Fine) which contains an autoencoder that emphasizes the aspect term by learning its representation from the category embedding. Similar to the Transformer, which does away with RNNs and CNNs and use only attention for translation, BIBREF26 design an attention model for ASC with the difference that they use lighter (weight-wise) multi-head attentions for context and target word modeling. Using bidirectional LSTMs BIBREF27, BIBREF28 propose a model that takes into account the history of aspects with an attention block called Truncated History Attention (THA). To capture the opinion summary, they also introduce Selective Transformation Network (STN) which highlights more important information with respect to a given aspect. BIBREF29 approach the aspect extraction in an unsupervised way.", "To perform the ablation study, first we initialize our model with post-trained BERT which has been trained on uncased version of $\\mathbf {BERT_{BASE}}$. We attempt to discover what number of training epochs and which dropout probability yield the best performance for BERT-PT. Since one and two training epochs result in very low scores, results of 3 to 10 training epochs have been depicted for all experiments. For AE, we experiment with 10 different dropout values in the fully connected (linear) layer. The results can be seen in Figure FIGREF6 for laptop and restaurant datasets. To be consistent with the previous work and because of the results having high variance, each point in the figure (F1 score) is the average of 9 runs. In the end, for each number of training epochs, a dropout value, which outperforms the other values, is found."]}
{"question_id": "9b536f4428206ef7afabc4ff0a2ebcbabd68b985", "predicted_answer": "", "predicted_evidence": ["where", "Since its introduction by BIBREF24, attention mechanism has become widely popular in many natural language processing tasks including sentiment analysis. BIBREF25 design a network to transfer aspect knowledge learned from a coarse-grained network which performs aspect category sentiment classification to a fine-grained one performing aspect term sentiment classification. This is carried out using an attention mechanism (Coarse2Fine) which contains an autoencoder that emphasizes the aspect term by learning its representation from the category embedding. Similar to the Transformer, which does away with RNNs and CNNs and use only attention for translation, BIBREF26 design an attention model for ASC with the difference that they use lighter (weight-wise) multi-head attentions for context and target word modeling. Using bidirectional LSTMs BIBREF27, BIBREF28 propose a model that takes into account the history of aspects with an attention block called Truncated History Attention (THA).", "In order to compare the effect of adversarial examples on the performance of the model, we choose the best dropout for each number of epochs and experiment with five different values for epsilon (perturbation size). The results for laptop and restaurant can be seen in Figure FIGREF7. As is noticeable, in terms of scores, they follow the same pattern as the original ones. Although most of the epsilon values improve the results, it can be seen in Figure FIGREF7 that not all of them will enhance the model's performance. In the case of $\\epsilon =5.0$ for AE, while it boosts the performance in the restaurant domain for most of the training epochs, it negatively affects the performance in the laptop domain. The reason for this could be the creation of adversarial examples which are not similar to the original ones but are labeled the same. In other words, the new examples greatly differ from the original ones but are fed to the net as being similar, leading to the network's poorer performance.", "While adversarial training has been utilized for sentence classification BIBREF16, its effects have not been studied in ABSA. Therefore, in this work, we study the impact of applying adversarial training to the powerful BERT language model.", "where the $[CLS]$ token is an indicator of the beginning of the sequence as well as its sentiment when performing sentiment classification. The $[SEP]$ token is a token to separate a sequence from the subsequent one. Finally, $w_{i}$ are the words of the sequence. After they go through the BERT model, for each item of the sequence, a vector representation of the size 768, size of BERT's hidden layers, is computed. Then, we apply a fully connected layer to classify each word vector as one of the three labels."]}
{"question_id": "9d04fc997689f44e5c9a551b8571a60b621d35c2", "predicted_answer": "", "predicted_evidence": ["Bidirectional Encoder Representations from Transformers (BERT) BIBREF13 is a deep and powerful language model which uses the encoder of the Transformer in a self-supervised manner to learn the language model. It has been shown to result in state-of-the-art performances on the GLUE benchmark BIBREF14 including text classification. BIBREF1 show that adding domain-specific information to this model can enhance its performance in ABSA. Using their post-trained BERT (BERT-PT), we add adversarial examples to further improve BERT's performance on Aspect Extraction (AE) and Aspect Sentiment Classification (ASC) which are two major tasks in ABSA. A brief overview of these two sub-tasks is given in Section SECREF3.", "BIBREF20 design a seven-layer CNN architecture and make use of both part of speech tagging and word embeddings as features. BIBREF21 use convolutional neural networks and domain-specific data for AE and ASC. They show that adding the word embeddings produced from the domain-specific data to the general purpose embeddings semantically enriches them regarding the task at hand. In a recent work BIBREF1, the authors also show that using in-domain data can enhance the performance of the state-of-the-art language model (BERT). Similarly, BIBREF22 also fine-tune BERT on domain-specific data for ASC. They perform a two-stage process, first of which is self-supervised in-domain fine-tuning, followed by supervised task-specific fine-tuning. Working on the same task, BIBREF23 apply graph convolutional networks taking into consideration the assumption that in sentences with multiple aspects, the sentiment about one aspect can help determine the sentiment of another aspect.", "Since its introduction by BIBREF24, attention mechanism has become widely popular in many natural language processing tasks including sentiment analysis. BIBREF25 design a network to transfer aspect knowledge learned from a coarse-grained network which performs aspect category sentiment classification to a fine-grained one performing aspect term sentiment classification. This is carried out using an attention mechanism (Coarse2Fine) which contains an autoencoder that emphasizes the aspect term by learning its representation from the category embedding. Similar to the Transformer, which does away with RNNs and CNNs and use only attention for translation, BIBREF26 design an attention model for ASC with the difference that they use lighter (weight-wise) multi-head attentions for context and target word modeling. Using bidirectional LSTMs BIBREF27, BIBREF28 propose a model that takes into account the history of aspects with an attention block called Truncated History Attention (THA).", "Traditional machine learning methods such as SVM BIBREF2, Naive Bayes BIBREF3, Decision Trees BIBREF4, Maximum Entropy BIBREF5 have long been practiced to acquire such knowledge. However, in recent years due to the abundance of available data and computational power, deep learning methods such as CNNs BIBREF6, BIBREF7, BIBREF8, RNNs BIBREF9, BIBREF10, BIBREF11, and the Transformer BIBREF12 have outperformed the traditional machine learning techniques in various tasks of sentiment analysis. Bidirectional Encoder Representations from Transformers (BERT) BIBREF13 is a deep and powerful language model which uses the encoder of the Transformer in a self-supervised manner to learn the language model. It has been shown to result in state-of-the-art performances on the GLUE benchmark BIBREF14 including text classification. BIBREF1 show that adding domain-specific information to this model can enhance its performance in ABSA.", "We attempt to discover what number of training epochs and which dropout probability yield the best performance for BERT-PT. Since one and two training epochs result in very low scores, results of 3 to 10 training epochs have been depicted for all experiments. For AE, we experiment with 10 different dropout values in the fully connected (linear) layer. The results can be seen in Figure FIGREF6 for laptop and restaurant datasets. To be consistent with the previous work and because of the results having high variance, each point in the figure (F1 score) is the average of 9 runs. In the end, for each number of training epochs, a dropout value, which outperforms the other values, is found. In our experiments, we noticed that the validation loss increases after 2 epochs as has been mentioned in the original paper. However, the test results do not follow the same pattern. Looking at the figures, it can be seen that as the number of training epochs increases, better results are produced in the restaurant domain while in the laptop domain the scores go down."]}
{"question_id": "8a0e1a298716698a305153c524bf03d18969b1c6", "predicted_answer": "", "predicted_evidence": ["Performing these tasks requires a deep understanding of the language. Traditional machine learning methods such as SVM BIBREF2, Naive Bayes BIBREF3, Decision Trees BIBREF4, Maximum Entropy BIBREF5 have long been practiced to acquire such knowledge. However, in recent years due to the abundance of available data and computational power, deep learning methods such as CNNs BIBREF6, BIBREF7, BIBREF8, RNNs BIBREF9, BIBREF10, BIBREF11, and the Transformer BIBREF12 have outperformed the traditional machine learning techniques in various tasks of sentiment analysis. Bidirectional Encoder Representations from Transformers (BERT) BIBREF13 is a deep and powerful language model which uses the encoder of the Transformer in a self-supervised manner to learn the language model. It has been shown to result in state-of-the-art performances on the GLUE benchmark BIBREF14 including text classification.", "Our contributions are twofold. First, by carrying out an ablation study on the number of training epochs and the values for dropout in the classification layer, we show that there are values that outperform the specified ones for BERT-PT. Second, we introduce the application of adversarial training in ABSA by proposing a novel architecture which combines adversarial training with the BERT language model for AE and ASC tasks. Our experiments show that the proposed model outperforms the best performance of BERT-PT in both tasks.", "where $r$ denotes the perturbations on the input and $\\hat{\\theta }$ is a constant copy of $\\theta $ in order not to allow the gradients to propagate in the process of constructing the artificial examples. Solving the above minimization problem means that we are searching for the worst perturbations while trying to minimize the loss of the model. An approximate solution for Equation DISPLAY_FORM3 is found by linearizing $\\log p(y|x;\\theta )$ around $x$ BIBREF0. Therefore, the following perturbations are added to the input embeddings to create new adversarial sentences in the embedding space.", "where the $[CLS]$ token is an indicator of the beginning of the sequence as well as its sentiment when performing sentiment classification. The $[SEP]$ token is a token to separate a sequence from the subsequent one. Finally, $w_{i}$ are the words of the sequence. After they go through the BERT model, for each item of the sequence, a vector representation of the size 768, size of BERT's hidden layers, is computed. Then, we apply a fully connected layer to classify each word vector as one of the three labels.", "In this section, we give a brief description of two major tasks in ABSA which are called Aspect Extraction (AE) and Aspect Sentiment Classification (ASC). These tasks were sub-tasks of task 4 in SemEval 2014 contest BIBREF30, and since then they have been the focus of attention in many studies."]}
{"question_id": "538430077b1820011c609c8ae147389b960932c8", "predicted_answer": "", "predicted_evidence": ["We would like to thank Adidas AG for funding this work.", "In this paper, we introduced the application of adversarial training in Aspect-Based Sentiment Analysis. The experiments with our proposed architecture show that the performance of the post-trained BERT on aspect extraction and aspect sentiment classification tasks are improved by utilizing adversarial examples during the network training. As future work, other white-box adversarial examples as well as black-box ones will be utilized for a comparison of adversarial training methods for various sentiment analysis tasks. Furthermore, the impact of adversarial training in the other tasks in ABSA namely Aspect Category Detection and Aspect Category Polarity will be investigated.", "Observing, from AE task, that higher dropouts perform poorly, we experiment with the 5 lower values for ASC task in BERT-PT experiments. In addition, for BAT experiments, two different values ($0.01, 0.1$) for epsilon are tested to make them more diverse. The results are depicted in Figures FIGREF9 and FIGREF10 for BERT-PT and BAT, respectively. While in AE, towards higher number of training epochs, there is an upward trend for restaurant and a downward trend for laptop, in ASC a clear pattern is not observed. Regarding the dropout, lower values ($0.1$ for laptop, $0.2$ for restaurant) yield the best results for BERT-PT in AE task, but in ASC a dropout probability of 0.4 results in top performance in both domains.", "Similar improvements can be seen in ASC results with an increase of +2.16 in MF1 score for restaurant compared to +0.81 for laptop which is due to the increase in the number of training epochs for restaurant domain since it exhibits better results with more training while the model reaches its peak performance for laptop domain in earlier training epochs. In addition, applying adversarial training improves the network's performance in both tasks, though at different rates. While for laptop there are similar improvements in both tasks (+0.69 in AE, +0.61 in ASC), for restaurant we observe different enhancements (+0.81 in AE, +0.12 in ASC). This could be attributed to the fact that these are two different datasets whereas the laptop dataset is the same for both tasks. Furthermore, the perturbation size plays an important role in performance of the system. By choosing the appropriate ones, as was shown, better results are achieved.", "and $\\epsilon $ is the size of the perturbations. In order to find values which outperform the original results, we carried out an ablation study on five values for epsilon whose results are presented in Figure FIGREF7 and discussed in Section SECREF6. After the adversarial examples go through the network, their loss is calculated as follows:"]}
{"question_id": "97055ab0227ed6ac7a8eba558b94f01867bb9562", "predicted_answer": "", "predicted_evidence": ["We then introduce in Table 3 another measure, the entropy of a sentence, defined as $-\\frac{1}{|R|}\\sum _{w\\in R}\\log p(w)$", "From the BLEU scores in Table 2 , we see biseq2seq significantly outperforms conventional seq2seq, showing that, if enriched with a retrieved human utterance as a candidate, the encoder-decoder framework can generate much more human-like utterances.", "Combining the retrieval system and the RNN generator by bi-sequence input and post-reranking, we achieve the highest performance in terms of both human evaluation and BLEU scores. Concretely, our model ensemble outperforms the state-of-the-practice retrieval system by $ +13.6\\%$ averaged human scores, which we believe is a large margin.", "In the retrieval system, we use the classifier's confidence as the relevance score. The training set consists of 10k samples, which are either in the original human-human utterance pairs or generated by negative sampling. We made efforts to collect binary labels from a crowd-sourcing platform, indicating whether a query is relevant to another query and whether it is relevant to a particular reply. We find using crowd-sourced labels results in better performance than original negative sampling.", "Figure 1 depicts the overall framework of our proposed ensemble of retrieval and generative dialog systems. It mainly consists of the following components."]}
{"question_id": "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8", "predicted_answer": "", "predicted_evidence": ["RQ1: What is the performance of biseq2seq (the 1 step in Figure 1 ) in comparison with traditional seq2seq?", "We compare our model ensemble with each individual component and provide a thorough ablation test. Listed below are the competing methods in our experiments.", "To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo, Baidu Zhidao, and Baidu Tieba. We filtered out short and meaningless replies like \u201c...\u201d and \u201cErrr.\u201d In total, the database contains 7 million query-reply pairs for retrieval.", "We present our main results in Table 2 . As shown, the retrieval system, which our model ensemble is based on, achieves better performance than RNN-based sequence generation. The result is not consistent with nbciteacl, where their RNNs are slightly better than retrieval-based methods. After closely examining their paper, we find that their database is multiple times smaller than ours, which may, along with different features and retrieval methods, explain the phenomenon. This also verifies that the retrieval-based dialog system in our experiment is a strong baseline to compare with.", "where $\\mathbf {t}^{(i)}$ is the one-hot vector of the next target word, serving as the groundtruth, $\\mathbf {y}$ is the output probability by softmax, and $V$ is the vocabulary size. We adopt mini-batched AdaDelta nbciteadadelta for optimization."]}
{"question_id": "bfcbb47f3c54ee1a459183e04e4c5a41ac9ae83b", "predicted_answer": "", "predicted_evidence": ["RQ2: How do the retrieval- and generation-based systems contribute to post-reranking (the 2 step in Figure 1 )?", "Notice that, automatic metrics were computed on the entire test set, whereas subjective evaluation was based on 79 randomly chosen test samples due to the limitation of human resources available.", "Finally, we use a reranker (which is a part of the retrieval system) to select either $r^*$ or $r^\\text{+}$ as the ultimate response to the original query $q$ .", "From the above process, we see that the retrieval and generative systems are integrated by two mechanisms: (1) The retrieved candidate is fed to the sequence generator to mitigate the \u201clow-substance\u201d problem; (2) The post-reranker can make better use of both the retrieved candidate and the generated utterance. In this sense, we call our overall approach an ensemble in this paper. To the best of our knowledge, we are the first to combine retrieval and generative models for open-domain conversation.", "where $R$ refers to all replies. Entropy is used in nbcitevariationalDialog and nbciteseq2BF to measure the serendipity of generated utterances. The results in Table 3 confirm that biseq2seq indeed integrates information from the retrieved candidate, so that it alleviates the \u201clow-substance\u201d problem of RNNs and can generate utterances more meaningful than traditional seq2seq. And the statistic result also displays that biseq2seq generates longer sentences than seq2seq approach."]}
{"question_id": "fe1a74449847755cd7a46647cc9d384abfee789e", "predicted_answer": "", "predicted_evidence": ["Given how tied humor is to the human condition, the phenomenon has challenged some of the greatest thinkers throughout history and has been the subject of much academic research across over 20 disciplines BIBREF2 , including computer science BIBREF3 , where researchers have developed algorithms for detecting, analyzing, and generating humorous utterances (cf. Sec. \"Related work\" ).", "Micro-punchlines. We now ask where in terms of location within a headline the humor tends to reside. To answer this question, we compute the position of the modified chunk in each headline's chunk sequence and plot the distribution of modified positions in Fig. 3 . We see that, regardless of headline length, modifications to the last chunk are particularly overrepresented. This is an important finding: we have previously (Sec. \"Introduction\" ) argued that satirical headlines consist of a punchline only, with minimal narrative structure, and indeed it was this very intuition that led us to investigate headlines in isolation. Given Fig. 3 , we need to revise this statement slightly: although satirical headlines consist of a single sentence, they are often structured\u2014at a microlevel\u2014akin to more narrative jokes, where the humorous effect also comes with the very last words. Put differently, the final words of satirical headlines often serve as a \u201cmicropunchline\u201d.", "The most widely accepted theory of verbal humor is the so-called General Theory of Verbal Humor by Attardo and Raskin attardo1991script, an extension of Raskin's raskin1985semantic Semantic-Script Theory of Humor, which we summarize when discussing our findings in its context in Sec. \"Discussion and future work\" .", "Fig. \"Related work\" , which plots the mean average seriousness rating $r(h^{\\prime })$ of modified headlines $h^{\\prime }$ as a function of the edit distance $d(h,h^{\\prime })$ , shows how this tradeoff plays out in practice. For edit distances between 1 and 5 (83% of all pairs, cf. Fig. \"Semantic analysis of aligned corpus\" ), seriousness ratings correlate positively with edit distance. In particular, it seems harder to remove the humor by changing one word than by changing two words, whereas the marginal effect is negligible when allowing for even larger edits. The positive correlation does not hold for the much smaller number (17%) of pairs with an edit distance above 5. Inspecting the data, we find that this is caused by headlines so inherently absurd that even large edits cannot manage to remove the humor from them.", "Chunking all 9,159 original headlines from our The Onion corpus, we find the most frequent chunk pattern to be NP VP NP PP NP (4.8%; e.g., H2 in Table 3 ), followed by NP VP NP (4.3%; e.g., H4) and NP VP PP NP (3.3%; e.g., H9)."]}
{"question_id": "425d17465ff91019eb87c28ff3942f781ba1bbcb", "predicted_answer": "", "predicted_evidence": ["One may, however, not just replace any noun phrase with any other noun phrase; rather, the corresponding scripts need to be opposed along one of a few dimensions essential to the human condition and typically pitting \u201cgood\u201d vs. \u201cbad\u201d. Also, the two opposing scripts need to be connected via certain subtle mechanisms, and we pointed out false analogy as one prominent mechanism. These findings echo the predictions made by the prevailing theory of humor. We now summarize this theory and discuss our results in its context.", "Chunk-based edit distance. Recomputing edit distances at the chunk level, rather than the token level, we obtain the chunkbased edit distance distribution of Fig. \"Conclusion\" . It resembles the tokenbased edit distance distribution of Fig. \"Semantic analysis of aligned corpus\" , with the difference that the smallest possible distance of 1 is even more prevalent (52% vs. 33% of pairs), due to the fact that modifying a single chunk frequently corresponds to modifying multiple tokens. Since, moreover, the vast majority (97%) of all singlechunk edits are substitutions, we now focus on 254 $(h,h^{\\prime })$ pairs where exactly one chunk of $h$ has been modified (henceforth singlesubstitution pairs). This accounts for about half of all successful pairs (after discarding pairs that were problematic for the chunker).", "Reward for task 1. As player $A$ is supposed to remove the humor from $h$ via a minimal modification, his reward $R_A(h,h^{\\prime })$ increases (1) with the average rating $r(h^{\\prime })$ that the modified headline $h^{\\prime }$ receives from all $n$ players $B_1, \\dots , B_n$ who rate it and (2) with the similarity $s(h,h^{\\prime })$ of $h$ and $h^{\\prime }$ :", "As the examples of Table 3 show, the analogy is never marked lexically via words such as like; rather, it is evoked implicitly, e.g., by blending the two realms of human psychiatry and biblical lore into a single headline. Only the satirical headline $H(x)$ itself (red box in Table 3 ) is explicit to the reader, whereas $x^{\\prime }$ and $P$ (and thus all the other 3 boxes) need to be inferred. A main advantage of our method is that it also makes $x^{\\prime }$ explicit and thereby facilitates inferring $P$ and thus the semantic structure that induces humor (as in Table 3 ).", "Fig. \"Related work\" , which plots the mean average seriousness rating $r(h^{\\prime })$ of modified headlines $h^{\\prime }$ as a function of the edit distance $d(h,h^{\\prime })$ , shows how this tradeoff plays out in practice. For edit distances between 1 and 5 (83% of all pairs, cf. Fig. \"Semantic analysis of aligned corpus\" ), seriousness ratings correlate positively with edit distance. In particular, it seems harder to remove the humor by changing one word than by changing two words, whereas the marginal effect is negligible when allowing for even larger edits. The positive correlation does not hold for the much smaller number (17%) of pairs with an edit distance above 5. Inspecting the data, we find that this is caused by headlines so inherently absurd that even large edits cannot manage to remove the humor from them."]}
{"question_id": "08561f6ba578ce8f8d284abf90f5b24eb1f804d3", "predicted_answer": "", "predicted_evidence": ["From tokens to chunks. We analyze syntax at an intermediate level of abstraction between simple sequences of part-of-speech (POS) tags and complex parse trees, by relying on a chunker (also called shallow parser). We use OpenNLP's maximum entropy chunker BIBREF10 , after retraining it to better handle pithy, headlinestyle text. The chunker takes POStagged text as input and groups subsequent tokens into meaningful phrases (chunks) without inferring the recursive structure of parse trees; e.g., our running example (Sec. \"Introduction\" ) is chunked as [NP Bob Dylan] [VP diagnosed] [PP with] [NP bipolar disorder] (chunk labels expanded in Table 2 ).", "Attardo [p. 27]attardo2001humorous gives a comprehensive list of 27 logical mechanisms. While our analysis (Sec. \"Semantic analysis of aligned corpus\" ) revealed that one mechanism\u2014false analogy\u2014dominates in satirical headlines, several others also occur: e.g., in figure\u2013ground reversal, the real problem (the \u201cfigure\u201d) is left implicit, while an unimportant side effect (the \u201cground\u201d) moves into the focus of attention (e.g., H12 in Table 3 : waterboarding, like baths, does waste water, but the real problem is ethical, not ecological). Another common mechanism\u2014cratylism\u2014plays with the assumption prevalent in puns that phonetic implies semantic similarity (e.g., H11 in Table 3 ).", "As the examples of Table 3 show, the analogy is never marked lexically via words such as like; rather, it is evoked implicitly, e.g., by blending the two realms of human psychiatry and biblical lore into a single headline. Only the satirical headline $H(x)$ itself (red box in Table 3 ) is explicit to the reader, whereas $x^{\\prime }$ and $P$ (and thus all the other 3 boxes) need to be inferred. A main advantage of our method is that it also makes $x^{\\prime }$ explicit and thereby facilitates inferring $P$ and thus the semantic structure that induces humor (as in Table 3 ).", "In typical jokes, one of the two scripts (the so-called bona fide interpretation) seems more likely given the text, so it is in the foreground of attention. But in the punchline it becomes clear that the bona fide interpretation cannot be true, causing initial confusion in the audience, followed by a search for a more appropriate interpretation, and finally surprise or relief when the actually intended, non\u2013bona fide script is discovered. To enable this process on the recipient side, the theory posits that the two scripts be connected in specific ways, via the so-called logical mechanism, which resolves the tension between the two opposed scripts.", "Beyond humor. The mechanism underlying Unfun.me defines a general procedure for identifying the essential portion of a text that causes the text to have a certain property. In our case, this property is humor, but when asking players instead to remove the rudeness, sexism, euphemism, hyperbole, etc., from a given piece of text, we obtain a scalable way of collecting finegrained supervised examples for better understanding these ways of speaking linguistically."]}
{"question_id": "ec2045e0da92989642a5b5f2b1130c8bd765bcc5", "predicted_answer": "", "predicted_evidence": ["One may, however, not just replace any noun phrase with any other noun phrase; rather, the corresponding scripts need to be opposed along one of a few dimensions essential to the human condition and typically pitting \u201cgood\u201d vs. \u201cbad\u201d. Also, the two opposing scripts need to be connected via certain subtle mechanisms, and we pointed out false analogy as one prominent mechanism. These findings echo the predictions made by the prevailing theory of humor. We now summarize this theory and discuss our results in its context.", "Satirical and serious headlines. The game requires corpora of satirical as well as serious news headlines as input. Our satirical corpus consists of 9,159 headlines published by the wellknown satirical newspaper The Onion; our serious corpus, of 9,000 headlines drawn from 9 major news websites.", "Reward for task 2. Since player $B$ 's very purpose is to determine whether $h^{\\prime }$ is without humor, we do not have a groundtruth rating for $h^{\\prime }$ . In order to still be able to reward player $B$ for participating in task 2, and to incentivize her to indicate her true opinion about $h^{\\prime }$ , we also ask her for her belief $p_B(g)$ regarding a headline $g$ for which we do have the ground truth of \u201cserious\u201d vs. \u201csatirical\u201d. The reward $R_B(g)$ that player $B$ receives for rating headline $g$ is then", "First, we present Unfun.me, an online game for collecting a corpus of pairs of satirical news headlines aligned to similarbutseriouslooking headlines (Sec. \"Game description: Unfun.me\" ). Second, our analysis of these pairs (Sec. \"Analysis of game dynamics\" \u2013 \"Semantic analysis of aligned corpus\" ) reveals key properties of satirical headlines at a much finer level of granularity than prior work (Sec. \"Related work\" ). Syntactically (Sec. \"Syntactic analysis of aligned corpus\" ), we conclude that the humor tends to reside in noun phrases, and with increased likelihood toward the end of headlines, giving rise to what we term \u201cmicropunchlines\u201d. Semantically (Sec. \"Semantic analysis of aligned corpus\" ), we observe that original and modified headlines are usually opposed to each other along certain dimensions crucial to the human condition (e.g., high vs. low stature, life vs.", "Overall game flow. Whenever a user wants to play, we generate a type-1 task with probability $\\alpha =1/3$ and a type-2 task with probability $1-\\alpha =2/3$ , such that we can collect two ratings per modified headline. As mentioned, ratings from task 2 can serve as a filter, and we can increase its precision at will by decreasing $\\alpha $ . To make rewards more intuitive and give more weight to the core task 1, we translate and scale rewards such that $R_A(\\cdot ,\\cdot ) \\in [0, 1000]$ and $R_B(\\cdot ) \\in [0, 200]$ . We also implemented additional incentive mechanisms such as badges, high-score tables, and immediate rewards for participating, but we omit the details for space reasons."]}
{"question_id": "25f699c7a33e77bd552782fb3886b9df9d02abb2", "predicted_answer": "", "predicted_evidence": ["The main challenge in non-diacritized text is that it is very ambiguous BIBREF3, BIBREF4, BIBREF1, BIBREF5. ADR attempts to decode the ambiguity present in undiacritized text. Adegbola et al. assert that for ADR the \u201cprevailing error factor is the number of valid alternative arrangements of the diacritical marks that can be applied to the vowels and syllabic nasals within the words\" BIBREF1.", "Yor\u00f9b\u00e1 is a tonal language spoken by more than 40 Million people in the countries of Nigeria, Benin and Togo in West Africa. The phonology is comprised of eighteen consonants, seven oral vowel and five nasal vowel phonemes with three kinds of tones realized on all vowels and syllabic nasal consonants BIBREF0. Yor\u00f9b\u00e1 orthography makes notable use of tonal diacritics, known as am\u00ed oh\u00f9n, to designate tonal patterns, and orthographic diacritics like underdots for various language sounds BIBREF1, BIBREF2.", "Data preprocessing, parallel text preparation and training hyper-parameters are the same as in BIBREF3. Experiments included evaluations of the effect of the various texts, notably for JW300, which is a disproportionately large contributor to the dataset. We also evaluated models trained with pre-trained FastText embeddings to understand the boost in performance possible with word embeddings BIBREF6, BIBREF7. Our training hardware configuration was an AWS EC2 p3.2xlarge instance with OpenNMT-py BIBREF8.", "To make the first open-sourced ADR models available to a wider audience, we tested extensively on colloquial and conversational text. These soft-attention seq2seq models BIBREF3, trained on the first three sources in Table TABREF5, suffered from domain-mismatch generalization errors and appeared particularly weak when presented with contractions, loan words or variants of common phrases. Because they were trained on majority Biblical text, we attributed these errors to low-diversity of sources and an insufficient number of training examples. To remedy this problem, we aggregated text from a variety of online public-domain sources as well as actual books. After scanning physical books from personal libraries, we successfully employed commercial Optical Character Recognition (OCR) software to concurrently use English, Romanian and Vietnamese characters, forming an approximative superset of the Yor\u00f9b\u00e1 character set. Text with inconsistent quality was put into a special queue for subsequent human supervision and manual correction. The post-OCR correction of H\u00e1\u00e0 \u00c8n\u00ecy\u00e0n, a work of fiction of some 20,038 words, took a single expert two weeks of part-time work by to review and correct.", "Diacritics provide morphological information, are crucial for lexical disambiguation and pronunciation, and are vital for any computational Speech or Natural Language Processing (NLP) task. To build a robust ecosystem of Yor\u00f9b\u00e1-first language technologies, Yor\u00f9b\u00e1 text must be correctly represented in computing environments. The ultimate objective of automatic diacritic restoration (ADR) systems is to facilitate text entry and text correction that encourages the correct orthography and promotes quotidian usage of the language in electronic media."]}
{"question_id": "3e4e415e346a313f5a7c3764fe0f51c11f51b071", "predicted_answer": "", "predicted_evidence": ["Even for situations described by Theorem SECREF2 where uniqueness is not strictly guaranteed, the probability for collision is extremely low in practice. Therefore, FOFE can be safely considered as an encoding mechanism that converts variable-length sequence into a fixed-size representation theoretically without any loss of information.", "The linguistic distribution hypothesis states that words that occur in close contexts should have similar meaning BIBREF8 . It implies that the particular sense of a polyseme is highly related to its surrounding context. Moreover, human decides the sense of a polyseme by firstly understanding its occurring context. Likewise, our proposed model has two stages, as shown in Figure FIGREF3 : training a FOFE-based pseudo language model that abstracts context as embeddings, and performing WSD classification over context embeddings.", "FFNN is constructed in fully-connected layers. Each layer receives values from previous layer as input, and produces values through a function over weighted input values as its output. FFNN increasingly abstracts the features of the data through the layers. As the pseudo language model is trained to predict the target word, the output layer is irrelevant to WSD task and hence can be discarded. However, the remaining layers still have learned the ability to generalize features from word to context during the training process. The values of the held-out layer (the second last layer) are extracted as context embedding, which provides a nice numerical abstraction of the surrounding context of a target word.", "where INLINEFORM0 is a constant between 0 and 1, called forgetting factor. For example, assuming A, B, C are three words with one-hot vectors INLINEFORM1 , INLINEFORM2 , INLINEFORM3 respectively. The FOFE encoding from left to right for ABC is [ INLINEFORM4 , INLINEFORM5 ,1] and for ABCBC is [ INLINEFORM6 , INLINEFORM7 , INLINEFORM8 ]. It becomes evident that the FOFE code is in fixed size, which is equal to the size of the one-hot vector, regardless of the length of the sequence INLINEFORM9 .", "As firstly proposed in BIBREF5 , FOFE provides a way to encode the entire sequence of words of variable length into an almost unique fixed-size representation, while also retain the positional information for words in the sequence. FOFE has been applied to several NLP problems in the past, such as language model BIBREF5 , named entity recognition BIBREF6 , and word embedding BIBREF7 . The promising results demonstrated by the FOFE approach in these areas inspired us to apply FOFE in solving the WSD problem. In this paper, we will first describe how FOFE is used to encode sequence of any length into a fixed-size representation. Next, we elaborate on how a pseudo language model is trained with the FOFE encoding from unlabelled data for the purpose of context abstraction, and how a classifier for each polyseme is built from context abstractions of its labelled training data. Lastly, we provide the experiment results of our method on several WSD data sets to justify the equivalent performance as the state-of-the-art approach."]}
{"question_id": "d622564b250cffbb9ebbe6636326b15ec3c622d9", "predicted_answer": "", "predicted_evidence": ["Even for situations described by Theorem SECREF2 where uniqueness is not strictly guaranteed, the probability for collision is extremely low in practice. Therefore, FOFE can be safely considered as an encoding mechanism that converts variable-length sequence into a fixed-size representation theoretically without any loss of information.", "The development of the so called \u201cfixed-size ordinally forgetting encoding\u201d (FOFE) has enabled us to consider more efficient method. As firstly proposed in BIBREF5 , FOFE provides a way to encode the entire sequence of words of variable length into an almost unique fixed-size representation, while also retain the positional information for words in the sequence. FOFE has been applied to several NLP problems in the past, such as language model BIBREF5 , named entity recognition BIBREF6 , and word embedding BIBREF7 . The promising results demonstrated by the FOFE approach in these areas inspired us to apply FOFE in solving the WSD problem. In this paper, we will first describe how FOFE is used to encode sequence of any length into a fixed-size representation. Next, we elaborate on how a pseudo language model is trained with the FOFE encoding from unlabelled data for the purpose of context abstraction, and how a classifier for each polyseme is built from context abstractions of its labelled training data.", "The FOFE encoding has the property that the original sequence can be unequivocally recovered from the FOFE encoding. According to BIBREF5 , the uniqueness for the FOFE encoding of a sequence is confirmed by the following two theorems:", "Table TABREF6 presents the micro F1 scores from different models. Note that we use a corpus with 0.8 billion words and vocabulary of 100,000 words when training the language model, comparing with BIBREF4 using 100 billion words and vocabulary of 1,000,000 words. The context abstraction using the language model is the most crucial step. The sizes of the training corpus and vocabulary significantly affect the performance of this process, and consequently the final WSD results. However, BIBREF4 did not publish the 100 billion words corpus used for training their LSTM language model.", "Words with the same sense mostly appear in similar contexts, hence the context embeddings of their contexts are supposed to be close in the embedding space. As the FOFE-based pseudo language model is capable of abstracting surrounding context for any target word as context embeddings, applying the language model on instances in annotated corpus produces context embeddings for senses."]}
{"question_id": "4367617c0b8c9f33051016e8d4fbb44831c54d0f", "predicted_answer": "", "predicted_evidence": ["The linguistic distribution hypothesis states that words that occur in close contexts should have similar meaning BIBREF8 . It implies that the particular sense of a polyseme is highly related to its surrounding context. Moreover, human decides the sense of a polyseme by firstly understanding its occurring context. Likewise, our proposed model has two stages, as shown in Figure FIGREF3 : training a FOFE-based pseudo language model that abstracts context as embeddings, and performing WSD classification over context embeddings.", "A language model is trained with large unlabelled corpus by BIBREF4 in order to overcome the shortage of WSD training data. A language model represents the probability distribution of a given sequence of words, and it is commonly used in predicting the subsequent word given preceding sequence. BIBREF5 proposed a FOFE-based neural network language model by feeding FOFE code of preceding sequence into FFNN. WSD is different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Hence, we build a pseudo language model that uses both preceding and succeeding sequence to accommodate the purpose of WSD tasks.", "The fact that human languages consist of variable-length sequence of words requires NLP models to be able to consume variable-length data. RNN/LSTM addresses this issue by recurrent connections, but such recurrence consequently increases the computational complexity. On the contrary, feed forward neural network (FFNN) has been widely adopted in many artificial intelligence problems due to its powerful modelling ability and fast computation, but is also limited by its requirement of fixed-size input. FOFE aims at encoding variable-length sequence of words into a fixed-size representation, which subsequently can be fed into an FFNN.", "Theorem 1 If the forgetting factor INLINEFORM0 satisfies INLINEFORM1 , FOFE is unique for any sequence of finite length INLINEFORM2 and any countable vocabulary INLINEFORM3 .", "where INLINEFORM0 is a constant between 0 and 1, called forgetting factor. For example, assuming A, B, C are three words with one-hot vectors INLINEFORM1 , INLINEFORM2 , INLINEFORM3 respectively. The FOFE encoding from left to right for ABC is [ INLINEFORM4 , INLINEFORM5 ,1] and for ABCBC is [ INLINEFORM6 , INLINEFORM7 , INLINEFORM8 ]. It becomes evident that the FOFE code is in fixed size, which is equal to the size of the one-hot vector, regardless of the length of the sequence INLINEFORM9 ."]}
{"question_id": "2c60628d54f2492e0cbf0fb8bacd8e54117f0c18", "predicted_answer": "", "predicted_evidence": ["A language model is trained with large unlabelled corpus by BIBREF4 in order to overcome the shortage of WSD training data. A language model represents the probability distribution of a given sequence of words, and it is commonly used in predicting the subsequent word given preceding sequence. BIBREF5 proposed a FOFE-based neural network language model by feeding FOFE code of preceding sequence into FFNN. WSD is different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Hence, we build a pseudo language model that uses both preceding and succeeding sequence to accommodate the purpose of WSD tasks.", "Given vocabulary INLINEFORM0 of size INLINEFORM1 , each word can be represented by a one-hot vector. FOFE can encode a sequence of words of any length using linear combination, with a forget factor to reflect the positional information. For a sequence of words INLINEFORM2 from V, let INLINEFORM3 denote the one-hot representation for the INLINEFORM4 word, then the FOFE code of S can be recursively obtained using following equation (set INLINEFORM5 ): INLINEFORM6", "The fact that human languages consist of variable-length sequence of words requires NLP models to be able to consume variable-length data. RNN/LSTM addresses this issue by recurrent connections, but such recurrence consequently increases the computational complexity. On the contrary, feed forward neural network (FFNN) has been widely adopted in many artificial intelligence problems due to its powerful modelling ability and fast computation, but is also limited by its requirement of fixed-size input. FOFE aims at encoding variable-length sequence of words into a fixed-size representation, which subsequently can be fed into an FFNN.", "A classifier can be built for each polyseme over the context embeddings of all its occurring contexts in the training corpus. When predict the sense of a polyseme, we similarly extract the context embedding from the context surrounding the predicting polyseme, and send it to the polyseme's classifier to decide the sense. If a classifier cannot be built for the predicting polyseme due to the lack of training instance, the first sense from the dictionary is used instead.", "For example, the word bank can either mean a \u201cfinancial establishment\u201d or \u201cthe land alongside or sloping down to a river or lake\u201d, based on different contexts. Such a word is called a \u201cpolyseme\u201d. The task to identify the meaning of a polyseme in its surrounding context is called word sense disambiguation (WSD). Word sense disambiguation is a long-standing problem in natural language processing (NLP), and has broad applications in other NLP problems such as machine translation BIBREF0 . Lexical sample task and all-word task are the two main branches of WSD problem. The former focuses on only a pre-selected set of polysemes whereas the later intends to disambiguate every polyseme in the entire text. Numerous works have been devoted in WSD task, including supervised, unsupervised, semi-supervised and knowledge based learning BIBREF1 . Our work focuses on using supervised learning to solve all-word WSD problem."]}
{"question_id": "77a331d4d909d92fab9552b429adde5379b2ae69", "predicted_answer": "", "predicted_evidence": ["In this experiment, we compared the performance from our baseline models GRURNN and LSTMRNN with our proposed GRURNTN and LSTMRNTN models. We used the same dimensions for the embedding matrix to represent the words and characters as the vectors of real numbers.", "E_i(\\theta )}{\\partial W_{tsr}^{[1:d]}} &=& \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial \\tanh (a_j)} \\frac{\\partial \\tanh (a_j)}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} i_j (1-\\tanh ^2(a_j)) \\begin{bmatrix}", "We used a PennTreeBank (PTB) corpus, which is a standard benchmark corpus for statistical language modeling. A PTB corpus is a subset of the WSJ corpus. In this experiment, we followed the standard preprocessing step that was done by previous research BIBREF23 . The PTB dataset is divided as follows: a training set from sections 0-20 with total 930.000 words, a validation set from sections 21-22 with total 74.000 words, and a test set from sections 23-24 with total 82.000 words. The vocabulary is limited to the 10.000 most common words, and all words outside are mapped into a \" $<$ unk $>$ \" token. We used the preprocessed PTB corpus from the RNNLM-toolkit website.", "\\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}} \\frac{\\partial \\tilde{h_j}}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}}\\frac{\\partial \\tilde{h_j}}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}} f^{\\prime }(a_j) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal } \\begin{bmatrix} (r_j \\odot h_{j-1}) \\end{bmatrix}", "&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}} \\frac{\\partial \\tilde{h_j}}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}}\\frac{\\partial \\tilde{h_j}}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}} f^{\\prime }(a_j) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal } \\begin{bmatrix} (r_j \\odot"]}
{"question_id": "516b691ef192f136bb037c12c3c9365ef5a6604c", "predicted_answer": "", "predicted_evidence": ["Representing hidden states with deeper operations was introduced just a few years ago BIBREF11 . In these works, Pascanu et al. BIBREF11 use additional nonlinear layers for representing the transition from input to hidden layers, hidden to hidden layers, and hidden to output layers. They also improved the RNN architecture by a adding shortcut connection in the deep transition by skipping the intermediate layers. Another work from BIBREF33 proposed a new RNN design for a stacked RNN model called Gated Feedback RNN (GFRNN), which adds more connections from all the previous time-step stacked hidden layers into the current hidden layer computations. Despite adding additional transition layers and connection weight from previous hidden layers, all of these models still represent the input and hidden layer relationships by using linear projection, addition and nonlinearity transformation.", "c_j} \\frac{\\partial c_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial \\tanh (a_j)} \\frac{\\partial \\tanh (a_j)}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} i_j (1-\\tanh ^2(a_j)) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal } \\begin{bmatrix} h_{j-1} \\end{bmatrix} $$   (Eq. 30)", "W_{tsr}^{[1:d]}} &=& \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial \\tanh (a_j)} \\frac{\\partial \\tanh (a_j)}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} i_j (1-\\tanh ^2(a_j)) \\begin{bmatrix} x_j", "A Gated Recurrent Unit (GRU) BIBREF4 is a gated RNN with similar properties to a LSTM. However, there are several differences: a GRU does not have separated memory cells BIBREF18 , and instead of three gating layers, it only has two gating layers: reset gates and update gates. The GRU hidden layer at time $t$ is defined by the following equations BIBREF4 :", "A Long Short Term Memory (LSTM) BIBREF9 is a gated RNN with three gating layers and memory cells. The gating layers are used by the LSTM to control the existing memory by retaining the useful information and forgetting the unrelated information. Memory cells are used for storing the information across time. The LSTM hidden layer at time $t$ is defined by the following equations BIBREF17 :"]}
{"question_id": "c53b036eff430a9d0449fb50b8d2dc9d2679d9fe", "predicted_answer": "", "predicted_evidence": [")}}{\\partial c_j} \\frac{\\partial c_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial \\tanh (a_j)} \\frac{\\partial \\tanh (a_j)}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} i_j (1-\\tanh ^2(a_j)) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal } \\begin{bmatrix} h_{j-1} \\end{bmatrix} $$   (Eq.", "\\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial \\tanh (a_j)} \\frac{\\partial \\tanh (a_j)}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} i_j (1-\\tanh ^2(a_j)) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal } \\begin{bmatrix} h_{j-1} \\end{bmatrix}", "E_i(\\theta )}{\\partial \\tilde{h_j}} \\frac{\\partial \\tilde{h_j}}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}}\\frac{\\partial \\tilde{h_j}}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n&=& \\sum _{j=1}^{i} \\frac{\\partial E_i(\\theta )}{\\partial \\tilde{h_j}} f^{\\prime }(a_j) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal } \\begin{bmatrix} (r_j \\odot h_{j-1}) \\end{bmatrix} $$", "&=& \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} \\frac{\\partial c_j}{\\partial \\tanh (a_j)} \\frac{\\partial \\tanh (a_j)}{\\partial a_j} \\frac{\\partial a_j}{\\partial W_{tsr}^{[1:d]}} \\nonumber \\\\\n& = & \\sum _{j=1}^{i} \\frac{\\partial E_i{(\\theta )}}{\\partial c_j} i_j (1-\\tanh ^2(a_j)) \\begin{bmatrix} x_j \\end{bmatrix}^{\\intercal }", "$$ a_j &=& \\left( \\begin{bmatrix} x_j \\end{bmatrix} W_{tsr}^{[1:d]} \\begin{bmatrix} (r_j \\odot h_{j-1}) \\end{bmatrix}^{\\intercal } \\right. \\nonumber \\\\ & & \\left. + x_j W_{xh} + (r_j \\odot h_{j-1}) W_{hh} + b_h \\right) \\nonumber $$   (Eq. 28)"]}
{"question_id": "5da9e2eef741bd7efccec8e441b8e52e906b2d2d", "predicted_answer": "", "predicted_evidence": ["The interactive demo interface of 360\u00b0 INLINEFORM0 INLINEFORM1 Stance Detection, which can be seen in Figure FIGREF9 , takes two inputs: a news search query, which is used to retrieve news articles using News API, and a stance target topic, which is used as the target of the stance detection model. For good results, the stance target should also be included as a keyword in the news search query. Multiple keywords can be provided as the query by connecting them with `AND' or `OR' as in Figure FIGREF9 .", "The demo then visualizes the collected news articles as a 2D scatter plot with each (x,y) coordinate representing a single news article from a particular outlet that matched the user query. The x-axis shows the stance of the article in the range INLINEFORM0 . The y-axis displays the prominence of the news outlet that published the article in the range INLINEFORM1 , measured by its Alexa ranking. A table displays the provided information in a complementary format, listing the news outlets of the articles, the stance labels, confidence scores, and prominence rankings. Excerpts of the articles can be scanned by hovering over the news outlets in the table and the original articles can be read by clicking on the source.", "When these two inputs are provided, the application retrieves a predefined number of news articles (up to 50) that match the first input, and analyzes their stance towards the target (the second input) using the stance detection model. The stance detection model is exposed as a web service and returns for each article-target entity pair a stance label (i.e. one of `in favour', `against' or `neutral') along with a probability.", "Specifically, we included all of the topics mentioned in the Wikipedia list of controversial issues and converted them to DBpedia resource URIs (e.g. http://en.wikipedia.org/wiki/Abortion INLINEFORM0 http://dbpedia.org/resource/Abortion) in order to facilitate linking between topics and DBpedia metadata. We then used DBpedia types BIBREF12 to filter out all entities of type Place, Person and Organisation. Finally, we ranked the remaining topics based on their number of unique outbound edges within the DBpedia graph as a measure of prominence, and picked the top 300. We show the final composition of topics in Table TABREF8 . For each topic, we retrieve the most relevant articles using the News API from November 2015 to July 2017.", "We define a topic to include named entities, but also more abstract, controversial keywords such as `gun control' and `abortion'. We compile a diverse list of topics that people are likely to be interested in from several sources: a) We retrieve the top 10 entities with the most mentions in each month from November 2015 to June 2017 and filter out entities that are not locations, persons, or organizations and those that are generally perceived as neutral; b) we manually curate a list of current important political figures; and c) we use DBpedia to retrieve a list of controversial topics. Specifically, we included all of the topics mentioned in the Wikipedia list of controversial issues and converted them to DBpedia resource URIs (e.g. http://en.wikipedia.org/wiki/Abortion INLINEFORM0 http://dbpedia.org/resource/Abortion) in order to facilitate linking between topics and DBpedia metadata. We then used DBpedia types BIBREF12 to filter out all entities of type Place, Person and Organisation."]}
{"question_id": "77bc886478925c8e9fb369b1ba5d05c42b3cd79a", "predicted_answer": "", "predicted_evidence": ["The growing epidemic of fake news in the wake of the election cycle for the 45th President of the United States has revealed the danger of staying within our filter bubbles. In light of this development, research in detecting false claims has received renewed interest BIBREF0 . However, identifying and flagging false claims may not be the best solution, as putting a strong image, such as a red flag, next to an article may actually entrench deeply held beliefs BIBREF1 .", "A better alternative would be to provide additional evidence that will allow a user to evaluate multiple viewpoints and decide with which they agree. To this end, we propose 360\u00b0 INLINEFORM0 INLINEFORM1 Stance Detection, a tool that provides a wide view of a topic from different perspectives to aid with forming a balanced opinion. Given a topic, the tool aggregates relevant news articles from different sources and leverages recent advances in stance detection to lay them out on a spectrum ranging from support to opposition to the topic.", "360\u00b0 INLINEFORM0 INLINEFORM1 Stance Detection is particularly useful to gain an overview of complex or controversial topics and to highlight differences in their perception across different outlets. We show visualizations for example queries and three controversial topics in Figure FIGREF14 . By extending the tool to enable retrieval of a larger number of news articles and more fine-grained filtering, we can employ it for general news analysis. For instance, we can highlight the volume and distribution of the stance of news articles from a single news outlet such as CNN towards a specified topic as in Figure FIGREF18 .", "The interactive demo interface of 360\u00b0 INLINEFORM0 INLINEFORM1 Stance Detection, which can be seen in Figure FIGREF9 , takes two inputs: a news search query, which is used to retrieve news articles using News API, and a stance target topic, which is used as the target of the stance detection model. For good results, the stance target should also be included as a keyword in the news search query. Multiple keywords can be provided as the query by connecting them with `AND' or `OR' as in Figure FIGREF9 .", "We define a topic to include named entities, but also more abstract, controversial keywords such as `gun control' and `abortion'. We compile a diverse list of topics that people are likely to be interested in from several sources: a) We retrieve the top 10 entities with the most mentions in each month from November 2015 to June 2017 and filter out entities that are not locations, persons, or organizations and those that are generally perceived as neutral; b) we manually curate a list of current important political figures; and c) we use DBpedia to retrieve a list of controversial topics. Specifically, we included all of the topics mentioned in the Wikipedia list of controversial issues and converted them to DBpedia resource URIs (e.g. http://en.wikipedia.org/wiki/Abortion INLINEFORM0 http://dbpedia.org/resource/Abortion) in order to facilitate linking between topics and DBpedia metadata."]}
{"question_id": "f15bc40960bd3f81bc791f43ab5c94c52378692d", "predicted_answer": "", "predicted_evidence": ["Specifically, we included all of the topics mentioned in the Wikipedia list of controversial issues and converted them to DBpedia resource URIs (e.g. http://en.wikipedia.org/wiki/Abortion INLINEFORM0 http://dbpedia.org/resource/Abortion) in order to facilitate linking between topics and DBpedia metadata. We then used DBpedia types BIBREF12 to filter out all entities of type Place, Person and Organisation. Finally, we ranked the remaining topics based on their number of unique outbound edges within the DBpedia graph as a measure of prominence, and picked the top 300. We show the final composition of topics in Table TABREF8 . For each topic, we retrieve the most relevant articles using the News API from November 2015 to July 2017.", "The growing epidemic of fake news in the wake of the election cycle for the 45th President of the United States has revealed the danger of staying within our filter bubbles. In light of this development, research in detecting false claims has received renewed interest BIBREF0 . However, identifying and flagging false claims may not be the best solution, as putting a strong image, such as a red flag, next to an article may actually entrench deeply held beliefs BIBREF1 .", "We have introduced 360\u00b0 INLINEFORM0 INLINEFORM1 Stance Detection, a tool that aims to provide evidence and context in order to assist the user with forming a balanced opinion towards a controversial topic. It aggregates news with multiple perspectives on a topic, annotates them with their stance, and visualizes them on a spectrum ranging from support to opposition, allowing the user to skim excerpts of the articles or read the original source. We hope that this tool will demonstrate how NLP can be used to help combat filter bubbles and fake news and to aid users in obtaining evidence on which they can base their opinions.", "The final dataset consists of 32,227 pairs of news articles and topics annotated with their stance. In particular, 47.67% examples have been annotated with `neutral', 21.9% with `against', 19.05% with `in favour', and 11.38% with `unrelated`. We use 70% of examples for training, 20% for validation, and 10% for testing according to a stratified split. As we expect to encounter novel and unknown entities in the wild, we ensure that entities do not overlap across splits and that we only test on unseen entities.", "We collect data using the AYLIEN News API, which provides search capabilities for news articles enriched with extracted entities and other metadata. As most extracted entities have a neutral stance or might not be of interest to users, we take steps to compile a curated list of topics, which we detail in the following."]}
{"question_id": "80d6b9123a10358f57f259b8996a792cac08cb88", "predicted_answer": "", "predicted_evidence": ["It would be interesting to test the ideas with a larger corpus of news articles for example the Google News articles used in the word2vec implementation BIBREF5 .", "For each article we extracted with the Python library \u201cSpacy\u201d the named entities labeled as person. \u201cSpacy\u201d was used because of its good performance BIBREF13 and it has pre-trained language models for English, German and others. The entity recognition is not perfect, so we have errors in the lists of persons. In a post processing step the terms from a list of common errors are removed. The names of the persons appear in different versions like \u201cDonald Trump\u201d or \u201cTrump\u201d. We map all names to the shorter version i.e. \u201cTrump\u201d in this example.", "Other approaches used large text corpora for trying to find connections and relatedness by making statistics over the words in the texts. This of course only works for people appearing in the texts and we will discuss this in section SECREF2 . All these methods do not cover the changes of relations of the persons over time, that may change over the years. Therefore the measure should have a time parameter, which can be set to the desired time we are investigating.", "For this vectors of INLINEFORM0 numbers for persons we can use different similarity measures. This choice has of course an impact of the results in applications BIBREF16 . A first choice could be the cosine similarity as used in the word2vec implementations BIBREF5 . We propose a different calculation for our setup, because we want to capture the high correlation of the series even if they are on different absolute levels of the total number of mentions, as in the example of Figure FIGREF19 .", "The time series of the correlations looks quite \u201cnoisy\u201d as you can see in Figure FIGREF6 , because the series of the mentions has a high variance. To reflect the change of the relation of the persons in a more stable way, you can take a higher value for the size of the calculation window of the correlation between the two series. In the example of Figure FIGREF20 we used a calculation window of 120 days instead of 30 days."]}
{"question_id": "5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f", "predicted_answer": "", "predicted_evidence": ["We collected datasets of news articles in English and German language from the news agency Reuters (Table TABREF13 ). After a data cleaning step, which was deleting meta information like author and editor name from the article, title, body and date were stored in a local database and imported to a Pandas data frame BIBREF12 . The English corpus has a dictionary of length 106.848, the German version has a dictionary of length 163.788.", "We propose to use the Pearson correlation coefficient instead. We can shift the window of calculation over time and therefore get the measure of relatedness as a function of time.", "Other approaches used large text corpora for trying to find connections and relatedness by making statistics over the words in the texts. This of course only works for people appearing in the texts and we will discuss this in section SECREF2 . All these methods do not cover the changes of relations of the persons over time, that may change over the years. Therefore the measure should have a time parameter, which can be set to the desired time we are investigating.", "An example from the US news corpus shows the time series of \u201cTrump\u201d and \u201cObama\u201d in Figure FIGREF18 and a zoom in to the first month of 2018 in Figure FIGREF19 . It shows that a high correlation can be on different absolute levels. Therefore we used Pearson correlation to calculate the relation of two persons. You can find examples of the similarities of some test persons from December 2017 in Table TABREF17", "A trained dictionary for more than 3 million words and phrases with 300-dim vectors is provided for download. We used the Python library Gensim from BIBREF8 for the calculation of the word distances of the multidimensional scaling in Figure FIGREF8 ."]}
{"question_id": "f010f9aa4ba1b4360a78c00aa0747d7730a61805", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF6 shows a chart of the Pearson correlation coefficient computed over a sliding window of 30 days from 2015-01-01 to 2018-02-26 for the persons \u201cMerkel\u201d and \u201cSchulz\u201d. The measure clearly covers the change in their relationship during this time period. We propose that 30 days is a good value for the time window, because on one hand it is large enough to have sufficient data for the calculation of the correlation, on the other hand it is sensitive enough to reflect changes over time. But the optimal value depends on the application for which the measure is used.", "Figure FIGREF18 shows that the mentions of a person and the correlation with the mentions of another person varies over time. We want to capture this in our relation measure. So we take a time window of INLINEFORM0 days and look at the time series in the segment back in time as shown in the example of Figure FIGREF5 .", "In Figure FIGREF15 you can see the time series of the mentions of \u201cTrump\u201d in the news, with a peak at the 8th of November 2016 the day of the election. It is also visible that the general level is changing with the election and is on higher level since then.", "Taking a look at the histograms of the most frequent persons in some timespan shows the top 20 persons in the English news articles from 2016 to 2018 (Figure FIGREF16 ). As expected the histogram has a distribution that follows Zipfs law BIBREF14 , BIBREF15 .", "A trained dictionary for more than 3 million words and phrases with 300-dim vectors is provided for download. We used the Python library Gensim from BIBREF8 for the calculation of the word distances of the multidimensional scaling in Figure FIGREF8 ."]}
{"question_id": "1e582319df1739dcd07ba0ba39e8f70187fba049", "predicted_answer": "", "predicted_evidence": ["Ablation Study. We show an ablation study in the last rows of Table TABREF23 . First, we share the parameters of INLINEFORM0 / INLINEFORM1 / INLINEFORM2 . The accuracy decreases significantly, indicating that it is crucial to learn role-sensitive units to update speaker embeddings. Second, to examine our joint selection, we fall back to selecting the addressee and response separately, as in Dynamic-RNN. We find that joint selection improves ADR and RES individually, and it is particularly helpful for pair selection ADR-RES.", "Overall Result. As shown in Table TABREF23 , SI-RNN significantly improves upon the previous state-of-the-art. In particular, addressee selection (ADR) benefits most, with different number of candidate responses (denoted as RES-CAND): around 12% in RES-CAND INLINEFORM0 and more than 10% in RES-CAND INLINEFORM1 . Response selection (RES) is also improved, suggesting role-sensitive GRUs and joint selection are helpful for response selection as well. The improvement is more obvious with more candidate responses (2% in RES-CAND INLINEFORM2 and 4% in RES-CAND INLINEFORM3 ). These together result in significantly better accuracy on the ADR-RES metric as well.", "Baselines. Apart from Dynamic-RNN, we also include several other baselines. Recent+TF-IDF always selects the most recent speaker (except the responding speaker INLINEFORM0 ) as the addressee and chooses the response to maximize the tf-idf cosine similarity with the context. We improve it by using a slightly different addressee selection heuristic (Direct-Recent+TF-IDF): select the most recent speaker that directly talks to INLINEFORM1 by an explicit addressee mention. We select from the previous 15 utterances, which is the longest context among all the experiments. This works much better when there are multiple concurrent sub-conversations, and INLINEFORM2 responds to a distant message in the context. We also include another GRU-based model Static-RNN from BIBREF4 ouchi-tsuboi:2016:EMNLP2016. Unlike Dynamic-RNN, speaker embeddings in Static-RNN are based on the order of speakers and are fixed.", "The key difference from Eq EQREF12 is that Eq EQREF19 is conditioned on the correct response INLINEFORM0 with embedding INLINEFORM1 . Similarly, for response selection, we calculate the probability of a candidate response INLINEFORM2 given the ground-truth addressee INLINEFORM3 : DISPLAYFORM0", "Figure FIGREF4 (Left) illustrates the dialog encoder in Dynamic-RNN on an example context. In this example, INLINEFORM0 says INLINEFORM1 to INLINEFORM2 , then INLINEFORM3 says INLINEFORM4 to INLINEFORM5 , and finally INLINEFORM6 says INLINEFORM7 to INLINEFORM8 . The context INLINEFORM9 will be: DISPLAYFORM0"]}
{"question_id": "aaf2445e78348dba66d7208b7430d25364e11e46", "predicted_answer": "", "predicted_evidence": ["The probability of an addressee and a response being the ground truth is calculated based on embedding similarity. To be specific, for addressee selection, the model compares the candidate speaker INLINEFORM0 , the dialog context INLINEFORM1 , and the responding speaker INLINEFORM2 : DISPLAYFORM0", "Figure FIGREF4 (Left) illustrates the dialog encoder in Dynamic-RNN on an example context. In this example, INLINEFORM0 says INLINEFORM1 to INLINEFORM2 , then INLINEFORM3 says INLINEFORM4 to INLINEFORM5 , and finally INLINEFORM6 says INLINEFORM7 to INLINEFORM8 . The context INLINEFORM9 will be: DISPLAYFORM0", "Algorithm SECREF4 gives a formal definition of the dialog encoder in SI-RNN. The dialog encoder is a function that takes as input a dialog context INLINEFORM0 (lines 1-5) and returns speaker embeddings at the final time step (lines 28-30). Speaker embeddings are initialized as INLINEFORM1 -dimensional zero vectors (lines 6-9). Speaker embeddings are updated by iterating over each line in the context (lines 10-27).", "In this paper, we study the problem of addressee and response selection in multi-party conversations: given a responding speaker and a dialog context, the task is to select an addressee and a response from a set of candidates for the responding speaker. The task requires modeling multi-party conversations and can be directly used to build retrieval-based dialog systems BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 .", "with the set of speakers INLINEFORM0 ."]}
{"question_id": "d98148f65d893101fa9e18aaf549058712485436", "predicted_answer": "", "predicted_evidence": ["In Eq EQREF21 , we decompose the joint probability into two terms: the first term selects the response given the context, and then selects the addressee given the context and the selected response; the second term selects the addressee and response in the reversed order.", "Baselines. Apart from Dynamic-RNN, we also include several other baselines. Recent+TF-IDF always selects the most recent speaker (except the responding speaker INLINEFORM0 ) as the addressee and chooses the response to maximize the tf-idf cosine similarity with the context. We improve it by using a slightly different addressee selection heuristic (Direct-Recent+TF-IDF): select the most recent speaker that directly talks to INLINEFORM1 by an explicit addressee mention. We select from the previous 15 utterances, which is the longest context among all the experiments. This works much better when there are multiple concurrent sub-conversations, and INLINEFORM2 responds to a distant message in the context. We also include another GRU-based model Static-RNN from BIBREF4 ouchi-tsuboi:2016:EMNLP2016. Unlike Dynamic-RNN, speaker embeddings in Static-RNN are based on the order of speakers and are fixed.", "Example (b) shows the advantage of SI-RNN in responding to a distant message. The responding speaker \u201cnicomachus\" is actively engaged with \u201cVeryBewitching\" in the sub-conversation 1 and is also loosely involved in the sub-conversation 2: \u201cchingao\" mentions \u201cnicomachus\" in the most recent utterance. SI-RNN remembers the distant sub-conversation 1 and responds to \u201cVeryBewitching\" with a detailed answer. Direct-Recent+TF-IDF selects the ground-truth addressee because \u201cVeryBewitching\" talks to \u201cnicomachus\", but the response is not helpful. Dynamic-RNN is biased to the recent speaker \u201cchingao\", yet the response is not relevant.", "where INLINEFORM0 INLINEFORM1 are learnable parameters. INLINEFORM2 uses the same formulation with a different set of parameters, as illustrated in the middle of Figure FIGREF17 . In addition, we update the observer embeddings from the utterance. INLINEFORM3 is implemented as the traditional GRU unit in the lower part of Figure FIGREF17 . Note that the parameters in INLINEFORM4 / INLINEFORM5 / INLINEFORM6 are not shared. This allows SI-RNN to learn role-dependent features to control speaker embedding updates. The formulations of INLINEFORM7 and INLINEFORM8 are similar.", "We improve it by using a slightly different addressee selection heuristic (Direct-Recent+TF-IDF): select the most recent speaker that directly talks to INLINEFORM1 by an explicit addressee mention. We select from the previous 15 utterances, which is the longest context among all the experiments. This works much better when there are multiple concurrent sub-conversations, and INLINEFORM2 responds to a distant message in the context. We also include another GRU-based model Static-RNN from BIBREF4 ouchi-tsuboi:2016:EMNLP2016. Unlike Dynamic-RNN, speaker embeddings in Static-RNN are based on the order of speakers and are fixed. Furthermore, inspired by BIBREF30 zhou16multi and BIBREF19 serban2016building, we implement Static-Hier-RNN, a hierarchical version of Static-RNN. It first builds utterance embeddings from words and then uses high-level RNNs to process utterance embeddings."]}
{"question_id": "34e9e54fa79e89ecacac35f97b33ef3ca3a00f85", "predicted_answer": "", "predicted_evidence": ["We propose a novel quadripartite analysis of the BLI models, in which we independently control for four different variables: (i) word form frequency, (ii) morphology, (iii) lexeme frequency and (iv) lexeme. We provide detailed descriptions for each of those conditions in the following sections. For each condition, we analyzed all 40 language pairs for each of our selected models\u2014a total of 480 experiments. In the body of the paper we only present a small representative subset of our results.", "Another difference lies in the morphological diversity of both dictionaries. The average proportion of paradigm covered for lemmata present in MUSE test dictionaries is 53% for nouns, 37% for adjectives and only 3% for verbs. We generally observe that for most lemmata the dictionaries contain only one inflection. In contrast, for our test dictionaries we get 97% coverage for nouns, 98% for adjectives and 67% for verbs. Note that we do not get 100% coverage as we are limited by the compatibility of source language and target language UniMorph resources.", "The task of bilingual lexicon induction is well established in the community BIBREF16, BIBREF17 and is the current standard choice for evaluation of cross-lingual word embedding models. Given a list of $N$ source language word forms $x_1, \\ldots , x_N$, the goal is to determine the most appropriate translation $t_i$, for each query form $x_i$. In the context of cross-lingual embeddings, this is commonly accomplished by finding a target language word that is most similar to $x_i$ in the shared semantic space, where words' similarity is usually computed using a cosine between their embeddings. The resulting set of $(x_i, t_i)$ pairs is then compared to the gold standard and evaluated using the precision at $k$ (P@$k$) metric, where $k$ is typically set to 1, 5 or 10. Throughout our evaluation we use P@1, which is equivalent to accuracy.", "Finally, we carefully analyze the magnitude of the train\u2013test paradigm leakage. We found that, on average 20% (299 out of 1500) of source words in MUSE test dictionaries share their lemma with a word in the corresponding train dictionary. E.g. the French\u2013Spanish test set includes the form perdent\u2014a third-person plural present indicative of perdre (to lose) which is present in the train set. Note that the splits we provide for our dictionaries do not suffer from any leakage as we ensure that each dictionary contains the full paradigm of every lemma.", "We focus on pairs of genetically-related languages for which we can cleanly map one morphological inflection onto another. We selected 5 languages from the Slavic family: Polish, Czech, Russian, Slovak and Ukrainian, and 5 Romance languages: French, Spanish, Italian, Portuguese and Catalan. Table TABREF5 presents an example extract from our resource; every source\u2013target pair is followed by their corresponding lemmata and a shared tag."]}
{"question_id": "6e63db22a2a34c20ad341eb33f3422f40d0001d3", "predicted_answer": "", "predicted_evidence": ["From the results of the previous section, it is not clear whether the models perform badly on inflections of generally infrequent lemmata or whether they fail on infrequent morphosyntactic categories, independently of the lexeme frequency. Indeed, the frequency of different morphosyntactic categories is far from uniform. To shed more light on the underlying cause of the performance drop in sec:freqcontrol, we first analyze the differences in the models' performance as they translate forms belonging to different categories and, next, look at the distribution of these categories across the frequency bins.", "Following our analysis in sec:parcontrol, we also examine how the performance on this new task differs for less and more frequent paradigms, as well as across different morphosyntactic categories. Here, we exhibit an unexpected result, which we present in the two right-hand side graphs of Figure FIGREF30: the state-of-the-art BLI models do generalize morphologically for frequent slots, but do not generalize for infrequent slots. For instance, for the Polish\u2013Czech pair, the models achieve 100% accuracy on identifying the correct inflection when this inflection is , , or for frequent and, for the first two categories, also the infrequent lexemes; all of which are common morphosyntactic categories (see Table TABREF26). The results from Figure FIGREF30 also demonstrate that the worst performing forms for the French\u2013Spanish language pair are indeed the infrequent verbal inflections.", "In this section we briefly outline important differences between our resource and the MUSE dictionaries BIBREF2 for Portuguese, Italian, Spanish, and French (12 dictionaries in total). We focus on MUSE as it is one of the few openly available resources that covers genetically-related language pairs.", "We split each test dictionary into 9 frequency bins, based on the relative frequencies of words in the original training corpus for the word embeddings (Wikipedia in the case of fastText). More specifically, a pair appears in a frequency bin if its source word belongs to that bin, according to its rank in the respective vocabulary. We also considered unseen words that appear in the test portion of our dictionaries, but do not occur in the training corpus for the embeddings. This is a fair experimental setting since most of those OOV words are associated with known lemmata. Note that it bears a resemblance to the classic Wug Test BIBREF20 in which a child is introduced to a single instance of a fictitious object\u2014`a wug'\u2014and is asked to name two instances of the same object\u2014`wugs'. However, in contrast to the original setup, we are interested in making sure the unseen inflection of a known lexeme is properly translated.", "We generated our dictionaries automatically based on openly available resources: Open Multilingual WordNet BIBREF12 and Extended Open Multilingual WordNet BIBREF13, both of which are collections of lexical databases which group words into sets of synonyms (synsets), and UniMorph BIBREF14\u2014a resource comprised of inflectional word paradigms for 107 languages, extracted from Wiktionary and annotated according to the UniMorph schema BIBREF15. For each language pair $(L1, L2)$ we first generated lemma translation pairs by mapping all $L1$ lemmata to all $L2$ lemmata for each synset that appeared in both $L1$ and $L2$ WordNets."]}
{"question_id": "58259f2e22363aab20c448e5dd7b6f432556b32d", "predicted_answer": "", "predicted_evidence": ["Pre-training large-capacity models (e.g., BERT, GPT BIBREF30, XLNet BIBREF31) on large corpora, then fine-tuning on more domain-specific information, has led to performance improvements on various tasks. Inspired by this, our goal in this section is to observe the effect of pre-training BERT on commonsense knowledge and refining the model on task-specific content from the CommonsenseQA dataset. Essentially, we would like to test if pre-training on our external knowledge resources can help the model acquire commonsense. For the ConceptNet pre-training procedure, pre-training BERT on pseudo-sentences formulated from ConceptNet knowledge triples does not provide much gain on performance. Instead, we trained BERT on the Open Mind Common Sense (OMCS) corpus BIBREF44, the originating corpus that was used to create ConceptNet.", "Given previously-extracted knowledge triples, we need to integrate them with the OCN component of our model. Inspired by BIBREF33, we propose to use attention-based injection. For ConceptNet knowledge triples, we first convert concept-relation entities into tokens from our lexicon, in order to generate a pseudo-sentence. For example, \u201c(book, AtLocation, library)\u201d would be converted to \u201cbook at location library.\u201d Next, we used the knowledge injection cell to fuse the commonsense knowledge into BERT's output, before feeding the fused output into the OCN cell. Specifically, in a knowledge-injection cell, a Bi-LSTM layer is used to encode these pseudo-sentences, before computing the attention with respect to BERT output, as illustrated in bottom left of figure FIGREF21.", "The first knowledge-base we consider for our experiments is ConceptNet BIBREF6. ConceptNet contains over 21 million edges and 8 million nodes (1.5 million nodes in the partition for the English vocabulary), from which one may generate triples of the form $(C1, r, C2)$, wherein the natural-language concepts $C1$ and $C2$ are associated by commonsense relation $r$, e.g., (dinner, AtLocation, restaurant). Thanks to its coverage, ConceptNet is one of the most popular semantic networks for commonsense. ATOMIC BIBREF7 is a knowledge-base that focuses on procedural knowledge. Triples are of the form (Event, r, {Effect$|$Persona$|$Mental-state}), where head and tail are short sentences or verb phrases and $r$ represents an if-then relation type: (X compliments Y, xIntent, X wants to be nice).", "In the case of OCN pre-trained on ATOMIC, although the overall performance is much lower than the OCN baseline, it is interesting to see that performance for the \u201cCauses\u201d type is not significantly affected. Moreover, performance for \u201cCausesDesire\u201d and \u201cDesires\u201d types actually got much better. As noted by BIBREF7, the \u201cCauses\u201d relation in ConceptNet is similar to \u201cEffects\u201d and \u201cReactions\u201d in ATOMIC; and \u201cCausesDesire\u201d in ConceptNet is similar to \u201cWants\u201d in ATOMIC. This result suggests that models with knowledge pre-training perform better on questions that fit the knowledge domain, but perform worse on others. In this case, pre-training on ATOMIC helps the model do better on questions that are similar to ATOMIC relations, even though overall performance is inferior. Finally, we noticed that questions of type \u201cAntonym\u201d appear to be the hardest ones. Many questions that fall into this category contain negations, and we hypothesize that the models still lack the ability to reason over negation sentences, suggesting another direction for future improvement.", "To better understand when a model performs better or worse with knowledge-injection, we analyzed model predictions by question type. Since all questions in CommonsenseQA require commonsense reasoning, we classify questions based on the ConceptNet relation between the question concept and correct answer concept. The intuition is that the model needs to capture this relation in order to answer the question. The accuracies for each question type are shown in Table TABREF32. Note that the number of samples by question type is very imbalanced. Thus due to the limited space, we omitted the long tail of the distribution (about 7% of all samples). We can see that with ConceptNet relation-injection, all question types got performance boosts|for both the OCN model and OCN model that was pre-trained on OMCS|suggesting that external knowledge is indeed helpful for the task."]}
{"question_id": "b9e0b1940805a5056f71c66d176cc87829e314d4", "predicted_answer": "", "predicted_evidence": ["Initially, for our study we had selected one algorithm from each class: TransE BIBREF19 to represent the transitional distance-based algorithms and RESCAL BIBREF20 to represent the semantic matching-based algorithms. However, after experimentation, RESCAL did not scale well for handling large KGs in our experiments. Therefore, we also included HolE BIBREF21|an efficient successor of RESCAL|in the evaluation. A brief summary of each algorithm is provided for each model, below:", "Realizing this vision requires robust ML/AI algorithms that are trained on massive amounts of data. Thousands of cars, equipped with various types of sensors (e.g., LIDAR, RGB, RADAR), are now deployed around the world to collect this heterogeneous data from real-world driving scenes. The primary objective for AD is to use these data to optimize the vehicle's perception pipeline on such tasks as: 3D object detection, obstacle tracking, object trajectory forecasting, and learning an ideal driving policy. Fundamental to all of these tasks will be the vehicle's context understanding capability, which requires knowledge of the time, location, detected objects, participating events, weather, and various other aspects of a driving scene. Even though state-of-the-art AI technologies are used for this purpose, their current effectiveness and scalability are insufficient to achieve full autonomy. Humans naturally exhibit context understanding behind the wheel, where the decisions we make are the result of a continuous evaluation of perceptual cues combined with background knowledge.", "For ease of quantitative evaluation, we consider a QA task in section SECREF17. In particular, the task is to select the correct answer from a pool of candidates, given a question that specifically requires commonsense to resolve. For example, the question, If electrical equipment won't power on, what connection should be checked? is associated with `company', `airport', `telephone network', `wires', and `freeway'(where `wires' is the correct answer choice). We demonstrate that our proposed hybrid architecture out-performs the state-of-the-art neural approaches that do not utilize structured commonsense knowledge bases. Furthermore, we discuss how our approach maintains explainability in the model's decision-making process: the model has the joint task of learning an attention distribution over the commonsense knowledge context which, in turn, depends on the knowledge triples that were conceptually most salient for selecting the correct answer candidate, downstream. Fundamentally, the goal of this project is to make human interaction with chatbots and personal assistants more robust.", "Since all questions in CommonsenseQA require commonsense reasoning, we classify questions based on the ConceptNet relation between the question concept and correct answer concept. The intuition is that the model needs to capture this relation in order to answer the question. The accuracies for each question type are shown in Table TABREF32. Note that the number of samples by question type is very imbalanced. Thus due to the limited space, we omitted the long tail of the distribution (about 7% of all samples). We can see that with ConceptNet relation-injection, all question types got performance boosts|for both the OCN model and OCN model that was pre-trained on OMCS|suggesting that external knowledge is indeed helpful for the task. In the case of OCN pre-trained on ATOMIC, although the overall performance is much lower than the OCN baseline, it is interesting to see that performance for the \u201cCauses\u201d type is not significantly affected. Moreover, performance for \u201cCausesDesire\u201d and \u201cDesires\u201d types actually got much better.", "Generating Knowledge Graphs. The Scene Ontology identifies events and features-of-interests (FoIs) as top-level concepts. An event or a FoI may be associated with a Scene via the includes relation. FoIs are associated with events through the isParticipantOf relation. Figure FIGREF8(b) shows a subset of the FoIs and events defined by the Scene Ontology. In generating the scenes' KG, each scene and sub-scene found in NuScenes is annotated using the Scene Ontology. Table TABREF9 shows some basic statistics of the generated KG."]}
{"question_id": "b54525a0057aa82b73773fa4dacfd115d8f86f1c", "predicted_answer": "", "predicted_evidence": ["We began the chapter by alluding to the way in which humans leverage a complex array of cognitive processes, in order to understand the environment; we further stated that one of the greatest challenges in AI research is learning how to endow machines with similar sense-making capabilities. In these final remarks, it is important to emphasize again (see footnote #3) that the capability we describe here need only follow from satisfying the functional requirements of context understanding, rather than concerning ourselves with how those requirements are specifically implemented in humans versus machines. In other words, our hybrid AI approach stems from the complementary nature of perception and knowledge, but does not commit to the notion of replicating human cognition in the machine: as knowledge graphs can only capture a stripped-down representation of what we know, deep neural networks can only approximate how we perceive the world and learn from it. Certainly, human knowledge (encoded in machine-consumable format) abounds in the digital world, and our work shows that these knowledge bases can be used to instruct ML models and, ultimately, enhance AI systems.", "For ease of quantitative evaluation, we consider a QA task in section SECREF17. In particular, the task is to select the correct answer from a pool of candidates, given a question that specifically requires commonsense to resolve. For example, the question, If electrical equipment won't power on, what connection should be checked? is associated with `company', `airport', `telephone network', `wires', and `freeway'(where `wires' is the correct answer choice). We demonstrate that our proposed hybrid architecture out-performs the state-of-the-art neural approaches that do not utilize structured commonsense knowledge bases. Furthermore, we discuss how our approach maintains explainability in the model's decision-making process: the model has the joint task of learning an attention distribution over the commonsense knowledge context which, in turn, depends on the knowledge triples that were conceptually most salient for selecting the correct answer candidate, downstream. Fundamentally, the goal of this project is to make human interaction with chatbots and personal assistants more robust.", "Essentially, we would like to test if pre-training on our external knowledge resources can help the model acquire commonsense. For the ConceptNet pre-training procedure, pre-training BERT on pseudo-sentences formulated from ConceptNet knowledge triples does not provide much gain on performance. Instead, we trained BERT on the Open Mind Common Sense (OMCS) corpus BIBREF44, the originating corpus that was used to create ConceptNet. We extracted about 930K English sentences from OMCS and randomly masked out 15% of the tokens; we then fine-tuned BERT, using a masked language model objective, where the model's objective is to predict the masked tokens, as a probability distribution over the entire lexicon. Finally, we load this fine-tuned model into OCN framework proceed with the downstream CommonsenseQA task. As for pre-training on ATOMIC, we follow previous work's pre-processing steps to convert ATOMIC knowledge triples into sentences BIBREF45; we created special tokens for 9 types of relations as well as blanks.", "In the case of OCN pre-trained on ATOMIC, although the overall performance is much lower than the OCN baseline, it is interesting to see that performance for the \u201cCauses\u201d type is not significantly affected. Moreover, performance for \u201cCausesDesire\u201d and \u201cDesires\u201d types actually got much better. As noted by BIBREF7, the \u201cCauses\u201d relation in ConceptNet is similar to \u201cEffects\u201d and \u201cReactions\u201d in ATOMIC; and \u201cCausesDesire\u201d in ConceptNet is similar to \u201cWants\u201d in ATOMIC. This result suggests that models with knowledge pre-training perform better on questions that fit the knowledge domain, but perform worse on others. In this case, pre-training on ATOMIC helps the model do better on questions that are similar to ATOMIC relations, even though overall performance is inferior. Finally, we noticed that questions of type \u201cAntonym\u201d appear to be the hardest ones. Many questions that fall into this category contain negations, and we hypothesize that the models still lack the ability to reason over negation sentences, suggesting another direction for future improvement.", "Visualizing KGEs. In order to visualize the generated KGE, a \u201cmini\" KG from the NuScenes-mini dataset was created. Specifically, 10 scenes were selected (along with their sub-scenes) to generate the KG, and the TransE algorithm was used to learn the embeddings. When training the KGEs, we chose the dimension of the vectors to be 100. To visualize the embeddings in 2-dimensional (2D) space, the dimensions are reduced using the t-Distributed Stochastic Neighbor Embedding (t-SNE) BIBREF22 projection. Figure FIGREF11(a) shows the resulting embeddings of the NuScenes dataset. To denote interesting patterns that manifest in the embeddings, instances of Car (a FoI) and the events in which they participate are highlighted. In this image, events such as parked car, moving car, and stopped car are clustered around entities of type Car. This shows that the isParticipantOf relations defined in the KG are maintained within the KG embeddings."]}
{"question_id": "f264612db9096caf938bd8ee4085848143b34f81", "predicted_answer": "", "predicted_evidence": ["We designed an experiment to compare the performance of human judges to our best model, the temporal-linguistic confusion model. The task had to be simple enough so that human judges could attempt it with ease. For example, it would have been ludicrous to ask the judges to sort $11,224$ accounts into $5,612$ matching pairs.", "We used Dirichlet distributions BIBREF24 as priors over multinomials. This method outperforms all other methods with an accuracy of $0.27$ and average rank of 859.", "For example, a post on Friday, August 5th at 2 AM would be translated to $\\lbrace w_8,w_{17},w_{48},w_{53}\\rbrace $ , corresponding to August, 5th, Friday, 2 AM respectively. Since we are only using unigram models, the order of words does not matter. As with the language models described in the last section, all of the probability distributions were calculated using Witten-Bell smoothing. We used the same four methods as in the last section to create our temporal models.", "Table 3 shows the performance of each of these models. Although the performance of the temporal models were not as strong as the linguistic ones, they all vastly outperformed the baseline. Also, note that here as with the linguistic models, the confusion model greatly outperformed the other models.", "We experimented with linguistic, temporal, and combined temporal-linguistic models using standard and novel techniques. The methods based on our novel confusion model outperformed the more standard ones in all cases. We showed that both temporal and linguistic information are useful for matching users, with the best temporal model performing with an accuracy of $.10$ and the best linguistic model performing with an accuracy of $0.27$ . Even though the linguistic models vastly outperformed the temporal models, when combined the temporal-linguistic models outperformed both with an accuracy of $0.31$ . The improvement in the performance of the combined models suggests that although temporal information is dwarfed by linguistic information, in terms of its contribution to digital stylometry, it nonetheless provides non-overlapping information with the linguistic data."]}
{"question_id": "da0a2195bbf6736119ff32493898d2aadffcbcb8", "predicted_answer": "", "predicted_evidence": ["Matching profiles across social networks is a hard task for humans. It is a task on par with detecting plagiarism, something a non-trained person (or sometimes even a trained person) cannot easily accomplish. (Hence the need for the development of the field of stylometry in early Renaissance.) Be that as it may, we wanted to evaluate our model against humans to make sure that it is indeed outperforming them.", "In addition to the technical contributions (such as our confusion model), we hope that this paper is able to shed light on the relative ease with which seemingly innocuous information can be used to track users across social networks, even when signing up on different services using completely different account and profile information. In the future, we hope to extend this work to other social network sites, and to incorporate more sophisticated techniques, such as topic modelling and opinion mining, into our models.", "We treated each of these bins as a word, so that we could use the same methods used in the last section to measure the similarity between the temporal activity patterns of pairs of accounts (this will also help greatly for creating the combined model, explained in the next section). In other word, the 12 bins in month were set to $w_1$ . . . $w_{12}$ , the 31 bins in day of month to $w_{13}$ . . . $w_{43}$ , the 7 bins in day of week to $w_{44}$ . . . $w_{50}$ , and the 24 bins in time were set to $w_{51}$ . . . $w_{74}$ . Thus, we had a corpus of 74 words.", "We had a total of 3 English speaking human judges from Amazon Mechanical Turk (which is an tool for crowd-sourcing of human annotation tasks) . For each task, the judges were shown the link to one of the 100 account, and its 10 corresponding candidate account links. The judges were allowed to explore each of the accounts as much as they wanted to make their decision (since all these accounts were public, there were no privacy concerns).", "We designed an experiment to compare the performance of human judges to our best model, the temporal-linguistic confusion model. The task had to be simple enough so that human judges could attempt it with ease. For example, it would have been ludicrous to ask the judges to sort $11,224$ accounts into $5,612$ matching pairs."]}
{"question_id": "f5513f9314b9d7b41518f98c6bc6d42b8555258d", "predicted_answer": "", "predicted_evidence": ["Motivated by the growing interest in matching user account across different social media and networking sites, in this paper we presented models for Digital Stylometry, which is a method for matching users through stylometry inspired techniques. We used temporal and linguistic patterns of users to do the matching.", "A baseline random choice ranker would have an accuracy of $1/N$ , and an average rank of $N/2$ (since $u\\prime $ may appear anywhere in the list of $N$ items).", "TF-IDF is a method of converting text into numbers so that it can be represented meaningfully by a vector BIBREF23 . TF-IDF is the product of two statistics, TF or Term Frequency and IDF or Inverse Document Frequency. Term Frequency measures the number of times a term (word) occurs in a document. Since each document will be of different size, we need to normalize the document based on its size. We do this by dividing the Term Frequency by the total number of terms.", "For this work, we used unigram models in Python, utilizing some components from NLTK BIBREF21 . Probability distributions were calculated using Witten-Bell smoothing BIBREF19 . Rather than assigning word $w_i$ the maximum likelihood probability estimate $p_i = \\frac{c_i}{N}$ , where $c_i$ is the number of observations of word $w_i$ and $N$ is the total number of observed tokens, Witten-Bell smoothing discounts the probability of observed words to $p_i^* = \\frac{c_i}{N+T}$ where $T$ is the total number of observed word types. The remaining $Z$ words in the vocabulary that are unobserved (i.e. where $c_i = 0$ ) are given by $p_i^* = \\frac{T}{Z(N+T)}$ .", "We developed several linguistic, temporal and combined temporal-linguistic models for our task. These models take as input a user, $u$ , from one of the sites (i.e., Twitter or Facebook) and a list of $N$ users from the other service, where one of the $N$ users, $u\\prime $ , is the same as $u$ . The models then provide a ranking among candidate matches between $u$ and each of the $N$ users. We used two criteria to evaluate our models:"]}
{"question_id": "d97843afec733410d2c580b4ec98ebca5abf2631", "predicted_answer": "", "predicted_evidence": ["The user interface allows the user to input an entity and a time period he wants to learn about, displaying four sections. In the first one, the most frequent terms used that day are shown inside circles. These circles have two properties: size and color. Size is defined by the term's frequency and the color by it's polarity, with green being positive, red negative and blue neutral. Afterwards, it displays some example tweets with the words contained in the circles highlighted with their respective sentiment color. The user may click a circle to display tweets containing that word. A trendline is also created, displaying in a chart the number of tweets per day, throughout the two years analyzed. Finally, the main topics identified are shown, displaying the identifying set of words for each topic.", "A set of portuguese and english stopwords are removed - these contain very common and not meaningful words such as \u201cthe\" or \u201ca\";", "If any tweet has less than 40 characters it is discarded. These tweets are considered too small to have any meaningful content;", "Keywords used to find a particular entity are removed from tweets associated to it. This is done because these words do not contribute to either topic or sentiment;", "With this in mind and using text mining techniques, this work explores and evaluates ways to characterize given entities by finding: (a) the main terms that define that entity and (b) the sentiment associated with it. To accomplish these goals we use topic modeling BIBREF1 to extract topics and relevant terms and phrases of daily entity-tweets aggregations, as well as, sentiment analysis BIBREF2 to extract polarity of frequent subjective terms associated with the entities. Since public opinion is, in most cases, not constant through time, this analysis is performed on a daily basis. Finally we create a data visualization of topics and sentiment that aims to display these two dimensions in an unified and intelligible way."]}
{"question_id": "813a8156f9ed8ead53dda60ef54601f6ca8076e9", "predicted_answer": "", "predicted_evidence": ["These steps serve the purpose of sanitizing and improving the text, as well as eliminating some words that may undermine the results of the remaining steps. The remaining words are then stored, organized by entity and day, e.g. all of the words in tweets related to Cristiano Ronaldo on the 10th of July, 2015.", "A set of portuguese and english stopwords are removed - these contain very common and not meaningful words such as \u201cthe\" or \u201ca\";", "A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. A visualization system was created that displays the most mentioned words for each entity/day and their respective polarity using correspondingly colored and sized circles, which are called SentiBubbles.", "If any tweet has less than 40 characters it is discarded. These tweets are considered too small to have any meaningful content;", "Topic extraction is achieved using LDA, BIBREF1 which can determine the topics in a set of documents (a corpus) and a document-topic distribution. Since we create each document in the corpus containing every word used in tweets related to an entity, during one day, we can retrieve the most relevant topics about an entity on a daily basis. From each of those topics we select the most related words in order to identify it. The system supports three different approaches with LDA, yielding varying results: (a) creating a single model for all entities (i.e. a single corpus), (b) creating a model for each group of entities that fit in a similar category (e.g. sports, politics) and (c) creating a single model for each entity."]}
{"question_id": "dd807195d10c492da2b0da8b2c56b8f7b75db20e", "predicted_answer": "", "predicted_evidence": ["Before actually analyzing the text in the tweets, we apply the following operations:", "Remove all hyperlinks and special characters and convert all alphabetic characters to lower case;", "Topic extraction is achieved using LDA, BIBREF1 which can determine the topics in a set of documents (a corpus) and a document-topic distribution. Since we create each document in the corpus containing every word used in tweets related to an entity, during one day, we can retrieve the most relevant topics about an entity on a daily basis. From each of those topics we select the most related words in order to identify it. The system supports three different approaches with LDA, yielding varying results: (a) creating a single model for all entities (i.e. a single corpus), (b) creating a model for each group of entities that fit in a similar category (e.g. sports, politics) and (c) creating a single model for each entity.", "These steps serve the purpose of sanitizing and improving the text, as well as eliminating some words that may undermine the results of the remaining steps. The remaining words are then stored, organized by entity and day, e.g. all of the words in tweets related to Cristiano Ronaldo on the 10th of July, 2015.", "The main goal of the proposed system is to obtain a characterization of a certain entity regarding both mentioned topics and sentiment throughout time, i.e. obtain a classification for each entity/day combination."]}
{"question_id": "aa287673534fc05d8126c8e3486ca28821827034", "predicted_answer": "", "predicted_evidence": ["Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. \u201cCristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.\u201cRonaldo\", \u201cCR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords.", "Topic extraction is achieved using LDA, BIBREF1 which can determine the topics in a set of documents (a corpus) and a document-topic distribution. Since we create each document in the corpus containing every word used in tweets related to an entity, during one day, we can retrieve the most relevant topics about an entity on a daily basis. From each of those topics we select the most related words in order to identify it. The system supports three different approaches with LDA, yielding varying results: (a) creating a single model for all entities (i.e. a single corpus), (b) creating a model for each group of entities that fit in a similar category (e.g. sports, politics) and (c) creating a single model for each entity.", "A set of portuguese and english stopwords are removed - these contain very common and not meaningful words such as \u201cthe\" or \u201ca\";", "Entities play a central role in the interplay between social media and online news BIBREF0 . Everyday millions of tweets are generated about local and global news, including people reactions and opinions regarding the events displayed on those news stories. Trending personalities, organizations, companies or geographic locations are building blocks of news stories and their comments. We propose to extract entities from tweets and their associated context in order to understand what is being said on Twitter about those entities and consequently to create a picture of people reactions to recent events.", "Before actually analyzing the text in the tweets, we apply the following operations:"]}
{"question_id": "8b8adb1d5a1824c8995b3eba668745c44f61c9c6", "predicted_answer": "", "predicted_evidence": ["The Zipf exponent measured in the overall corpus is also much lower than the INLINEFORM0 from the original law BIBREF39 . We do not observe the second power-law regime either, as suggested by BIBREF57 and BIBREF48 . Because most observations so far hold only for books or corpora that contain longer texts than tweets, our results suggest that the nature of communication, in our case Twitter itself affects the parameters of linguistic laws.", "In this paper, we investigated the scaling relations in citywise Twitter corpora coming from the Metropolitan and Micropolitan Statstical Areas of the United States. We could observe a slightly superlinear scaling decreasing with the city population for the total volume of the tweets and words created in a city. When observing the scaling of individual words, we found that a certain core vocabulary follows the scaling relationship of that of the bulk text, but most words are sensitive to city size, and their frequencies either increase at a higher or a lower rate with city size than that of the total word volume. At both ends of the spectrum, the meaning of the most superlinearly or most sublinearly scaling words is representative of their exponent. We also examined the increase in the number of words with city size, which has an exponent in the sublinear range. This shows that there is a decreasing amount of new words introduced in larger Twitter corpora.", "We use the following form for Zipf's law that is proposed in BIBREF48 , and that fits the probability distribution of the word frequencies apart from the very rare words: INLINEFORM0", "From online user activity and content, it is often possible to infer different socio-economic variables on various aggregation scales. Ranging from showing correlation between the main language features on Twitter and several demographic variables BIBREF10 , through predicting heart-disease rates of an area based on its language use BIBREF11 or relating unemployment to social media content and activity BIBREF12 , BIBREF13 , BIBREF14 to forecasting stock market moves from search semantics BIBREF15 , many studies have attempted to connect online media language and metadata to real-world outcomes. Various studies have analyzed spatial variation in the text of OSN messages and its applicability to several different questions, including user localization based on the content of their posts BIBREF16 , BIBREF17 , empirical analysis of the geographic diffusion of novel words, phrases, trends and topics of interest BIBREF18 , BIBREF19 , measuring public mood BIBREF20 .", "That the relative frequency of some words changes with city size means that the frequency of words versus their rank, Zipf's law, can vary from metropolitan area to metropolitan area. We obtained that the exponent of Zipf's law depends on city size, namely that the exponent decreases as text size increases. It means that with the growth of a city, rarer words tend to appear in greater numbers. The values obtained for the Zipf exponent are in line with the theoretical bounds 1.6-2.4 of BIBREF54 . In the communication efficiency framework BIBREF54 , BIBREF55 , decreasing INLINEFORM0 can be understood as decreased communication efficiency due to the increased number of different tokens, that requires more effort in the process of understanding from the reader."]}
{"question_id": "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "predicted_answer": "", "predicted_evidence": ["Their exponent differs significantly from that of the total word count, and their meaning can usually be linked to the exponent range qualitatively. The sublinearly scaling words mostly correspond to weather services reporting (flood 0.54, thunderstorm 0.61, wind 0.85), some certain slang and swearword forms (shxt 0.81, dang 0.88, damnit 0.93), outdoor-related activities (fishing 0.82, deer 0.81, truck 0.90, hunting 0.87) and certain companies (walmart 0.83). There is a longer tail in the range of superlinearly scaling words than in the sublinear regime in Figure FIGREF11 . This tail corresponds to Spanish words (gracias 1.41, por 1.40, para 1.39 etc.), that could not be separated from the English text, since the shortness of tweets make automated language detection very noisy.", "In our paper, we aim to capture the effect of city size on language use via individual urban scaling laws of words. By examining the so-called scaling exponents, we are able to connect geographical size effects to systematic variations in word use frequencies. We show that the sensitivity of words to population size is also reflected in their meaning. We also investigate how social media language and city size affects the parameters of Zipf's law BIBREF39 , and how the exponent of Zipf's law is different from that of the literature value BIBREF39 , BIBREF40 . We also show that the number of new words needed in longer texts, the Heaps law BIBREF1 exhibits a power-law form on Twitter, indicating a decelerating growth of distinct tokens with city size.", "Thus, when compared to the slightly nonlinear scaling of total amount of words, not all words follow the growth homogeneously with this same exponent. Though a significant amount remains in the linear or inconclusive range according to the statistical model test, most words are sensitive to city size and exhibit a super- or sublinear scaling. Those that fit the linear model the best, correspond to a kind of 'core-Twitter' vocabulary, which has a lot in common with the most common words of the English language, but also shows some Twitter-specific elements. A visible group of words that are amongst the most super- or sublinearly scaling words are related to the abundance or lack of the elements of urban lifestyle (e.g. deer, fitness). Thus, the imprint of the physical environment appears in a quantifiable way in the growths of word occurrences as a function of urban populations.", "related to the city, INLINEFORM1 is a multiplication factor, and INLINEFORM2 is the size of the city in terms of its population, and INLINEFORM3 denotes a scaling exponent, that captures the dynamics of the change of the quantity INLINEFORM4 with city population INLINEFORM5 . INLINEFORM6 describes a linear relationship, where the quantity INLINEFORM7 is linearly proportional to the population, which is usually associated with individual human needs such as jobs, housing or water consumption. The case INLINEFORM8 is called superlinear scaling, and it means that larger cities exhibit disproportionately more of the quantity INLINEFORM9 than smaller cities. This type of scaling is usually related to larger cities being disproportionately the centers of innovation and wealth. The opposite case is when INLINEFORM10 , that is called sublinear scaling, and is usually related to infrastructural quantities such as road network length, where urban agglomeration effects create more efficiency. BIBREF26", "From the first 5000 words according to word rank by occurrence, the most sublinearly and superlinearly scaling words can be seen in Table TABREF13 . Their exponent differs significantly from that of the total word count, and their meaning can usually be linked to the exponent range qualitatively. The sublinearly scaling words mostly correspond to weather services reporting (flood 0.54, thunderstorm 0.61, wind 0.85), some certain slang and swearword forms (shxt 0.81, dang 0.88, damnit 0.93), outdoor-related activities (fishing 0.82, deer 0.81, truck 0.90, hunting 0.87) and certain companies (walmart 0.83). There is a longer tail in the range of superlinearly scaling words than in the sublinear regime in Figure FIGREF11 . This tail corresponds to Spanish words (gracias 1.41, por 1.40, para 1.39 etc."]}
{"question_id": "74cef0205e0f31d0ab28d0e4d96c1e8ef62d4cce", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 denotes a quantity (economic output, number of patents, crime rate etc.) related to the city, INLINEFORM1 is a multiplication factor, and INLINEFORM2 is the size of the city in terms of its population, and INLINEFORM3 denotes a scaling exponent, that captures the dynamics of the change of the quantity INLINEFORM4 with city population INLINEFORM5 . INLINEFORM6 describes a linear relationship, where the quantity INLINEFORM7 is linearly proportional to the population, which is usually associated with individual human needs such as jobs, housing or water consumption. The case INLINEFORM8 is called superlinear scaling, and it means that larger cities exhibit disproportionately more of the quantity INLINEFORM9 than smaller cities. This type of scaling is usually related to larger cities being disproportionately the centers of innovation and wealth.", "This enables us to compare our result to several others in the literature.", "Most urban socioeconomic indicators follow the certain relation for a certain urban system: DISPLAYFORM0", "We use the 'Person Model' of Leitao et al. BIBREF47 , where this conservation is ensured by the normalization factor, and where the assumption is that out of the total number of INLINEFORM0 units of output that exists in the whole urban system, the probability INLINEFORM1 for one person INLINEFORM2 to obtain one unit of output depends only on the population INLINEFORM3 of the city where person INLINEFORM4 lives as INLINEFORM5", "The decrease in INLINEFORM0 for bigger cities (or bigger Twitter corpora) suggesting a decreasing number of words with lower frequencies is thus confirmed. There is evidence, that as languages grow, there is a decreasing marginal need for new words BIBREF58 . In this sense, the decelerated extension of the vocabulary in bigger cities can also be regarded as language growth."]}
{"question_id": "200c37060d037dee33f3b7c8b1a2aaa58376566e", "predicted_answer": "", "predicted_evidence": ["We use the 'Person Model' of Leitao et al. BIBREF47 , where this conservation is ensured by the normalization factor, and where the assumption is that out of the total number of INLINEFORM0 units of output that exists in the whole urban system, the probability INLINEFORM1 for one person INLINEFORM2 to obtain one unit of output depends only on the population INLINEFORM3 of the city where person INLINEFORM4 lives as INLINEFORM5", "related to the city, INLINEFORM1 is a multiplication factor, and INLINEFORM2 is the size of the city in terms of its population, and INLINEFORM3 denotes a scaling exponent, that captures the dynamics of the change of the quantity INLINEFORM4 with city population INLINEFORM5 . INLINEFORM6 describes a linear relationship, where the quantity INLINEFORM7 is linearly proportional to the population, which is usually associated with individual human needs such as jobs, housing or water consumption. The case INLINEFORM8 is called superlinear scaling, and it means that larger cities exhibit disproportionately more of the quantity INLINEFORM9 than smaller cities. This type of scaling is usually related to larger cities being disproportionately the centers of innovation and wealth. The opposite case is when INLINEFORM10 , that is called sublinear scaling, and is usually related to infrastructural quantities such as road network length, where urban agglomeration effects create more efficiency. BIBREF26", "First, we checked how some aggregate metrics: the total number of users, the total number of individual words and the total number of tweets change with city size. Figures FIGREF6 , FIGREF7 and FIGREF8 show the scaling relationship data on a log-log scale, and the result of the fitted model. In all cases, INLINEFORM0 was greater than 6, which confirmed nonlinear scaling. The the total count of tweets and words both have a slightly superlinear exponents around 1.02. The deviation from the linear exponent may seem small, but in reality it means that for a tenfold increase in city size, the abundance of the quantity INLINEFORM1 measured increases by 5%, which is already a significant change. The number of users scales sublinearly ( INLINEFORM2 ) with the city population, though.", "While many of the above cited studies exploit the fact that language use or social media activity varies in space, it is hard to capture the impact of the geographic environment on the used words or concepts. There is a growing literature on how the sheer size of a settlement influences the number of patents, GDP or the total road length driven by universal laws BIBREF21 .", "where the index INLINEFORM0 denotes different cities, the total number of cities is INLINEFORM1 , and INLINEFORM2 is the population of the city with index INLINEFORM3 ."]}
{"question_id": "415014a5bcd83df52c9307ad16fab1f03d80f705", "predicted_answer": "", "predicted_evidence": ["These words and phrases are called n-grams (an n-gram is a contiguous sequence of n words). Given the relatively short sentences on Twitter, we decided to only consider unigram, bigram and trigram phrases. We generated a list of all of the unigrams, bigrams and trigrams that appear at least five times in our tweets for a total of 6,738 n-grams. From that list we selected a total of 1,415 n-grams that were most predictive of the speech act of their corresponding tweets but did not contain topic-specific terms (such as Boston, Red Sox, etc). There is a binary feature for each of these sub-trees indicating their appearance.", "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.", "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.", "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.", "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols."]}
{"question_id": "b79c85fa84712d3028cb5be2af873c634e51140e", "predicted_answer": "", "predicted_evidence": ["Speech act recognition is a multi-class classification problem. As with any other supervised classification problem, a large labelled dataset is needed. In order to create such a dataset we first created a taxonomy of speech acts for Twitter by identifying and defining a set of commonly occurring speech acts. Next, we manually annotated a large collection of tweets using our taxonomy. Our primary task was to use the expertly annotated dataset to analyse and select various syntactic and semantic features derived from tweets that are predictive of their corresponding speech acts. Using our labelled dataset and robust features we trained standard, off-the-shelf classifiers (such as SVMs, Naive Bayes, etc) for our speech act recognition task.", "There has been extensive research done on speech act (also known as dialogue act) classification in computational linguistics, e.g., BIBREF4 . Unfortunately, these methods do not map well to Twitter, given the noisy and unconventional nature of the language used on the platform. In this work, we created a supervised speech act classifier for Twitter, using a manually annotated dataset of a few thousand tweets, in order to be better understand the meaning and intention behind tweets and uncover the rich interactions between the users of Twitter. Knowing the speech acts behind a tweet can help improve analysis of tweets and give us a better understanding of the state of mind of the users. Moreover, ws we have shown in our previous works BIBREF5 , BIBREF6 , speech act classification is essential for detection of rumors on Twitter.", "There has been extensive research done on speech act (also known as dialogue act) classification in computational linguistics, e.g., BIBREF4 . Unfortunately, these methods do not map well to Twitter, given the noisy and unconventional nature of the language used on the platform. In this work, we created a supervised speech act classifier for Twitter, using a manually annotated dataset of a few thousand tweets, in order to be better understand the meaning and intention behind tweets and uncover the rich interactions between the users of Twitter. Knowing the speech acts behind a tweet can help improve analysis of tweets and give us a better understanding of the state of mind of the users. Moreover, ws we have shown in our previous works BIBREF5 , BIBREF6 , speech act classification is essential for detection of rumors on Twitter. Finally, knowing the distribution of speech acts of tweets about a particular topic can reveal a lot about the general attitude of users about that topic (e.g., are they confused and are asking a lot of questions?", "These words and phrases are called n-grams (an n-gram is a contiguous sequence of n words). Given the relatively short sentences on Twitter, we decided to only consider unigram, bigram and trigram phrases. We generated a list of all of the unigrams, bigrams and trigrams that appear at least five times in our tweets for a total of 6,738 n-grams. From that list we selected a total of 1,415 n-grams that were most predictive of the speech act of their corresponding tweets but did not contain topic-specific terms (such as Boston, Red Sox, etc). There is a binary feature for each of these sub-trees indicating their appearance.", "We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets)."]}
{"question_id": "dc473819b196c0ea922773e173a6b283fa778791", "predicted_answer": "", "predicted_evidence": ["Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.", "These words and phrases are called n-grams (an n-gram is a contiguous sequence of n words). Given the relatively short sentences on Twitter, we decided to only consider unigram, bigram and trigram phrases. We generated a list of all of the unigrams, bigrams and trigrams that appear at least five times in our tweets for a total of 6,738 n-grams. From that list we selected a total of 1,415 n-grams that were most predictive of the speech act of their corresponding tweets but did not contain topic-specific terms (such as Boston, Red Sox, etc). There is a binary feature for each of these sub-trees indicating their appearance.", "In recent years, the micro-blogging platform Twitter has become a major social media platform with hundreds of millions of users. People turn to Twitter for a variety of purposes, from everyday chatter to reading about breaking news. The volume plus the public nature of Twitter (less than 10% of Twitter accounts are private BIBREF0 ) have made Twitter a great source of data for social and behavioural studies. These studies often require an understanding of what people are tweeting about. Though this can be coded manually, in order to take advantage of the volume of tweets available automatic analytic methods have to be used. There has been extensive work done on computational methods for analysing the linguistic content of tweets. However, there has been very little work done on classifying the pragmatics of tweets. Pragmatics looks beyond the literal meaning of an utterance and considers how context and intention contribute to meaning.", "These studies often require an understanding of what people are tweeting about. Though this can be coded manually, in order to take advantage of the volume of tweets available automatic analytic methods have to be used. There has been extensive work done on computational methods for analysing the linguistic content of tweets. However, there has been very little work done on classifying the pragmatics of tweets. Pragmatics looks beyond the literal meaning of an utterance and considers how context and intention contribute to meaning. A major element of pragmatics is the intended communicative act of an utterance, or what the utterance was meant to achieve. It is essential to study pragmatics in any linguistic system because at the core of linguistic analysis is studying what language is used for or what we do with language. Linguistic communication and meaning can not truly be studied without studying pragmatics.", "There has been extensive research done on speech act (also known as dialogue act) classification in computational linguistics, e.g., BIBREF4 . Unfortunately, these methods do not map well to Twitter, given the noisy and unconventional nature of the language used on the platform. In this work, we created a supervised speech act classifier for Twitter, using a manually annotated dataset of a few thousand tweets, in order to be better understand the meaning and intention behind tweets and uncover the rich interactions between the users of Twitter. Knowing the speech acts behind a tweet can help improve analysis of tweets and give us a better understanding of the state of mind of the users. Moreover, ws we have shown in our previous works BIBREF5 , BIBREF6 , speech act classification is essential for detection of rumors on Twitter."]}
{"question_id": "9207f19e65422bdf28f20e270ede6c725a38e5f9", "predicted_answer": "", "predicted_evidence": ["Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories.", "We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific. All of our classifiers are evaluated using 20-fold cross validation. Table TABREF9 shows the performance of our five classifiers trained and evaluated on all of the data. We report the F1 score for each class. As shown in Table TABREF9 , the logistic regression was the performing classifier with a weighted average F1 score of INLINEFORM0 . Thus we picked logistic regression as our classier and the rest of the results reported will be for LR only. Table TABREF10 shows the average performance of the LR classifier for Twitter-wide, type and topic specific classifiers.", "Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.", "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.", "The distribution of speech acts for each of the six topics and three types is shown in Figure FIGREF2 . There is much greater similarity between the distribution of speech acts of topics of the same type (e.g, Ashton Kutcher and Red Sox) compared to topics of different types. Though each topic type seems to have its own distinct distribution, Entity and Event types have much closer resemblance to each other than Long-standing. Assertions and expressions dominate in Entity and Event types with questions beings a distant third, while in Long-standing, recommendations are much more dominant with assertions being less so. This agrees with Zhao et al.'s BIBREF7 findings that tweets about Long-standings topics tend to be more opinionated which would result in more recommendations and expressions and fewer assertions."]}
{"question_id": "8ddf78dbdc6ac964a7102ae84df18582841f2e3c", "predicted_answer": "", "predicted_evidence": ["We extracted sub-trees of length one and two (the length refers to the number of edges) from each dependency tree. Overall we collected 5,484 sub-trees that appeared at least five times. We then used a filtering process identical to the one used for n-grams, resulting in 1,655 sub-trees. There is a binary feature for each of these sub-trees indicating their appearance.", "Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.", "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.", "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.", "We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets)."]}
{"question_id": "079e654c97508c521c07ab4d24cdaaede5602c61", "predicted_answer": "", "predicted_evidence": ["Finally, we compared the performance of our classifier (called TweetAct) to a logistic regression classifier trained on features proposed by, as far as we know, the only other supervised Twitter speech act classifier by Zhang et al. (called Zhang). Table TABREF12 shows the results. Not only did our classifier outperform the Zhang classifier for every class, both the semantic and syntactic classifiers (see Table TABREF11 ) also generally outperformed the Zhang classifier.", "These words and phrases are called n-grams (an n-gram is a contiguous sequence of n words). Given the relatively short sentences on Twitter, we decided to only consider unigram, bigram and trigram phrases. We generated a list of all of the unigrams, bigrams and trigrams that appear at least five times in our tweets for a total of 6,738 n-grams. From that list we selected a total of 1,415 n-grams that were most predictive of the speech act of their corresponding tweets but did not contain topic-specific terms (such as Boston, Red Sox, etc). There is a binary feature for each of these sub-trees indicating their appearance.", "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech.", "These studies often require an understanding of what people are tweeting about. Though this can be coded manually, in order to take advantage of the volume of tweets available automatic analytic methods have to be used. There has been extensive work done on computational methods for analysing the linguistic content of tweets. However, there has been very little work done on classifying the pragmatics of tweets. Pragmatics looks beyond the literal meaning of an utterance and considers how context and intention contribute to meaning. A major element of pragmatics is the intended communicative act of an utterance, or what the utterance was meant to achieve. It is essential to study pragmatics in any linguistic system because at the core of linguistic analysis is studying what language is used for or what we do with language. Linguistic communication and meaning can not truly be studied without studying pragmatics.", "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question."]}
{"question_id": "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c", "predicted_answer": "", "predicted_evidence": ["We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.", "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.", "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.", "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.", "Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs."]}
{"question_id": "95bbd91badbfe979899cca6655afc945ea8a6926", "predicted_answer": "", "predicted_evidence": ["Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.", "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.", "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.", "Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.", "These words and phrases are called n-grams (an n-gram is a contiguous sequence of n words). Given the relatively short sentences on Twitter, we decided to only consider unigram, bigram and trigram phrases. We generated a list of all of the unigrams, bigrams and trigrams that appear at least five times in our tweets for a total of 6,738 n-grams. From that list we selected a total of 1,415 n-grams that were most predictive of the speech act of their corresponding tweets but did not contain topic-specific terms (such as Boston, Red Sox, etc). There is a binary feature for each of these sub-trees indicating their appearance."]}
{"question_id": "76ae794ced3b5ae565f361451813f2f3bc85b214", "predicted_answer": "", "predicted_evidence": ["Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.", "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.", "Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories.", "We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific. All of our classifiers are evaluated using 20-fold cross validation. Table TABREF9 shows the performance of our five classifiers trained and evaluated on all of the data. We report the F1 score for each class. As shown in Table TABREF9 , the logistic regression was the performing classifier with a weighted average F1 score of INLINEFORM0 . Thus we picked logistic regression as our classier and the rest of the results reported will be for LR only. Table TABREF10 shows the average performance of the LR classifier for Twitter-wide, type and topic specific classifiers.", "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations."]}
{"question_id": "2a9c7243744b42f1e9fed9ff2ab17c6f156b1ba4", "predicted_answer": "", "predicted_evidence": ["Fig. FIGREF5 shows the overall structure based on Kaldi BIBREF22 for training the acoustic models used in this work. The right-most block is the vocal data, and the series of blocks on the left are the feature extraction processes over the vocal data. Features I, II, III, IV represent four different versions of features used here. For example, Feature IV was derived from splicing Feature III with 4 left-context and 4 right-context frames, and Feature III was obtained by performing fMLLR transformation over Feature II, while Feature I has been mean and variance normalized, etc.", "In this paper, we wish our work can be compatible to more available singing content, therefore in the initial effort we collected about five hours of music-removed version of English songs directly from commercial singing content on YouTube. The descriptive term \"music-removed\" implies the background music have been removed somehow. Because many very impressive works were based on Japanese songs BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , the comparison is difficult. We analyzed various approaches with HMM, deep learning with data augmentation, and acoustic adaptation on fragment, song, singer, and genre levels, primarily based on fMLLR BIBREF19 . We also trained the language model with a corpus of lyrics, and modify the pronunciation lexicon and increase the transition probability of HMM for prolonged vowels. Initial results are reported.", "In addition to the data set from LibriSpeech (803M words, 40M sentences), we collected 574k pieces of lyrics text (totally 129.8M words) from lyrics.wikia.com, a lyric website, and the lyrics were normalized by removing punctuation marks and unnecessary words (like \u2019[CHORUS]\u2019). Also, those lyrics for songs within our vocal data were removed from the data set.", "Applying ASR technologies to singing voice has been studied for long. However, not too much work has been reported, probably because the recognition accuracy remained to be relatively low compared to the experiences for speech. But such low accuracy is actually natural considering the various difficulties caused by the significant differences between singing voice and speech. An extra major problem is probably the lack of singing voice database, which pushed the researchers to collect their own closed datasets BIBREF12 , BIBREF15 , BIBREF17 , which made it difficult to compare results from different works.", "The exploding multimedia content over the Internet, has created a new world of spoken content processing, for example the retrieval BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , browsing BIBREF5 , summarization BIBREF0 , BIBREF5 , BIBREF6 , BIBREF7 , and comprehension BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 of spoken content. On the other hand, we may realize there still exists a huge part of multimedia content not yet taken care of, i.e., the singing content or those with audio including songs. Songs are human voice carrying plenty of semantic information just as speech. It will be highly desired if the huge quantities of singing content can be similarly retrieved, browsed, summarized or comprehended by machine based on the lyrics just as speech. For example, it is highly desired if song retrieval can be achieved based on the lyrics in addition."]}
{"question_id": "f8f64da7172e72e684f0e024a19411b43629ff55", "predicted_answer": "", "predicted_evidence": ["Rows (4)(5)(6) for Models B, C, D show the incremental improvements when training the acoustic models with a series of improved alignments a, b, c, which led to the Model E-4 in row (7). Some preliminary tests with p-norm DNN with varying parameters were then performed. The best results for the moment were obtained with 4 hidden layers, 600 and 150 hidden units for p-norm nonlinearity BIBREF26 . The result in rows (9) shows absolute improvements of 1.52% (row (9) for Model F-1 vs. row (7)) for regular DNN. Rows(10) is for Models F-1 DNN (multi-target).", "In Fig. FIGREF5 Model E includes different models obtained with fMLLR over different levels, Models E-1,2,3,4. But in Table. TABREF14 only Model E-4 is listed. Complete results for Models E-1,2,3,4 are listed in Table. TABREF19 , all for Lyrics Language Model with extended lexicon. Row (4) here is for Model E-4, or fMLLR over fragment level, exactly row (7) of Table. TABREF14 . Rows (1)(2)(3) are the same as row (5) here, except over levels of genre, singer and song. We see fragment level is the best, probably because fragment(10-35 sec long) is the smallest unit and the acoustic characteristic of signals within a fragment is almost uniform (same genre, same singer and the same song).", "Rows (11)(12)(13) show the results of BLSTMs with different factors of data augmentation described in SECREF6 . Models G-1,2,3 used three layers with 400 hidden states and 100 units for recurrent and projection layer, however, since the amount of training data were different, the number of training epoches were 15, 7 and 5 respectively. Data augmentation brought much improvement of 5.62% (rows (12) v.s.(11)), while 3-fold BLSTM outperformed 5-fold by 1.03%. Trend for Model H (rows (14)(15)(16)) is the same as Model G, 3-fold turned out to be the best.", "In this paper we report some initial results of transcribing lyrics from commercial song audio using different sets of acoustic models, adaptation approaches, language models and lexicons. Techniques for special characteristics of song audio were considered. The achieved WER was relatively high compared to experiences in speech recognition. However, considering the much more difficult problems in song audio and the wide difference between speech and singing voice, the results here may serve as good references for future work to be continued.", "From the data, we found errors frequently occurred under some specific circumstances, such as high-pitched voice, widely varying phone duration, overlapping verses (multiple people sing simultaneously), and residual background music."]}
{"question_id": "8da8c4651979a4b1d1d3008c1f77bc7e9397183b", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is word vector representation from the previous layer.", "where the difference and element-wise product are concatenated with the original vectors.", "Prediction Layer. We feed INLINEFORM0 in Equation into a 2-layer fully-connected feed-forward neural network with ReLu activation. In the last layer the sigmoid function is used. We minimize binary cross-entropy loss for training.", "Attention Matching Layer. As in ESIM model, the co-attention matrix INLINEFORM0 where INLINEFORM1 . INLINEFORM2 computes the similarity of hidden states between context and response. For each word in context, we find the most relevant response word by computing the attended response vector in Equation EQREF8 . The similar operation is used to compute attended context vector in Equation . DISPLAYFORM0", "Matching Aggregation Layer. As in ESIM model, BiLSTM is used to aggregate response-aware context representation as well as context-aware response representation. The high-level formula is given by DISPLAYFORM0"]}
{"question_id": "8cf52ba480d372fc15024b3db704952f10fdca27", "predicted_answer": "", "predicted_evidence": ["In this section, we first review ESIM model BIBREF10 and introduce our modifications and extensions. Then we introduce a string matching algorithm for out-of-vocabulary words.", "The other work related to enrich word representation is to combine the pre-built embedding produced by GloVe and word2vec with structured knowledge from semantic network ConceptNet BIBREF17 and merge them into a common representation BIBREF18 . The method obtained very good performance on word-similarity evaluations. But it is not very clear how useful the method is for other tasks such as question answering. Furthermore, this method does not directly address out-of-vocabulary issue.", "where the difference and element-wise product are concatenated with the original vectors.", "Context Representation Layer. As in base model, context and response embedding vector sequences are fed into BiLSTM. Here BiLSTM learns to represent word and its local sequence context. We concatenate the hidden states at each time step for both directions as local context-aware new word representation, denoted by INLINEFORM0 and INLINEFORM1 for context and response, respectively. DISPLAYFORM0", "We evaluate our model on the public Ubuntu Dialogue Corpus V2 BIBREF29 since this corpus is designed for response selection study of multi turns human-computer conversations. The corpus is constructed from Ubuntu IRC chat logs. The training set consists of 1 million INLINEFORM0 triples where the original context and corresponding response are labeled as positive and negative response are selected randomly on the dataset. On both validation and test sets, each context contains one positive response and 9 negative responses. Some statistics of this corpus are presented in Table TABREF15 ."]}
{"question_id": "d8ae36ae1b4d3af5b59ebd24efe94796101c1c12", "predicted_answer": "", "predicted_evidence": ["It can be observed that the performance is significantly degraded without two special tags. In order to understand how the two tags helps the model identify the important information, we perform a case study. We randomly selected a context-response pair where model trained with tags succeeded and model trained without tags failed. Since max pooling is used in Equations EQREF11 and , we apply max operator to each context token vector in Equation EQREF10 as the signal strength. Then tokens are ranked in a descending order by it. The same operation is applied to response tokens.", "It can be observed that tuning word embedding vectors during the training obtained the worse performance. The ensemble of word embedding from ConceptNet NumberBatch did not perform well since it still suffers from out-of-vocabulary issues. In order to get insights into the performance improvement of WP5, we show word coverage on Ubuntu Dialogue Corpus.", "Used the fixed pre-built FastText vectors where word vectors for out-of-vocabulary words were computed based on built model.", "In this section, we first review ESIM model BIBREF10 and introduce our modifications and extensions. Then we introduce a string matching algorithm for out-of-vocabulary words.", "In this section we evaluated word representation with the following cases on Ubuntu Dialogue corpus and compared them with that in algorithm SECREF12 ."]}
{"question_id": "2bd702174e915d97884d1571539fb1b5b0b7123a", "predicted_answer": "", "predicted_evidence": ["In this section we evaluated word representation with the following cases on Ubuntu Dialogue corpus and compared them with that in algorithm SECREF12 .", "[H] InputInputOutputOutput A dictionary with word embedding vectors of dimension INLINEFORM0 for INLINEFORM1 . INLINEFORM2", "We evaluate our model on the public Ubuntu Dialogue Corpus V2 BIBREF29 since this corpus is designed for response selection study of multi turns human-computer conversations. The corpus is constructed from Ubuntu IRC chat logs. The training set consists of 1 million INLINEFORM0 triples where the original context and corresponding response are labeled as positive and negative response are selected randomly on the dataset. On both validation and test sets, each context contains one positive response and 9 negative responses. Some statistics of this corpus are presented in Table TABREF15 .", "Used the fixed pre-built FastText vectors where word vectors for out-of-vocabulary words were computed based on built model.", "where INLINEFORM0 is vector concatenation operator. The remaining words which are in INLINEFORM1 and are not in the above output dictionary are initialized with zero vectors. The above algorithm not only alleviates out-of-vocabulary issue but also enriches word embedding representation."]}
{"question_id": "0c247a04f235a4375dd3b0fd0ce8d0ec72ef2256", "predicted_answer": "", "predicted_evidence": ["We initialize the edge scores using BERT BIBREF4 finetuned on the semantic textual similarity task for computing the semantic similarity (SS) between two sentences. Refer to the Supplementary Material for more details regarding the SS model. Note that this representation drops the sentence order information but is better able to capture the interaction between far off sentences within a document.", "CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer BIBREF11 with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.", "LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.", "We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,", "BIBREF16 introduced graph attention networks to address various shortcomings of GCNs. Most importantly, they enable nodes to attend over their neighborhoods\u2019 features without depending on the graph structure upfront. The key idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention BIBREF15 strategy. By default, there is one attention head in the GAT model. For our GAT + 2 Attn Heads model, we use two attention heads and concatenate the node embeddings obtained from different heads before passing it to the pooling layer. For a fully connected graph, the GAT model allows every node to attend on every other node and learn the edge weights. Thus, initializing the edge weights using the SS model is useless as they are being learned. Mathematical details are provided in the Supplementary Material."]}
{"question_id": "66dfcdab1db6a8fcdf392157a478b4cca0d87961", "predicted_answer": "", "predicted_evidence": ["4-way classification b/w satire, propaganda, hoax and trusted articles: We split the LUN-train into a 80:20 split to create our training and development set. We use the LUN-test as our out of domain test set.", "We initialize the edge scores using BERT BIBREF4 finetuned on the semantic textual similarity task for computing the semantic similarity (SS) between two sentences. Refer to the Supplementary Material for more details regarding the SS model. Note that this representation drops the sentence order information but is better able to capture the interaction between far off sentences within a document.", "BIBREF16 introduced graph attention networks to address various shortcomings of GCNs. Most importantly, they enable nodes to attend over their neighborhoods\u2019 features without depending on the graph structure upfront. The key idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention BIBREF15 strategy. By default, there is one attention head in the GAT model. For our GAT + 2 Attn Heads model, we use two attention heads and concatenate the node embeddings obtained from different heads before passing it to the pooling layer. For a fully connected graph, the GAT model allows every node to attend on every other node and learn the edge weights. Thus, initializing the edge weights using the SS model is useless as they are being learned. Mathematical details are provided in the Supplementary Material.", "In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset BIBREF0 and Satirical Legitimate News dataset BIBREF2. Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method.", "LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes."]}
{"question_id": "7ef34b4996ada33a4965f164a8f96e20af7470c0", "predicted_answer": "", "predicted_evidence": ["Here, $Z^l$ is the output feature corresponding to the nodes after $l^{th}$ convolution. $W^l$ is the parameter associated with the $l^{th}$ layer. We set $Z^0 = S$. Based on the above operation, we can define arbitrarily deep networks. For our experiments, we just use a single layer unless stated otherwise. By default, the adjacency matrix ($E$) is fully connected i.e. all the elements are 1 except the diagonal elements which are all set to 0. We set $E$ based on semantic similarity model in our GCN + SS model. For the GCN + Attn model, we just add a self attention layer BIBREF15 after the GCN layer and before the pooling layer.", "Capturing sentence interactions in long documents is not feasible using a recurrent network because of the vanishing gradient problem BIBREF13. Thus, we propose a novel way of encoding documents as described in the next subsection. Figure FIGREF5 shows the overall framework of our graph based neural network.", "We conduct experiments across various settings and datasets. We report macro-averaged scores in all the settings.", "LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.", "BERT: In this model, we extract the sentence vector (representation corresponding to [CLS] token) using BERT (Bidirectional Encoder Representations from Transformers) BIBREF4 for each sentence in the document. We then apply a LSTM layer on the sentence embeddings, followed by a projection layer to make the prediction for each document."]}
{"question_id": "6e80386b33fbfba8bc1ab811a597d844ae67c578", "predicted_answer": "", "predicted_evidence": ["Capturing sentence interactions in long documents is not feasible using a recurrent network because of the vanishing gradient problem BIBREF13. Thus, we propose a novel way of encoding documents as described in the next subsection. Figure FIGREF5 shows the overall framework of our graph based neural network.", "We conduct experiments across various settings and datasets. We report macro-averaged scores in all the settings.", "This paper introduces a novel way of encoding articles for fake news classification. The intuition behind representing documents as a graph is motivated by the fact that sentences interact differently with each other across different kinds of article. Recurrent networks are unable to maintain long term dependencies in large documents, whereas a fully connected graph captures the interaction between sentences at unit distance. The quantitative result shows the effectiveness of our proposed model and the qualitative results validate our hypothesis about difference in sentence interaction across different articles. Further, we show that our proposed model generalizes to unseen datasets.", "Satire, according to BIBREF5, is complicated because it occupies more than one place in the framework for humor, proposed by BIBREF6: it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. BIBREF2 defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources BIBREF2. BIBREF7 hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources.", "In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset BIBREF0 and Satirical Legitimate News dataset BIBREF2. Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method."]}
{"question_id": "1c182b4805b336bd6e1a3f43dc84b07db3908d4a", "predicted_answer": "", "predicted_evidence": ["2-way classification b/w satire and trusted articles: We use the satirical and trusted news articles from LUN-train for training, and from LUN-test as the development set. We evaluate our model on the entire SLN dataset. This is done to emulate a real-world scenario where we want to see the performance of our classifier on an out of domain dataset. We don't use SLN for training purposes because it just contains 360 examples which is too little for training our model and we want to have an unseen test set. The best performing model on SLN is used to evaluate the performance on RPN.", "We use a randomly initialized embedding matrix with 100 dimensions. We use a single layer LSTM to encode the sentences prior to the graph neural networks. All the hidden dimensions used in our networks are set to 100. The node embedding dimension is 32. For GCN and GAT, we set $\\sigma $ as LeakyRelU with slope 0.2. We train the models for a maximum of 10 epochs and use Adam optimizer with learning rate 0.001. For all the models, we use max-pool for pooling, which is followed by a fully connected projection layer with output nodes equal to the number of classes for classification.", "LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.", "BIBREF0 extends BIBREF2's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. BIBREF0 found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model. Recent works by BIBREF8, BIBREF9 show that sophisticated neural models can be used for satirical news detection. To the best of our knowledge, none of the previous works represent individual documents as graphs where the nodes represent the sentences for performing classification using a graph neural network.", "We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,"]}
{"question_id": "f71b95001dce46ee35cdbd8d177676de19ca2611", "predicted_answer": "", "predicted_evidence": ["Common Vulnerabilities and Exposures (CVE) is a list of publicly known cybersecurity vulnerabilities, each with an identification number. These entries are used in the National Vulnerability Database (NVD), the U.S. government repository of standards based vulnerability management data. The NVD suffers from poor coverage, as it contains only 10% of the open-source vulnerabilities that have received a CVE identifier BIBREF2. This could be due to the fact that a number of security vulnerabilities are discovered and fixed through informal communication between maintainers and their users in an issue tracker. To make things worse, these public databases are too slow to add vulnerabilities as they lag behind a private database such as Snyk's DB by an average of 92 days BIBREF0 All of the above pitfalls of public vulnerability management databases (such as NVD) call for a mechanism to automatically infer the presence of security threats in open-source projects, and their corresponding fixes, in a timely manner.", "In this study, we seek to answer the following research questions:", "[leftmargin=*]", "The results for all of our models on both the ground-truth and augmented datasets are given in Table TABREF22.", "RQ1: Can we effectively identify security-relevant commits using only the commit diff? For this research question, we do not use any of the commit metadata such as the commit message or information about the author. We treat source code changes like unstructured text without using path-based representations from the abstract syntax tree."]}
{"question_id": "5aa6556ffd7142933f820a015f1294d38e8cd96c", "predicted_answer": "", "predicted_evidence": ["RQ1: Can we effectively identify security-relevant commits using only the commit diff?", "RQ3: Does exploiting path-based representations of the Java classes before and after the change improve the identification of security-relevant commits?", "In this study, we seek to answer the following research questions:", "When extracting features from the complete source code of the Java classes which are modified in the commit, the performance of HR-CNN increases noticeably. Table TABREF22, row 9, shows that the accuracy of HR-CNN when using pre-trained embeddings increases to 72.6% and $\\text{F}_1$increases to 79.7%. This is considerably above the LR baseline and justifies the use of a more complex deep learning model. Meanwhile, the performance of H-CNN with randomly-initialized embeddings (Table TABREF22, row 6) does not improve when learning on entire Java classes, but there is a marked improvement in $\\text{F}_1$of about 6 points when using pre-trained embeddings. Hence, we find that extracting class-level features from the source code before and after the change, instead of using only the commit diff, improves the identification of security-relevant commits.", "We propose a novel approach using deep learning in order to identify commits in open-source repositories that are security-relevant. We build regularized hierarchical deep learning models that encode features first at the file level, and then aggregate these file-level representations to perform the final classification. We also show that code2vec, a model that learns from path-based representations of code and claimed by BIBREF3 to be suitable for a wide range of source code classification tasks, performs worse than our logistic regression baseline."]}
{"question_id": "10edfb9428b8a4652274c13962917662fdf84f8a", "predicted_answer": "", "predicted_evidence": ["A comparative analysis of how various deep learning models perform across different input representations and how various regularization techniques help with the generalization of our models.", "For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.", "We learn token-level vectors for code using the CBOW architecture BIBREF4, with negative sampling and a context window size of 5. Using CBOW over skip-gram is a deliberate design decision. While skip-gram is better for infrequent words, we felt that it is more important to focus on the more frequent words (inevitably, the keywords in a programming language) when it comes to code. Since we only perform minimal preprocessing on the code (detailed below), the most infrequent words will usually be variable identifiers. Following the same line of reasoning, we choose negative sampling over hierarchical-softmax as the training algorithm.", "We envision that this work would ultimately allow for monitoring open-source repositories in real-time, in order to automatically detect security-relevant changes such as vulnerability fixes.", "Deep neural networks are prone to overfitting due to the possibility of the network learning complicated relationships that exist in the training set but not in unseen test data. Dropout prevents complex co-adaptations of hidden units on training data by randomly removing (i.e. dropping out) hidden units along with their connections during training BIBREF10. Embedding dropout, used by BIBREF11 for neural language modeling, performs dropout on entire word embeddings. This effectively removes a proportion of the input tokens randomly at each training iteration, in order to condition the model to be robust against missing input."]}
{"question_id": "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "predicted_answer": "", "predicted_evidence": ["Common Vulnerabilities and Exposures (CVE) is a list of publicly known cybersecurity vulnerabilities, each with an identification number. These entries are used in the National Vulnerability Database (NVD), the U.S. government repository of standards based vulnerability management data. The NVD suffers from poor coverage, as it contains only 10% of the open-source vulnerabilities that have received a CVE identifier BIBREF2. This could be due to the fact that a number of security vulnerabilities are discovered and fixed through informal communication between maintainers and their users in an issue tracker. To make things worse, these public databases are too slow to add vulnerabilities as they lag behind a private database such as Snyk's DB by an average of 92 days BIBREF0 All of the above pitfalls of public vulnerability management databases (such as NVD) call for a mechanism to automatically infer the presence of security threats in open-source projects, and their corresponding fixes, in a timely manner.", "We propose a novel approach using deep learning in order to identify commits in open-source repositories that are security-relevant. We build regularized hierarchical deep learning models that encode features first at the file level, and then aggregate these file-level representations to perform the final classification. We also show that code2vec, a model that learns from path-based representations of code and claimed by BIBREF3 to be suitable for a wide range of source code classification tasks, performs worse than our logistic regression baseline.", "For RQ1, we use a hierarchical CNN (H-CNN) with either randomly-initialized or pre-trained word embeddings in order to extract features from the commit diff. We represent the commit diff as a concatenation of 300-dimensional vectors for each corresponding token from that diff. This resultant matrix is then passed through three temporal convolutional layers in parallel, with filter windows of size 3, 5, and 7. A temporal max-pooling operation is applied to these feature maps to retain the feature with the highest value in every map. We also present a regularized version of this model (henceforth referred to as HR-CNN) with embedding dropout applied on the inputs, and DropBlock on the activations of the convolutional layers.", "[leftmargin=*]", "We modify our model accordingly for every research question, based on changes in the input representation. To benchmark the performance of our deep learning models, we compare them against a logistic regression (LR) baseline that learns on one-hot representations of the Java tokens extracted from the commit diffs. For all of our models, we employ dropout on the fully-connected layer for regularization. We use Adam BIBREF25 for optimization, with a learning rate of 0.001, and batch size of 16 for randomly initialized embeddings and 8 for pre-trained embeddings."]}
{"question_id": "0b5a7ccf09810ff5a86162d502697d16b3536249", "predicted_answer": "", "predicted_evidence": ["In future work, we are investigating whether the SEPT model can be jointly trained with relation and other metadata from papers.", "Our model is consists of four parts as illustrated in figure FIGREF2: Embedding layer, sampling layer, span extractor, classification layer.", "Experiments demonstrate that even simplified architecture achieves the same performance and SEPT achieves a new state of the art result compared to existing transformer-based systems.", "The first Span-based model was proposed by BIBREF8, who apply this model to a coreference resolution task. Later, BIBREF3, BIBREF2 extend it to various tasks, such as semantic role labeling, named entity recognition and relation extraction. BIBREF2 is the first one to perform a scientific information extraction task by a span-based model and construct a dataset called SCIERC, which is the only computer science-related fine-grained information extraction dataset to our best knowledge. BIBREF9 further introduces a general framework for the information extraction task by adding a dynamic graph network after span extractors.", "In this paper, we propose an approach to improve span-based scientific named entity recognition. Unlike previous papers, we focus on named entity recognition rather than multitask framework because the multitask framework is natural to help. We work on single-tasking and if we can improve the performance on a single task, the benefits on many tasks are natural."]}
{"question_id": "8f00859f74fc77832fa7d38c22f23f74ba13a07e", "predicted_answer": "", "predicted_evidence": ["To balance the positive and negative samples and reduce the search space, we remove the pruner and modify the model by under-sampling. Furthermore, because there is a multi-head self-attention mechanism in transformers and they can capture interactions between tokens, we don't need more attention or LSTM network in span extractors. So we simplify the origin network architecture and extract span representation by a simple pooling layer. We call the final scientific named entity recognizer SEPT.", "This is a simple but effective way to improve both performance and efficiency. For those ground truth, we keep them all. In this way, we can obtain a balanced span set: $S = S_{neg} \\cup S_{pos} $. In which $S_{neg} = \\lbrace s^{\\prime }_1, s^{\\prime }_2, \\dots , s^{\\prime }_p\\rbrace $, $S_{pos} = \\lbrace s_1, s_2, \\dots , s_q\\rbrace $. Both $s$ and $s^{\\prime }$ is consist of $\\lbrace \\mathbf {e}_i ,\\dots ,\\mathbf {e}_j\\rbrace $, $i$ and $j$ are the start and end index of the span. $p$ is a hyper-parameter: the negative sample number. $q$ is the positive sample number.", "Due to the scarcity of annotated corpus of scientific papers, the pre-trained language model is an important role in the task. Recent progress such as ELMo BIBREF4, GPT BIBREF5, BERT BIBREF6 improves the performance of many NLP tasks significantly including named entity recognition. In the scientific domain, SciBERT BIBREF7 leverages a large corpus of scientific text, providing a new resource of the scientific language model. After combining the pre-trained language model with span extractors, we discover that the performance between span-based models and sequence labeling models become similar.", "In the sampling layer, we sample continuous sub-strings from the embedding layer, which is also called span. Because we know the exact label of each sample in the training phase, so we can train the model in a particular way. For those negative samples, which means each span does not belong to any entity class, we randomly sampling them rather than enumerate them all. This is a simple but effective way to improve both performance and efficiency. For those ground truth, we keep them all. In this way, we can obtain a balanced span set: $S = S_{neg} \\cup S_{pos} $. In which $S_{neg} = \\lbrace s^{\\prime }_1, s^{\\prime }_2, \\dots , s^{\\prime }_p\\rbrace $, $S_{pos} = \\lbrace s_1, s_2, \\dots , s_q\\rbrace $.", "Our model is consists of four parts as illustrated in figure FIGREF2: Embedding layer, sampling layer, span extractor, classification layer."]}
{"question_id": "bda21bfb2dd74085cbc355c70dab5984ef41dba7", "predicted_answer": "", "predicted_evidence": ["Table TABREF20 shows the results obtained using the multimodal model for different sets of input features. The model that uses all the input features available leads to the best results, improving significantly over the text-only and video-only methods.", "A distinctive aspect of this work is that we label actions in videos based on the language that accompanies the video. This has the potential to create a large repository of visual depictions of actions, with minimal human intervention, covering a wide spectrum of actions that typically occur in everyday life.", "We build a similar model that embeds actions using ELMo (composed of 2 bi-LSTMs). We pass these embeddings through the same feed forward network and sigmoid activation function. The results for both the LSTM and ELMo models are shown in Table TABREF20 .", "Context Embeddings. Context can be helpful to determine if an action is visible or not. We use two types of context information, action-level and sentence-level. Action-level context takes into account the previous action and the next action; we denote it as Context INLINEFORM0 . These are each calculated by taking the average of the action's GloVe embeddings. Sentence-level context considers up to five words directly before the action and up to five words after the action (we do not consider words that are not in the same sentence as the action); we denote it as Context INLINEFORM1 . Again, we average the GLoVe embeddings of the preceding and following words to get two context vectors.", "Yolo Object Detection. Our final baseline leverages video information from the YOLO9000 object detector. This baseline builds on the intuition that many visible actions involve visible objects. We thus label an action as visible if it contains at least one noun similar to objects detected in its corresponding miniclip. To measure similarity, we compute both the Wu-Palmer (WUP) path-length-based semantic similarity BIBREF52 and the cosine similarity on the GloVe word embeddings. For every action in a miniclip, each noun is compared to all detected objects and assigned a similarity score. As in our concreteness baseline, the action is assigned the highest score of its corresponding nouns. We use the validation data to fine tune the similarity threshold that decides if an action is visible or not. The results are reported in Table TABREF20 . Examples of actions that contain one or more words similar to detected objects by Yolo can be seen in Figure FIGREF18 ."]}
{"question_id": "c2497552cf26671f6634b02814e63bb94ec7b273", "predicted_answer": "", "predicted_evidence": ["For sequence-level video features, we use the C3D model BIBREF27 pre-trained on the Sports-1M dataset BIBREF23 . Similarly, we take the feature map of the sixth fully connected layer. Because C3D captures motion information, it is important that it is applied on consecutive frames. We take each frame used to extract the Inception features and extract C3D features from the 16 consecutive frames around it.", "Moreover, the addition of extra information improves the results for both modalities. Specifically, the addition of context is found to bring improvements. The use of POS is also found to be generally helpful.", "Our goal is to determine if actions mentioned in the transcript of a video are visually represented in the video. We develop a multimodal model that leverages both visual and textual information, and we compare its performance with several single-modality baselines.", "Context Embeddings. Context can be helpful to determine if an action is visible or not. We use two types of context information, action-level and sentence-level. Action-level context takes into account the previous action and the next action; we denote it as Context INLINEFORM0 . These are each calculated by taking the average of the action's GloVe embeddings. Sentence-level context considers up to five words directly before the action and up to five words after the action (we do not consider words that are not in the same sentence as the action); we denote it as Context INLINEFORM1 . Again, we average the GLoVe embeddings of the preceding and following words to get two context vectors.", "In this paper, we address the task of identifying human actions visible in online videos. We focus on the genre of lifestyle vlogs, and construct a new dataset consisting of 1,268 miniclips and 14,769 actions out of which 4,340 have been labeled as visible. We describe and evaluate several text-based and video-based baselines, and introduce a multimodal neural model that leverages visual and linguistic information as well as additional information available in the input data. We show that the multimodal model outperforms the use of one modality at a time."]}
{"question_id": "441a2b80e82266c2cc2b306c0069f2b564813fed", "predicted_answer": "", "predicted_evidence": ["There has been a surge of recent interest in detecting human actions in videos. Work in this space has mainly focused on learning actions from articulated human pose BIBREF7 , BIBREF8 , BIBREF9 or mining spatial and temporal information from videos BIBREF10 , BIBREF11 . A number of resources have been produced, including Action Bank BIBREF12 , NTU RGB+D BIBREF13 , SBU Kinect Interaction BIBREF14 , and PKU-MMD BIBREF15 .", "An alternative approach is to start with a set of videos, and identify all the actions present in these videos BIBREF17 , BIBREF18 . This approach has been referred to as implicit data gathering, and it typically leads to the identification of a larger number of actions, possibly with a small number of examples per action.", "Context Embeddings. Context can be helpful to determine if an action is visible or not. We use two types of context information, action-level and sentence-level. Action-level context takes into account the previous action and the next action; we denote it as Context INLINEFORM0 . These are each calculated by taking the average of the action's GloVe embeddings. Sentence-level context considers up to five words directly before the action and up to five words after the action (we do not consider words that are not in the same sentence as the action); we denote it as Context INLINEFORM1 . Again, we average the GLoVe embeddings of the preceding and following words to get two context vectors.", "Most research on video action detection has gathered video information for a set of pre-defined actions BIBREF2 , BIBREF16 , BIBREF1 , an approach known as explicit data gathering BIBREF0 . For instance, given an action such as \u201copen door,\u201d a system would identify videos that include a visual depiction of this action. While this approach is able to detect a specific set of actions, whose choice may be guided by downstream applications, it achieves high precision at the cost of low recall. In many cases, the set of predefined actions is small (e.g., 203 activity classes in BIBREF2 ), and for some actions, the number of visual depictions is very small.", "Moreover, the addition of extra information improves the results for both modalities. Specifically, the addition of context is found to bring improvements. The use of POS is also found to be generally helpful."]}
{"question_id": "e462efb58c71f186cd6b315a2d861cbb7171f65b", "predicted_answer": "", "predicted_evidence": ["Table TABREF8 shows statistics for our final dataset of videos labeled with actions, and Figure 2 shows a sample video and transcript, with annotations.", "After spam removal, we compute the agreement score between the turkers using Fleiss kappa BIBREF43 . Over the entire data set, the Fleiss agreement score is 0.35, indicating fair agreement. On the ground truth data, the Fleiss kappa score is 0.46, indicating moderate agreement. This fair to moderate agreement indicates that the task is difficult, and there are cases where the visibility of the actions is hard to label. To illustrate, Figure FIGREF9 shows examples where the annotators had low agreement.", "The goal of our dataset is to capture naturally-occurring, routine actions. Because the same action can be identified in different ways (e.g., \u201cpop into the freezer\u201d, \u201cstick into the freezer\"), our dataset has a complex and diverse set of action labels. These labels demonstrate the language used by humans in everyday scenarios; because of that, we choose not to group our labels into a pre-defined set of actions. Table TABREF1 shows the number of unique verbs, which can be considered a lower bound for the number of unique actions in our dataset. On average, a single verb is used in seven action labels, demonstrating the richness of our dataset.", "We use this approach because combining Inception V3 and C3D features has been shown to work well in other video-based models BIBREF30 , BIBREF25 , BIBREF1 .", "Similar to previous research on multimodal methods BIBREF39 , BIBREF40 , BIBREF41 , BIBREF30 , we also perform feature ablation to determine the role played by each modality in solving the task. Consistent with earlier work, we observe that the textual modality leads to the highest performance across individual modalities, and that the multimodal model combining textual and visual clues has the best overall performance."]}
{"question_id": "84f9952814d6995bc99bbb3abb372d90ef2f28b4", "predicted_answer": "", "predicted_evidence": ["In this paper, we address the task of identifying human actions visible in online videos. We focus on the genre of lifestyle vlogs, and construct a new dataset consisting of 1,268 miniclips and 14,769 actions out of which 4,340 have been labeled as visible. We describe and evaluate several text-based and video-based baselines, and introduce a multimodal neural model that leverages visual and linguistic information as well as additional information available in the input data. We show that the multimodal model outperforms the use of one modality at a time.", "We build a similar model that embeds actions using ELMo (composed of 2 bi-LSTMs). We pass these embeddings through the same feed forward network and sigmoid activation function. The results for both the LSTM and ELMo models are shown in Table TABREF20 .", "Moreover, the addition of extra information improves the results for both modalities. Specifically, the addition of context is found to bring improvements. The use of POS is also found to be generally helpful.", "Video Representations. We use Yolo9000 BIBREF47 to identify objects present in each miniclip. We choose YOLO9000 for its high and diverse number of labels (9,000 unique labels). We sample the miniclips at a rate of 1 frame-per-second, and we use the Yolo9000 model pre-trained on COCO BIBREF48 and ImageNet BIBREF49 .", "Feature-based Classifier. For our second set of baselines, we run a classifier on subsets of all of our features. We use an SVM BIBREF50 , and perform five-fold cross-validation across the train and validation sets, fine tuning the hyper-parameters (kernel type, C, gamma) using a grid search. We run experiments with various combinations of features: action GloVe embeddings; POS embeddings; embeddings of sentence-level context (Context INLINEFORM0 ) and action-level context (Context INLINEFORM1 ); concreteness score. The combinations that perform best during cross-validation on the combined train and validation sets are shown in Table TABREF20 ."]}
{"question_id": "5364fe5f256f1263a939e0a199c3708727ad856a", "predicted_answer": "", "predicted_evidence": ["LSTM and ELMo. We also consider an LSTM model BIBREF36 that takes as input the tokenized action sequences padded to the length of the longest action. These are passed through a trainable embedding layer, initialized with GloVe embeddings, before the LSTM. The LSTM output is then passed through a feed forward network of fully connected layers, each followed by a dropout layer BIBREF51 at a rate of 50%. We use a sigmoid activation function after the last hidden layer to get an output probability distribution. We fine tune the model on the validation set for the number of training epochs, batch size, size of LSTM, and number of fully-connected layers.", "Each of our baselines considers only a single modality, either text or video. While each of these modalities contributes important information, neither of them provides a full picture. The visual modality is inherently necessary, because it shows the visibility of an action. For example, the same spoken action can be labeled as either visible or non-visible, depending on its visual context; we find 162 unique actions that are labeled as both visible and not visible, depending on the miniclip. This ambiguity has to be captured using video information. However, the textual modality provides important clues that are often missing in the video. The words of the person talking fill in details that many times cannot be inferred from the video. For our full model, we combine both textual and visual information to leverage both modalities.", "Moreover, the addition of extra information improves the results for both modalities. Specifically, the addition of context is found to bring improvements. The use of POS is also found to be generally helpful.", "After spam removal, we compute the agreement score between the turkers using Fleiss kappa BIBREF43 . Over the entire data set, the Fleiss agreement score is 0.35, indicating fair agreement. On the ground truth data, the Fleiss kappa score is 0.46, indicating moderate agreement. This fair to moderate agreement indicates that the task is difficult, and there are cases where the visibility of the actions is hard to label. To illustrate, Figure FIGREF9 shows examples where the annotators had low agreement.", "We find that using only Yolo to find visible objects does not provide sufficient information to solve this task. This is due to both the low number of objects that Yolo is able to detect, and the fact that not all actions involve objects. For example, visible actions from our datasets such as \u201cget up\", \u201ccut them in half\", \u201cgetting ready\", and \u201cchopped up\" cannot be correctly labeled using only object detection. Consequently, we need to use additional video information such as Inception and C3D information."]}
{"question_id": "e500948fa01c74e5cb3e6774f66aaa9ad4b3e435", "predicted_answer": "", "predicted_evidence": ["Yes, if making something implies owning the brand", "P: Mental health problems in children and adolescents are on the rise, the British Medical Association has warned, and services are ill-equipped to cope.", "P: Catastrophic floods in Europe endanger lives and cause human tragedy as well as heavy economic losses", "(The coalition may have collapsed at the time of solving the problem)", "The main advantages of the RTE challenges is their use of examples from natural text and the inclusion of cases that require presupposed information, mostly world knowledge. Indeed, the very definition of inference assumed in a number of the examples is problematic. As BIBREF1 have pointed out, RTE platforms suffer from cases of inference that should not be categorized as such. For these cases, a vast amount of world knowledge needs to be taken into consideration (that most importantly not every linguistic agent has). In this paper, and having the RTE as our starting point challenges, we claim that RTE is insufficiently precise to perform logical reasoning or precise reasoning tasks and we take up the task of validating our working hypothesis and proposing a method for doing proper collection of precise entailment pairs in the style of RTE. Of course, the creators of RTE had in mind a more loose definition of inference where both a precise and an imprecise definition of entailment would be at play. BIBREF2 mention that \u201cour applied notion of textual entailment is also related, of course, to classical semantic entailment in the linguistics literature... a common definition of entailment specifies that a text t entails another text h (hypothesis, in our terminology) if h is true in every circumstance (possible world) in which t is true.\""]}
{"question_id": "b8b79a6123716cb9fabf751b31dff424235a2ee2", "predicted_answer": "", "predicted_evidence": ["In our compilation of answers, we have marked 42 problems as straight \u201cNo\u201d, 64 as \u201cYes\u201d with missing implicit hypotheses and \u201c44\u201d as plain \u201cYes\u201d. This means that, we expect, in our opinion, 28% of problems to be incorrectly labeled in RTE3 even assuming reasonable world knowledge. An additional 42% of problems require additional (yet reasonable to assume) hypotheses for entailment to hold formally, as prescribed by RTE3. This leaves only 30% of problems to acceptable as such. The reason that the amount of doubt is larger than in the average numbers quoted above is that, for many problems, certain missing hypotheses and/or error were not detected by a majority experts, but, after careful inspection, we judge that the minority report is justified.", "The classification between \u201cyes\u201d with missing hypotheses and \u201cno\u201d is sometimes a tenuous one \u2014 which is why we elected to group those categories in our summaries above. Indeed, consider the following example:", "However, each problem was classified by three experts. The histogram below shows the distribution of number of experts casting doubt on entailment, over all problems.", "H: In 1945, an atomic bomb was dropped on Hiroshima. (Bombs can explode without being dropped.)", "We have additionally tagged each missing hypothesis according to the following classification:"]}
{"question_id": "00f507053c47e55d7e72bebdbd8a75b3ca88cf85", "predicted_answer": "", "predicted_evidence": ["For the studies of generative methods, a huge amount of work aims to mitigate the \u201csafe response\" issue from different perspectives. Most of work build models under a sequence to sequence framework BIBREF18 , and introduce other elements, such as latent variables BIBREF4 , topic information BIBREF19 , and dynamic vocabulary BIBREF20 to increase response diversity. Furthermore, the reranking technique BIBREF10 , reinforcement learning technique BIBREF15 , and adversarial learning technique BIBREF16 , BIBREF21 have also been applied to response generation. Apart from work on \u201csafe response\", there is a growing body of literature on style transfer BIBREF22 , BIBREF23 and emotional response generation BIBREF17 . In general, most of previous work generates a response from scratch either left-to-right or conditioned on a latent vector, whereas our approach aims to generate a response by editing a prototype. Prior works have attempted to utilize prototype responses to guide the generation process BIBREF24 , BIBREF25 , in which prototype responses are encoded into vectors and feed to a decoder along with a context representation.", "Compared to ensemble models, our model performs much better on diversity and originality, that is because we regard prototype response instead of the current context as source sentence in the Seq2Seq, which keeps most of content in prototype but slightly revises it based on the context difference. Both of the ensemble and edit model are improved when the original retrieval candidates are considered in the rerank process.", "For an unconditional sentence editing setting BIBREF11 , an edit vector is randomly sampled from a distribution because how to edit the sentence is not constrained. In contrast, we should take both of INLINEFORM0 and INLINEFORM1 into consideration when we revise a prototype response INLINEFORM2 . Formally, INLINEFORM3 is firstly transformed to hidden vectors INLINEFORM4 through a biGRU parameterized as Equation ( EQREF10 ). DISPLAYFORM0", "Suppose that we have a data set INLINEFORM0 . INLINEFORM1 , INLINEFORM2 comprises a context INLINEFORM3 and its response INLINEFORM4 , where INLINEFORM5 is the INLINEFORM6 -th word of the context INLINEFORM7 and INLINEFORM8 is the INLINEFORM9 -th word of the response INLINEFORM10 . It should be noted that INLINEFORM11 can be either a single turn input or a multiple turn input. As the first step, we assume INLINEFORM12 is a single turn input in this work, and leave the verification of the same technology for multi-turn response generation to future work. Our full model is shown in Figure FIGREF3 , consisting of a prototype selector INLINEFORM13 and a context-aware neural editor INLINEFORM14 . Given a new conversational context INLINEFORM15 , we first use INLINEFORM16 to retrieve a context-response pair INLINEFORM17 .", "We present a new paradigm, prototype-then-edit, for open domain response generation, that enables a generation-based chatbot to leverage retrieved results. We propose a simple but effective model to edit context-aware responses by taking context differences into consideration. Experiment results on a large-scale dataset show that our model outperforms traditional methods on some metrics. In the future, we will investigate how to jointly learn the prototype selector and neural editor."]}
{"question_id": "e14e3e0944ec3290d1985e9a3da82a7df17575cd", "predicted_answer": "", "predicted_evidence": ["Prior works on retrieval-based methods mainly focus on the matching model architecture for single turn conversation BIBREF5 and multi-turn conversation BIBREF6 , BIBREF8 , BIBREF9 . For the studies of generative methods, a huge amount of work aims to mitigate the \u201csafe response\" issue from different perspectives. Most of work build models under a sequence to sequence framework BIBREF18 , and introduce other elements, such as latent variables BIBREF4 , topic information BIBREF19 , and dynamic vocabulary BIBREF20 to increase response diversity. Furthermore, the reranking technique BIBREF10 , reinforcement learning technique BIBREF15 , and adversarial learning technique BIBREF16 , BIBREF21 have also been applied to response generation. Apart from work on \u201csafe response\", there is a growing body of literature on style transfer BIBREF22 , BIBREF23 and emotional response generation BIBREF17 . In general, most of previous work generates a response from scratch either left-to-right or conditioned on a latent vector, whereas our approach aims to generate a response by editing a prototype.", "The decoder takes INLINEFORM0 as an input and generates a response by a GRU language model with attention. The hidden state of the decoder is acquired by DISPLAYFORM0", "In this paper, we only consider single turn response generation. We collected over 20 million human-human context-response pairs (context only contains 1 turn) from Douban Group . After removing duplicated pairs and utterance longer than 30 words, we split 19,623,374 pairs for training, 10,000 pairs for validation and 10,000 pairs for testing. The average length of contexts and responses are 11.64 and 12.33 respectively. The training data mentioned above is used by retrieval models and generative models.", "Our contributions are listed as follows: 1) this paper proposes a new paradigm, prototype-then-edit, for response generation; 2) we elaborate a simple but effective context-aware editing model for response generation; 3) we empirically verify the effectiveness of our method in terms of relevance, diversity, fluency and originality.", "A good prototype selector INLINEFORM0 plays an important role in the prototype-then-edit paradigm. We use different strategies to select prototypes for training and testing. In testing, as we described above, we retrieve a context-response pair INLINEFORM1 from a pre-defined index for context INLINEFORM2 according to the similarity of INLINEFORM3 and INLINEFORM4 . Here, we employ Lucene to construct the index and use its inline algorithm to compute the context similarity."]}
{"question_id": "f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "predicted_answer": "", "predicted_evidence": ["where the input of INLINEFORM0 -th time step is the last step hidden state and the concatenation of the INLINEFORM1 -th word embedding and the edit vector obtained in Equation EQREF14 . Then we compute a context vector INLINEFORM2 , which is a linear combination of INLINEFORM3 : DISPLAYFORM0", "Following BIBREF10 , we evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct-1 and Distinct-2. In this paper, we define a new metric, originality, that is defined as the ratio of generated responses that do not appear in the training set. Here, \u201cappear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0. +2: The response is fluent and grammatically correct. +1: There are a few grammatical errors in the response but readers could understand it. 0: The response is totally grammatically broken, making it difficult to understand. As how to evaluate response generation automatically is still an open problem BIBREF35 , we further conduct human evaluations to compare our models with baselines.", "We learn our response generation model by minimizing the negative log likelihood of INLINEFORM0 DISPLAYFORM0", "Prior works on retrieval-based methods mainly focus on the matching model architecture for single turn conversation BIBREF5 and multi-turn conversation BIBREF6 , BIBREF8 , BIBREF9 . For the studies of generative methods, a huge amount of work aims to mitigate the \u201csafe response\" issue from different perspectives. Most of work build models under a sequence to sequence framework BIBREF18 , and introduce other elements, such as latent variables BIBREF4 , topic information BIBREF19 , and dynamic vocabulary BIBREF20 to increase response diversity. Furthermore, the reranking technique BIBREF10 , reinforcement learning technique BIBREF15 , and adversarial learning technique BIBREF16 , BIBREF21 have also been applied to response generation. Apart from work on \u201csafe response\", there is a growing body of literature on style transfer BIBREF22 , BIBREF23 and emotional response generation BIBREF17 . In general, most of previous work generates a response from scratch either left-to-right or conditioned on a latent vector, whereas our approach aims to generate a response by editing a prototype.", "where INLINEFORM0 is given by DISPLAYFORM0"]}
{"question_id": "0b5bf00d2788c534c4c6c007b72290c48be21e16", "predicted_answer": "", "predicted_evidence": ["Research on chatbots goes back to the 1960s when ELIZA was designed BIBREF12 with a huge amount of hand-crafted templates and rules. Recently, researchers have paid more and more attention on data-driven approaches BIBREF13 , BIBREF14 due to their superior scalability. Most of these methods are classified as retrieval-based methods BIBREF14 , BIBREF7 and generation methods BIBREF15 , BIBREF16 , BIBREF17 . The former one aims to select a relevant response using a matching model, while the latter one generates a response with natural language generative models.", "We evaluate our model on four criteria: fluency, relevance, diversity and originality. We employ Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) BIBREF35 to evaluate response relevance, which are better correlated with human judgment than BLEU. Following BIBREF10 , we evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct-1 and Distinct-2. In this paper, we define a new metric, originality, that is defined as the ratio of generated responses that do not appear in the training set. Here, \u201cappear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0.", "Furthermore, the reranking technique BIBREF10 , reinforcement learning technique BIBREF15 , and adversarial learning technique BIBREF16 , BIBREF21 have also been applied to response generation. Apart from work on \u201csafe response\", there is a growing body of literature on style transfer BIBREF22 , BIBREF23 and emotional response generation BIBREF17 . In general, most of previous work generates a response from scratch either left-to-right or conditioned on a latent vector, whereas our approach aims to generate a response by editing a prototype. Prior works have attempted to utilize prototype responses to guide the generation process BIBREF24 , BIBREF25 , in which prototype responses are encoded into vectors and feed to a decoder along with a context representation. Our work differs from previous ones on two aspects. One is they do not consider prototype context in the generation process, while our model utilizes context differences to guide editing process. The other is that we regard prototype responses as a source language, while their works formulate it as a multi-source seq2seq task, in which the current context and prototype responses are all source languages in the generation process.", "Ensemble Model: Song et al song2016two propose an ensemble of retrieval and generation methods. It encodes current context and retrieved responses (Top-k retrieved responses are all used in the generation process.) into vectors, and feeds these representations to a decoder to generate a new response. As there is no official code, we implement it carefully by ourselves. We use the top-1 response returned by beam search as a baseline, denoted as Ensemble-default. For a fair comparison, we further rerank top 20 generated results with the same LSTM based matching model, and denote it as Ensemble-Rerank. We further create a candidate pool by merging the retrieval and generation results, and rerank them with the same ranker. The method is denoted as Ensemble-Merge.", "Here, \u201cappear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0. +2: The response is fluent and grammatically correct. +1: There are a few grammatical errors in the response but readers could understand it. 0: The response is totally grammatically broken, making it difficult to understand. As how to evaluate response generation automatically is still an open problem BIBREF35 , we further conduct human evaluations to compare our models with baselines. We ask the same three native speakers to do a side-by-side comparison BIBREF15 on the 1,000 contexts. Given a context and two responses generated by different models, we ask annotators to decide which response is better (Ties are permitted)."]}
{"question_id": "86c867b393db0ec4ad09abb48cc1353cac47ea4c", "predicted_answer": "", "predicted_evidence": ["Here, \u201cappear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0. +2: The response is fluent and grammatically correct. +1: There are a few grammatical errors in the response but readers could understand it. 0: The response is totally grammatically broken, making it difficult to understand. As how to evaluate response generation automatically is still an open problem BIBREF35 , we further conduct human evaluations to compare our models with baselines. We ask the same three native speakers to do a side-by-side comparison BIBREF15 on the 1,000 contexts. Given a context and two responses generated by different models, we ask annotators to decide which response is better (Ties are permitted).", "Correspondingly, we evaluate three variants of our model. Specifically, Edit-default and Edit-1-Rerank edit top-1 response yielded by Retrieval-default and Retrieval-Rerank respectively. Edit-N-Rerank edits all 20 responses returned by Lucene and then reranks the revised results with the dual-LSTM model. We also merge edit results of Edit-N-Rerank and candidates returned by the search engine, and then rerank them, which is denoted as Edit-Merge. In practice, the word embedding size and editor vector size are 512, and both of the encoder and decoder are a 1-layer GRU whose hidden vector size is 1024. Message and response vocabulary size are 30000, and words not covered by the vocabulary are represented by a placeholder $UNK$. Word embedding size, hidden vector size and attention vector size of baselines and our models are the same.", "It is interesting to explore the semantic gap between prototype and revised response. We ask annotators to conduct 4-scale rating on 500 randomly sampled prototype-response pairs given by Edit-default and Edit-N-Rerank respectively. The 4-scale is defined as: identical, paraphrase, on the same topic and unrelated.", "We present a new paradigm, prototype-then-edit, for open domain response generation, that enables a generation-based chatbot to leverage retrieved results. We propose a simple but effective model to edit context-aware responses by taking context differences into consideration. Experiment results on a large-scale dataset show that our model outperforms traditional methods on some metrics. In the future, we will investigate how to jointly learn the prototype selector and neural editor.", "We give three examples to show how our model works in Table TABREF30 . The first case illustrates the effect of word insertion. Our editing model enriches a short response by inserting words from context, that makes the conversation informative and coherent. The second case gives an example of word deletion, where a phrase \u201cbraised pork rice\" is removed as it does not fit current context. Phrase \u201cbraised pork rice\" only appears in the prototype context but not in current context, so it is in the deletion word set INLINEFORM0 , that makes the decoder not generate it. The third one is that our model forms a relevant query by deleting some words in the prototype while inserting other words to it. Current context is talking about \u201cclean tatoo\", but the prototype discusses \u201cclean hair\", leading to an irrelevant response. After the word substitution, the revised response becomes appropriated for current context."]}
{"question_id": "8f6b11413a19fe4639b3fba88fc6b3678286fa0c", "predicted_answer": "", "predicted_evidence": ["Table TABREF27 shows the statistics for the examined MRC datasets.", "We report the variance for shuffling methods $s_6$ context words shuffle, $s_7$ sentence words shuffle, and $s_8$ sentence order shuffle in Table TABREF26.", "In this appendix, we describe the results for the reading and reasoning skills not mentioned in Section 4.2. $s_1$: recognizing question words. For the first four answer-extraction datasets, the performance decreased by more than 70%. For the multiple-choice datasets, the performance decreased by an average of 23.9%.", "Hyperparameters used in the baseline model are shown in Table TABREF25.", "Our methodology uses a set of requisite skills and corresponding ablation methods. By checking the solvability of questions after applying the ablation methods, we can quantify to what degree the questions allow unintended solutions that do not require the requisite skills. Users can define an arbitrary set of skills to suit their purposes. We develop a method $\\sigma _i$ that ablates features necessary for the corresponding skill $s_i$ in a set of requisite skills $S$. For $(x, y) \\in X \\times Y$, whenever $f(x) = y$, if $f(\\sigma _i(x)) = y$, we recognize that $x$ is solvable without $s_i$."]}
{"question_id": "141f23e87c10c2d54d559881e641c983e3ec8ef3", "predicted_answer": "", "predicted_evidence": ["This result means that most of the solved questions are solvable even with the sentence words shuffled. However, we should not say that all questions must require this skill; a question can require the performance of some complex reasoning (e.g., logical and multi-hop reasoning) and merely need to identify the sentence that gives the correct answer without precisely understanding that sentence. Nevertheless, if the question is not intended to require such reasoning, we should care whether it can be solved with only a (sentence-level) bag of words. In order to ensure that a model can understand the precise meaning of a described event, we may need to include questions to evaluate the grammatical and syntactic understanding into a dataset. $s_8$: discourse relation understanding (sentence order shuffle). The smallest drop, excluding SWAG, which has one context sentence, was $-$1.3%, on SQuAD v1.1. Except for HotpotQA, the datasets show small drops (less than 10%), which indicates that most solved questions do not require understanding of adjacent discourse relations and are solvable even if the sentences appear in an unnatural order.", "For the other answer extraction datasets, the largest drop (73.6% relative) is by HotpotQA; it has longer context documents than the other datasets, which seemingly makes its questions more difficult. To verify the effect of its longer documents, we also evaluated the baseline model on HotpotQA without distracting paragraphs. We found that the model's performance was 56.4% F1 (the original performance was 76.3% F1 and its relative drop was 26.1%) which is much higher than that on the context with distracting paragraphs (16.8% F1). This indicates that adding longer distracting documents contributes to encouraging machines to understand a given context beyond matching word patterns. On the other hand, the performance on the multiple choice datasets was significantly worse; if multiple choices do not have sufficient word overlap with the given context, there is no way to infer the correct answer option. Therefore, this result shows that multiple choice datasets might have a capacity for requiring more complex understanding beyond matching patterns between the question and the context than the answer extraction datasets.", "Results. For both datasets, the annotation shows that, not surprisingly, almost all features are unreconstructable in the shuffled sentence/context words and the vocabulary anonymization (except for one example in RACE). When these questions are solvable / unsolvable by humans, we can say that features are unnecessary (Case $\\alpha $) / necessary (Case $\\beta $) for answering the questions. In contrast, the annotators could guess function words for some questions even if these words are dropped (SQuAD: 55.0% and RACE: 15.0%). The annotation of the necessity also shows that, however, reconstructable features (function words in this case) for all the questions are not necessary to answer them (i.e., Case $\\gamma $). Therefore, we could not find any question in Case $\\delta $. We report the annotation results in Appendix H. It is not easy for the annotator to completely ignore the information of reconstructed features.", "Surprisingly, for SQuAD v1.1, the baseline model achieved 61.2% F1. It only uses 248 tokens as the vocabulary with the anonymization tags and no other actual tokens. For the other answer extraction datasets, the largest drop (73.6% relative) is by HotpotQA; it has longer context documents than the other datasets, which seemingly makes its questions more difficult. To verify the effect of its longer documents, we also evaluated the baseline model on HotpotQA without distracting paragraphs. We found that the model's performance was 56.4% F1 (the original performance was 76.3% F1 and its relative drop was 26.1%) which is much higher than that on the context with distracting paragraphs (16.8% F1). This indicates that adding longer distracting documents contributes to encouraging machines to understand a given context beyond matching word patterns. On the other hand, the performance on the multiple choice datasets was significantly worse; if multiple choices do not have sufficient word overlap with the given context, there is no way to infer the correct answer option.", "We note that we cannot draw general conclusions for instances given by conditions other than the abovementioned one. Consider the case where $f(x) = y$ and $f(\\sigma _i(x)) \\ne y$, for example. This only means that $f$ cannot solve $x$ without the features ablated by $\\sigma _i$. We cannot conclude that $x$ requires $s_i$ in every model because there might exist a model that can solve $x$ without $s_i$. However, if there is at least one model $f$ that solves $x$ without $s_i$, this may indicate an unintended way to solve $x$ while ignoring $s_i$. Therefore our methodology only requires a single baseline model. Users can choose an arbitrary model for their purposes."]}
{"question_id": "45e6532ac06a59cb6a90624513242b06d7391501", "predicted_answer": "", "predicted_evidence": ["BIBREF2 recently propose to use local attention and block attention to sparsify the transformer. Our approach differs from them in that our method does not need to block sentences and still capture long distance dependencies. Besides, we demonstrate the importance of Explicit Sparse Transformer in sequence to sequence learning. Although the variants of sparsemax BIBREF3, BIBREF42, BIBREF43 improve in machine translation tasks, we empirically demonstrate in SECREF24 that our method introduces less computation in the standard transformer and is much faster than those sparse attention methods on GPUs.", "where $C^{(i)}$ refers to the output of the head, $Q^{(i)}$, $K^{(i)}$ and $V^{(i)}$ are the query, key and value of the head, and $d_k$ refers to the size of each head ($d_k = d/g$). Finally, the output of each head are concatenated for the output:", "Figure FIGREF56 shows the code for the idea in case of single head self-attention, the proposed method is easy to implement and plug in the successful Transformer model.", "The softmax function is evidently differentiable, therefore, we have calculated the gradient involved in top-k selection.", "We use the default setting in BIBREF0 for the implementation of our proposed Explicit Sparse Transformer. The hyper parameters including beam size and training steps are tuned on the valid set."]}
{"question_id": "a98ae529b47362f917a398015c8525af3646abf0", "predicted_answer": "", "predicted_evidence": ["The natural question of how to choose the optimal $k$ comes with the proposed method. We compare the effect of the value of $k$ at exponential scales. We perform experiments on En-Vi and De-En from 3 different initializations for each value of $K$, and report the mean BLEU scores on the valid set. The figure FIGREF27 shows that regardless of the value of 16 on the En-Vi dataset, the model performance generally rises first and then falls as $k$ increases. For $k\\in \\lbrace 4,8,16,32\\rbrace $, setting the value of $k$ to 8 achieves consistent improvements over the transformer baseline.", "where $C^{(i)}$ refers to the output of the head, $Q^{(i)}$, $K^{(i)}$ and $V^{(i)}$ are the query, key and value of the head, and $d_k$ refers to the size of each head ($d_k = d/g$). Finally, the output of each head are concatenated for the output:", "where $A$ refers to the normalized scores. As the scores that are smaller than the top k largest scores are assigned with negative infinity by the masking function $\\mathcal {M}(\\cdot , \\cdot )$, their normalized scores, namely the probabilities, approximate 0. We show the back-propagation process of Top-k selection in SECREF50. The output representation of self-attention $C$ can be computed as below:", "We are surprised to find that only adding the sparsification in the training phase can also bring an improvement in the performance. We experiment this idea on IWSLT En-Vi and report the results on the valid set in Table TABREF30, . The improvement of 0.3 BLEU scores shows that vanilla Transformer may be overparameterized and the sparsification encourages the simplification of the model.", "Take the original attention mechanism in NMT as an example. Both key $K \\in \\mathbb {R}^{n \\times d}$ and value $V \\in \\mathbb {R}^{n \\times d} $ are the sequence of output states from the encoder. Query $Q \\in \\mathbb {R}^{m \\times d}$ is the sequence of output states from the decoder, where $m$ is the length of $Q$, $n$ is the length of $K$ and $V$, and $d$ is the dimension of the states. Thus, the attention mechanism is formulated as:"]}
{"question_id": "58df55002fbcba76b9aeb2181d78378b8c01a827", "predicted_answer": "", "predicted_evidence": ["Spoken dialogue systems typically consist of a spoken language understanding (SLU) component that performs slot-filling to detect slot-value pairs expressed in the user utterance. This information is then used by dialogue state tracker(DST) BIBREF1, BIBREF2. Recent research has focused on jointly modeling the SLU and the DST BIBREF4, BIBREF5. For such joint models, deep neural network techniques have been the choice of use because of their proven ability to extract features from a given input and their generalization capability BIBREF4, BIBREF5, BIBREF6, BIBREF7. Following this research line, BIBREF4 proposed a word-based DST (based on a delexicalisation approach) that jointly models SLU and DST, and directly maps from the utterances to an updated belief state. BIBREF6 proposed a data-driven approach for DST, named neural belied tracker (NBT), which learns a vector representation for each slot-value pair and compares them with the vector representation of the user utterance to predict if the user has expressed the corresponding slot-value pair.", "GLAD (Global-Locally Self-Attentive Dialogue State Tracker) BIBREF8 consists of a shared global bidirectional-LSTM BIBREF14 for all slots, and a local bidirectional-LSTM for each slot. The global and local representations are then combined using attention, which then is used by a scoring module to obtain scores for each slot-value pair. GLAD also relies on pre-trained embeddings, and since it consists of multiple recurrent modules, the latency of the model is quite high. In BIBREF9, a Globally conditioned encoder (GCE) is used as a shared encoder for all slots and aim to address this issue by proposing a single encoder with global conditioning. While this approach reduced the latency of GLAD, it still has a considerable time complexity for real-world applications, which is discussed in Section SECREF6.", "Current DST models use recurrent neural networks (RNN), as they are able to capture temporal dependencies in the input sentence. A RNN processes each token in the input sequentially, one after the other, and so can incur significant latency if not modeled well. Apart from the architecture, the number of slots and values of the domain ontology also affects the time complexity of the DST. Recent works BIBREF6, BIBREF8, BIBREF7 use RNNs to obtain very high performance for DST, but nevertheless are quite limited as far as the efficiency of the models are concerned. For instance, the GCE model BIBREF9 addresses time complexity within the same architectural framework used by of GLAD BIBREF8, although the latency prediction of the model is still quite poor, at least for a production system (more details in Section SECREF5). This limitation could be attributed to the fact that both GLAD and GCE use separate recurrent modules to output representations for user utterance, system action and slot-value pairs.", "further experiments show that the proposed model is highly robust when either pre-trained embeddings are used or when they are not used, in this case outperforming state-of-art systems.", "Both GLAD and GCE, by default, use embeddings of size 400, while our G-SAT model has a default embedding size of 128. So we also investigated the effect of embedding dimension on these different models, to understand if results are consistent, or if the choice of the embedding size has a significant role in the performance of the models (as the embeddings are learned during training). First, we experimented our approach with the same embedding size as GLAD and GCE, which is of dimension 400. In this case G-SAT achieved 88.6 and 86.7 on the dev and test on English, respectively, still outperforming GLAD (dev:88.4, test:84.6) and GCE (dev:89.0, test:85.1)."]}
{"question_id": "7a60f29e28063f50c2a7afd1c2a7668fb615cd53", "predicted_answer": "", "predicted_evidence": ["Recent research has focused on jointly modeling the SLU and the DST BIBREF4, BIBREF5. For such joint models, deep neural network techniques have been the choice of use because of their proven ability to extract features from a given input and their generalization capability BIBREF4, BIBREF5, BIBREF6, BIBREF7. Following this research line, BIBREF4 proposed a word-based DST (based on a delexicalisation approach) that jointly models SLU and DST, and directly maps from the utterances to an updated belief state. BIBREF6 proposed a data-driven approach for DST, named neural belied tracker (NBT), which learns a vector representation for each slot-value pair and compares them with the vector representation of the user utterance to predict if the user has expressed the corresponding slot-value pair. The NBT model uses pre-trained semantic embeddings to train a model without semantic lexicon.", "Table TABREF33 shows the joint goal performance of the models on both the development and test data for three different languages. We can see that our model (G-SAT) outperforms both GLAD and GCE on the three languages of the WOZ2.0 dataset when no pre-trained resources are available, and that the model performance is consistent across both the development and the test data.", "The proposed approach, G-SAT (Global encoder and Slot-Attentive decoders), is designed to predict slot value pairs for a given turn in the dialogue. For a dialogue turn, given the user utterance $U$, the previous system action $A$ and the value set $V_s$ for slot $s \\in S$, the proposed model provides a probability distribution over slot-value set $V_s$.", "Since our model uses a shared encoder for all slots, the outputs of the encoder $h_t$ and $h_L$ are used by slot specific classifiers for prediction on corresponding slots.", "The encoder of the model is shared across all slots and a separate classifier is defined for each slot. The number of hidden units of the LSTM is set to 64 and a dropout of 0.2 is applied between different layers. We use Adam optimizer with a learning rate of 0.001. The embedding dimension of the default model is set to 128, and embeddings are learned during training. In order to have a fair comparison with other models that use pre-trained embeddings, we also experiment our approach using pre-trained GloVe embeddings (of dimension 300) BIBREF19, and character n-gram embeddings (of dimension 100) BIBREF20 as used in GLAD, leading to embedding of size 400. The turn-level predictions are accumulated forward through the dialogue and the goal for slot $s$ is None until it is predicted as value $v$ by the model. The implemented model is experimented with 10 different random initializations for each language, and the scores reported in Section SECREF6 are the mean and standard deviation obtained in the experiments."]}
{"question_id": "6371c6863fe9a14bf67560e754ce531d70de10ab", "predicted_answer": "", "predicted_evidence": ["The overall statistics of ReClor and comparison with other similar multiple-choice MRC datasets are summarized in Table TABREF9. As shown, ReClor is of comparable size and relatively large vocabulary size. Compared with RACE, the length of the context of ReCor is much shorter. In RACE, there are many redundant sentences in context to answer a question. However, in ReClor, every sentence in the context passages is important, which makes this dataset focus on evaluating the logical reasoning ability of models rather than the ability to extract relevant information from a long context. The length of answer options of ReClor is largest among these datasets. We analyze and manually annotate the types of questions on the testing set and group them into 17 categories, whose percentages and descriptions are shown in Table TABREF11. The percentages of different types of questions reflect those in the logical reasoning module of GMAT and LSAT. Some examples of different types of logical reasoning are listed in Figure FIGREF12, and more examples are listed in the Appendix .", "Logical Reasoning in NLP. There are several tasks and datasets introduced to investigate logical reasoning in NLP. The task of natural language inference, also known as recognizing textual entailment BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30 requires models to take a pair of sentence as input and classify their relationship types, i.e., Entailment, Neutral, or Contradiction. SNLI BIBREF31 and MultiNLI BIBREF32 datasets are proposed for this task. However, this task only focuses on sentence-level logical relationship reasoning and the relationships are limited to only a few types. Another task related to logical reasoning in NLP is argument reasoning comprehension task introduced by BIBREF33 with a dataset of this task. Given an argument with a claim and a premise, this task aims to select the correct implicit warrant from two options. Although the task is on passage-level logical reasoning, it is limited to only one logical reasoning type, i.e., identifying warrants.", "We construct a dataset containing 6,138 logical reasoning questions sourced from open websites and books. In the original problems, there are five answer options in which only one is right. To comply with fair use of law, we shuffle the order of answer options and randomly delete one of the wrong options for each data point, which results in four options with one right option and three wrong options. Furthermore, similar to ImageNet dataset, ReClor is available for non-commercial research purpose only. We are also hosting a public evaluation server on EvalAI BIBREF37 to benchmark progress on Reclor.", "As mentioned earlier, biases prevalently exist in human-annotated datasets BIBREF16, BIBREF17, BIBREF18, BIBREF42, which are often exploited by models to perform well without truly understanding the text. Therefore, it is necessary to find out the biased data points in ReClor in order to evaluate models in a more comprehensive manner BIBREF43. To this end, we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question.", "As mentioned above, we collect 6,138 data points, in which 91.22% are from actual exams of GMAT and LSAT while others are from high-quality practice exams. They are divided into training set, validation set and testing set with 4,638, 500 and 1,000 data points respectively. The overall statistics of ReClor and comparison with other similar multiple-choice MRC datasets are summarized in Table TABREF9. As shown, ReClor is of comparable size and relatively large vocabulary size. Compared with RACE, the length of the context of ReCor is much shorter. In RACE, there are many redundant sentences in context to answer a question. However, in ReClor, every sentence in the context passages is important, which makes this dataset focus on evaluating the logical reasoning ability of models rather than the ability to extract relevant information from a long context."]}
{"question_id": "28a8a1542b45f67674a2f1d54fff7a1e45bfad66", "predicted_answer": "", "predicted_evidence": ["To this end, we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question. However, the setting of this task is a multiple-choice question with 4 probable options, and even a chance baseline could have 25% probability to get it right. To eliminate the effect of random guess, we set four different random seeds for each model and pick the data points that are predicted correctly in all four cases to form the EASY set. Then, the data points which are predicted correctly by the models at random could be nearly eliminated, since any data point only has a probability of $(25\\%)^{4}=0.39\\%$ to be guessed right consecutively for four times.", "Reading Comprehension Datasets. A variety of reading comprehension datasets have been introduced to promote the development of this field. MCTest BIBREF10 is a dataset with 2,000 multiple-choice reading comprehension questions about fictional stories in the format similar to ReClor. BIBREF4 proposed SQuAD dataset, which contains 107,785 question-answer pairs on 536 Wikipedia articles. The authors manually labeled 192 examples of the dataset and found that the examples mainly require reasoning of lexical or syntactic variation. In an analysis of the above-mentioned datasets, BIBREF11 found that none of questions requiring logical reasoning in MCTest dataset BIBREF10 and only 1.2% in SQuAD dataset BIBREF4. BIBREF5 introduced RACE dataset by collecting the English exams for middle and high school Chinese students in the age range between 12 to 18. They hired crowd workers on Amazon Mechanical Turk to label the reasoning type of 500 samples in the dataset and show that around 70 % of the samples are in the category of word matching, paraphrasing or single-sentence reasoning.", "On HARD set, the three models perform poorly on certain types such as Strengthen, Weaken and Role which require extensive logical reasoning. However, they perform relatively better on other certain types, such as Conclusion/Main Point and Match Structures that are more straight-forward. For the result of transfer learning, we analyze XLNet$_{\\small \\textsc {LARGE}}$ in detail. Though the overall performance is significantly boosted after fine-tuning on RACE first, the histograms in the bottom of Figure FIGREF22 show that on EASY set, accuracy of the model with fine-tuning on RACE is similar to that without it among most question types, while on HARD set, significant improvement on some question types is observed, such as Conclusion/Main Point and Most Strongly Supported. This may be because these types require less logical reasoning to some extent compared with other types, and similar question types may also be found in RACE dataset. Thus, the pre-training on RACE helps enhance the ability of logical reasoning especially of relatively simple reasoning types, but more methods are still needed to further enhance the ability especially that of relatively complex reasoning types.", "", "They hired crowd workers on Amazon Mechanical Turk to label the reasoning type of 500 samples in the dataset and show that around 70 % of the samples are in the category of word matching, paraphrasing or single-sentence reasoning. To encourage progress on deeper comprehension of language, more reading comprehension datasets requiring more complicated reasoning types are introduced, such as iterative reasoning about the narrative of a story BIBREF20, multi-hop reasoning across multiple sentences BIBREF21 and multiple documents BIBREF22, commonsense knowledge reasoning BIBREF23, BIBREF24, BIBREF25 and numerical discrete reasoning over paragraphs BIBREF8. However, to the best of our knowledge, although there are some datasets targeting logical reasoning in other NLP tasks mentioned in the next section, there is no dataset targeting evaluating logical reasoning in reading comprehension task. This work introduces a new dataset to fill this gap."]}
{"question_id": "539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2", "predicted_answer": "", "predicted_evidence": ["We propose a hybrid TN system as in Fig. FIGREF3, which combines the rule-based model and a neural model to make up the shortcomings of one another. The system inputs are raw texts. The NSW are first extracted from the original text using regular expressions. We only extract NSW that are digit and symbol related, and other NSW like English abbreviations will be processed in the rule-based model. Then the system performs a priority check on the NSW, and all matched strings will be sent into the rule-based model. The priority patterns include definite NSW such as \u201c911\u201d and other user-defined strings. Then the remaining patterns are passed through the neural model to be classified into one of the pattern groups. Before normalizing the classified NSW in the pattern reader, the format of each classified NSW is checked with regular expressions, and the illegal ones will be filtered back to the rule-based system. For example, classifying \u201c10%\u201d to read as year is illegal.", "FIGREF3, which combines the rule-based model and a neural model to make up the shortcomings of one another. The system inputs are raw texts. The NSW are first extracted from the original text using regular expressions. We only extract NSW that are digit and symbol related, and other NSW like English abbreviations will be processed in the rule-based model. Then the system performs a priority check on the NSW, and all matched strings will be sent into the rule-based model. The priority patterns include definite NSW such as \u201c911\u201d and other user-defined strings. Then the remaining patterns are passed through the neural model to be classified into one of the pattern groups. Before normalizing the classified NSW in the pattern reader, the format of each classified NSW is checked with regular expressions, and the illegal ones will be filtered back to the rule-based system. For example, classifying \u201c10%\u201d to read as year is illegal. In the pattern reader, each pattern label has a unique process function to perform the NSW-SFW transformation.", "The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units. However, improving the performance of this system on more general cases becomes a bottleneck.", "Multi-head self-attention was proposed by GoogleBIBREF12 in the model of transformer. The model uses self-attention in the encoder and decoder and encoder-decoder attention in between. Motivated by the structure of transformer, multi-head self-attention is adopted in our neural model and the structure is shown in Fig. FIGREF4. Compared with other modules like LSTM and GRU, self-attention can efficiently extract the information of the NSW with all context in parallel and is fast to train. The core part of the neural model is similar to the encoder of a transformer. The inputs of the model are the sentences with their manually labeled NSW. We take a 30-character context window around each NSW and send it to the embedding layer. Padding is used when the window exceeds the sentence range. After 8 heads of self-attention, the model outputs a vector with dimension of the number of patterns. Finally, the highest masked softmax probability is chosen as the classified pattern group.", "The system inputs are raw texts. The NSW are first extracted from the original text using regular expressions. We only extract NSW that are digit and symbol related, and other NSW like English abbreviations will be processed in the rule-based model. Then the system performs a priority check on the NSW, and all matched strings will be sent into the rule-based model. The priority patterns include definite NSW such as \u201c911\u201d and other user-defined strings. Then the remaining patterns are passed through the neural model to be classified into one of the pattern groups. Before normalizing the classified NSW in the pattern reader, the format of each classified NSW is checked with regular expressions, and the illegal ones will be filtered back to the rule-based system. For example, classifying \u201c10%\u201d to read as year is illegal. In the pattern reader, each pattern label has a unique process function to perform the NSW-SFW transformation. Finally, all of the normalized SFW are inserted back to the text segmentations to form the output sentences."]}
{"question_id": "aa7c5386aedfb13a361a2629b67cb54277e208d2", "predicted_answer": "", "predicted_evidence": ["It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units. However, improving the performance of this system on more general cases becomes a bottleneck. For example, in a report of a football game, it cannot transform \u201c1-3\u201d to score if there are no keywords like \u201cscore\u201d or \u201cgame\u201d close to it.", "The model uses self-attention in the encoder and decoder and encoder-decoder attention in between. Motivated by the structure of transformer, multi-head self-attention is adopted in our neural model and the structure is shown in Fig. FIGREF4. Compared with other modules like LSTM and GRU, self-attention can efficiently extract the information of the NSW with all context in parallel and is fast to train. The core part of the neural model is similar to the encoder of a transformer. The inputs of the model are the sentences with their manually labeled NSW. We take a 30-character context window around each NSW and send it to the embedding layer. Padding is used when the window exceeds the sentence range. After 8 heads of self-attention, the model outputs a vector with dimension of the number of patterns. Finally, the highest masked softmax probability is chosen as the classified pattern group. The mask uses a regular expression to check if the NSW contain symbols and filters illegal ones such as classifying \u201c12:00\u201d as pure number, which is like a bi-class classification before softmax is applied.", "Imbalanced dataset is a challenge for the task because the top patterns are taking too much attention so that most weights might be determined by the easier ones. We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2. The loss function helps the model to focus on harder cases in different classes and therefore reduce the impact of the imbalanced data. The experimental results are in SECREF11.", "For the loss function, in order to solve the problem of imbalanced dataset, which will be talked about in SECREF7, the final selection of the loss function is motivated by BIBREF13:", "Currently, based on the traditional taxonomy approach for NSWBIBREF0, the Mandarin TN tasks are generally resolved by rule-based systems which use keywords and regular expressions to determine the SFW of ambiguous wordsBIBREF1, BIBREF2. These systems typically classify NSW into different pattern groups, such as abbreviations, numbers, etc., and then into sub-groups, such as phone number, year, etc., which has corresponding NSW-SFW transformations. ZhouBIBREF3 and JiaBIBREF4 proposed systems which use maximum entropy (ME) to further disambiguate the NSW with multiple pattern matches. For the NSW given the context constraints, the highest probability corresponds to the highest entropy. LiouBIBREF5 proposed a system of data-driven models which combines a rule-based and a keyword-based TN module. The second module classifies preceding and following words around the keywords and then trains a CRF model to predict the NSW patterns based on the classification results."]}
{"question_id": "9b3371dcd855f1d3342edb212efa39dfc9142ae3", "predicted_answer": "", "predicted_evidence": ["The model uses self-attention in the encoder and decoder and encoder-decoder attention in between. Motivated by the structure of transformer, multi-head self-attention is adopted in our neural model and the structure is shown in Fig. FIGREF4. Compared with other modules like LSTM and GRU, self-attention can efficiently extract the information of the NSW with all context in parallel and is fast to train. The core part of the neural model is similar to the encoder of a transformer. The inputs of the model are the sentences with their manually labeled NSW. We take a 30-character context window around each NSW and send it to the embedding layer. Padding is used when the window exceeds the sentence range. After 8 heads of self-attention, the model outputs a vector with dimension of the number of patterns. Finally, the highest masked softmax probability is chosen as the classified pattern group. The mask uses a regular expression to check if the NSW contain symbols and filters illegal ones such as classifying \u201c12:00\u201d as pure number, which is like a bi-class classification before softmax is applied.", "The future work includes other aspects of model explorations. Mandarin word segmentation methods will be applied to replace the character-wise embedding with word-level embedding. More sequence learning models and attention mechanisms will be experimented. And more labeled dataset in other corpus will be supplemented for training.", "The training data is split into 36 different classes, each of which has its own NSW-SFW transformation. The distribution of the dataset is the same with the NSW in our internal news corpus and is imbalanced, which is one of the challenges for our neural model. The approaches to deal with the imbalanced dataset are discussed in the next section.", "The paper is organized as follows. Section SECREF2 introduces the detailed structure of the proposed hybrid system and its training and inference. In Section SECREF3, the performance of different system configurations is evaluated on different datasets. And the conclusion is given in Section SECREF4.", "Our experiments produce similar errors. For example, \u201cPodnieks, Andrew 2000\u201d is transformed to \u201cPodncourt, Andrew Two Thousand\u201d, changing \u201cPodnieks\u201d to \u201cPodncourt\u201d. These errors cannot be detected by the model itself. In BIBREF11, rules are applied to two specific categories to resolve silly errors, but this method is hard to apply to all cases. Another challenge in Mandarin is the word segmentation since words are not separated by spaces and the segmentation could depend on the context. Besides, some NSW may have more than one SFW in Mandarin, making the seq2seq model hard to train. For example, \u201cUTF8gbsn\u4e24\u5343\u96f6\u516b\u5e74\u201d and \u201cUTF8gbsn\u4e8c\u96f6\u96f6\u516b\u5e74\u201d are both acceptable SFW for \u201c2008UTF8gbsn\u5e74\u201d. The motivation of this paper is to combine the advantages of a rule-based model for its flexibility and a neural model to enhance the performance on more general cases."]}
{"question_id": "b02a6f59270b8c55fa4df3751bcb66fca2371451", "predicted_answer": "", "predicted_evidence": ["For recent NLP studies, sequence-to-sequence (seq2seq) models have achieved impressive progress in TN tasks in English and RussianBIBREF8, BIBREF9. Seq2seq models typically encode sequences into a state vector, which is decoded into an output vector from its learnt vector representation and then to a sequence. Different seq2seq models with bi-LSTM, bi-GRU with attention are proposed in BIBREF9, BIBREF10. Zhang and Sproat proposed a contextual seq2seq model, which uses a sliding-window and RNN with attentionBIBREF8. In this model, bi-directional GRU is used in both encoder and decoder, and the context words are labeled with \u201c$\\langle $self$\\rangle $\u201d, helping the model distinguish the NSW and the context.", "The contributions of this paper include the following. First, this is the first known TN system for Mandarin which uses a neural model with multi-head self-attention. Second, we propose a hybrid system combining a rule-based model and a neural model. Third, we experiment with different approaches to deal with imbalanced dataset in the TN task.", "However, seq2seq models have several downsides when directly applied in Mandarin TN tasks. As mentioned in BIBREF8, the sequence output directly from a seq2seq model can lead to unrecoverable errors. The model sometimes changes the context words which should be kept the same. Our experiments produce similar errors. For example, \u201cPodnieks, Andrew 2000\u201d is transformed to \u201cPodncourt, Andrew Two Thousand\u201d, changing \u201cPodnieks\u201d to \u201cPodncourt\u201d. These errors cannot be detected by the model itself. In BIBREF11, rules are applied to two specific categories to resolve silly errors, but this method is hard to apply to all cases. Another challenge in Mandarin is the word segmentation since words are not separated by spaces and the segmentation could depend on the context. Besides, some NSW may have more than one SFW in Mandarin, making the seq2seq model hard to train.", "The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units. However, improving the performance of this system on more general cases becomes a bottleneck.", "The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units."]}
{"question_id": "3a3c372b6d73995adbdfa26103c85b32d071ff10", "predicted_answer": "", "predicted_evidence": ["These errors cannot be detected by the model itself. In BIBREF11, rules are applied to two specific categories to resolve silly errors, but this method is hard to apply to all cases. Another challenge in Mandarin is the word segmentation since words are not separated by spaces and the segmentation could depend on the context. Besides, some NSW may have more than one SFW in Mandarin, making the seq2seq model hard to train. For example, \u201cUTF8gbsn\u4e24\u5343\u96f6\u516b\u5e74\u201d and \u201cUTF8gbsn\u4e8c\u96f6\u96f6\u516b\u5e74\u201d are both acceptable SFW for \u201c2008UTF8gbsn\u5e74\u201d. The motivation of this paper is to combine the advantages of a rule-based model for its flexibility and a neural model to enhance the performance on more general cases. To avoid the problems of seq2seq models, we consider the TN task as a multi-class classification problem with carefully designed patterns for the neural model.", "The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units. However, improving the performance of this system on more general cases becomes a bottleneck.", "These systems typically classify NSW into different pattern groups, such as abbreviations, numbers, etc., and then into sub-groups, such as phone number, year, etc., which has corresponding NSW-SFW transformations. ZhouBIBREF3 and JiaBIBREF4 proposed systems which use maximum entropy (ME) to further disambiguate the NSW with multiple pattern matches. For the NSW given the context constraints, the highest probability corresponds to the highest entropy. LiouBIBREF5 proposed a system of data-driven models which combines a rule-based and a keyword-based TN module. The second module classifies preceding and following words around the keywords and then trains a CRF model to predict the NSW patterns based on the classification results. There are some other hybrid systemsBIBREF6, BIBREF7 which use NLP models and rules separately to help normalize hard cases in TN.", "The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units.", "However, seq2seq models have several downsides when directly applied in Mandarin TN tasks. As mentioned in BIBREF8, the sequence output directly from a seq2seq model can lead to unrecoverable errors. The model sometimes changes the context words which should be kept the same. Our experiments produce similar errors. For example, \u201cPodnieks, Andrew 2000\u201d is transformed to \u201cPodncourt, Andrew Two Thousand\u201d, changing \u201cPodnieks\u201d to \u201cPodncourt\u201d. These errors cannot be detected by the model itself. In BIBREF11, rules are applied to two specific categories to resolve silly errors, but this method is hard to apply to all cases. Another challenge in Mandarin is the word segmentation since words are not separated by spaces and the segmentation could depend on the context. Besides, some NSW may have more than one SFW in Mandarin, making the seq2seq model hard to train. For example, \u201cUTF8gbsn\u4e24\u5343\u96f6\u516b\u5e74\u201d and \u201cUTF8gbsn\u4e8c\u96f6\u96f6\u516b\u5e74\u201d are both acceptable SFW for \u201c2008UTF8gbsn\u5e74\u201d."]}
{"question_id": "952fe4fbf4e0bcfcf44fab2dbd3ed85dd961eff3", "predicted_answer": "", "predicted_evidence": ["Instances of the categories of HASHTAG-LIKE and CONTRACTED are observed in 38 and 35 names, respectively. A sample name variant marked with HASHTAG-LIKE is SabriSar\u0131o\u011flu where this person name should have been written as Sabri Sar\u0131o\u011flu. A contracted name instance in the dataset is Diyanet which is an organization name with the correct open form of Diyanet \u0130\u015fleri Ba\u015fkanl\u0131\u011f\u0131.", "We have annotated the PLOs in the tweet dataset (already-annotated for named entities as described in BIBREF5) with the name variant category labels of WELL-FORMED, ABBREVIATION, CAPITALIZATION, DIACRITICS, HASHTAG-LIKE, CONTRACTED, HYPOCORISM, and ERROR, as described in the previous subsection. Although there are 980 PLOs in the dataset, since 44 names have two name variant category labels, the total number of name variant annotations is 1,024.", "The name variant annotations described in the study are made publicly available at https://github.com/dkucuk/Name-Variants-Turkish-Tweets as a text file, for research purposes. Each line in the annotation file denotes triplets, separated by semicolons. The first item in each triplet is the tweet id, the second item is another triplet denoting the already-existing named entity boundaries and type, and the final item is a comma-separated list of name variant annotations for the named entity under consideration. Below provided are two sample lines from the annotation file. The first line indicates a person name (between the non-white-space characters of 0 and 11 in the tweet text) annotated with CAPITALIZATION category, as it lacks proper capitalization. The second line denotes an organization name (between the non-white-space characters of 0 and 19 in the tweet) which has issues related to characters with diacritics and proper capitalization.", "360731728177922048;0,11,PERSON;CAPITALIZATION", "The instances of HYPOCORISM and ERROR are comparatively low, where 10 instances of hyprocorism and 11 instances of other errors are seen in the dataset. An instance of the former category is Nazl\u0131\u015f which is a hypocoristic use of the female person name Nazl\u0131. An instance of the ERROR category is the use of FENEBAH\u00c7E instead of the correct sports club name FENERBAH\u00c7E."]}
{"question_id": "1dc5bf9dca7de2ba21db10e9056d3906267ef5d5", "predicted_answer": "", "predicted_evidence": ["We have annotated the PLOs in the tweet dataset (already-annotated for named entities as described in BIBREF5) with the name variant category labels of WELL-FORMED, ABBREVIATION, CAPITALIZATION, DIACRITICS, HASHTAG-LIKE, CONTRACTED, HYPOCORISM, and ERROR, as described in the previous subsection. Although there are 980 PLOs in the dataset, since 44 names have two name variant category labels, the total number of name variant annotations is 1,024.", "Instances of the categories of HASHTAG-LIKE and CONTRACTED are observed in 38 and 35 names, respectively. A sample name variant marked with HASHTAG-LIKE is SabriSar\u0131o\u011flu where this person name should have been written as Sabri Sar\u0131o\u011flu. A contracted name instance in the dataset is Diyanet which is an organization name with the correct open form of Diyanet \u0130\u015fleri Ba\u015fkanl\u0131\u011f\u0131.", "The rest of the paper is organized as follows: In Section 2, an analysis of the named entities in the publicly-available Turkish tweet dataset with respect to their being name variants or not is presented together with the descriptions of name variant categories. In Section 3, details and samples of the related finer-grained annotations of named entities are described and Section 4 concludes the paper with a summary of main points.", "The number of names having issues about characters with diacritics is 45, and similarly there are 45 abbreviations (of mostly organization names) in the dataset. As examples of names having issues with diacritics, people use Kutahya istead of the correct form K\u00fctahya, and similarly Besiktas instead of Be\u015fikta\u015f. Abbreviations in the dataset include national corporations like TRT and SGK, and international organizations like UEFA.", "HYPOCORISM: Hypocorism or hypocoristic use is defined as the phenomenon of deliberately modifying a name, in the forms of nicknames, diminutives, and terms of endearment, to show familiarity and affection BIBREF8, BIBREF9. An example hypocoristic use in English is using Bobby instead of the name Bob BIBREF8. Such name variants observed in the tweet dataset are marked with the category label of HYPOCORISM."]}
{"question_id": "8faec509406d33444bd620afc829adc9eae97644", "predicted_answer": "", "predicted_evidence": ["Automatic extraction and classification of named entities in natural language texts (i.e., named entity recognition (NER)) is a significant topic of natural language processing (NLP), both as a stand-alone research problem and as a subproblem to facilitate solutions of other related NLP problems. NER has been studied for a long time and in different domains, and there are several survey papers on NER including BIBREF0.", "Conducting NLP research (such as NER) on microblog texts like tweets poses further challenges, due to the particular nature of this text genre. Contractions, writing/grammatical errors, and deliberate distortions of words are common in this informal text genre which is produced with character limitations and published without a formal review process before publication. There are several studies that propose tweet normalization schemes BIBREF1 to alleviate the negative effects of such language use in microblogs, for the other NLP tasks to be performed on the normalized microblogs thereafter. Yet, particularly regarding Turkish content, a related study on NER on Turkish tweets BIBREF2 claims that normalization before the actual NER procedure on tweets may not guarantee improved NER performance.", "The analysis leads to a breakdown of different named entity variants into eight categories. Although about 60% of the names are in their correct and canonical forms, about 40% of them either appear as abbreviations or suffer from a deviation from the standard form due to multiple reasons including violations of the writing rules of the language. Hence, it provides an insight about the extent of the use of different name variants as named entities in Turkish tweets.", "360731728177922048;0,11,PERSON;CAPITALIZATION", "The number of names having issues about characters with diacritics is 45, and similarly there are 45 abbreviations (of mostly organization names) in the dataset. As examples of names having issues with diacritics, people use Kutahya istead of the correct form K\u00fctahya, and similarly Besiktas instead of Be\u015fikta\u015f. Abbreviations in the dataset include national corporations like TRT and SGK, and international organizations like UEFA."]}
{"question_id": "e3c2b6fcf77a7b1c76add2e6e1420d07c29996ea", "predicted_answer": "", "predicted_evidence": ["While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: Sutskever2014 used a 4-layer LSTM with 1000 hidden units per layer (herein INLINEFORM0 ) and Zhou2016 obtained state-of-the-art results on English INLINEFORM1 French with a 16-layer LSTM with 512 units per layer. The sheer size of the models requires cutting-edge hardware for training and makes using the models on standard setups very challenging.", "For models trained with word-level knowledge distillation, we also tried regressing the student network's top-most hidden layer at each time step to the teacher network's top-most hidden layer as a pretraining step, noting that Romero2015 obtained improvements with a similar technique on feed-forward models. We found this to give comparable results to standard knowledge distillation and hence did not pursue this further.", "Finally, although past work has shown that models with lower perplexity generally tend to have higher BLEU, our results indicate that this is not necessarily the case. The perplexity of the baseline INLINEFORM0 English INLINEFORM1 German model is INLINEFORM2 while the perplexity of the corresponding Seq-KD model is INLINEFORM3 , despite the fact that Seq-KD model does significantly better for both greedy ( INLINEFORM4 BLEU) and beam search ( INLINEFORM5 BLEU) decoding.", "Let INLINEFORM0 and INLINEFORM1 be (random variable sequences representing) the source/target sentence, with INLINEFORM2 and INLINEFORM3 respectively being the source/target lengths. Machine translation involves finding the most probable target sentence given the source: INLINEFORM4", "where INLINEFORM0 is the observed sequence. Of course, this just shows that from a negative log likelihood perspective, minimizing word-level NLL and sequence-level NLL are equivalent in this model."]}
{"question_id": "ee2c2fb01d67f4c58855bf23186cbd45cecbfa56", "predicted_answer": "", "predicted_evidence": ["Since the second term is intractable, we could again apply the mode approximation from the previous section, INLINEFORM0", "Note that INLINEFORM0 is inherently different from INLINEFORM1 , as the sum is over an exponential number of terms. Despite its intractability, we posit that this sequence-level objective is worthwhile. It gives the teacher the chance to assign probabilities to complete sequences and therefore transfer a broader range of knowledge. We thus consider an approximation of this objective.", "where INLINEFORM0 is the target vocabulary set. The student can further be trained to optimize the mixture of INLINEFORM1 and INLINEFORM2 . In the context of NMT, we refer to this approach as word-level knowledge distillation and illustrate this in Figure 1 (left).", "We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher's mode) instead of `wasting' parameters on trying to model the entire space of translations. Our results suggest that this is indeed the case: the probability mass that Seq-KD models assign to the approximate mode is much higher than is the case for baseline models trained on original data (Table 1: INLINEFORM0 ). For example, on English INLINEFORM1 German the (approximate) INLINEFORM2 for the INLINEFORM3 Seq-KD model (on average) accounts for INLINEFORM4 of the total probability mass, while the corresponding number is INLINEFORM5 for the baseline. This also explains the success of greedy decoding for Seq-KD models\u2014since we are only modeling around the teacher's mode, the student's distribution is more peaked and therefore the INLINEFORM6 is much easier to find.", "Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses, INLINEFORM0"]}
{"question_id": "f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3", "predicted_answer": "", "predicted_evidence": ["Since the second term is intractable, we could again apply the mode approximation from the previous section, INLINEFORM0", "Other approaches for compression involve low rank factorizations of weight matrices BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , sparsity-inducing regularizers BIBREF24 , binarization of weights BIBREF25 , BIBREF26 , and weight sharing BIBREF27 , BIBREF9 . Finally, although we have motivated sequence-level knowledge distillation in the context of training a smaller model, there are other techniques that train on a mixture of the model's predictions and the data, such as local updating BIBREF12 , hope/fear training BIBREF28 , SEARN BIBREF29 , DAgger BIBREF30 , and minimum risk training BIBREF31 , BIBREF32 .", "There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 . Combining such methods with knowledge distillation/pruning to further reduce the memory footprint of NMT systems remains an avenue for future work.", "Concretely, assume we are learning a multi-class classifier over a data set of examples of the form INLINEFORM0 with possible classes INLINEFORM1 . The usual training criteria is to minimize NLL for each example from the training data, INLINEFORM2", "Word-level knowledge distillation allows transfer of these local word distributions. Ideally however, we would like the student model to mimic the teacher's actions at the sequence-level. The sequence distribution is particularly important for NMT, because wrong predictions can propagate forward at test-time."]}
{"question_id": "a0197894ee94b01766fa2051f50f84e16b5c9370", "predicted_answer": "", "predicted_evidence": ["The English-German data comes from WMT 2014. The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a INLINEFORM0 LSTM (as in Luong2015) and we train two student models: INLINEFORM1 and INLINEFORM2 . The Thai-English data comes from IWSLT 2015. There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k. Size of the teacher model is INLINEFORM3 (which performed better than INLINEFORM4 , INLINEFORM5 models), and the student model is INLINEFORM6 .", "Since the second term is intractable, we could again apply the mode approximation from the previous section, INLINEFORM0", "The English-German data comes from WMT 2014. The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a INLINEFORM0 LSTM (as in Luong2015) and we train two student models: INLINEFORM1 and INLINEFORM2 . The Thai-English data comes from IWSLT 2015. There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k.", "Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses, INLINEFORM0", "Neural machine translation (NMT) BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 is a deep learning-based method for translation that has recently shown promising results as an alternative to statistical approaches. NMT systems directly model the probability of the next word in the target sentence simply by conditioning a recurrent neural network on the source sentence and previously generated target words."]}
{"question_id": "55bafa0f7394163f4afd1d73340aac94c2d9f36c", "predicted_answer": "", "predicted_evidence": ["We observed the unstructured inference mainly helps for two classes of questions: (1) questions involving aggregation operations (Questions 1-3); (2) questions involving sub-lexical compositionally (Questions 4-5). Questions 1 and 2 contain the predicate $largest$ an aggregation operator. A semantic parsing method should explicitly handle this predicate to trigger $max(.)$ operator. For Question 3, structured inference predicts the Freebase relation fb:teams..from retrieving all the years in which Ray Allen has played basketball. Note that Ray Allen has joined Connecticut University's team in 1993 and NBA from 1996. To answer this question a semantic parsing system would require a min( $\\cdot $ ) operator along with an additional constraint that the year corresponds to the NBA's term. Interestingly, without having to explicitly model these complex predicates, the unstructured inference helps in answering these questions more accurately. Questions 4-5 involve sub-lexical compositionally BIBREF25 predicates father and college.", "Suppose the pair $(e_{gold}, r_{gold})$ represents the gold entity/relation pair for a question $q$ . We take all our entity and relation predictions for $q$ , create a list of entity and relation pairs $\\lbrace (e_{0}, r_{0}), (e_{1}, r_{1}), ..., (e_{n}, r_{n})\\rbrace $ from $q$ and rank them using an svm rank classifier BIBREF21 which is trained to predict a rank for each pair. Ideally higher rank indicates the prediction is closer to the gold prediction. For training, svm rank classifier requires a ranked or scored list of entity-relation pairs as input. We create the training data containing ranked input pairs as follows: if both $e_{pred} = e_{gold}$ and $r_{pred} = r_{gold}$ , we assign it with a score of 3.", "Since the advent of large structured knowledge bases (KBs) like Freebase BIBREF0 , YAGO BIBREF1 and DBpedia BIBREF2 , answering natural language questions using those structured KBs, also known as KB-based question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities.", "Since this dataset contains only question-answer pairs and annotated topic entities, instead of relying on gold relations we rely on surrogate gold relations which produce answers that have the highest overlap with gold answers. Specifically, for a given question, we first locate the topic entity $e$ in the Freebase graph, then select 1-hop and 2-hop relations connected to the topic entity as relation candidates. The 2-hop relations refer to the $n$ -ary relations of Freebase, i.e., first hop from the subject to a mediator node, and the second from the mediator to the object node. For each relation candidate $r$ , we issue the query ( $e$ , $r$ , $?$ ) to the KB, and label the relation that produces the answer with minimal $F_1$ -loss against the gold answer, as the surrogate gold relation. From the training set, we collect 461 relations to train the MCCNN, and the target prediction during testing time is over these relations.", "In this section we introduce the experimental setup, the main results and detailed analysis of our system."]}
{"question_id": "cbb4eba59434d596749408be5b923efda7560890", "predicted_answer": "", "predicted_evidence": ["Our work also intersects with relation extraction methods. While these methods aim to predict a relation between two entities in order to populate KBs BIBREF46 , BIBREF47 , BIBREF48 , we work with sentence level relation extraction for question answering. krishnamurthy2012weakly and fader2014open adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries. Closest to our extraction method is yao-jacana-freebase-acl2014 and yao-scratch-qa-naacl2015 who also uses sentence level relation extraction for QA. Unlike them, we can predict multiple relations per question, and our MCCNN architecture is more robust to unseen contexts compared to their logistic regression models.", "Since we pipeline structured inference first and then unstructured inference, our method is limited by the coverage of Freebase. Our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.", "dong-EtAl:2015:ACL-IJCNLP1 were the first to use MCCNN for question answering. Yet our approach is very different in spirit to theirs. Dong et al. aim to maximize the similarity between the distributed representation of a question and its answer entities, whereas our network aims to predict Freebase relations. Our search space is several times smaller than theirs since we do not require potential answer entities beforehand (the number of relations is much smaller than the number of entities in Freebase). In addition, our method can explicitly handle compositional questions involving multiple relations, whereas Dong et al. learn latent representation of relation joins which is difficult to comprehend. Moreover, we outperform their method by 7 points even without unstructured inference.", "Questions 1 and 2 contain the predicate $largest$ an aggregation operator. A semantic parsing method should explicitly handle this predicate to trigger $max(.)$ operator. For Question 3, structured inference predicts the Freebase relation fb:teams..from retrieving all the years in which Ray Allen has played basketball. Note that Ray Allen has joined Connecticut University's team in 1993 and NBA from 1996. To answer this question a semantic parsing system would require a min( $\\cdot $ ) operator along with an additional constraint that the year corresponds to the NBA's term. Interestingly, without having to explicitly model these complex predicates, the unstructured inference helps in answering these questions more accurately. Questions 4-5 involve sub-lexical compositionally BIBREF25 predicates father and college. For example in Question 5, the user queries for the colleges that John Steinbeck attended.", "We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction."]}
{"question_id": "1d9d7c96c5e826ac06741eb40e89fca6b4b022bd", "predicted_answer": "", "predicted_evidence": ["For each question, we use hand-built sequences of part-of-speech categories to identify all possible named entity mention spans, e.g., the sequence NN (shaq) may indicate an entity. For each mention span, we use the entity linking tool S-MART BIBREF15 to retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity.", "dong-EtAl:2015:ACL-IJCNLP1 were the first to use MCCNN for question answering. Yet our approach is very different in spirit to theirs. Dong et al. aim to maximize the similarity between the distributed representation of a question and its answer entities, whereas our network aims to predict Freebase relations. Our search space is several times smaller than theirs since we do not require potential answer entities beforehand (the number of relations is much smaller than the number of entities in Freebase). In addition, our method can explicitly handle compositional questions involving multiple relations, whereas Dong et al. learn latent representation of relation joins which is difficult to comprehend. Moreover, we outperform their method by 7 points even without unstructured inference.", "Ideally higher rank indicates the prediction is closer to the gold prediction. For training, svm rank classifier requires a ranked or scored list of entity-relation pairs as input. We create the training data containing ranked input pairs as follows: if both $e_{pred} = e_{gold}$ and $r_{pred} = r_{gold}$ , we assign it with a score of 3. If only the entity or relation equals to the gold one (i.e., $e_{pred}=e_{gold}$ , $r_{pred}\\ne r_{gold}$ or $e_{pred}\\ne e_{gold}$ , $q$0 ), we assign a score of 2 (encouraging partial overlap). When both entity and relation assignments are wrong, we assign a score of 1.", "Note that, in the Wikipedia page of the topic entity, we may collect more than one sentence that contain a candidate answer. However, not all sentences are relevant, therefore we consider the candidate answer as correct if at least there is one positive evidence. On the other hand, sometimes, we may not find any evidence for the candidate answer. In these cases, we fall back to the results of the KB-based approach.", "The above two feature classes indicate local features. From the entity-relation $(e,r)$ pair, we create the query triple $(e,r,?)$ to retrieve the answers, and further extract features from the answers. These features are non-local since we require both $e$ and $r$ to retrieve the answer. One such feature is using the co-occurrence of the answer type and the question word based on the intuition that question words often indicate the answer type, e.g., the question word when usually indicates the answer type type.datetime. Another feature is the number of answer entities retrieved."]}
{"question_id": "d1d37dec9053d465c8b6f0470e06316bccf344b3", "predicted_answer": "", "predicted_evidence": ["On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 or distributed representations BIBREF11 , BIBREF12 . Designing large training datasets for these methods is relatively easy BIBREF7 , BIBREF13 , BIBREF14 . These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry.", "And when Structured + Joint uses unstructured inference, the performance boosts by 6.2% (from 47.1% to 53.3%) achieving a new state-of-the-art result. For the latter, we manually analyzed the cases in which unstructured inference helps. Table 4 lists some of these questions and the corresponding answers before and after the unstructured inference. We observed the unstructured inference mainly helps for two classes of questions: (1) questions involving aggregation operations (Questions 1-3); (2) questions involving sub-lexical compositionally (Questions 4-5). Questions 1 and 2 contain the predicate $largest$ an aggregation operator. A semantic parsing method should explicitly handle this predicate to trigger $max(.)$ operator. For Question 3, structured inference predicts the Freebase relation fb:teams..from retrieving all the years in which Ray Allen has played basketball.", "Using textual evidence not only mitigates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella's mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer's gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by wang2015). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, ...her mother was Isabella of Barcelos ..., can act as a further constraint to answer the question correctly.", "We use the shortest path between an entity mention and the question word in the dependency tree as input to the first channel. Similar to xu-EtAl:2015:EMNLP1, we treat the path as a concatenation of vectors of words, dependency edge directions and dependency labels, and feed it to the convolution layer. Note that, the entity mention and the question word are excluded from the dependency path so as to learn a more general relation representation in syntactic level. As shown in Figure 2 , the dependency path between who and shaq is $\\leftarrow $ dobj \u2013 play \u2013 nsubj $\\rightarrow $ .", ")$ operator. For Question 3, structured inference predicts the Freebase relation fb:teams..from retrieving all the years in which Ray Allen has played basketball. Note that Ray Allen has joined Connecticut University's team in 1993 and NBA from 1996. To answer this question a semantic parsing system would require a min( $\\cdot $ ) operator along with an additional constraint that the year corresponds to the NBA's term. Interestingly, without having to explicitly model these complex predicates, the unstructured inference helps in answering these questions more accurately. Questions 4-5 involve sub-lexical compositionally BIBREF25 predicates father and college. For example in Question 5, the user queries for the colleges that John Steinbeck attended. However, Freebase defines the relation fb:education..institution to describe a person's educational information without discriminating the specific periods such as high school or college. Inference using unstructured data helps in alleviating these representational issues."]}
{"question_id": "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b", "predicted_answer": "", "predicted_evidence": ["Questions 1 and 2 contain the predicate $largest$ an aggregation operator. A semantic parsing method should explicitly handle this predicate to trigger $max(.)$ operator. For Question 3, structured inference predicts the Freebase relation fb:teams..from retrieving all the years in which Ray Allen has played basketball. Note that Ray Allen has joined Connecticut University's team in 1993 and NBA from 1996. To answer this question a semantic parsing system would require a min( $\\cdot $ ) operator along with an additional constraint that the year corresponds to the NBA's term. Interestingly, without having to explicitly model these complex predicates, the unstructured inference helps in answering these questions more accurately. Questions 4-5 involve sub-lexical compositionally BIBREF25 predicates father and college. For example in Question 5, the user queries for the colleges that John Steinbeck attended. However, Freebase defines the relation fb:education..institution to describe a person's educational information without discriminating the specific periods such as high school or college.", "Over time, the QA task has evolved into two main streams \u2013 QA on unstructured data, and QA on structured data. TREC QA evaluations BIBREF26 were a major boost to unstructured QA leading to richer datasets and sophisticated methods BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . While initial progress on structured QA started with small toy domains like GeoQuery BIBREF34 , recent focus has shifted to large scale structured KBs like Freebase, DBPedia BIBREF35 , BIBREF36 , BIBREF3 , BIBREF4 , BIBREF37 , and on noisy KBs BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 . An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly BIBREF43 , BIBREF44 , BIBREF45 .", "We use the pipelined EL and RE along with inference on Wikipedia as described in sec:refine.", "Knowledge bases like Freebase capture real world facts, and Web resources like Wikipedia provide a large repository of sentences that validate or support these facts. For example, a sentence in Wikipedia says, Denali (also known as Mount McKinley, its former official name) is the highest mountain peak in North America, with a summit elevation of 20,310 feet (6,190 m) above sea level. To answer our example question against a KB using a relation extractor, we can use this sentence as external evidence, filter out wrong answers and pick the correct one.", "Since the advent of large structured knowledge bases (KBs) like Freebase BIBREF0 , YAGO BIBREF1 and DBpedia BIBREF2 , answering natural language questions using those structured KBs, also known as KB-based question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities."]}
{"question_id": "e057fa254ea7a4335de22fd97a0f08814b88aea4", "predicted_answer": "", "predicted_evidence": ["BBN: weischedel2005bbn annotated entities in Wall Street Journal using 93 types. We use the train/test splits in Ren:2016:LNR:2939672.2939822 and randomly hold out 2,000 pairs for dev. Document contexts are retrieved from the original corpus.", "On BBN (tab:bbn), while C16-1017's label embedding algorithm holds the best strict INLINEFORM0 , our approach notably improves both macro INLINEFORM1 and micro INLINEFORM2 . The performance drops to a competitive level with other approaches if adaptive thresholds or document-level contexts are removed.", "tab:cases shows examples illustrating the benefits brought by our proposed approach. Example A illustrates that sentence-level context sometimes is not informative enough, and attention, though already placed on the head verbs, can be misleading. Including document-level context (i.e., \u201cCanada's declining crude output\u201d in this case) helps preclude wrong predictions (i.e., /other/health and /other/health/treatment). Example B shows that the semantic patterns learnt by our attention mechanism help make the correct prediction. As we observe in tab:ontonotes and tab:figer, adding hand-crafted features to our approach does not improve the results. One possible explanation is that hand-crafted features are mostly about syntactic-head or topic information, and such information are already covered by our attention mechanism and document-level contexts as shown in tab:cases.", "Loose Macro: INLINEFORM0", "To overcome these drawbacks, we propose a neural architecture (fig:arch) which learns more context-aware representations by using a better attention mechanism and taking advantage of semantic discourse information available in both the document as well as sentence-level contexts. Further, we find that adaptive classification thresholds leads to further improvements. Experiments demonstrate that our approach, without any reliance on hand-crafted features, outperforms prior work on three benchmark datasets."]}
{"question_id": "134a66580c363287ec079f353ead8f770ac6d17b", "predicted_answer": "", "predicted_evidence": ["tab:cases shows examples illustrating the benefits brought by our proposed approach. Example A illustrates that sentence-level context sometimes is not informative enough, and attention, though already placed on the head verbs, can be misleading. Including document-level context (i.e., \u201cCanada's declining crude output\u201d in this case) helps preclude wrong predictions (i.e., /other/health and /other/health/treatment). Example B shows that the semantic patterns learnt by our attention mechanism help make the correct prediction. As we observe in tab:ontonotes and tab:figer, adding hand-crafted features to our approach does not improve the results. One possible explanation is that hand-crafted features are mostly about syntactic-head or topic information, and such information are already covered by our attention mechanism and document-level contexts as shown in tab:cases. Compared to hand-crafted features that heavily rely on system or human annotations, attention mechanism requires significantly less supervision, and document-level or paragraph-level contexts are much easier to get.", "On FIGER (tab:figer) where no document-level context is currently available, our proposed approach still achieves the state-of-the-art strict and micro INLINEFORM0 . If compared with the ablation variant of the Neural approach, i.e., w/o hand-crafted features, our approach gains significant improvement. We notice that removing adaptive thresholds only causes a small performance drop; this is likely because the train and test splits of FIGER are from different sources, and adaptive thresholds are not generalized well enough to the test data. Kwasibie, Attentive and Fnet were trained on a different dataset, so their results are not directly comparable.", "Sentence-level Context Encoder: The encoder INLINEFORM0 for sentence-level context INLINEFORM1 employs a single bi-directional RNN to encode INLINEFORM2 . Formally, let the tokens in INLINEFORM3 be INLINEFORM4 . The hidden state INLINEFORM5 for token INLINEFORM6 is a concatenation of a left-to-right hidden state INLINEFORM7 and a right-to-left hidden state INLINEFORM8 , DISPLAYFORM0", "At inference, the predicted type set INLINEFORM0 assigned to entity INLINEFORM1 is carried out by DISPLAYFORM0", "Attention: The feature representation for INLINEFORM0 is a weighted sum of the hidden states: INLINEFORM1 , where INLINEFORM2 is the attention to hidden state INLINEFORM3 . We employ the dot-product attention BIBREF11 . It computes attention based on the alignment between the entity and its context: DISPLAYFORM0"]}
{"question_id": "610fc593638c5e9809ea9839912d0b282541d42d", "predicted_answer": "", "predicted_evidence": ["We conduct experiments on three publicly available datasets. tab:stat shows the statistics of these datasets.", "We adopt the metrics used in Ling2012 where results are evaluated via strict, loose macro, loose micro INLINEFORM0 scores. For the INLINEFORM1 -th instance, let the predicted type set be INLINEFORM2 , and the reference type set INLINEFORM3 . The precision ( INLINEFORM4 ) and recall ( INLINEFORM5 ) for each metric are computed as follow.", "Given a type embedding vector INLINEFORM0 and a featurizer INLINEFORM1 that takes entity INLINEFORM2 and its context INLINEFORM3 , we employ the logistic regression (as shown in fig:arch) to model the probability of INLINEFORM4 assigned INLINEFORM5 (i.e., INLINEFORM6 ) DISPLAYFORM0", "and we seek to learn a type embedding matrix INLINEFORM0 and a featurizer INLINEFORM1 such that DISPLAYFORM0", "with INLINEFORM0 the threshold for predicting INLINEFORM1 has type INLINEFORM2 ."]}
{"question_id": "ab895ed198374f598e13d6d61df88142019d13b8", "predicted_answer": "", "predicted_evidence": ["Unlike traditional coreference annotations in datasets like those of BIBREF4, BIBREF10, BIBREF11 and BIBREF7, which aim to obtain complete coreference clusters, our questions require understanding coreference between only a few spans. While this means that the notion of coreference captured by our dataset is less comprehensive, it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing.", "We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table .", "To better understand the phenomena present in Quoref , we manually analyzed a random sample of 100 paragraph-question pairs. The following are some empirical observations.", "Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets BIBREF0, BIBREF1, BIBREF2, BIBREF3. However, these datasets focus largely on understanding local predicate-argument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances BIBREF4, BIBREF5, BIBREF6, BIBREF7, and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaways that let models arrive at the correct answer without performing the desired reasoning (see \u00a7SECREF3 for examples).", "We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions."]}
{"question_id": "8795bb1f874e5f3337710d8c3d5be49e672ab43a", "predicted_answer": "", "predicted_evidence": ["We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks.", "We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table .", "We introduce a new dataset, Quoref , that contains questions requiring coreferential reasoning (see examples in Figure FIGREF1). The questions are derived from paragraphs taken from a diverse set of English Wikipedia articles and are collected using an annotation process (\u00a7SECREF2) that deals with the aforementioned issues in the following ways: First, we devise a set of instructions that gets workers to find anaphoric expressions and their referents, asking questions that connect two mentions in a paragraph. These questions mostly revolve around traditional notions of coreference (Figure FIGREF1 Q1), but they can also involve referential phenomena that are more nebulous (Figure FIGREF1 Q3). Second, inspired by BIBREF8, we disallow questions that can be answered by an adversary model (uncased base BERT, BIBREF9, trained on SQuAD 1.1, BIBREF0) running in the background as the workers write questions. This adversary is not particularly skilled at answering questions requiring coreference, but can follow obvious lexical cues\u2014it thus helps workers avoid writing questions that shortcut coreferential reasoning.", "Unlike traditional coreference annotations in datasets like those of BIBREF4, BIBREF10, BIBREF11 and BIBREF7, which aim to obtain complete coreference clusters, our questions require understanding coreference between only a few spans. While this means that the notion of coreference captured by our dataset is less comprehensive, it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing.", "We scraped paragraphs from Wikipedia pages about English movies, art and architecture, geography, history, and music. For movies, we followed the list of English language films, and extracted plot summaries that are at least 40 tokens, and for the remaining categories, we followed the lists of featured articles. Since movie plot summaries usually mention many characters, it was easier to find hard Quoref questions for them, and we sampled about 60% of the paragraphs from this category."]}
{"question_id": "c30b0d6b23f0f01573eea315176c5ffe4e0c6b5c", "predicted_answer": "", "predicted_evidence": ["Decoding-based methods use a standard Seq2Seq training setup but modify the system during decoding to control a given attribute. For instance, the length of summaries was controlled by preventing the decoder from generating the End-Of-Sentence token before reaching the desired length or by only selecting hypotheses of a given length during the beam search BIBREF5. Weighted decoding (i.e. assigning weights to specific words during decoding) was also used with dialog models BIBREF18 or poetry generation models BIBREF21 to control the number of repetitions, alliterations, sentiment or style.", "Deep semantics sentence representation fed to a monolingual MT system.", "In Natural Language Processing, the Text Simplification task aims at making a text easier to read and understand. Text simplification can be beneficial for people with cognitive disabilities such as aphasia BIBREF0, dyslexia BIBREF1 and autism BIBREF2 but also for second language learners BIBREF3 and people with low literacy BIBREF4. The type of simplification needed for each of these audiences is different. Some aphasic patients struggle to read sentences with a high cognitive load such as long sentences with intricate syntactic structures, whereas second language learners might not understand texts with rare or specific vocabulary. Yet, research in text simplification has been mostly focused on developing models that generate a single generic simplification for a given source text with no possibility to adapt outputs for the needs of various target populations.", "In this paper, we propose a controllable simplification model that provides explicit ways for users to manipulate and update simplified outputs as they see fit. This work only considers the task of Sentence Simplification (SS) where the input of the model is a single source sentence and the output can be composed of one sentence or splitted into multiple. Our work builds upon previous work on controllable text generation BIBREF5, BIBREF6, BIBREF7, BIBREF8 where a Sequence-to-Sequence (Seq2Seq) model is modified to control attributes of the output text. We tailor this mechanism to the task of SS by considering relevant attributes of the output sentence such as the output length, the amount of paraphrasing, lexical complexity, and syntactic complexity. To this end, we condition the model at train time, by feeding those parameters along with the source sentence as additional inputs.", "Seq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI."]}
{"question_id": "311f9971d61b91c7d76bba1ad6f038390977a8be", "predicted_answer": "", "predicted_evidence": ["Text simplification has gained more and more interest through the years and has benefited from advances in Natural Language Processing and notably Machine Translation.", "In Natural Language Processing, the Text Simplification task aims at making a text easier to read and understand. Text simplification can be beneficial for people with cognitive disabilities such as aphasia BIBREF0, dyslexia BIBREF1 and autism BIBREF2 but also for second language learners BIBREF3 and people with low literacy BIBREF4. The type of simplification needed for each of these audiences is different. Some aphasic patients struggle to read sentences with a high cognitive load such as long sentences with intricate syntactic structures, whereas second language learners might not understand texts with rare or specific vocabulary. Yet, research in text simplification has been mostly focused on developing models that generate a single generic simplification for a given source text with no possibility to adapt outputs for the needs of various target populations.", "Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by choosing relevant parameters; (2) We show through a detailed analysis that our model can indeed control the considered attributes, making the simplifications potentially able to fit the needs of various end audiences; (3) With careful calibration, our controllable parametrization improves the performance of out-of-the-box Seq2Seq models leading to a new state-of-the-art score of 41.87 SARI BIBREF9 on the WikiLarge benchmark BIBREF10, a +1.42 gain over previous scores, without requiring any external resource or modified training objective.", "Phrase-based and Syntax-based MT was successfully used for SS BIBREF11 and further tailored to the task using deletion models BIBREF13 and candidate reranking BIBREF12. The candidate reranking method by BIBREF12 favors simplifications that are most dissimilar to the source using Levenshtein distance. The authors argue that dissimilarity is a key factor of simplification.", "Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization BIBREF5, BIBREF6, dialog BIBREF18, sentence compression BIBREF19, BIBREF20 or poetry generation BIBREF21."]}
{"question_id": "23cbf6ab365c1eb760b565d8ba51fb3f06257d62", "predicted_answer": "", "predicted_evidence": ["NbChars: character length ratio between source sentence and target sentence (compression level). This parameter accounts for sentence compression, and content deletion. Previous work showed that simplicity is best correlated with length-based metrics, and especially in terms of number of characters BIBREF23. The number of characters indeed accounts for the lengths of words which is itself correlated to lexical complexity.", "Decoding-based methods use a standard Seq2Seq training setup but modify the system during decoding to control a given attribute. For instance, the length of summaries was controlled by preventing the decoder from generating the End-Of-Sentence token before reaching the desired length or by only selecting hypotheses of a given length during the beam search BIBREF5. Weighted decoding (i.e. assigning weights to specific words during decoding) was also used with dialog models BIBREF18 or poetry generation models BIBREF21 to control the number of repetitions, alliterations, sentiment or style.", "We compute FKGL and SARI using the EASSE python package for SS BIBREF31. We do not use BLEU because it is not suitable for evaluating SS systems BIBREF32, and favors models that do not modify the source sentence BIBREF9.", "We evaluate our methods with FKGL (Flesch-Kincaid Grade Level) BIBREF30 to account for simplicity and SARI BIBREF9 as an overall score. FKGL is a commonly used metric for measuring readability however it should not be used alone for evaluating systems because it does not account for grammaticality and meaning preservation BIBREF12. It is computed as a linear combination of the number of words per simple sentence and the number of syllables per word:", "Syntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI."]}
{"question_id": "6ec267f66a1c5f996519aed8aa0befb5e5aec205", "predicted_answer": "", "predicted_evidence": ["Most of the studies in the literature to classify PD from speech are based on computing hand-crafted features and using classifiers such as support vector machines (SVMs) or K-nearest neighbors (KNN). For instance, in BIBREF3, the authors computed features related to perturbations of the fundamental frequency and amplitude of the speech signal to classify utterances from 20 PD patients and 20 HC subjects, Turkish speakers. Classifiers based on KNN and SVMs were considered, and accuracies of up to 75% were reported. Later, in BIBREF4 the authors proposed a phonation analysis based on several time frequency representations to assess tremor in the speech of PD patients. The extracted features were based on energy and entropy computed from time frequency representations. Several classifiers were used, including Gaussian mixture models (GMMs) and SVMs. Accuracies of up to 77% were reported in utterances of the PC-GITA database BIBREF5, formed with utterances from 50 PD patients and 50 HC subjects, Colombian Spanish native speakers.", "The authors studied the rapid repetition of the syllables /pa-ta-ka/ pronounced by 24 Czech native speakers, and reported an accuracy of 88% discriminating between PD patients and HC speakers, using an SVM classifier. Additional articulation features were proposed in BIBREF7, where the authors modeled the difficulty of PD patients to start/stop the vocal fold vibration in continuous speech. The model was based on the energy content in the transitions between unvoiced and voiced segments. The authors classified PD patients and HC speakers with speech recordings in three different languages (Spanish, German, and Czech), and reported accuracies ranging from 80% to 94% depending on the language; however, the results were optimistic, since the hyper-parameters of the classifier were optimized based on the accuracy on the test set. Another articulation model was proposed in BIBREF8. The authors considered a forced alignment strategy to segment the different phonetic units in the speech utterances.", "In addition to the hand-crafted feature extraction models, there is a growing interest in the research community to consider deep learning models in the assessment of the speech of PD patients BIBREF10, BIBREF11, BIBREF12. Deep learning methods have the potential to extract more abstract and robust features than those manually computed. These features could help to improve the accuracy of different models to classify pathological speech, such as PD BIBREF13. A deep learning based articulation model was proposed in BIBREF11 to model the difficulties of the patients to stop/start the vibration of the vocal folds. Transitions between voiced and unvoiced segments were modeled with time-frequency representations and convolutional neural networks (CNNs). The authors considered speech recordings of PD patients and HC speakers in three languages: Spanish, German, and Czech, and reported accuracies ranging from 70% to 89%, depending on the language. However, in a language independent scenario, i.e., training the CNN with utterances from one language and testing with the remaining two, the results were not satisfactory (accuracy$<60\\%$).", "Parkinson's disease (PD) is a neurodegenerative disorder characterized by the progressive loss of dopaminergic neurons in the mid-brain producing several motor and non-motor impairments in the patients BIBREF0. Motor symptoms include among others, bradykinesia, rigidity, resting tremor, micrographia, and different speech impairments. The speech impairments observed in PD patients are typically grouped as hypokinetic dysarthria, and include symptoms such as vocal folds rigidity, bradykinesia, and reduced control of muscles and limbs involved in the speech production. The effects of dysarthria in the speech of PD patients include increased acoustic noise, reduced intensity, harsh and breathy voice quality, increased voice nasality, monopitch, monoludness, speech rate disturbances, imprecise articulation of consonants BIBREF1, and involuntary introduction of pauses BIBREF2. Clinical observations in the speech of patients can be objectively and automatically measured by using computer aided methods supported in signal processing and pattern recognition with the aim to address two main aspects: (1) to support the diagnosis of the disease by classifying healthy control (HC) subjects and patients, and (2) to predict the level of degradation of the speech of the patients according to a specific clinical scale.", "The model was based on the energy content in the transitions between unvoiced and voiced segments. The authors classified PD patients and HC speakers with speech recordings in three different languages (Spanish, German, and Czech), and reported accuracies ranging from 80% to 94% depending on the language; however, the results were optimistic, since the hyper-parameters of the classifier were optimized based on the accuracy on the test set. Another articulation model was proposed in BIBREF8. The authors considered a forced alignment strategy to segment the different phonetic units in the speech utterances. The phonemes were segmented and grouped to train different GMMs. The classification was performed based on a threshold of the difference between the posterior probabilities from the models created for HC subjects and PD patients. The model was tested with Colombian Spanish utterances from the PC-GITA database BIBREF5 and with the Czech data from BIBREF9. The authors reported accuracies of up to 81% for the Spanish data, and of up to 94% for the Czech data."]}
{"question_id": "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e", "predicted_answer": "", "predicted_evidence": ["Parkinson's disease (PD) is a neurodegenerative disorder characterized by the progressive loss of dopaminergic neurons in the mid-brain producing several motor and non-motor impairments in the patients BIBREF0. Motor symptoms include among others, bradykinesia, rigidity, resting tremor, micrographia, and different speech impairments. The speech impairments observed in PD patients are typically grouped as hypokinetic dysarthria, and include symptoms such as vocal folds rigidity, bradykinesia, and reduced control of muscles and limbs involved in the speech production. The effects of dysarthria in the speech of PD patients include increased acoustic noise, reduced intensity, harsh and breathy voice quality, increased voice nasality, monopitch, monoludness, speech rate disturbances, imprecise articulation of consonants BIBREF1, and involuntary introduction of pauses BIBREF2.", "The classification of PD from speech in different languages has to be carefully conducted to avoid bias towards the linguistic content present in each language. For instance, Czech and German languages are richer than Spanish language in terms of consonant production, which may cause that it is easier to produce consonant sounds by Czech PD patients than by Spanish PD patients. Despite these language dependent issues, the results in the classification of PD in different languages could be improved using a transfer learning strategy among languages, i.e., to train a base model with utterances from one language, and then, to perform a fine-tuning of the weights with utterances from the target language BIBREF14. Similar approaches based on transfer learning have been recently considered to classify PD using handwriting BIBREF15. In the present study, we propose a methodology to classify PD via a transfer learning strategy with the aim to improve the accuracy in different languages. CNNs trained with utterances from one language are used to initialize a model to classify speech utterances from PD patients in a different language.", "A total of 100 native Czech speakers (50 PD, 50 HC) were considered BIBREF18. The speech tasks performed by the participants include the rapid repetition of the syllables /pa-ta-ka/, a read text with 80 words, and a monologue.", "The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.", "The features extracted from the transitions include 12 Mel-Frequency Cepstral Coefficients (MFCCs) with their first and second derivatives, and the log energy of the signal distributed into 22 Bark bands. The total number of descriptors corresponds to 58. Four statistical functionals (mean, standard deviation, skewness, and kurtosis) are computed for each descriptor, obtaining a 232-dimensional feature-vector per utterance. The classification of PD patients and HC speakers is performed with a radial basis SVM with margin parameter $C=10$ and a Gaussian kernel with parameter $\\gamma =0.0001$. The SVM is tested following a 10-fold Cross-Validation strategy, speaker independent."]}
{"question_id": "a9d5f83f4b32c52105f2ae1c570f1c590ac52487", "predicted_answer": "", "predicted_evidence": ["Social media websites have become the main platform for users to browse information and share opinions, facilitating news dissemination greatly. However, the characteristics of social media also accelerate the rapid spread and dissemination of unverified information, i.e., rumors BIBREF0. The definition of rumor is \u201citems of information that are unverified at the time of posting\u201d BIBREF1. Ubiquitous false rumors bring about harmful effects, which has seriously affected public and individual lives, and caused panic in society BIBREF2, BIBREF3. Because online content is massive and debunking rumors manually is time-consuming, there is a great need for automatic methods to identify false rumors BIBREF4.", "In all experiments, the number of GCN layers is set to $L=2$. We list the implementation details in Appendix A.", "The state-of-the-art methods for rumor stance classification are proposed to model the sequential property BIBREF13 or the temporal property BIBREF14 of a Twitter conversation thread. In this paper, we propose a new perspective based on structural property: learning tweet representations through aggregating information from their neighboring tweets. Intuitively, a tweet's nearer neighbors in its conversation thread are more informative than farther neighbors because the replying relationships of them are closer, and their stance expressions can help classify the stance of the center tweet (e.g., in Figure FIGREF1, tweets \u201c1\u201d, \u201c4\u201d and \u201c5\u201d are the one-hop neighbors of the tweet \u201c2\u201d, and their influences on predicting the stance of \u201c2\u201d are larger than that of the two-hop neighbor \u201c3\u201d). To achieve this, we represent both tweet contents and conversation structures into a latent space using a graph convolutional network (GCN) BIBREF15, aiming to learn stance feature for each tweet by aggregating its neighbors' features.", "Now we detail Conversational-GCN, the bottom component of our framework. We first adopt a bidirectional GRU (BGRU) BIBREF43 layer to learn the content feature for each tweet in the thread $\\mathcal {C}$. For a tweet $t_i$ ($i\\in [1,|\\mathcal {C}|]$), we run the BGRU over its word embedding sequence, and use the final step's hidden vector to represent the tweet. The content feature representation of $t_i$ is denoted as $\\mathbf {c}_i\\in \\mathbb {R}^{d}$, where $d$ is the output size of the BGRU.", "where $\\mathbf {h}_i^{\\text{in}}\\in \\mathbb {R}^{d_{\\text{in}}}$ and $\\mathbf {h}_i^{\\text{out}}\\in \\mathbb {R}^{d_{\\text{out}}}$ denote the input and output feature representations of the tweet $t_i$ respectively. The convolution filter $\\mathbf {W}\\in \\mathbb {R}^{d_{\\text{in}}\\times d_{\\text{out}}}$ and the bias $\\mathbf {b}\\in \\mathbb {R}^{d_{\\text{out}}}$ are shared over all tweets in a conversation."]}
{"question_id": "288f0c003cad82b3db5e7231c189c0108ae7423e", "predicted_answer": "", "predicted_evidence": ["Performance Comparison Table TABREF20 shows the results of different methods for rumor stance classification. Clearly, the macro-averaged $F_1$ of Conversational-GCN is better than all baselines.", "The first is SemEval-2017 task 8 BIBREF16 dataset. It includes 325 rumorous conversation threads, and has been split into training, development and test sets. These threads cover ten events, and two events of that only appear in the test set. This dataset is used to evaluate both stance classification and veracity prediction tasks.", "Effect of Temporal Evolution Modeling We modify the Stance-Aware RNN by two ways: (i) we replace the GRU layer by a CNN that only captures local temporal information; (ii) we remove the GRU layer. Results in Table TABREF30 verify that replacing or removing the GRU block hurts the performance, and thus modeling the stance evolution of public reactions towards a rumorous message is indeed necessary for effective veracity prediction.", "Rumor Veracity Prediction Previous studies have proposed methods based on various features such as linguistics, time series and propagation structures BIBREF30, BIBREF31, BIBREF32, BIBREF33. Neural networks show the effectiveness of modeling time series BIBREF34, BIBREF35 and propagation paths BIBREF36. BIBREF37's model adopted recursive neural networks to incorporate structure information into tweet representations and outperformed previous methods.", "$\\bullet $ TD-RvNN BIBREF37 models the top-down tree structure using a recursive neural network for veracity classification."]}
{"question_id": "562a995dfc8d95777aa2a3c6353ee5cd4a9aeb08", "predicted_answer": "", "predicted_evidence": ["$\\bullet $ We design a novel graph convolution operation customized to encode conversation structures for learning stance features. To our knowledge, we are the first to employ graph convolution for modeling the structural property of Twitter conversations.", "Recent work has focused on Twitter conversations discussing rumors. BIBREF12 proposed to capture the sequential property of conversations with linear-chain CRF, and also used a tree-structured CRF to consider the conversation structure as a whole. BIBREF27 developed a novel feature set that scores the level of users' confidence. BIBREF28 designed affective and dialogue-act features to cover various facets of affect. BIBREF29 proposed a semi-supervised method that propagates the stance labels on similarity graph. Beyond feature-based methods, BIBREF13 utilized an LSTM to model the sequential branches in a conversation, and their system ranked the first in SemEval-2017 task 8. BIBREF14 adopted attention to model the temporal property of a conversation and achieved the state-of-the-art performance.", "$\\bullet $ Experimental results on two benchmark datasets verify that our hierarchical framework performs better than existing methods in both rumor stance classification and veracity prediction.", "Some studies utilized stance labels as the input feature of veracity classifiers to improve the performance BIBREF9, BIBREF38. BIBREF39 proposed to recognize the temporal patterns of true and false rumors' stances by two hidden Markov models (HMMs). Unlike their solution, our method learns discriminative features of stance evolution with an RNN. Moreover, our method jointly predicts stance and veracity by exploiting both structural and temporal characteristics, whereas HMMs need stance labels as the input sequence of observations.", "The loss function of $\\mathcal {C}$ for veracity prediction is also computed by cross-entropy criterion:"]}
{"question_id": "71e1f06daf6310609d00850340e64a846fbe2dfb", "predicted_answer": "", "predicted_evidence": ["In this paper, we propose a novel approach to compressing a large BERT model into a shallow one via Patient Knowledge Distillation. To fully utilize the rich information in deep structure of the teacher network, our Patient-KD approach encourages the student model to patiently learn from the teacher through a multi-layer distillation process. Extensive experiments over four NLP tasks demonstrate the effectiveness of our proposed model.", "Besides encouraging the student model to imitate the teacher's behavior, we can also fine-tune the student model on target tasks, where task-specific cross-entropy loss is included for model training:", "Furthermore, in 5 tasks out of 7 (SST-2 (-2.3% compared to BERT-Base teacher), QQP (-0.1%), MNLI-m (-2.2%), MNLI-mm (-1.8%), and QNLI (-1.4%)), the proposed 6-layer student coached by the patient teacher achieved similar performance to the original BERT-Base, demonstrating the effectiveness of our approach. Interestingly, all those 5 tasks have more than 60k training samples, which indicates that our method tends to perform better when there is a large amount of training data.", "Thus, the final objective function for knowledge distillation can be formulated as:", "We denote the set of intermediate layers to distill knowledge from as $I_{pt}$. Take distilling from BERT$_{12}$ to BERT$_6$ as an example. For the PKD-Skip strategy, $I_{pt} = \\lbrace 2,4,6,8,10\\rbrace $; and for the PKD-Last strategy, $I_{pt} = \\lbrace 7,8,9,10,11\\rbrace $. Note that $k=5$ for both cases, because the output from the last layer (e.g., Layer 12 for BERT-Base) is omitted since its hidden states are connected to the softmax layer, which is already included in the KD loss defined in Eqn. (DISPLAY_FORM10). In general, for BERT student with $n$ layers, $k$ always equals to $n-1$."]}
{"question_id": "ebb4db9c24aa36db9954dd65ea079a798df80558", "predicted_answer": "", "predicted_evidence": ["For future work, we plan to pre-train BERT from scratch to address the initialization mismatch issue, and potentially modify the proposed method such that it could also help during pre-training. Designing more sophisticated distance metrics for loss functions is another exploration direction. We will also investigate Patient-KD in more complex settings such as multi-task learning and meta learning.", "The original large teacher network is represented by a function $f(\\mathbf {x};\\mathbf {\\theta })$, where $\\mathbf {x}$ is the input to the network, and $\\mathbf {\\theta }$ denotes the model parameters. The goal of knowledge distillation is to learn a new set of parameters $\\mathbf {\\theta }^{\\prime }$ for a shallower student network $g(\\mathbf {x};\\mathbf {\\theta }^{\\prime })$, such that the student network achieves similar performance to the teacher, with much lower computational cost. Our strategy is to force the student model to imitate outputs from the teacher model on the training dataset with a defined objective $L_{KD}$.", "We further investigate the performance gain from two different patient teacher designs: PKD-Last vs. PKD-Skip. Results of both PKD variants on the GLUE benchmark (with BERT$_6$ as the student) are summarized in Table TABREF23. Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last. Presumably, this might be due to the fact that distilling information across every $k$ layers captures more diverse representations of richer semantics from low-level to high-level, while focusing on the last $k$ layers tends to capture relatively homogeneous semantic information.", "To test the inference speed, we ran experiments on 105k samples from QNLI training set BIBREF20. Inference is performed on a single Titan RTX GPU with batch size set to 128, maximum sequence length set to 128, and FP16 activated. The inference time for the embedding layer is negligible compared to the Transformer layers. Results in Table TABREF26 show that the proposed Patient-KD approach achieves an almost linear speedup, 1.94 and 3.73 times for BERT$_6$ and BERT$_3$, respectively.", "Besides encouraging the student model to imitate the teacher's behavior, we can also fine-tune the student model on target tasks, where task-specific cross-entropy loss is included for model training:"]}
{"question_id": "7a212a34e9dbb0ba52c40471842b2e0e3e14f276", "predicted_answer": "", "predicted_evidence": ["Feature-based methods mainly focus on learning: ($i$) context-independent word representation (e.g., word2vec BIBREF9, GloVe BIBREF10, FastText BIBREF11); ($ii$) sentence-level representation (e.g., BIBREF12, BIBREF13, BIBREF14); and ($iii$) contextualized word representation (e.g., Cove BIBREF15, ELMo BIBREF0). Specifically, ELMo BIBREF0 learns high-quality, deep contextualized word representation using bidirectional language model, which can be directly plugged into standard NLU models for performance boosting.", "Despite its empirical success, BERT's computational efficiency is a widely recognized issue because of its large number of parameters. For example, the original BERT-Base model has 12 layers and 110 million parameters. Training from scratch typically takes four days on 4 to 16 Cloud TPUs. Even fine-tuning the pre-trained model with task-specific dataset may take several hours to finish one epoch. Thus, reducing computational costs for such models is crucial for their application in practice, where computational resources are limited.", "Different from previous knowledge distillation methods BIBREF6, BIBREF7, BIBREF8, we adopt a patient learning mechanism: instead of learning parameters from only the last layer of the teacher, we encourage the student model to extract knowledge also from previous layers of the teacher network. We call this `Patient Knowledge Distillation'. This patient learner has the advantage of distilling rich information through the deep structure of the teacher network for multi-layer knowledge distillation.", "Language model pre-training has proven to be highly effective in learning universal language representations from large-scale unlabeled data. ELMo BIBREF0, GPT BIBREF1 and BERT BIBREF2 have achieved great success in many NLP tasks, such as sentiment classification BIBREF3, natural language inference BIBREF4, and question answering BIBREF5.", "We evaluate the proposed approach on several NLP tasks, including Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension. Experiments on seven datasets across these four tasks demonstrate that the proposed Patient-KD approach achieves superior performance and better generalization than standard knowledge distillation methods BIBREF6, with significant gain in training efficiency and storage reduction while maintaining comparable model accuracy to original large models. To the authors' best knowledge, this is the first known effort for BERT model compression."]}
{"question_id": "ed15a593d64a5ba58f63c021ae9fd8f50051a667", "predicted_answer": "", "predicted_evidence": ["One full training iteration consisting of an insertion phase followed by a deletion phase can be represented by the following steps:", "We parametrize both the insertion and deletion probability distributions with two stacked transformer decoders, where $\\theta _i$ denotes the parameters of the insertion model and $\\theta _d$ of the deletion model. The models are trained at the same time, where the deletion model's signal is dependent on the state of the current insertion model. For sampling from the insertion model we take the argument that maximizes the probability of the current sequence via parallel decoding: $\\hat{c}_l = \\arg \\max _{c}p(c, \\mid l, \\hat{x}_t)$. We do not backpropagate through the sampling process, i.e., the gradient during training can not flow from the output of the deletion model through the insertion model. Both models are trained to maximize the log-probability of their respective distributions. A graphical depiction of the model is shown in Figure FIGREF7.", "The output of the deletion model represents the probability distribution $p(d_l \\mid l, \\hat{x}^*_t) \\quad \\forall \\quad l \\in \\lbrace 1, \\dots , t\\rbrace $", "Pass this sequence through the insertion model to get the probability distribution over $p(c_i^z \\mid x_{1:i-1}^{z, i-1})$ (denote $\\hat{x}_t$ short for $x_{1:i-1}^{z, i-1}$).", "In this section, we describe our Insertion-Deletion model. We extend the Insertion Transformer BIBREF3, an insertion-only framework to handle both insertions and deletions."]}
{"question_id": "e86fb784011de5fda6ff8ccbe4ee4deadd7ee7d6", "predicted_answer": "", "predicted_evidence": ["In this section, we describe our Insertion-Deletion model. We extend the Insertion Transformer BIBREF3, an insertion-only framework to handle both insertions and deletions.", "Target $ g\\ j\\ a\\ d\\ s $", "Source $ h\\ k\\ b\\ e\\ t $", "Target $ m\\ n\\ o\\ p\\ q $", "The training algorithm used in the LevT framework uses an expert policy. This expert policy requires dynamic programming to minimize Levenshtein distance between the current input and the target. This approach was also explored by BIBREF8, BIBREF9. Their learning algorithm arguably adds more complexity than needed over the simple on-policy method we propose. The LevT framework consists of three stages, first the number of tokens to be inserted is predicted, then the actual tokens are predicted, and finally the deletion actions are emitted. The extra classifier to predict the number of tokens needed to be inserted adds an additional Transformer pass to each generation step. In practice, it is also unclear whether the LevT exhibits speedups over an insertion-based model following a balanced binary tree order. In contrast, our Insertion-Deletion framework only has one insertion phase and one deletion phase, without the need to predict the number of tokens needed to be inserted. This greatly simplifies the model architecture, training procedure and inference runtime."]}
{"question_id": "d206f2cbcc3d2a6bd0ccaa3b57fece396159f609", "predicted_answer": "", "predicted_evidence": ["Count: A Numerical entity that represents the number of times the action should take place.", "Mod Link: A Modifier entity is linked to any entity that it is attempting to modify using this relation.", "Coreference: A link that associates two phrases when those two phrases refer to the same entity.", "Mention: Words that can refer to an object mentioned earlier in the sentence.", "Creates: This relation marks the physical entity that the action creates."]}
{"question_id": "633e2210c740b4558b1eea3f041b3ae8e0813293", "predicted_answer": "", "predicted_evidence": ["Mod Link: A Modifier entity is linked to any entity that it is attempting to modify using this relation.", "Count: A Numerical entity that represents the number of times the action should take place.", "pH: measure of acidity or alkalinity of a solution.", "Numerical: A generic tag for a number that doesn't fit time, temp, etc and which isn't accompanied by its unit of measure.", "Measure: A link that associates the various numerical measures to the entity its trying to measure directly."]}
{"question_id": "bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b", "predicted_answer": "", "predicted_evidence": ["Our base configuration implemented the Listener using 4 bidirectional LSTM layers of 256 units per direction (512 total), interleaved with 3 time-pooling layers which resulted in an 8-fold reduction of the input sequence length, approximately equating the length of hidden activations to the number of characters in the transcript. The Speller was a single LSTM layer with 256 units. Input characters were embedded into 30 dimensions. The attention MLP used 128 hidden units, previous attention weights were accessed using 3 convolutional filters spanning 100 frames. LSTM weights were initialized uniformly over the range INLINEFORM0 . Networks were trained using 8 asynchronous replica workers each employing the ADAM algorithm BIBREF20 with default parameters and the learning rate set initially to INLINEFORM1 , then reduced to INLINEFORM2 and INLINEFORM3 after 400k and 500k training steps, respectively. Static Gaussian weight noise with standard deviation 0.075 was applied to all weight matrices after 20000 training steps.", "The Speller was a single LSTM layer with 256 units. Input characters were embedded into 30 dimensions. The attention MLP used 128 hidden units, previous attention weights were accessed using 3 convolutional filters spanning 100 frames. LSTM weights were initialized uniformly over the range INLINEFORM0 . Networks were trained using 8 asynchronous replica workers each employing the ADAM algorithm BIBREF20 with default parameters and the learning rate set initially to INLINEFORM1 , then reduced to INLINEFORM2 and INLINEFORM3 after 400k and 500k training steps, respectively. Static Gaussian weight noise with standard deviation 0.075 was applied to all weight matrices after 20000 training steps. We have also used a small weight decay of INLINEFORM4 .", "Table TABREF21 gathers results that use the extended trigram language model. We report averages of two runs. For each run we have tuned beam search parameters on the validation set and applied them on the test set. A typical setup used beam width 200, language model weight INLINEFORM0 , coverage weight INLINEFORM1 and coverage threshold INLINEFORM2 . Our best result surpasses CTC-based networks BIBREF5 and matches the results of a DNN-HMM and CTC ensemble BIBREF22 .", "Our base configuration implemented the Listener using 4 bidirectional LSTM layers of 256 units per direction (512 total), interleaved with 3 time-pooling layers which resulted in an 8-fold reduction of the input sequence length, approximately equating the length of hidden activations to the number of characters in the transcript. The Speller was a single LSTM layer with 256 units. Input characters were embedded into 30 dimensions. The attention MLP used 128 hidden units, previous attention weights were accessed using 3 convolutional filters spanning 100 frames. LSTM weights were initialized uniformly over the range INLINEFORM0 . Networks were trained using 8 asynchronous replica workers each employing the ADAM algorithm BIBREF20 with default parameters and the learning rate set initially to INLINEFORM1 , then reduced to INLINEFORM2 and INLINEFORM3 after 400k and 500k training steps, respectively.", "Due to the recurrent formulation of the speller function, the most probable transcript cannot be found exactly using the Viterbi algorithm. Instead, approximate search methods are used. Typically, best results are obtained using beam search. The search begins with the set (beam) of hypotheses containing only the empty transcript. At every step, candidate transcripts are formed by extending hypothesis in the beam by one character. The candidates are then scored using the model, and a certain number of top-scoring candidates forms the new beam. The model indicates that a transcript is considered to be finished by emitting a special EOS (end-of-sequence) token."]}
{"question_id": "6c4e1a1ccc0c5c48115864a6928385c248f4d8ad", "predicted_answer": "", "predicted_evidence": ["We have compared two label smoothing methods: unigram smoothing BIBREF16 with the probability of the correct label set to INLINEFORM0 and neighborhood smoothing with the probability of correct token set to INLINEFORM1 and the remaining probability mass distributed symmetrically over neighbors at distance INLINEFORM2 and INLINEFORM3 with a INLINEFORM4 ratio. We have tuned the smoothing parameters with a small grid search and have found that good results can be obtained for a broad range of settings.", "Label smoothing was proposed as an efficient regularizer for the Inception architecture BIBREF15 . Several improved smoothing schemes were proposed, including sampling erroneous labels instead of using a fixed distribution BIBREF24 , using the marginal label probabilities BIBREF16 , or using early errors of the model BIBREF25 . Smoothing techniques increase the entropy of a model's predictions, a technique that was used to promote exploration in reinforcement learning BIBREF26 , BIBREF27 , BIBREF28 . Label smoothing prevents saturating the SoftMax nonlinearity and results in better gradient flow to lower layers of the network BIBREF15 . A similar concept, in which training targets were set slightly below the range of the output nonlinearity was proposed in BIBREF29 .", "Table TABREF21 gathers results that use the extended trigram language model. We report averages of two runs. For each run we have tuned beam search parameters on the validation set and applied them on the test set. A typical setup used beam width 200, language model weight INLINEFORM0 , coverage weight INLINEFORM1 and coverage threshold INLINEFORM2 . Our best result surpasses CTC-based networks BIBREF5 and matches the results of a DNN-HMM and CTC ensemble BIBREF22 .", "We have gathered results obtained without language models in Table TABREF20 . We have used a beam size of 10 and no mechanism to promote longer sequences. We report averages of two runs taken at the epoch with the lowest validation WER. Label smoothing brings a large error rate reduction, nearly matching the performance achieved with very deep and sophisticated encoders BIBREF21 .", "Our seq2seq networks are locally normalized, i.e. the speller produces a probability distribution at every step. Alternatively normalization can be performed globally on whole transcripts. In discriminative training of classical ASR systems normalization is performed over lattices BIBREF30 . In the case of recurrent networks lattices are replaced by beam search results. Global normalization has yielded important benefits on many NLP tasks including parsing and translation BIBREF31 , BIBREF32 . Global normalization is expensive, because each training step requires running beam search inference. It remains to be established whether globally normalized models can be approximated by cheaper to train locally normalized models with proper regularization such as label smoothing."]}
{"question_id": "55bde89fc5822572f794614df3130d23537f7cf2", "predicted_answer": "", "predicted_evidence": ["where Jji = diag(( xj,ipost,3(w11,j)), ...,( xj,ipost,3(wd1,j))) Rd d", "Using H\u00f6lder's inequality, we have", "For simplicity, we denote $x_l=\\text{Concat}(x_{l,1},...,x_{l,n})\\in \\mathbb {R}^{nd}$ and $x_l^k=\\text{Concat}(x_{l,1}^k,...,x_{l,n}^k)\\in \\mathbb {R}^{nd}$ for $k=\\lbrace 1,2,3,4,5\\rbrace $. Then in the Post-LN Transformer, the gradient of the parameters in the $l$-th layer (take $W^{2,l}$ as an example) can be written as", "According to union bound, with probability $0.99-\\delta (\\epsilon )-\\frac{\\epsilon }{1+\\epsilon -\\alpha _0}$ we have $|\\frac{\\partial \\mathcal {L}(x_{Final,i}^{pre})}{\\partial W^{2,L}_{pq}}|^2=\\mathcal {O}(\\left[\\Vert \\mathbf {J}_{LN}(x_{L+1,i}^{pre})\\Vert ^2_2|[\\text{ReLU}(x_{L,i}^{pre,4}W^{1,L})]_p|^2\\right])\\le \\mathcal {O}(\\frac{2d\\ln {100d}}{\\Vert x_{L+1,i}^{pre}\\Vert _2^2})\\le \\mathcal {O}(\\frac{d\\ln {d}}{\\alpha _0\\mathbb {E}\\Vert x_{L+1,", "Lemma 5 Let $Y$ be a random variable that is never larger than B. Then for all $a<B$,"]}
{"question_id": "523bc4e3482e1c9a8e0cb92cfe51eea92c20e8fd", "predicted_answer": "", "predicted_evidence": ["If $l$ is sufficiently large, the norm of $\\mathbf {J}_{LN}(x_{j,i}^{pre})$ and $\\mathbf {J}_{LN}(x^{pre,3}_{j,i})$ are very small (of order $\\mathcal {O}(\\frac{1}{\\sqrt{j}})$) as $j$ is between $l+1$ and $L$, which means the eigenvalues of matrix $\\frac{\\partial x_{j+1}^{pre}}{\\partial x_j^{pre,3}}$ and $\\frac{\\partial x_{j}^{pre,3}}{\\partial x_j^{pre}}$ are close to 1. Then we can see that $\\mathbb {E}\\Vert \\frac{\\partial x^{pre}_{j+1}}{\\partial x_j^{pre,3}}\\Vert _2$ and $\\mathbb {E}\\Vert \\frac{\\partial x_{j}^{pre,3}}{\\partial x_j^{pre}}\\Vert _2$ are nearly 1, and the norm of $\\frac{\\partial \\tilde{\\mathcal {L}}}{\\partial W^{2,l}}$ for pre-LN transformer is independent of $l$ when $l$ is large.", "Theoretically, we find that the gradients of the parameters near the output layers are very large for the Post-LN Transformer and suggest using large learning rates to those parameters makes the training unstable. To verify whether using small-step updates mitigates the issue, we use a very small but fixed learning rate and check whether it can optimize the Post-LN Transformer (without the learning rate warm-up step) to a certain extent. In detail, we use a fixed learning rate of $1e^{-4}$ at the beginning of the optimization, which is much smaller than the $\\text{lr}_{max}= 1e^{-3}$ in the paper. Please note that as the learning rates during training are small, the training converges slowly, and this setting is not very practical in real large-scale tasks. We plot the validation curve together with other baseline approaches in Figure 6. We can see from the figure, the validation loss (pink curve) is around 4.3 in 27 epochs.", "We have provided a formal proof on the gradients of the last FFN sub-layer as above. In order to fully understand the optimization, we also make some preliminary analysis for other layers and other parameters. Our main result is that the gradient norm in the Post-LN Transformer is large for the parameters near the output and will be likely to decay as the layer index $l$ decreases. On the contrary, the gradient norm in the Pre- Transformer will be likely to stay the same for any layer $l$. All the preliminary theoretical results are provided in the supplementary material.", "Since $\\Vert x_{L,i}^{post,3}\\Vert _2^2=d$, $[x_{L,i}^{post,3}W^{1,L}]_p$ has distribution $N(0,1)$, using Chernoff bound we have", "Since all the derivatives are bounded, we have $\\Vert \\frac{\\partial \\mathcal {L}(x_{L+1,i}^{post})}{\\partial x_{L+1,i}^{post}}\\Vert ^2_2=\\mathcal {O}(1)$. So"]}
{"question_id": "6073be8b88f0378cd0c4ffcad87e1327bc98b991", "predicted_answer": "", "predicted_evidence": ["To verify whether using small-step updates mitigates the issue, we use a very small but fixed learning rate and check whether it can optimize the Post-LN Transformer (without the learning rate warm-up step) to a certain extent. In detail, we use a fixed learning rate of $1e^{-4}$ at the beginning of the optimization, which is much smaller than the $\\text{lr}_{max}= 1e^{-3}$ in the paper. Please note that as the learning rates during training are small, the training converges slowly, and this setting is not very practical in real large-scale tasks. We plot the validation curve together with other baseline approaches in Figure 6. We can see from the figure, the validation loss (pink curve) is around 4.3 in 27 epochs. This loss is much lower than that of the Post-LN Transformer trained using a large learning rate (blue curve).", "To verify whether using small-step updates mitigates the issue, we use a very small but fixed learning rate and check whether it can optimize the Post-LN Transformer (without the learning rate warm-up step) to a certain extent. In detail, we use a fixed learning rate of $1e^{-4}$ at the beginning of the optimization, which is much smaller than the $\\text{lr}_{max}= 1e^{-3}$ in the paper. Please note that as the learning rates during training are small, the training converges slowly, and this setting is not very practical in real large-scale tasks. We plot the validation curve together with other baseline approaches in Figure 6. We can see from the figure, the validation loss (pink curve) is around 4.3 in 27 epochs. This loss is much lower than that of the Post-LN Transformer trained using a large learning rate (blue curve). But it is still worse than the SOTA performance (green curve).", "We also test different $\\text{lr}_{max}$ for both optimizers. For Adam, we set $\\text{lr}_{max}=5e^{-4}$ or $1e^{-3}$, and for SGD, we set $\\text{lr}_{max}=5e^{-3}$ or $1e^{-3}$. When the warm-up stage is used, we set $T_{\\text{warmup}}=4000$ as suggested by the original paper BIBREF0. To study the second aspect, we set $T_{\\text{warmup}}$ to be 1/500/4000 (\u201c1\u201d refers to the no warm-up setting) and use $\\text{lr}_{max}=5e^{-4}$ or $1e^{-3}$ with Adam. For all experiments, a same inverse square root learning rate scheduler is used after the warm-up stage. We use both validation loss and BLEU BIBREF36 as the evaluation measure of the model performance.", "Let $X=B-Y$, then $X\\ge 0$ and Markov's inequality tells us that", "which implies"]}
{"question_id": "f3b4e52ba962a0004064132d123fd9b78d9e12e2", "predicted_answer": "", "predicted_evidence": ["We use the streaming multi-layer truncated attention model (SMLTA) trained on the large-scale speech corpus (more than 10,000 hours) and fine-tuned on a number of talk related corpora (more than 1,000 hours), to generate the 5-best automatic recognized text for each acoustic speech.", "Effectiveness on Translation Quality. As shown in Table TABREF49 , there is a great deal of difference between the sub-sentence and the baseline model. On an average the sub-sentence shows weaker performance by a 3.08 drop in BLEU score (40.39 INLINEFORM0 37.31). Similarly, the wait-k model also brings an obvious decrease in translation quality, even with the best wait-15 policy, its performance is still worse than the baseline system, with a 2.15 drop, averagely, in BLEU (40.39 INLINEFORM1 38.24). For a machine translation product, a large degradation in translation quality will largely affect the use experience even if it has low latency.", "Effects of Discarding Preceding Generated Tokens. As mentioned and depicted in Figure FIGREF28 , we discard one token in the previously generated translation in our context-aware NMT model. One may be interested in whether discarding more generated translation leads to better translation quality. However, when decoding on the sub-sentence, even the best discard 4 tokens model brings no significant improvement (39.66 INLINEFORM0 39.82) but a slight cost of latency (see in Figure FIGREF58 for visualized latency). While decoding on the segment, even discarding two tokens can bring significant improvement (37.96 INLINEFORM1 39.00). This finding proves that our partial decoding model is able to generate accurate translation by anticipating the future content. It also indicates that the anticipation based on a larger context presents more robust performance than the aggressive anticipation in the wait-k model, as well as in the segment based decoding model.", "To make the simultaneous machine translation more accessible and producible, we borrow SI strategies used by human interpreters to create our model. As shown in Figure FIGREF3 , this model is able to constantly read streaming text from the ASR model, and simultaneously determine the boundaries of Information Units (IUs) one after another. Each detected IU is then translated into a fluent translation with two simple yet effective decoding strategies: partial decoding and context-aware decoding. Specifically, IUs at the beginning of each sentence are sent to the partial decoding module. Other information units, either appearing in the middle or at the end of a sentence, are translated into target language by the context-aware decoding module. Notice that this module is able to exploit additional context from the history so that the model can generate coherent translation. This method is derived from the \u201csalami technique\u201d BIBREF13 , BIBREF14 , or \u201cchunking\u201d, one of the most commonly used strategies by human interpreters to cope with the linearity constraint in simultaneous interpreting.", "During decoding, we discard a few previously generated translations, in order to make more fluent translations."]}
{"question_id": "ea6edf45f094586caf4684463287254d44b00e95", "predicted_answer": "", "predicted_evidence": ["Beyond the research on translation models, there are many research on the other relevant problems, such as sentence boundary detection for realtime speech translation BIBREF31 , BIBREF18 , BIBREF32 , BIBREF33 , BIBREF34 , low-latency simultaneous interpreting BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF35 , BIBREF36 , automatic punctuation annotation for speech transcription BIBREF37 , BIBREF38 , and discussion about human and machine in simultaneous interpreting BIBREF39 .", "Speech irregularities are kept in transcription while omitted in translation (eg. filler words like \u201c\u55ef, \u5443, \u554a\u201d, and unconscious repetitions like \u201c\u8fd9\u4e2a\u8fd9\u4e2a\u5462\u201d), which can be used to evaluate the robustness of the NMT model dealing with spoken language.", "baseline: A standard Transformer based model with big version of hyper parameters.", "As shown in Figure FIGREF19 , during training, we do not mask the source input, instead we mask the target sequence aligned to the first sub-sentence. This strategy will force the model to learn to complete the half-way done translation, rather than to concentrate on generating a translation of the full sentence.", "However, existing work pays less attention to the fluency of translation, which is extremely important in the context of simultaneous translation. For example, we have a sub-sentence NMT model that starts to translate after reading a sub-sentence rather than waiting until the end of a sentence like the full-sentence models does. This will definitely reduce the time waiting for the source language speech. However, as shown in the Figure FIGREF2 , the translation for each sub-sentence is barely adequate, whereas the translation of the entire source sentence lacks coherence and fluency. Moreover, it is clear that the model produces an inappropriate translation \u201cyour own\u201d for the source token \u201c\u81ea\u5df1\u201d due to the absence of the preceding sub-sentence."]}
{"question_id": "ba406e07c33a9161e29c75d292c82a15503beae5", "predicted_answer": "", "predicted_evidence": ["The existing research on speech translation can be divided into two types: the End-to-End model BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 and the cascaded model. The former approach directly translates the acoustic speech in one language, into text in another language without generating the intermediate transcription for the source language. Depending on the complexity of the translation task as well as the scarce training data, previous literatures explore effective techniques to boost the performance. For example pre-training BIBREF29 , multi-task learning BIBREF24 , BIBREF27 , attention-passing, BIBREF30 , and knowledge distillation BIBREF28 etc.,. However, the cascaded model remains the dominant approach and presents superior performance practically, since the ASR and NMT model can be optimized separately training on the large-scale corpus.", "We randomly extract several talks from the dataset, and divide them into the development and test set. In Table TABREF34 , we summarize the statistics of our dataset. The average number of utterances per talk is 152.6 in the training set, 59.75 in the dev set, and 162.5 in the test set.", "Many studies have proposed to synthesize realistic ASR errors, and augment them with translation training data, to enhance the robustness of the NMT model towards ASR errors BIBREF2 , BIBREF3 , BIBREF4 . However, most of these approaches depend on simple heuristic rules and only evaluate on artificially noisy test set, which do not always reflect the real noises distribution on training and inference BIBREF5 , BIBREF6 , BIBREF7 .", "Many studies present methods to improve the translation quality by enhancing the robustness of translation model against ASR errors BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . On the other hand, to reduce latency, some researchers propose models that start translating after reading a few source tokens BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF1 . As one representative work related to this topic, recently, we present a translation model using prefix-to-prefix framework with INLINEFORM0 policy BIBREF0 . This model is simple yet effective in practice, achieving impressive performance both on translation quality and latency.", "To validate the effectiveness of our translation model, we run two baseline models, baseline and sub-sentence. We also compare the translation quality as well as latency of our models with the wait-k model."]}
{"question_id": "3d662fb442d5fc332194770aac835f401c2148d9", "predicted_answer": "", "predicted_evidence": ["The English-German translation models are trained on WMT datasets, including News Commentary 13, Europarl v7, and Common Crawl, and evaluated on newstest2013 for early stopping. On the newstest2013 dev set, the En$\\rightarrow $De model reaches a BLEU-4 score of 19.6, and the De$\\rightarrow $En model reaches a BLEU-4 score of 24.6.", "A variety of paraphrase generation techniques have been proposed and studied BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17. Recently, BIBREF18 use a variational autoencoder to generate paraphrases from sentences and BIBREF19 use deep reinforcement learning to generate paraphrases. Several have generated paraphrases by separately modeling syntax and semantics BIBREF20, BIBREF21.", "We propose the task of question rewriting: converting textual ill-formed questions to well-formed ones while preserving their semantics.", "As the MQR dataset is constructed from 303 sub areas of the Stack Exchange networks, it covers a wide range of question domains. Table TABREF16 summarizes the number of categories in the TRAIN and DEVTEST portions of the MQR dataset, as well as the mean, standard deviation, minimum, and maximum number of instances per categories.", "We benchmark a variety of neural models trained on the MQR dataset, neural models trained with other question rewriting datasets, and other paraphrasing techniques. We find that models trained on the MQR and Quora datasets combined followed by grammatical error correction perform the best in the MQR question rewriting task."]}
{"question_id": "2280ed1e2b3e99921e2bca21231af43b58ca04f0", "predicted_answer": "", "predicted_evidence": ["The question is explicit. A well-formed question must be explicit and end with a question mark. A command or search query-like fragment is not well-formed.", "In following sections, when a transformer model is used, we follow the same setting as described above.", "We randomly sample 100 question pairs from DEVTEST for annotation of semantic equivalence. Two annotators produced binary judgments for all 100 pairs. Example pairs are shown in Table TABREF14.", "We use the Tensor2Tensor BIBREF31 implementation of the transformer model BIBREF3. We use their \u201ctransformer_base\u201d hyperparameter setting. The details are as follows: batch size 4096, hidden size 512, 8 attention heads, 6 transformer encoder and decoder layers, learning rate 0.1 and 4000 warm-up steps. We train the model for 250,000 steps and perform early stopping using the loss values on the DEV set.", "We also tried applying the transformer (trained on MQR) twice, but it hurts the performance compared to applying it only once (see Table TABREF32)."]}
{"question_id": "961a97149127e1123c94fbf7e2021eb1aa580ecb", "predicted_answer": "", "predicted_evidence": ["As the MQR dataset is constructed from 303 sub areas of the Stack Exchange networks, it covers a wide range of question domains. Table TABREF16 summarizes the number of categories in the TRAIN and DEVTEST portions of the MQR dataset, as well as the mean, standard deviation, minimum, and maximum number of instances per categories.", "A variety of paraphrase generation techniques have been proposed and studied BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17. Recently, BIBREF18 use a variational autoencoder to generate paraphrases from sentences and BIBREF19 use deep reinforcement learning to generate paraphrases. Several have generated paraphrases by separately modeling syntax and semantics BIBREF20, BIBREF21.", "To evaluate model performance, we apply our trained models to rewrite the ill-formed questions in TEST and treat the well-formed question in each pair as the reference sentence. We then compute BLEU-4 BIBREF28, ROUGE-1, ROUGE-2, ROUGE-L BIBREF29, and METEOR BIBREF30 scores. As a baseline, we also evaluate the original ill-formed question using the automatic metrics.", "This yields the MQR dataset. We use the following heuristic criteria to split MQR into TRAIN, DEV, and TEST sets:", "Is the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors."]}
{"question_id": "1e4f45c956dfb40fadb8e10d4c1bfafa8968be4d", "predicted_answer": "", "predicted_evidence": ["As the MQR dataset is constructed from 303 sub areas of the Stack Exchange networks, it covers a wide range of question domains. Table TABREF16 summarizes the number of categories in the TRAIN and DEVTEST portions of the MQR dataset, as well as the mean, standard deviation, minimum, and maximum number of instances per categories.", "Round trip neural machine translation is an effective approach for question or sentence paraphrasing BIBREF32, BIBREF9, BIBREF20. It first translates a sentence to another pivot language, then translates it back to the original language. We consider the use of both German (De) and French (Fr) as the pivot language, so we require translation systems for En$\\leftrightarrow $De and En$\\leftrightarrow $Fr.", "Is the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.", "We also benchmark other methods involving different training datasets and models. All the methods in this subsection use transformer models.", "We use the attention mechanism proposed by BIBREF2. We use the Tensor2Tensor implementation BIBREF31 with their provided Luong Attention hyperparameter settings. We set batch size to 4096. The hidden size is 1000 and we use 4 LSTM hidden layers following BIBREF2."]}
{"question_id": "627ce8a1db08a732d5a8f7e1f8a72e3de89847e6", "predicted_answer": "", "predicted_evidence": ["Examples are shown in Table TABREF2. We release our TRAIN/DEV/TEST splits of the MQR dataset to encourage research in question rewriting.", "To evaluate model performance, we apply our trained models to rewrite the ill-formed questions in TEST and treat the well-formed question in each pair as the reference sentence. We then compute BLEU-4 BIBREF28, ROUGE-1, ROUGE-2, ROUGE-L BIBREF29, and METEOR BIBREF30 scores. As a baseline, we also evaluate the original ill-formed question using the automatic metrics.", "Table TABREF34 shows two example questions rewritten by different methods. The questions rewritten by GEC remain unchanged but are still of low quality, whereas ParaNMT and round trip NMT make a variety of changes, resulting in large variations in question quality and semantics. Methods trained on MQR excel at converting ill-formed questions into explicit ones (e.g., adding \u201cWhat is\u201d in the first example and \u201cHow to\u201d in the second example), but sometimes make grammatical errors (e.g., Trans. (MQR + Quora) misses \u201ca\u201d in the second example). According to Table TABREF32, combining neural models trained on MQR and GEC achieves the best results in automatic metrics. However, they still suffer from semantic drift. In the first example of Table TABREF34, the last two rewrites show significant semantic mistakes, generating non-existent words \u201cwidebutcherblock\u201d and \u201cwidebitcherblock\u201d.", "To better evaluate model performance, we conduct a human evaluation on the model rewritten questions following the same guidelines from the \u201cDataset Quality\u201d subsection. Among the 300 questions annotated earlier, we chose the ill-formed questions from the TEST split, which yields 75 questions. We evaluate questions rewritten by three methods (Transformer (MQR + Quora), GEC, and Transformer (MQR + Quora) $\\rightarrow $ GEC), and ask annotators to determine the qualities of the rewritten questions. To understand if question meanings change after rewriting, we also annotate whether a model rewritten question is semantically equivalent to the ill-formed question or equivalent to the well-formed one.", "The results of adding training data are summarized in Table TABREF31. Adding the identity pairs improves the ROUGE and METEOR scores, which are focused more on recall, while harming BLEU, which is focused on precision. We hypothesize that adding auto-encoding data improves semantic preservation, which is expected to help the recall-oriented metrics. Adding Quora Question Pairs improves performance on TEST but adding Paralex pairs does not. The reason may stem from domain differences: WikiAnswers (used in Paralex) is focused on factoid questions answered by encyclopedic knowledge while Quora and Stack Exchange questions are mainly answered by community contributors. Semantic drift occurs more often in Paralex question pairs as Paralex is constructed from question clusters, and a cluster often contains more than 5 questions with significant variation."]}
{"question_id": "80bb07e553449bde9ac0ff35fcc718d7c161f2d4", "predicted_answer": "", "predicted_evidence": ["Supervised (Word-Level):", "Supervised (Word-Level):", "Supervised (Byte Pair Encoding):", "There is an increasing need to use neural machine translation techniques for African languages. Due to the low-resourced nature of these languages, these techniques can help build useful translation models that could hopefully help with the preservation and discoverability of these languages.", "Supervised (Byte Pair Encoding):"]}
{"question_id": "c8f8ecac23a991bceb8387e68b3b3f2a5d8cf029", "predicted_answer": "", "predicted_evidence": ["Surprisingly, the unsupervised model performs better at some relatively simple translation examples than both supervised models. The third example is a typical such case.", "Special thanks to the Masakhane group for catalysing this work.", "Supervised (Byte Pair Encoding):", "The only known natural language processing work done on any variant of Pidgin English is by BIBREF6. The authors provided the largest known Nigerian Pidgin English corpus and trained the first ever translation models between both languages via unsupervised neural machine translation due to the absence of parallel training data at the time.", "Taking a look at the results from the word-level tokenization Pidgin to English models, the supervised model outperforms the unsupervised model, achieving a BLEU score of 24.67 in comparison to the BLEU score of 7.93 achieved by the unsupervised model. The supervised model trained with byte pair encoding tokenization achieved a BLEU score of 13.00. One thing that is worthy of note is that word-level tokenization methods seem to perform better on Pidgin to English translation models, in comparison to English to Pidgin translation models."]}
{"question_id": "28847b20ca63dc56f2545e6f6ec3082d9dbe1b3f", "predicted_answer": "", "predicted_evidence": ["When analyzed by L1 speakers, the translation qualities were rated very well. In particular, the unsupervised model makes many translations that did not exactly match the reference translation, but conveyed the same meaning. More analysis and translation examples are in the Appendix.", "Taking a look at the results from the word-level tokenization Pidgin to English models, the supervised model outperforms the unsupervised model, achieving a BLEU score of 24.67 in comparison to the BLEU score of 7.93 achieved by the unsupervised model. The supervised model trained with byte pair encoding tokenization achieved a BLEU score of 13.00. One thing that is worthy of note is that word-level tokenization methods seem to perform better on Pidgin to English translation models, in comparison to English to Pidgin translation models.", "Supervised (Word-Level):", "English to Pidgin:", "There is an increasing need to use neural machine translation techniques for African languages. Due to the low-resourced nature of these languages, these techniques can help build useful translation models that could hopefully help with the preservation and discoverability of these languages."]}
{"question_id": "2d5d0b0c54105717bf48559b914fefd0c94964a6", "predicted_answer": "", "predicted_evidence": ["Some work has been done on developing neural machine translation baselines for African languages. BIBREF4 implemented a transformer model which significantly outperformed existing statistical machine translation architectures from English to South-African Setswana. Also, BIBREF5 went further, to train neural machine translation models from English to five South African languages using two different architectures - convolutional sequence-to-sequence and transformer. Their results showed that neural machine translation models are very promising for African languages.", "The unsupervised model performed poorly at some simple translation examples, such as the first translation example.", "Over 500 languages are spoken in Nigeria, but Nigerian Pidgin is the uniting language in the country. Between three and five million people are estimated to use this language as a first language in performing their daily activities. Nigerian Pidgin is also considered a second language to up to 75 million people in Nigeria, accounting for about half of the country's population according to BIBREF0.", "The only known natural language processing work done on any variant of Pidgin English is by BIBREF6. The authors provided the largest known Nigerian Pidgin English corpus and trained the first ever translation models between both languages via unsupervised neural machine translation due to the absence of parallel training data at the time.", "Supervised (Byte Pair Encoding):"]}
{"question_id": "dd81f58c782169886235c48b8f9a08e0954dd3ae", "predicted_answer": "", "predicted_evidence": ["The language is considered an informal lingua franca and offers several benefits to the country. In 2020, 65% of Nigeria's population is estimated to have access to the internet according to BIBREF1. However, over 58.4% of the internet's content is in English language, while Nigerian languages, such as Igbo, Yoruba and Hausa, account for less than 0.1% of internet content according to BIBREF2. For Nigerians to truly harness the advantages the internet offers, it is imperative that English content is able to be translated to Nigerian languages, and vice versa.", "Unsupervised (Word-Level):", "Over 500 languages are spoken in Nigeria, but Nigerian Pidgin is the uniting language in the country. Between three and five million people are estimated to use this language as a first language in performing their daily activities. Nigerian Pidgin is also considered a second language to up to 75 million people in Nigeria, accounting for about half of the country's population according to BIBREF0.", "The dataset used for the supervised was obtained from the JW300 large-scale, parallel corpus for Machine Translation (MT) by BIBREF8. The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs. Both the supervised and unsupervised models were evaluated on a test set of 2101 sentences preprocessed by the Masakhane group. The model with the highest test BLEU score is selected as the best.", "The supervised translation models seem to perform better at longer example translations than the unsupervised example."]}
{"question_id": "c138a45301713c1a9f6edafeef338ba2f99220ce", "predicted_answer": "", "predicted_evidence": ["Similarity to known positive/negative examples (kNN) (2+1 S+C features): We used three more features inspired by $k$-nearest neighbor (kNN) classification. The first one (sentence-level) uses the maximum over the training sentences of the number of matching words between the testing and the training sentence, which is further multiplied by $-1$ if the latter was not check-worthy. We also used another version of the feature, where we multiplied it by 0 if the speakers were different (contextual). A third version took as a training set all claims checked by PolitiFact (excluding the target sentence).", "We developed a rich input representation in order to model and to learn the check-worthiness concept. The feature types we implemented operate at the sentence- (S) and at the context-level (C), in either case targeting segments by the same speaker. The context features are novel and a contribution of this study. We also implemented a set of core features to compare to the state of the art. All of them are described below.", "", "We experimented with two learning algorithms. The first one is an SVM classifier with an RBF kernel. The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU BIBREF25 as the activation function and we trained the network with Stochastic Gradient Descent BIBREF26.", ""]}
{"question_id": "56d788af4694c1cd1eebee0b83c585836d1f5f99", "predicted_answer": "", "predicted_evidence": ["Metadata (8 C features): Check-worthy claims often contain mutual accusations between the opponents, as the following example shows (from the 2nd presidential debate):", "Thus, we use a feature that indicates whether the target sentence mentions the name of the opponent, whether the speaker is the moderator, and also who is speaking (3 features). We further use three binary features, indicating whether the target sentence is followed by a system message: applause, laugh, or cross-talk.", "In this paper, we focus on the first step: predicting check-worthiness of claims. Our contributions can be summarized as follows:", "Modeling the context: We develop a novel approach for automatically predicting which claims should be prioritized for fact-checking, based on a rich input representation. In particular, we model not only the textual content, but also the context: how the target claim relates to the current segment, to neighboring segments and sentences, and to the debate as a whole, and also how the opponents and the public react to it.", "where $n$ is the number of sentences to rank in the debate, $P(k)$ is the precision at $k$ and $rel(k)=1$ if the utterance at position $k$ is check-worthy, and it is 0 otherwise."]}
{"question_id": "34b434825f0ca3225dc8914f9da865d2b4674f08", "predicted_answer": "", "predicted_evidence": ["The feature groups in this subsection contain a mixture of sentence- and of contextual-level features. For example, if we use a discourse parser to parse the target sentence only, any features we extract from the parse would be sentence-level. However, if we parse an entire segment, we would also have contextual features.", "Metadata (8 C features): Check-worthy claims often contain mutual accusations between the opponents, as the following example shows (from the 2nd presidential debate):", "Note that the investigative journalists did not select the check-worthy claims in isolation. Our analysis shows that these include claims that were highly disputed during the debate, that were relevant to the topic introduced by the moderator, etc. We will make use of these contextual dependencies below, which is something that was not previously tried in related work.", "We experimented with two learning algorithms. The first one is an SVM classifier with an RBF kernel. The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU BIBREF25 as the activation function and we trained the network with Stochastic Gradient Descent BIBREF26.", "We also measure the recall at the $R$-th position of returned sentences for each debate. $R$ is the number of relevant documents for that debate and the metric is known as $R$-Precision ($R$-Pr)."]}
{"question_id": "61a2599acfbd3d75de58e97ecdba2d9cf0978324", "predicted_answer": "", "predicted_evidence": ["Ultimately, we ended up with a dataset of four debates, with a total of 5,415 sentences. The agreement between the sources was low as Table TABREF8 shows: only one sentence was selected by all nine sources, 57 sentences by at least five, 197 by at least three, 388 by at least two, and 880 by at least one. The reason for this is that the different media aimed at annotating sentences according to their own editorial line, rather than trying to be exhaustive in any way. This suggests that the task of predicting which sentence would contain check-worthy claims will be challenging. Thus, below we focus on a ranking task rather than on absolute predictions. Moreover, we predict which sentence would be selected (i) by at least one of the media, or (ii) by a specific medium.", "This research was performed by the Arabic Language Technologies group at Qatar Computing Research Institute, HBKU, within the Interactive sYstems for Answer Search project (Iyas).", "The models were trained to classify sentences as positive if one or more media had fact-checked a claim inside the target sentence, and negative otherwise. We then used the classifier scores to rank the sentences with respect to check-worthiness.", "We model this by counting the negations in the target sentence as found in a dictionary of negation cues such as not, didn't, and never. We further model the context as the number of such cues in the two neighboring sentences from the same segment and the two neighboring segments.", ""]}
{"question_id": "cf58d25bfa2561a359fdd7b6b20aef0b41dc634e", "predicted_answer": "", "predicted_evidence": ["The process starts when a document is made public. First, an intrinsic analysis is carried out in which check-worthy text fragments are identified. Then, other documents that might support or rebut a claim in the document are retrieved from various sources. Finally, by comparing a claim against the retrieved evidence, a system can determine whether the claim is likely true or likely false. For instance, BIBREF8 do this on the basis of a knowledge graph derived from Wikipedia. The outcome could then be presented to a human expert for final judgment.", "We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7. This could include not just a statement about factuality, but any free text that journalists decided to add, e.g., links to biographies or behavioral analysis of the opponents and moderators. We converted this to binary annotation about whether a particular sentence was annotated for factuality by a given source. Whenever one or more annotations were about part of a sentence, we selected the entire sentence, and when an annotation spanned over multiple sentences, we selected each of them.", "We performed error analysis of the decisions made by the Neural Network that uses all available features. Below we present some examples of False Positives (FP) and False Negatives (FN):", "In future work, we plan to extend our dataset with additional debates, e.g., from other elections, but also with interviews and general discussions. We would also like to experiment with distant supervision, which would allow us to gather more training data, thus facilitating deep learning. We further plan to extend our system with finding claims at the sub-sentence level, as well as with automatic fact-checking of the identified claims.", "First, there is a random baseline, followed by an SVM classifier based on a bag-of-words representation with TF.IDF weights learned on the training data. Then come three versions of the ClaimBuster system: CB-Platform uses scores from the online demo, which we accessed on December 20, 2016, and SVM$_{CBfeat}$ and FNN$_{CBfeat}$ are our re-implementations, trained on our dataset."]}
{"question_id": "e86b9633dc691976dd00ed57d1675e1460f7167b", "predicted_answer": "", "predicted_evidence": ["We use a bert classifier to implement this classification model.", "Since the correct answer to test set has not yet been released, we are unable to verify the accuracy of each model. According to the final version submitted on the website, our model has a F1-score of 70.45% in test set.", "Score7 Similarity between Entity-mention and Question. We simply fine-tune bert model to measure how well an entity matches a question. Matching rate is the corresponding score.", "We combine the above two methods. On the one hand, we use the retrieve based method to sort KB relationships and entities, and on the other hand, we use the most related relationship and entity to generate the sparql statement to query the final answer.", "(3) When the two-hop object appears in the sentence, select the intermediate result ?y is the final result. Still consider question \u201cWhat TV series did actor A and actor B play together?\u201d, the highest score is \u201cactor A ==act in==> TV serie X <==act in== actor B\u201d, but ?y(TV serie X ) rather than ?x(actor B) is the correct result. Thus, the sparql should be \u201cselect ?y where <actor A> <act in> ?y. <actor B> <act in> ?y.\u201d"]}
{"question_id": "b0edb9023f35a5a02eb8fb968e880e36233e66b3", "predicted_answer": "", "predicted_evidence": ["When constructing the training set for the similarity scoring model, since the positive samples are significantly smaller than the negative samples, we performed 5 oversamplings on the positive samples and 5 randomly selected training sets from all negative samples of each data. The positive and negative relationship of the one-hop relationship is easier to understand. The positive sample of the two-hop relationship is the concatenation of the correct one-hop and two-hop relationship, and the wrong relationship is not put into the negative sample because it may interfere with the one-hop relationship score. In addition, entities in all questions are replaced with <e> in order to reduce entity interference. We tried several common models, the results of relation scoring model is shown in table TABREF20. Bert model has the highest accuracy of 95.7%.", "Semantic Parsing based approach is a linguistic method that transforms natural language into logic forms and queries them in the knowledge graph through corresponding semantic representations, such as lambda-Caculus, to arrive at an answer. Semantic parsing-based methods, including semantic parsing based on Lambda Dependency-Based Compositional Semantics (Lambda-DCS) BIBREF0, BIBREF1, semantic parsing based on Combinatory Categorical Grammars (CCG), semantic parsing based on Neural Machine Translation(NMT), and semantic parsing based on deep learning BIBREF2 released by Microsoft in 2015.", "Since the correct answer to test set has not yet been released, we are unable to verify the accuracy of each model. According to the final version submitted on the website, our model has a F1-score of 70.45% in test set.", "Score2 The Out-degree of Entity in the KB. An entity with higher out-degree is more likely to be the topic entity. Meanwhile, it is more efficient to query the nodes pointed to other entity in the knowledge base than to be pointed.", "(2) When time appears in the question, it is unified into the format of \u201cYYYY-MM-DD\u201d (consistent with the knowledge graph)."]}
{"question_id": "8c872236e4475d5d0969fb90d2df94589c7ab1c4", "predicted_answer": "", "predicted_evidence": ["Sequence classification. Bag-of-n-gram models are used for language identification BIBREF86 , BIBREF87 , topic labeling BIBREF88 , authorship attribution BIBREF89 , word/text similarity BIBREF2 , BIBREF90 , BIBREF4 and word sense disambiguation BIBREF3 .", "Table TABREF15 shows that sequence-preserving position embeddings perform better than bag-of-ngram representations.", "We ran some experiments with more epochs, but this did not improve the results.", "We group character-level models that are based on tokenization as a necessary preprocessing step in the category of tokenization-based approaches. Those can be either models with tokenized text as input or models that operate only on individual tokens (such as studies on morphological inflection of words).", "In the following, we explore a subset of bag-of-ngram models that are used for representation learning, information retrieval, and sequence classification tasks."]}
{"question_id": "f6ba0a5cfd5b35219efe5e52b0a5b86ae85c5abd", "predicted_answer": "", "predicted_evidence": ["Sequence classification. Another recent end-to-end model uses character-level inputs for document classification BIBREF110 , BIBREF111 , BIBREF112 . To capture long-term dependencies of the input, the authors combine convolutional layers with recurrent layers. The model is evaluated on sentiment analysis, ontology classification, question type classification and news categorization.", "A more recent study BIBREF4 trains character n-gram embeddings in an end-to-end fashion with a neural network. They are evaluated on word similarity, sentence similarity and part-of-speech tagging.", "W2V hyperparameter settings. size of word vectors: 200, max skip length between words: 5, threshold for occurrence of words: 0, hierarchical softmax: 0, number of negative examples: 5, threads: 50, training iterations: 1, min-count: 5, starting learning rate: .025, classes: 0", "The most important challenge that we need to address is how to use nonsymbolic text representation for tasks that are word-based like part-of-speech tagging. This may seem like a contradiction at first, but gillick16 have shown how character-based methods can be used for \u201csymbolic\u201d tasks. We are currently working on creating an analogous evaluation for our nonsymbolic text representation.", "Character n-grams have a long history as features for specific NLP applications, such as information retrieval. However, there is also work on representing words or larger input units, such as phrases, with character n-gram embeddings. Those embeddings can be within-token or cross-token, i.e., there is no tokenization necessary."]}
{"question_id": "b21f61c0f95fefdb1bdb90d51cbba4655cd59896", "predicted_answer": "", "predicted_evidence": ["The learning procedure takes a set of ngrams and their embeddings as input. It then exhaustively searches for all pairs of ngrams, for all pairs of characters INLINEFORM7 / INLINEFORM8 , for each of the three templates. (This takes about 10 hours on a multicore server.) When two matching embeddings exist, we compute their cosine. For example, for the operation \u201cdelete space before M\u201d, an ngram pair from our embeddings that matches is \u201c@Mercedes\u201d / \u201cMercedes\u201d and we compute its cosine. As the characteristic statistic of an operation we take the average of all cosines; e.g., for \u201cdelete space before M\u201d the average cosine is .7435. We then rank operations according to average cosine and take the first INLINEFORM9 as the definition of INLINEFORM10 where INLINEFORM11 is a parameter. For characters that are replaced by each other (e.g., 1, 2, 3 in Table TABREF7 ), we compute the equivalence class and then replace the learned operations with ones that replace a character by the canonical member of its equivalence class (e.g., 2 INLINEFORM12 1, 3 INLINEFORM13 1).", "Future work.", "Similar to bag-of-n-gram models, end-to-end models are tokenization-free. Their input is a sequence of characters or bytes and they are directly optimized on a (task-specific) objective. Thus, they learn their own, task-specific representation of the input sequences. Recently, character-based end-to-end models have gained a lot of popularity due to the success of neural networks.", "We evaluate the three models on an entity typing task, similar to BIBREF14 , but based on an entity dataset released by xie16entitydesc2 in which each entity has been assigned one or more types from a set of 50 types. For example, the entity \u201cHarrison Ford\u201d has the types \u201cactor\u201d, \u201ccelebrity\u201d and \u201caward winner\u201d among others. We extract mentions from FACC (http://lemurproject.org/clueweb12/FACC1) if an entity has a mention there or we use the Freebase name as the mention otherwise. This gives us a data set of 54,334, 6085 and 6747 mentions in train, dev and test, respectively. Each mention is annotated with the types that its entity has been assigned by xie16entitydesc2. The evaluation has a strong cross-domain aspect because of differences between FACC and Wikipedia, the training corpus for our representations. For example, of the 525 mentions in dev that have a length of at least 5 and do not contain lowercase characters, more than half have 0 or 1 occurrences in the Wikipedia corpus, including many like \u201cJOHNNY CARSON\u201d that are frequent in other case variants.", "A more recent study BIBREF4 trains character n-gram embeddings in an end-to-end fashion with a neural network. They are evaluated on word similarity, sentence similarity and part-of-speech tagging."]}
{"question_id": "0dbb5309d2be97f6eda29d7ae220aa16cafbabb7", "predicted_answer": "", "predicted_evidence": ["We explore the subset of these models that are used for sequence generation, sequence labeling, language modeling and sequence classification tasks.", "Sequence classification. Bag-of-n-gram models are used for language identification BIBREF86 , BIBREF87 , topic labeling BIBREF88 , authorship attribution BIBREF89 , word/text similarity BIBREF2 , BIBREF90 , BIBREF4 and word sense disambiguation BIBREF3 .", "Processing text vs. speech vs. images. gillick16 write: \u201cIt is worth noting that noise is often added ... to images ... and speech where the added noise does not fundamentally alter the input, but rather blurs it. [bytes allow us to achieve] something like blurring with text.\u201d It is not clear to what extent blurring on the byte level is useful; e.g., if we blur the bytes of the word \u201cuniversity\u201d individually, then it is unlikely that the noise generated is helpful in, say, providing good training examples in parts of the space that would otherwise be unexplored. In contrast, the text representation we have introduced in this paper can be blurred in a way that is analogous to images and speech. Each embedding of a position is a vector that can be smoothly changed in every direction. We have showed that the similarity in this space gives rise to natural variation.", "Recently, character-based neural network sequence-to-sequence models have been applied to instances of generation tasks like machine translation BIBREF93 , BIBREF94 , BIBREF95 , BIBREF96 , BIBREF97 (which was previously proposed on the token-level BIBREF98 ), question answering BIBREF99 and speech recognition BIBREF100 , BIBREF101 , BIBREF102 , BIBREF103 .", "Sequence-to-sequence generation. In 2011, the authors of BIBREF91 already proposed an end-to-end model for generating text. They train RNNs with multiplicative connections on the task of character-level language modeling. Afterwards, they use the model to generate text and find that the model captures linguistic structure and a large vocabulary. It produces only a few uncapitalized non-words and is able to balance parantheses and quotes even over long distances (e.g., 30 characters). A similar study by BIBREF92 uses a long short-term memory network to create character sequences."]}
{"question_id": "c27b885b1e38542244f52056abf288b2389b9fc6", "predicted_answer": "", "predicted_evidence": ["First, candidate images for each synset were sourced from commercial image search engines, including Google, Yahoo!, Microsoft's Live Search, Picsearch and Flickr BIBREF10 . Gender BIBREF11 and racial BIBREF12 biases have been demonstrated to exist in image search results (i.e. images of occupations), demonstrating that a more curated approach at the top of the funnel may be necessary to mitigate inherent biases of search engines. Second, English search queries were translated into Chinese, Spanish, Dutch and Italian using WordNet databases and used for image retrieval. While this is a step in the right direction, Chinese was the only non-Western European language used, and there exists, for example, Universal Multilingual WordNet which includes over 200 languages for translation BIBREF13 . Third, the authors quantify image diversity by computing the average image of each synset and measuring the lossless JPG file size. They state that a diverse synset will result in a blurrier average image and smaller file, representative of diversity in appearance, position, viewpoint and background.", "This lack of scrutiny into ImageNet's contents is concerning. Without a conscious effort to incorporate diversity in data collection, undesirable biases can collect and propagate. These biases can manifest in the form of patterns learned from data that are influential in the decision of a model, but are not aligned with values of society BIBREF6 . Age, gender and racial biases have been exposed in word embeddings BIBREF7 , image captioning models BIBREF8 , and commercial computer vision gender classifiers BIBREF9 . In the case of ImageNet, there is some evidence that CNNs pretrained on its data may also encode undesirable biases. Using adversarial examples as a form of model criticism, Stock and Cisse BIBREF6 discovered that prototypical examples of the synset `basketball' contain images of black persons, despite a relative balance of race in the class. They hypothesized that an under-representation of black persons in other classes may lead to a biased representation of `basketball'.", "ImageNet BIBREF0 , released in 2009, is a canonical dataset in computer vision. ImageNet follows the WordNet lexical database of English BIBREF1 , which groups words into synsets, each expressing a distinct concept. ImageNet contains 14,197,122 images in 21,841 synsets, collected through a comprehensive web-based search and annotated with Amazon Mechanical Turk (AMT) BIBREF0 . The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) BIBREF2 , held annually from 2010 to 2017, was the catalyst for an explosion of academic and industry interest in deep learning. A subset of 1,000 synsets were used in the ILSVRC classification task. Seminal work by Krizhevsky et al. BIBREF3 in the 2012 event cemented the deep convolutional neural network (CNN) as the preeminent model in computer vision.", "Today, work in computer vision largely follows a standard process: a pretrained CNN is downloaded with weights initialized to those trained on the 2012 ILSVRC subset of ImageNet, the network is adjusted to fit the desired task, and transfer learning is performed, where the CNN uses the pretrained weights as a starting point for training new data on the new task. The use of pretrained CNNs is instrumental in applications as varied as instance segmentation BIBREF4 and chest radiograph diagnosis BIBREF5 .", "Before proceeding with annotation, there is merit in contextualizing this study with a look at the methodology proposed by Deng et al. in the construction of ImageNet. A close reading of their data collection and quality assurance processes demonstrates that the conscious inclusion of demographic diversity in ImageNet was lacking BIBREF0 ."]}
{"question_id": "1ce6c09cf886df41a3d3c52ce82f370c5a30334a", "predicted_answer": "", "predicted_evidence": ["The FaceBoxes network BIBREF15 is employed for face detection, consisting of a lightweight CNN that incorporates novel Rapidly Digested and Multiple Scale Convolutional Layers for speed and accuracy, respectively. This model was trained on the WIDER FACE dataset BIBREF16 and achieves average precision of 95.50% on the Face Detection Data Set and Benchmark (FDDB) BIBREF17 . On a subset of 1,000 images from FDDB hand-annotated by the author for apparent age and gender, the model achieves a relative fair performance across intersectional groups, as show in Table TABREF1 .", "By convention, computer vision practitioners have effectively abstracted away the details of ImageNet. While this has proved successful in practical applications, there is merit in taking a step back and scrutinizing common practices. In the ten years following the release of ImageNet, there has not been a comprehensive study into the composition of images in the classes it contains.", "We recognize that a binary representation of gender does not adequately capture the complexities of gender or represent transgender identities. In this work, we express gender as a continuous value between 0 and 1. When thresholding at 0.5, we use the sex labels of `male' and `female' to define gender classes, as training datasets and evaluation benchmarks use this binary label system. We again follow Merler et al. BIBREF18 and employ a DEX model to annotate the gender of an individual. When tested on APPA-REAL, with enhanced annotations provided by BIBREF21 , the model achieves an accuracy of 91.00%, however its errors are not evenly distributed, as shown in Table TABREF3 . The model errs more on younger and older age groups and on those with a female gender label.", "The task of apparent age annotation arises as ground-truth ages of individuals in images are not possible to obtain in the domain of web-scraped datasets. In this work, we follow Merler et al. BIBREF18 and employ the Deep EXpectation (DEX) model of apparent age BIBREF19 , which is pre-trained on the IMDB-WIKI dataset of 500k faces with real ages and fine-tuned on the APPA-REAL training and validation sets of 3.6k faces with apparent ages, crowdsourced from an average of 38 votes per image BIBREF20 . As show in Table TABREF2 , the model achieves a mean average error of 5.22 years on the APPA-REAL test set, but exhibits worse performance on younger and older age groups.", "First, candidate images for each synset were sourced from commercial image search engines, including Google, Yahoo!, Microsoft's Live Search, Picsearch and Flickr BIBREF10 . Gender BIBREF11 and racial BIBREF12 biases have been demonstrated to exist in image search results (i.e. images of occupations), demonstrating that a more curated approach at the top of the funnel may be necessary to mitigate inherent biases of search engines. Second, English search queries were translated into Chinese, Spanish, Dutch and Italian using WordNet databases and used for image retrieval. While this is a step in the right direction, Chinese was the only non-Western European language used, and there exists, for example, Universal Multilingual WordNet which includes over 200 languages for translation BIBREF13 . Third, the authors quantify image diversity by computing the average image of each synset and measuring the lossless JPG file size. They state that a diverse synset will result in a blurrier average image and smaller file, representative of diversity in appearance, position, viewpoint and background."]}
{"question_id": "5429add4f166a3a66bec2ba22232821d2cbafd62", "predicted_answer": "", "predicted_evidence": ["ImageNet BIBREF0 , released in 2009, is a canonical dataset in computer vision. ImageNet follows the WordNet lexical database of English BIBREF1 , which groups words into synsets, each expressing a distinct concept. ImageNet contains 14,197,122 images in 21,841 synsets, collected through a comprehensive web-based search and annotated with Amazon Mechanical Turk (AMT) BIBREF0 . The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) BIBREF2 , held annually from 2010 to 2017, was the catalyst for an explosion of academic and industry interest in deep learning. A subset of 1,000 synsets were used in the ILSVRC classification task. Seminal work by Krizhevsky et al. BIBREF3 in the 2012 event cemented the deep convolutional neural network (CNN) as the preeminent model in computer vision.", "Gender BIBREF11 and racial BIBREF12 biases have been demonstrated to exist in image search results (i.e. images of occupations), demonstrating that a more curated approach at the top of the funnel may be necessary to mitigate inherent biases of search engines. Second, English search queries were translated into Chinese, Spanish, Dutch and Italian using WordNet databases and used for image retrieval. While this is a step in the right direction, Chinese was the only non-Western European language used, and there exists, for example, Universal Multilingual WordNet which includes over 200 languages for translation BIBREF13 . Third, the authors quantify image diversity by computing the average image of each synset and measuring the lossless JPG file size. They state that a diverse synset will result in a blurrier average image and smaller file, representative of diversity in appearance, position, viewpoint and background. This method, however, cannot quantify diversity with respect to demographic characteristics such as age, gender, and skin type.", "First, candidate images for each synset were sourced from commercial image search engines, including Google, Yahoo!, Microsoft's Live Search, Picsearch and Flickr BIBREF10 . Gender BIBREF11 and racial BIBREF12 biases have been demonstrated to exist in image search results (i.e. images of occupations), demonstrating that a more curated approach at the top of the funnel may be necessary to mitigate inherent biases of search engines. Second, English search queries were translated into Chinese, Spanish, Dutch and Italian using WordNet databases and used for image retrieval. While this is a step in the right direction, Chinese was the only non-Western European language used, and there exists, for example, Universal Multilingual WordNet which includes over 200 languages for translation BIBREF13 . Third, the authors quantify image diversity by computing the average image of each synset and measuring the lossless JPG file size.", "By convention, computer vision practitioners have effectively abstracted away the details of ImageNet. While this has proved successful in practical applications, there is merit in taking a step back and scrutinizing common practices. In the ten years following the release of ImageNet, there has not been a comprehensive study into the composition of images in the classes it contains.", "The FaceBoxes network BIBREF15 is employed for face detection, consisting of a lightweight CNN that incorporates novel Rapidly Digested and Multiple Scale Convolutional Layers for speed and accuracy, respectively. This model was trained on the WIDER FACE dataset BIBREF16 and achieves average precision of 95.50% on the Face Detection Data Set and Benchmark (FDDB) BIBREF17 . On a subset of 1,000 images from FDDB hand-annotated by the author for apparent age and gender, the model achieves a relative fair performance across intersectional groups, as show in Table TABREF1 ."]}
{"question_id": "d3d6a4a721b8bc9776f62759b8d9be1a19c6b0d2", "predicted_answer": "", "predicted_evidence": ["The encoder adopts the form of a bidirectional RNN (BiRNN) BIBREF9 , in which the hidden units can be either GRUs BIBREF15 or LSTMs BIBREF16 . In this paper, we used the GRU units. This BiRNN decoder consists of a forward RNN INLINEFORM0 and a backward RNN INLINEFORM1 . The forward RNN reads the source sentence from left to right and generates a sequence of forward annotations: INLINEFORM2", "Similarly, the backward RNN reads the input sequence from right to left and generates a sequence of backward annotations: INLINEFORM0", "where INLINEFORM0 is the decoding step, INLINEFORM1 is the context vector produced by the attention mechanism on the original input, and the INLINEFORM2 is the context vector produced by the attention mechanism on the draft translation. These two context vectors are computed exactly as the attention mechanism of the conventional attention-based NMT model, as presented in the previous section.", "The training of the double-attention NMT model is similar to the conventional attention-based NMT model, though the log likelihood function now depends on two input sequences INLINEFORM0 and INLINEFORM1 . This is written as follows: DISPLAYFORM0", "where INLINEFORM0 is the hidden state of the decoder at step INLINEFORM1 , and INLINEFORM2 is the attention function that can be implemented by a neural network. The context vector INLINEFORM3 is calculated as a weighted sum of annotations INLINEFORM4 , given by: DISPLAYFORM0"]}
{"question_id": "cc8f495cac0af12054c746a5b796e989ff0e5d5f", "predicted_answer": "", "predicted_evidence": ["All the parameters in the attention-based NMT model are optimized by maximizing the following conditional log-likelihood on the training dataset: DISPLAYFORM0", "where INLINEFORM0 is the the hidden state of the decoder at the INLINEFORM1 step, and it is updated according to the previous hidden state INLINEFORM2 , the previous output INLINEFORM3 , and the context vector INLINEFORM4 : DISPLAYFORM0", "where INLINEFORM0 is the softmax function, and INLINEFORM1", "This typical attention-based model is shown in Fig. 1, where the encoder and decoders are implemented as two RNNs. Put it in brief, a source sentence INLINEFORM0 is encoded by the encoder RNN into a sequence of annotations INLINEFORM1 . Then the decoder RNN initiates a decoding process from a `start' symbol. At each decoding step INLINEFORM2 , the decoder computes the relevance between the decoder state INLINEFORM3 and each annotation INLINEFORM4 , resulting in the attention weight INLINEFORM5 . The target word is generated by maximizing the conditional probability INLINEFORM6 .", "In this paper, we propose a two-stage translation approach to tackle this problem. This approach is based on the idea of drafting-and-refinement, by which a draft translation is produced at the first stage, and at the second stage, the draft is refined by referring to the draft translation. Since the draft has given a rough idea what the translation would be, the right context can be obtained and utilized to make the refinement. In our implementation, the first stage (drafting) uses a typical attention-based NMT system, and the second stage (refinement) uses a double-attention NMT model that we will present shortly after."]}
{"question_id": "64c45fdb536ae294cf06716ac20d08b5fdb7944d", "predicted_answer": "", "predicted_evidence": ["For the attention-based NMT, the posterior probability for the target word prediction is in the form INLINEFORM0 . Notice that it is conditioned on the entire source sentence INLINEFORM1 and the decoding history INLINEFORM2 , which is the left context. However, it does not involve any right context, although that information might be useful.", "As soon as we get the context vector INLINEFORM0 from INLINEFORM1 at decoding step INLINEFORM2 , the conditional probability of selecting a word INLINEFORM3 is calculated as: DISPLAYFORM0", "The encoder adopts the form of a bidirectional RNN (BiRNN) BIBREF9 , in which the hidden units can be either GRUs BIBREF15 or LSTMs BIBREF16 . In this paper, we used the GRU units. This BiRNN decoder consists of a forward RNN INLINEFORM0 and a backward RNN INLINEFORM1 . The forward RNN reads the source sentence from left to right and generates a sequence of forward annotations: INLINEFORM2", "All the parameters in the attention-based NMT model are optimized by maximizing the following conditional log-likelihood on the training dataset: DISPLAYFORM0", "The training of the double-attention NMT model is similar to the conventional attention-based NMT model, though the log likelihood function now depends on two input sequences INLINEFORM0 and INLINEFORM1 . This is written as follows: DISPLAYFORM0"]}
{"question_id": "bab4ae97afd598a11d1fc7c05c6fdb98c30cafe0", "predicted_answer": "", "predicted_evidence": ["\u2022 INLINEFORM0 Efficient: it supports both shortcut and command line annotation models to accelerate the annotating process.", "There exists a range of text span annotation tools which focus on different aspects of the annotation process. Stanford manual annotation tool is a lightweight tool but does not support result analysis and system recommendation. Knowtator BIBREF6 is a general-task annotation tool which links to a biomedical onto ontology to help identify named entities and relations. It supports quality control during the annotation process by integrating simple inter-annotator evaluation, while it cannot figure out the detailed disagreed labels. WordFreak BIBREF3 adds a system recommendation function and integrates active learning to rank the unannotated sentences based on the recommend confidence, while the post-annotation analysis is not supported.", "The differences between Yedda and related work are summarised in Table TABREF2 . Here \u201cSelf Consistency\u201d represents whether the tool works independently or it relies on pre-installed packages. Compared to these tools, Yedda provides a lighter but more systematic choice with more flexibility, efficiency and less dependence on system environment for text span annotation. Besides, Yedda offers administrator useful toolkits for evaluating the annotation quality and analyze the detailed disagreements within annotators.", "\u2022 INLINEFORM0 Content comparison: this function gives the detailed comparison of two annotated files in whole content. It highlights the annotated parts of two annotators and assigns different colors for the agreed and disagreed span.", "It is inevitable that the annotator or the recommending system gives incorrect annotations or suggestions. Based on our annotation experience, we found that the time cost of annotation correction cannot be neglected. Therefore, Yedda provides several efficient modification actions to revise the annotation:"]}
{"question_id": "f5913e37039b9517a323ec700b712e898316161b", "predicted_answer": "", "predicted_evidence": ["\u2022 INLINEFORM0 Comprehensive: it integrates useful toolkits to give the statistical index of analyzing multi-user annotation results and generate detailed content comparison for annotation pairs.", "\u2022 INLINEFORM0 Content comparison: this function gives the detailed comparison of two annotated files in whole content. It highlights the annotated parts of two annotators and assigns different colors for the agreed and disagreed span.", "It is inevitable that the annotator or the recommending system gives incorrect annotations or suggestions. Based on our annotation experience, we found that the time cost of annotation correction cannot be neglected. Therefore, Yedda provides several efficient modification actions to revise the annotation:", "The client is designed to accelerate the annotation process as much as possible. It supports shortcut annotation to reduce the user operation time. Command line annotation is designed to annotate multi-span in batch. In addition, the client provides system recommendations to lessen the workload of duplicated span annotation.", "If an administrator wants to look into the detailed disagreement of annotators, it is quite convenient by using the Pairwise Annotators Comparison (PAC). PAC loads two annotated files and generates a specific comparison report file for the two annotators. As shown in Figure FIGREF21 , the report is mainly in two parts:"]}
{"question_id": "a064d01d45a33814947161ff208abb88d4353b26", "predicted_answer": "", "predicted_evidence": ["\u2022 INLINEFORM0 Overall statistics: it shows the specific precision, recall and F1-score of two files in all labels. It also gives the three accuracy indexes on overall full level and boundary level in the end.", "Yedda offers annotators with system recommendation based on the existing annotation history. The current recommendation system incrementally collects annotated text spans from sentences that have been labeled, thus gaining a dynamically growing lexicon. Using the lexicon, the system automatically annotates sentences that are currently being annotated by leveraging the forward maximum matching algorithm. The automatically suggested text spans and their types are returned with colors in the user interface, as shown in green in Figure FIGREF4 . Annotators can use the shortcut to confirm, correct or veto the suggestions. The recommending system keeps online updating during the whole annotation process, which learns the up-to-date and in-domain annotation information. The recommending system is designed as \u201cpluggable\u201d which ensures that the recommending algorithm can be easily extended into other sequence labeling models, such as Conditional Random Field (CRF) BIBREF15 . The recommendation can be controlled through two buttons \u201cRMOn\u201d and \u201cRMOff\u201d, which enables and disables the recommending function, respectively.", "We thank Yanxia Qin, Hongmin Wang, Shaolei Wang, Jiangming Liu, Yuze Gao, Ye Yuan, Lu Cao, Yumin Zhou and other members of SUTDNLP group for their trials and feedbacks. Yue Zhang is the corresponding author. Jie is supported by the YEDDA grant 52YD1314.", "\u2022 INLINEFORM0 Content comparison: this function gives the detailed comparison of two annotated files in whole content. It highlights the annotated parts of two annotators and assigns different colors for the agreed and disagreed span.", "Web-based annotation tools have been developed to build operating system independent annotation environments. Gate BIBREF11 includes a web-based with collaborative annotation framework which allows users to work collaboratively by annotating online with shared text storage. Brat BIBREF7 is another web-based tool, which has been widely used in recent years, it provides powerful annotation functions and rich visualization ability, while it does not integrate the result analysis function. Anafora BIBREF4 and Atomic BIBREF5 are also web-based and lightweight annotation tools, while they don't support the automatic annotation and quality analysis either. WebAnno BIBREF12 , BIBREF13 supports both the automatic annotation suggestion and annotation quality monitoring such as inter-annotator agreement measurement, data curation, and progress monitoring."]}
{"question_id": "3d5b4aa1ce99903b1fcd257c1e394f7990431d13", "predicted_answer": "", "predicted_evidence": ["Roughly speaking, in each parse tree of the grammar, the leaves are the words of a sentence, and the internal nodes indicate which database entries are expressed by each subtree. The grammar is constructed using hand-crafted templates of rewrite rules and a parallel training corpus of database entries and sentences; a generative model based on the work of Liang et al. BIBREF42 is employed to estimate the probabilities of the grammar. Subsequently, all the parse trees of the grammar for the sentences of the training corpus and the corresponding database entries are represented as a weighted directed hypergraph BIBREF50 . The hypergraph's weights are estimated using the inside-outside algorithm BIBREF51 on the training corpus. Following Huang and Chiang BIBREF52 , the hypergraph nodes are then integrated with an INLINEFORM0 -gram language model trained on the sentences of the corpus. Given a new set of database entries, the most probable derivation is found in the hypergraph using a INLINEFORM1 -best Viterbi search with cube pruning BIBREF53 and the final sentence is obtained from the derivation.", "Focusing on Naturalowl, a publicly available state of the art natural language generator for owl ontologies, we proposed methods to automatically or semi-automatically extract from the Web sentence plans and natural language names, two of the most important types of domain-specific generation resources. We showed experimentally that texts generated using linguistic resources produced by our methods in a semi-automatic manner, with minimal human involvement, are perceived as being almost as good as texts generated using manually authored linguistic resources, and much better than texts produced by using linguistic resources extracted from the relation and entity identifiers of the ontologies. Using our methods, constructing sentence plans and natural language names requires human effort of a few minutes or hours, respectively, per ontology, whereas constructing them manually from scratch is typically a matter of days. Also, our methods do not require any familiarity with the internals of Naturalowl and the details of its linguistic resources. Furthermore, unlike previous related work, no parallel corpus of sentences and semantic representations is required.", "Naturalowl is based on ideas from ilex BIBREF35 and m-piro BIBREF27 , BIBREF24 . Excluding simple verbalizers, it is the only publicly available nlg system for owl, which is why we based our work on it. Nevertheless, its processing stages and linguistic resources are typical of nlg systems BIBREF2 , BIBREF25 . Hence, we believe that our work is also applicable, at least in principle, to other nlg systems. For example, ontosum BIBREF3 , which generates natural language descriptions of individuals, but apparently not classes, from rdf schema and owl ontologies, uses similar processing stages, and linguistic resources corresponding to nl names and sentence plans. Reiter et al. BIBREF36 discuss the different types of knowledge that nlg systems require and the difficulties of obtaining them (e.g., by interviewing experts or analyzing corpora). Unlike Reiter et al., we assume that domain knowledge is already available, in the form of owl ontologies.", "", "True if all the sentences INLINEFORM0 was derived from were well-formed, according to the parser."]}
{"question_id": "8d3f79620592d040f9f055b4fce0f73cc45aab63", "predicted_answer": "", "predicted_evidence": ["Full Matching: $\\overrightarrow{u}_i^{\\text{full}}=g(\\overrightarrow{x}_i,\\overrightarrow{x}_{l_x^{\\prime }},\\mathbf {W}_{o1}),$ $\\overleftarrow{u}_i^{\\text{full}}=g(\\overleftarrow{x}_i,\\overleftarrow{x}^{\\prime }_{1},\\mathbf {W}_{o2}). $", "We also examined the strategy choices with respect to certain keywords. For each word $w$ in vocabulary, we computed the distribution $\\Pr [G,T|w\\in Q]$ , i.e., the conditional distribution of strategy and step when $w$ appeared in the question. Table 4 provides some keywords and their associated dominant strategies and step choices. The results validate the assumption that DFN dynamically selects specific attention strategy based on different question types. For example, the underline \u201c_\u201d indicates that the question and choice should be concatenated to form a sentence. This led to Integral Attention being most favorable when \u201c_\u201d is present. In another example, \u201cnot\u201d and \u201cexcept\u201d usually appear in questions like \u201cWhich of the following is not TRUE\u201d. Such questions usually have long answer candidates that require more reasoning. So Answer-only Attention with Reasoning Step#5 became dominant.", "In this work, we propose a novel neural model - Dynamic Fusion Network (DFN), for MRC. For a given input sample, DFN can dynamically construct an model instance with a sample-specific network structure by picking an optimal attention strategy and an optimal number of reasoning steps on the fly. The capability allows DFN to adapt effectively to handling questions of different types. By training the policy of model construction with reinforcement learning, our DFN model can substantially outperform previous state-of-the-art MRC models on the challenging RACE dataset. Experiments show that by marrying dynamic fusion (DF) with multi-step reasoning (MR), the performance boost of DFN over baseline models is statistically significant. For future directions, we plan to incorporate more comprehensive attention strategies into the DFN model, and to apply the model to other challenging MRC tasks with more complex questions that need DF and MR jointly. Future extension also includes constructing a \u201ccomposable\u201d structure on the fly - by making the Dynamic Fusion Layer more flexible than it is now.", "Question 3 and 4 in Figure 5 provide two instances that use answer-only attentions. As shown in these examples, Answer-only attention usually deals with long and natural language answer candidates. Such answers cannot be derived without the model reading through multiple sentences in the passage, and this requires multi-step reasoning. So in both examples, the system went through 5 steps of reasoning.", "For ablation studies, we conducted experiments with 4 different model configurations:"]}
{"question_id": "65e30c842e4c140a6cb8b2f9498fcc6223ed49c0", "predicted_answer": "", "predicted_evidence": ["We trained the KDG model on unpruned logical form candidates generated using the DPD algorithm, and found its accuracy to drop to 36.3% (from 43.3%); all configuring parameters were left unchanged. This implies that pruning out spurious logical forms before training is necessary for the performance improvement achieved by the KDG model.", "BIBREF0 present several ablation studies to identify the sources of the performance improvement achieved by the KDG model. These studies comprehensively cover novel aspects of the model architecture. On the training side, the studies only vary the number of logical forms per question in the training dataset. Pruning of the logical forms was not considered. This may have happened inadvertently as the KDG system may have downloaded the logical forms dataset made available by Pasupat et al. without noticing that it had been pruned out.", "BIBREF3 claimed \u201cthe pruned set of logical forms would provide a stronger supervision signal for training a semantic parser\u201d. This paper provides empirical evidence in support of this claim. We further believe that the pruning algorithm may also be valuable to models that score logical forms. Such scoring models are typically used by grammar-based semantic parsers such as the one in BIBREF1 . Using the pruning algorithm, the scoring model can be trained to down-score spurious logical forms. Similarly, neural semantic parsers trained using reinforcement learning may use the pruning algorithm to only assign rewards to non-spurious logical forms.", "The KDG system operates by translating a natural language question and a table to a logical form in Lambda-DCS BIBREF4 . A logical form is an executable formal expression capturing the question's meaning. It is executed on the table to obtain the final answer.", "The translation to logical forms is carried out by a deep neural network, also called a neural semantic parser. Training the network requires a dataset of questions mapped to one or more logical forms. The WTQ dataset only contains the correct answer label for question-table instances. To obtain the desired training data, the KDG system enumerates consistent logical form candidates for each INLINEFORM0 triple in the WTQ dataset, i.e., it enumerates all logical forms that lead to the correct answer INLINEFORM1 on the given table INLINEFORM2 . For this, it relies on the dynamic programming algorithm of BIBREF3 . This algorithm is called dynamic programming on denotations (DPD)."]}
{"question_id": "65e26b15e087bedb6e8782d91596b35e7454b16b", "predicted_answer": "", "predicted_evidence": ["One of the significant challenges in contemporary information processing is the sheer volume of available data. BIBREF0 , for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing BIBREF1 , relies on hashing data into short, locality-preserving binary codes BIBREF2 . The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image.", "In case of English Wikipedia we held out for testing randomly selected 10% of the documents. We perform document retrieval by selecting queries from the test set and ordering other test documents according to the similarity of the inferred codes. We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) BIBREF16 . The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup. In RCV1 each document belongs to a hierarchy of topics, making the definition of relevancy less obvious. In this case we adopted the relevancy measure used by BIBREF3 .", "The 20 Newsgroups dataset comes with reference train/test sets. In case of RCV1 we used half of the documents for training and the other half for evaluation. In case of English Wikipedia we held out for testing randomly selected 10% of the documents. We perform document retrieval by selecting queries from the test set and ordering other test documents according to the similarity of the inferred codes. We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) BIBREF16 . The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup.", "Recently several works explored simple neural models for unsupervised learning of distributed representations of words, sentences and documents. BIBREF6 proposed log-linear models that learn distributed representations of words by predicting a central word from its context (CBOW model) or by predicting context words given the central word (Skip-gram model). The CBOW model was then extended by BIBREF7 to learn distributed representations of documents. Specifically, they proposed Paragraph Vector Distributed Memory (PV-DM) model, in which the central word is predicted given the context words and the document vector. During training, PV-DM learns the word embeddings and the parameters of the softmax that models the conditional probability distribution for the central words. During inference, word embeddings and softmax weights are fixed, but the gradients are backpropagated to the inferred document vector.", "We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) BIBREF16 . The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup. In RCV1 each document belongs to a hierarchy of topics, making the definition of relevancy less obvious. In this case we adopted the relevancy measure used by BIBREF3 . That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows BIBREF3 , enabling comparison with semantic hashing codes."]}
{"question_id": "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7", "predicted_answer": "", "predicted_evidence": ["One advantage of using the Real-Binary PV-DBOW model over two separate networks is that we need to store only one set of softmax parameters (and a small projection matrix) in the memory, instead of two large weight matrices. Additionally, only one model needs to be trained, rather than two distinct networks.", "Binary document codes can also be learned by extending distributed memory models. BIBREF7 suggest that in PV-DM, a context of the central word can be constructed by either concatenating or averaging the document vector and the embeddings of the surrounding words. However, in Binary PV-DM (Figure FIGREF3 ) we always construct the context by concatenating the relevant vectors before applying the sigmoid nonlinearity. This way, the length of binary codes is not tied to the dimensionality of word embeddings.", "Softmax layers in the models described above should be trained to predict words in documents given binary context vectors. Training should therefore encourage binary activations in the preceding sigmoid layers. This can be done in several ways. In semantic hashing autoencoders BIBREF3 added noise to the sigmoid coding layer. Error backpropagation then countered the noise, by forcing the activations to be close to 0 or 1. Another approach was used by BIBREF12 in autoencoders that learned binary codes for small images. During the forward pass, activations in the coding layer were rounded to 0 or 1. Original (i.e. not rounded) activations were used when backpropagating errors. Alternatively, one could model the document codes with stochastic binary neurons. Learning in this case can still proceed with error backpropagation, provided that a suitable gradient estimator is used alongside stochastic activations. We experimented with the methods used in semantic hashing and Krizhevsky's autoencoders, as well as with the two biased gradient estimators for stochastic binary neurons discussed by BIBREF13 .", "Softmax layers in the models described above should be trained to predict words in documents given binary context vectors. Training should therefore encourage binary activations in the preceding sigmoid layers. This can be done in several ways. In semantic hashing autoencoders BIBREF3 added noise to the sigmoid coding layer. Error backpropagation then countered the noise, by forcing the activations to be close to 0 or 1. Another approach was used by BIBREF12 in autoencoders that learned binary codes for small images. During the forward pass, activations in the coding layer were rounded to 0 or 1. Original (i.e. not rounded) activations were used when backpropagating errors. Alternatively, one could model the document codes with stochastic binary neurons. Learning in this case can still proceed with error backpropagation, provided that a suitable gradient estimator is used alongside stochastic activations.", "In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by BIBREF3 . Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TF-IDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TF-IDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such high-dimensional vectors is impractical."]}
{"question_id": "eafea4a24d103fdecf8f347c7d84daff6ef828a3", "predicted_answer": "", "predicted_evidence": ["Note that we cannot simply increase the embedding dimensionality in Binary PV-DBOW in order to learn better codes: binary vectors learned in this way would be too long to be useful in document hashing. The retrieval performance can, however, be improved by using binary codes for initial filtering of documents, and then using a representation with higher capacity to rank the remaining documents by their similarity to the query. BIBREF3 , for example, used semantic hashing codes for initial filtering and TF-IDF for ranking. A similar document retrieval strategy can be realized with binary paragraph vectors. Furthermore, we can extend the Binary PV-DBOW model to simultaneously learn short binary codes and higher-dimensional real-valued representations. Specifically, in the Real-Binary PV-DBOW model (Figure FIGREF2 ) we introduce a linear projection between the document embedding matrix and the sigmoid nonlinearity. During training, we learn the softmax parameters and the projection matrix. During inference, softmax weights and the projection matrix are fixed.", "For comparison we also report performance of real-valued paragraph vectors. Note that the binary codes perform very well, despite their far lower capacity: on 20 Newsgroups and RCV1 the 128-bit Binary PV-DBOW trained with bigrams approaches the performance of the real-valued paragraph vectors, while on English Wikipedia its performance is slightly lower. Furthermore, Binary PV-DBOW with bigrams outperforms semantic hashing codes: comparison of precision-recall curves from Figures FIGREF7 a and FIGREF7 b with BIBREF3 shows that 128-bit codes learned with this model outperform 128-bit semantic hashing codes on 20 Newsgroups and RCV1. Moreover, the 32-bit codes from this model outperform 128-bit semantic hashing codes on the RCV1 dataset, and on the 20 Newsgroups dataset give similar precision up to approximately 3% recall and better precision for higher recall levels. Note that the difference in this case lies not only in retrieval precision: the short 32-bit Binary PV-DBOW codes are more efficient for indexing than long 128-bit semantic hashing codes.", "The retrieval performance can, however, be improved by using binary codes for initial filtering of documents, and then using a representation with higher capacity to rank the remaining documents by their similarity to the query. BIBREF3 , for example, used semantic hashing codes for initial filtering and TF-IDF for ranking. A similar document retrieval strategy can be realized with binary paragraph vectors. Furthermore, we can extend the Binary PV-DBOW model to simultaneously learn short binary codes and higher-dimensional real-valued representations. Specifically, in the Real-Binary PV-DBOW model (Figure FIGREF2 ) we introduce a linear projection between the document embedding matrix and the sigmoid nonlinearity. During training, we learn the softmax parameters and the projection matrix. During inference, softmax weights and the projection matrix are fixed. This way, we simultaneously obtain a high-capacity representation of a document in the embedding matrix, e.g.", "The retrieval performance can, however, be improved by using binary codes for initial filtering of documents, and then using a representation with higher capacity to rank the remaining documents by their similarity to the query. BIBREF3 , for example, used semantic hashing codes for initial filtering and TF-IDF for ranking. A similar document retrieval strategy can be realized with binary paragraph vectors. Furthermore, we can extend the Binary PV-DBOW model to simultaneously learn short binary codes and higher-dimensional real-valued representations. Specifically, in the Real-Binary PV-DBOW model (Figure FIGREF2 ) we introduce a linear projection between the document embedding matrix and the sigmoid nonlinearity. During training, we learn the softmax parameters and the projection matrix. During inference, softmax weights and the projection matrix are fixed. This way, we simultaneously obtain a high-capacity representation of a document in the embedding matrix, e.g. 300-dimensional real-valued vector, and a short binary representation from the sigmoid activations.", "Note that we cannot simply increase the embedding dimensionality in Binary PV-DBOW in order to learn better codes: binary vectors learned in this way would be too long to be useful in document hashing. The retrieval performance can, however, be improved by using binary codes for initial filtering of documents, and then using a representation with higher capacity to rank the remaining documents by their similarity to the query. BIBREF3 , for example, used semantic hashing codes for initial filtering and TF-IDF for ranking. A similar document retrieval strategy can be realized with binary paragraph vectors. Furthermore, we can extend the Binary PV-DBOW model to simultaneously learn short binary codes and higher-dimensional real-valued representations. Specifically, in the Real-Binary PV-DBOW model (Figure FIGREF2 ) we introduce a linear projection between the document embedding matrix and the sigmoid nonlinearity. During training, we learn the softmax parameters and the projection matrix."]}
{"question_id": "e099a37db801718ab341ac9a380a146c7452fd21", "predicted_answer": "", "predicted_evidence": ["We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) BIBREF16 . The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup. In RCV1 each document belongs to a hierarchy of topics, making the definition of relevancy less obvious. In this case we adopted the relevancy measure used by BIBREF3 . That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows BIBREF3 , enabling comparison with semantic hashing codes.", "Information retrieval results for Real-Binary PV-DBOW are summarized in Table TABREF19 . The model gives higher NDCG@10 than 32-bit Binary PV-DBOW codes (Table TABREF8 ). The difference is large when the initial filtering is restrictive, e.g. when using 28-bit codes and 1-2 bit Hamming distance limit. Real-Binary PV-DBOW can therefore be useful when one needs to quickly find a short list of relevant documents in a large text collection, and the recall level is not of primary importance. If needed, precision can be further improved by using plain Binary PV-DBOW codes for filtering and standard DBOW representation for raking (Table TABREF19 , column B). Note, however, that PV-DBOW model would then use approximately 10 times more parameters than Real-Binary PV-DBOW.", "Softmax layers in the models described above should be trained to predict words in documents given binary context vectors. Training should therefore encourage binary activations in the preceding sigmoid layers. This can be done in several ways. In semantic hashing autoencoders BIBREF3 added noise to the sigmoid coding layer. Error backpropagation then countered the noise, by forcing the activations to be close to 0 or 1. Another approach was used by BIBREF12 in autoencoders that learned binary codes for small images. During the forward pass, activations in the coding layer were rounded to 0 or 1. Original (i.e. not rounded) activations were used when backpropagating errors. Alternatively, one could model the document codes with stochastic binary neurons. Learning in this case can still proceed with error backpropagation, provided that a suitable gradient estimator is used alongside stochastic activations.", "In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by BIBREF3 . Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TF-IDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TF-IDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions.", "One of the significant challenges in contemporary information processing is the sheer volume of available data. BIBREF0 , for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing BIBREF1 , relies on hashing data into short, locality-preserving binary codes BIBREF2 . The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image."]}
{"question_id": "ead7704a64447dccd504951618d3be463eba86bf", "predicted_answer": "", "predicted_evidence": ["english", "We also plan to combine machine learning techniques with a dictionary-based approach. Our system can already detect and replace typos and abbreviations to help machine learning techniques increase their performance.", "A limitation of the proposed algorithm that impacted recall was the absence of term detection when adjectives were isolated. For example, in the sentence \"metastase hepatique et renale\", \"metastase renale\" was not recognized even though the term existed. This situation seemed to be quite frequent.", "We submitted two runs on the C\u00e9piDC test set, one used all the terms entered by human coders in the training set only (run 2), the other (run 1) added the 2015 ICD-10 dictionary provided by the task organizers to the set the terms of run 1. We obtained our best precision (0.794) and recall (0.779) with run 2.", "Surprisingly, adding more terms (run 1) did not improve the recall, which appears to be even slightly worse. The results were quite promising for our first participation in this task, using a general purpose annotation tool."]}
{"question_id": "8476d0bf5962f4ed619a7b87415ebe28c38ce296", "predicted_answer": "", "predicted_evidence": ["The present study is part of the Drugs Systematized Assessment in real-liFe Environment (DRUGS-SAFE) research platform that is funded by the French Medicines Agency (Agence Nationale de S\u00e9curit\u00e9 du M\u00e9dicament et des Produits de Sant\u00e9, ANSM). This platform aims at providing an integrated system allowing the concomitant monitoring of drug use and safety in France. The funder had no role in the design and conduct of the studies ; collection, management, analysis, and interpretation of the data ; preparation, review, or approval of the manuscript ; and the decision to submit the manuscript for publication. This publication represents the views of the authors and does not necessarily represent the opinion of the French Medicines Agency.", "In this paper, we describe our approach and present the results for our participation in the task 1, i.e. multilingual information extraction, of the CLEF eHealth 2018 challenge BIBREF0 . More precisely, this task consists in automatically coding death certificates using the International Classification of Diseases, 10th revision (ICD-10) BIBREF1 .", "We addressed the challenge by matching ICD-10 terminology entries to text phrases in death certificates. Matching text phrases to medical concepts automatically is important to facilitate tasks such as search, classification or organization of biomedical textual contents BIBREF2 . Many concept recognition systems already exist BIBREF2 , BIBREF3 . They use different approaches and some of them are open source. We developed a general purpose biomedical semantic annotation tool for our own needs. The algorithm was initially implemented to detect drugs in a social media corpora as part of the Drugs-Safe project BIBREF4 . We adapted the algorithm for the ICD-10 coding task. The main motivation in participating in the challenge was to evaluate and compare our system with others on a shared task.", "For each token, the algorithm used three matching techniques : perfect match, abbreviation match and Levenshtein match. The abbreviation match technique used a dictionary of abbreviations. We manually added nine frequent abbreviations after looking at some examples. The Levenshtein matching technique used the Levenshtein distance. It corresponds to the minimum number of single-character edits (insertions, deletions or substitutions) required to change one token into the other. The Lucene\u2122implementation of the Levenshtein distance was used.", "The data set for the coding of death certificates is called the C\u00e9piDC corpus. Three CSV files (AlignedCauses) were provided by task organizers containing annotated death certificates for different periods : 2006 to 2012, 2013 and 2014. This training set contained 125383 death certificates. Each certificate contains one or more lines of text (medical causes that led to death) and some metadata. Each CSV file contains a \"Raw Text\" column entered by a physician, a \"Standard Text\" column entered by a human coder that supports the selection of an ICD-10 code in the last column. Table TABREF2 presents an excerpt of these files. Zero to multiples ICD-10 codes can be assigned to each line of a death certificate."]}
{"question_id": "bbfe7e131ed776c85f2359b748db1325386c1af5", "predicted_answer": "", "predicted_evidence": ["We also plan to combine machine learning techniques with a dictionary-based approach. Our system can already detect and replace typos and abbreviations to help machine learning techniques increase their performance.", "Some frequent abbreviations were manually added to improve the recall in this corpora. Improvement at this stage may be possible by automating the abbreviation detection or by adding more entries manually.", "DRUGS-SAFE National Platform of Pharmacoepidemiology, France", "Surprisingly, adding more terms (run 1) did not improve the recall, which appears to be even slightly worse. The results were quite promising for our first participation in this task, using a general purpose annotation tool.", "english"]}
{"question_id": "b6dae03d56dff0db8ad2a1bff9c7dd3f87551cd1", "predicted_answer": "", "predicted_evidence": ["A shortcoming of the method used here is the rather limited amount of party specific data: the quality and the quantity of the text data used varies drastically between parties, as can be seen in Table 2 . Using, for example, parliamentary debates, opinion pieces, and other official party communication might improve data coverage.", "Norrbottens Kuriren", "In this paper we have introduced some very preliminary results on how to measure similarities in language use, conditioned on discourse, e.g. \u201chow similar is The BBC to The Daily Mail, when talking about Climate Change\". The end goal is to measure aggregate similarity in specific issues, answering questions such as \u201cwhen talking about health policy, to which extent does the general language use align with Source A, Source B, etc.\", and use such an aggregate measure to study issue ownership at scale.", "Furthermore, we see a strong dissimilarity between nativist media and all parties regarding nativist issues. This is particularly true for parties promoting liberal immigration policy: The Left Party, The Social Democrats, The Green Party, The Centre Party, and The Moderates are all currently or historically promoting liberal immigration policy at odds with nativist sentiment.", "Arbetarbladet"]}
{"question_id": "f93bad406e004014618dd64f6c604b1a9ee6a371", "predicted_answer": "", "predicted_evidence": ["Expressen", "Fria Tider", "Norrbottens Kuriren", "Furthermore, we see a strong dissimilarity between nativist media and all parties regarding nativist issues. This is particularly true for parties promoting liberal immigration policy: The Left Party, The Social Democrats, The Green Party, The Centre Party, and The Moderates are all currently or historically promoting liberal immigration policy at odds with nativist sentiment.", "Nerikes Allehanda"]}
{"question_id": "c5ea4da3c760ba89194ad807bc1ef60e1761429f", "predicted_answer": "", "predicted_evidence": ["We acknowledge Gama System (http://www.gama-system.si) who collected most of the tweets (except English), and Sowa Labs (http://www.sowalabs.com) for providing the Goldfinch platform for sentiment annotations. Special thanks go to Sa\u0161o Rutar who implemented several classification algorithms and evaluation procedures in the LATINO library for text mining (https://github.com/latinolib). We thank Mojca Mikac for computing the Krippendorff's INLINEFORM0 confidence intervals, and Dragi Kocev for help with the Friedman-Nemenyi test.", "All the above algorithms were applied to the 13 language datasets and evaluated by 10-fold cross-validation. Standard 10-fold cross-validation randomly partitions the whole labeled set into 10 equal folds. One is set apart for testing, the remaining nine are used to train the model, and the train-test procedure is run over all 10 folds. Cross-validation is stratified when the partitioning is not completely random, but each fold has roughly the same class distribution. With time-ordered data, as is the Twitter stream, one should also consider blocked form of cross-validation BIBREF41 , where there is no randomization, and each fold is a block of consecutive tweets. There are also other evaluation procedures suitable for time-ordered data, different than cross-validation, like ordered sub-sampling, but this is beyond the scope of the paper. In this study we applied blocked, stratified, 10-fold cross-validation in all the experiments.", "Go et al. BIBREF25 employ the keyword-based approach, Naive Bayes, Maximum Entropy, and SVM, and show that the best performing algorithm is Maximum Entropy. The authors in BIBREF28 show that Maximum Entropy outperforms Naive Bayes. In contrast, the authors in BIBREF29 report that Naive Bayes performs considerably better than Maximum Entropy. Pak and Paroubek BIBREF30 show that Naive Bayes outperforms the SVM and Conditional Random Fields algorithms. Asiaee et al. BIBREF31 employ a dictionary learning approach, weighted SVM, k-Nearest-Neighbor, and Naive Bayes\u2014Naive Bayes and its weighted variant are among the best performing algorithms. Saif et al. BIBREF32 employ Naive Bayes for predicting sentiment in tweets.", "Second, we can use the agreement as a proxy to measure the \u201cdistance\u201d between the sentiment classes. Lets assume that the difficulty of distinguishing between the extreme classes ( INLINEFORM0 , +), as measured by INLINEFORM1 , is normalized to 1. If it is more difficult to distinguish between the neutral (0) and each extreme ( INLINEFORM2 or +) then the normalized agreement will be lower than 1, otherwise it will be greater than 1. The results in Table TABREF37 , columns three and four, indicate that for almost all the datasets the normalized agreement is lower than 1. The only exceptions are Slovak and Spanish. If we ignore the Albanian, Spanish, and Emojis we observe the following average differences: (i) it is 27% ( INLINEFORM3 ) more difficult to distinguish between the negative ( INLINEFORM4 ) and neutral (0) than between the negative ( INLINEFORM5 ) and positive (+); and (ii) it is 35% ( INLINEFORM6 ) more difficult to distinguish between the positive (+) and neutral (0) than between the positive (+) and negative ( INLINEFORM7 ).", "Are there significant differences between the six classifiers, in terms of their performance? The results depend on the evaluation measure used, but generally the top classifiers are not distinguishable."]}
{"question_id": "4a093a9af4903a59057a4372ac1b01603467ca58", "predicted_answer": "", "predicted_evidence": ["If one wants to compare a control classifier to other classifiers, the Bonferroni-Dunn post-hoc test is used. In our case, however, all the classifiers are compared to each other, and the weaker Nemenyi test BIBREF15 is used. The Nemenyi test computes the critical distance between any pair of classifiers. The performance of the two classifiers is significantly different if the corresponding average ranks differ by at least the critical distance.", "(5) What are acceptable levels of the annotators agreement?", "We acknowledge Gama System (http://www.gama-system.si) who collected most of the tweets (except English), and Sowa Labs (http://www.sowalabs.com) for providing the Goldfinch platform for sentiment annotations. Special thanks go to Sa\u0161o Rutar who implemented several classification algorithms and evaluation procedures in the LATINO library for text mining (https://github.com/latinolib). We thank Mojca Mikac for computing the Krippendorff's INLINEFORM0 confidence intervals, and Dragi Kocev for help with the Friedman-Nemenyi test.", "The second corpus of data are four application datasets, used in different application scenarios and already published BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 .", "NaiveBayes is a well-know supervised machine learning algorithm, and is included here for reference. It is a probabilistic classifier based on the Bayes theorem, and does not assume ordering of the sentiment classes."]}
{"question_id": "f4e16b185b506713ff99acc4dbd9ec3208e4997b", "predicted_answer": "", "predicted_evidence": ["The inter-annotator agreement for the German dataset is low, INLINEFORM0 is 0.344. The classifier's performance is higher already with the initial small datasets, and soon starts dropping (Fig FIGREF16 , chart on the left). It turns out that over 90% of the German tweets were labeled by two annotators only, dubbed annotator A and B. The annotation quality of the two annotators is very different, the self-agreement INLINEFORM1 for the annotator A is 0.590, and for the annotator B is 0.760. We consider the German tweets labeled by A and B separately (Fig FIGREF16 , charts in the middle and on the right). The lower quality A dataset reaches its maximum at 30,000 tweets, while the performance of the higher quality B dataset is still increasing. There was also a relatively high disagreement between the two annotators which resulted in a low classifier's performance. A conclusions drawn from this dataset, as well as from the Bulgarian, is that one should constantly monitor the self- and inter-annotator agreements, and promptly notify the annotators as soon as the agreements drop too low.", "We observe a similar performance drop for the Albanian dataset (not shown). The main annotator (who annotated over 22% of the Albanian tweets) has self-agreement INLINEFORM0 only 0.269 (computed from 1,963 tweets annotated twice). The inter-annotator agreement INLINEFORM1 is only 0.126.", "There exist several publicly available and manually labeled Twitter datasets. They vary in the number of examples from several hundreds to several thousands, but to the best of our knowledge, none exceeds 20,000 entries. Saif et al. BIBREF16 describe eight Twitter sentiment datasets and also introduce a new one which contains separate sentiment labels for tweets and entities. Rosenthal et al. BIBREF26 provide statistics for several of the 2013\u20132015 SemEval datasets. Haldenwang and Vornberger BIBREF27 present a publicly available collection of Twitter posts, which were labeled not only with the positive or negative sentiment, but also as uncertain or spam. Finally, several Twitter sentiment datasets are publicly available in CrowdFlower's \u201cData for Everyone\u201d collection.", "There are 44,583 Bosnian tweets, 6,519 annotated twice, and the self-agreement INLINEFORM3 is 0.722. We can conclude that the annotation quality of the Croatian and Bosnian tweets is considerably higher than of the Serbian. If we construct separate sentiment classifiers for each language we observe very different performance (see Fig FIGREF18 ). The Serbian classifier reaches the inter-annotator agreement (albeit low) at 70,000 tweets. The Croatian classifier has much higher performance, and reaches it maximum at 50,000 tweets ( INLINEFORM4 is 0.590). The performance of the Bosnian classifier is also higher, and is still increasing at 40,000 tweets ( INLINEFORM5 is 0.494). The individual classifiers are \u201cwell-behaved\u201d in contrast to the joint Ser/Cro/Bos model in Fig FIGREF17 . In retrospect, we can conclude that datasets with no overlapping annotations and different annotation quality are better not merged.", "There is no automated sentiment classification with the Emojis dataset. From the 13 language datasets which consist in total of over 1.6 labeled tweets, we selected only the tweets that contain emojis, about 70,000 in total. The goal was to attribute the sentiment to emojis, based on the sentiment of all the tweets in which they occur. Fig FIGREF8 shows that Emojis is the only dataset where the self-agreement ( INLINEFORM0 is 0.544) is lower than the inter-annotator agreement ( INLINEFORM1 is 0.597). The reason for this anomaly is a large share of Spanish tweets with emojis (about 20,000) that have very low self-agreement ( INLINEFORM2 is 0.245). If we remove them from the Emojis set, the self-agreement increases considerably (new INLINEFORM3 is 0.720), while the inter-annotators agreement remains almost unchanged (new INLINEFORM4 is 0.598)."]}
{"question_id": "4683812cba21c92319be68c03260b5a8175bbb6e", "predicted_answer": "", "predicted_evidence": ["There is no inter-annotator agreement for the Portuguese dataset because only one annotator was engaged. However, the classifier shows interesting performance variability (Fig FIGREF20 ). After an initial peak is reached at 50,000 tweets ( INLINEFORM0 is 0.394), there is a considerable drop and a very high variability of performance. Inspection of the tweets (the set of 10,000 tweets added to the first 50,000 tweets at stage 6) revealed that at the beginning of November 2013, the Portuguese government approved additional austerity measures, affecting mainly public sector, to avoid the second international bailout. This provoked a flood of negative reactions on social media, in particular on Twitter, and a considerable shift of focus and sentiment of Twitter discussions.", "First, lets compare the agreements in terms of two variants of INLINEFORM0 : INLINEFORM1 (interval) and INLINEFORM2 (nominal). The difference between the two measures is that INLINEFORM3 assigns four times higher cost to extreme disagreements (between the negative and positive classes) than INLINEFORM4 . A measure which yields higher agreements hints at the nature of sentiment class ordering as perceived by humans. The results in Table TABREF37 , column two, show that INLINEFORM5 always yields higher agreement than INLINEFORM6 , except for Spanish. We compute the average relative agreement gains by ignoring the Albanian and Spanish datasets (which have poor annotation quality), and Emojis (which are already subsumed by the 13 language datasets). We observe that the average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8 . This gives a strong indication that the sentiment classes are perceived as ordered by the annotators.", "Evaluation results, in terms of INLINEFORM0 , are summarized in Fig FIGREF40 . The classifiers are ordered by their average performance rank across the 13 datasets. More detailed results, in terms of all four evaluation measures, and also including the application datasets, are in Table TABREF41 . Note that the sizes of the training datasets are lower than the numbers of annotated tweets in Table TABREF30 . Namely, tweets annotated several times are first merged into single training examples, thus forming the \u201cgold standard\u201d for training and testing. If all the annotations are the same, the assigned label is obvious. If the annotations differ, the following merging rules are applied: neutral and negative INLINEFORM1 negative; neutral and positive INLINEFORM2 positive; and negative and positive INLINEFORM3 neutral.", "NaiveBayes is a well-know supervised machine learning algorithm, and is included here for reference. It is a probabilistic classifier based on the Bayes theorem, and does not assume ordering of the sentiment classes.", "In general, the agreement can be estimated between any two methods of generating data. One of the main ideas of this work is to use the same measures to estimate the agreement between the human annotators as well as the agreement between the results of automated classification and the \u201cgold standard\u201d. There are different measures of agreement, and to get robust estimates we apply four well-known measures from the fields of inter-rater agreement and machine learning."]}
{"question_id": "c25014b7e57bb2949138d64d49f356d69838bc25", "predicted_answer": "", "predicted_evidence": ["Many people book vacations using travel agents, so the idea of booking travel through conversation is already familiar. Thus, we emulate the role of a travel agent, who talks to the customer while performing searches on various supplier databases on his behalf.", "Task-oriented chatbots have recently been applied to many areas in e-commerce. In this paper, we describe a task-oriented chatbot system that provides hotel recommendations and deals. Users access the chatbot through third-party messaging platforms, such as Facebook Messenger (Figure FIGREF4), Amazon Alexa, and WhatsApp. The chatbot elicits information, such as travel dates and hotel preferences, through a conversation, then recommends a set of suitable hotels that the user can then book. Our system uses a dialogue manager that integrates a combination of NLP models to handle the most frequent scenarios, and defer to a human support agent for more difficult situations.", "For queries identified as search intent, we perform named entity recognition (NER) to extract spans from the query representing names of hotels and cities. Recently, neural architectures have shown to be successful for NER BIBREF9, BIBREF10. Typically, they are trained on the CoNLL-2003 Shared Task BIBREF11 which features four entity types (persons, organizations, locations, and miscellaneous).", "Deep learning has been applied to short text ranking, for example, using LSTMs BIBREF13, or CNN-based architectures BIBREF14, BIBREF15. We experiment with several neural architectures, which take in the user query as one input and the hotel or city name as the second input. The model is trained to classify the match as relevant or irrelevant to the query. We compare the following models:", "We collect labelled training data from two sources. First, data for the intent model is extracted from conversations between users and customer support agents. To save time, the model suggests a pre-written response to the user, which the agent either accepts by clicking a button, or composes a response from scratch. This action is logged, and after being checked by a professional annotator, is added to our training data."]}
{"question_id": "25a8d432bf94af1662837877bc6c284e2fc3fbe2", "predicted_answer": "", "predicted_evidence": ["Typical online travel agencies provide a web interface (such as buttons, dropdowns, and checkboxes) to enter information and filter search results; this can be difficult to navigate. In contrast, chatbot have a much gentler learning curve, since users interact with the bot using natural language. Additionally, chatbots are lightweight as they are embedded in an instant messaging platform that handles authentication. All of these factors contribute to higher user convenience BIBREF0.", "Numerous task-oriented chatbots have been developed for commercial and recreational purposes. Most commercial chatbots today use a frame-based dialogue system, which was first proposed in 1977 for a flight booking task BIBREF1. Such a system uses a finite-state automaton to direct the conversation, which fills a set of slots with user-given values before an action can be taken. Modern frame-based systems often use machine learning for the slot-filling subtask BIBREF2.", "Natural language processing has been applied to other problems in the travel industry, for example, text mining hotel information from user reviews for a recommendation system BIBREF3, or determining the economic importance of various hotel characteristics BIBREF4. Sentiment analysis techniques have been applied to hotel reviews for classifying polarity BIBREF5 and identifying common complaints to report to hotel management BIBREF6.", "We evaluate our methods using per-category precision, recall, and F1 scores. These are more informative metrics than accuracy because of the class imbalance, and also because some intent classes are easier to classify than others. In particular, it is especially important to accurately classify the search intent, because more downstream models depend on this output.", "Our conversational AI uses machine learning for three separate, cascading tasks: intent classification, named entity recognition (NER), and information retrieval (IR). That is, the intent model is run on all messages, NER is run on only a subset of messages, and IR is run on a further subset of those. In this section, we give an overview of each task's model and evaluation metrics."]}
{"question_id": "be632f0246c2e5f049d12e796812f496e083c33e", "predicted_answer": "", "predicted_evidence": ["Second, we employ professional annotators to create training data for each of our models, using a custom-built interface. A pool of relevant messages is selected from past user conversations; each message is annotated once and checked again by a different annotator to minimize errors. We use the PyBossa framework to manage the annotation processes.", "BERT + fine-tuning: We follow the procedure for BERT sentence pair classification. That is, we feed the query as sentence A and the hotel name as sentence B into BERT, separated by a [SEP] token, then take the output corresponding to the [CLS] token into a final linear layer to predict the label. We initialize the weights with the pretrained checkpoint and fine-tune all layers for 3 epochs (Figure FIGREF19).", "Deep learning has been applied to short text ranking, for example, using LSTMs BIBREF13, or CNN-based architectures BIBREF14, BIBREF15. We experiment with several neural architectures, which take in the user query as one input and the hotel or city name as the second input. The model is trained to classify the match as relevant or irrelevant to the query. We compare the following models:", "Our conversational AI uses machine learning for three separate, cascading tasks: intent classification, named entity recognition (NER), and information retrieval (IR). That is, the intent model is run on all messages, NER is run on only a subset of messages, and IR is run on a further subset of those. In this section, we give an overview of each task's model and evaluation metrics.", "Figure FIGREF7 shows part of the state machine, invoked when a user starts a new hotel search. Figure FIGREF8 shows a typical conversation between a user and the bot, annotated with the corresponding state transitions and calls to our machine learning models."]}
{"question_id": "415b42ef6ff92553d04bd44ed0cbf6b3d6c83e51", "predicted_answer": "", "predicted_evidence": ["Averaged GloVe + feedforward: We use 100-dimensional, trainable GloVe embeddings BIBREF16 trained on Common Crawl, and produce sentence embeddings for each of the two inputs by averaging across all tokens. The sentence embeddings are then given to a feedforward neural network to predict the label.", "We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match. This model only returns the top match, so only top-1 recall is evaluated, and top-3 recall is not applicable. Both neural models outperform the baseline, but by far the best performing model is BERT with fine-tuning, which retrieves the correct match for nearly 90% of queries (Table TABREF21).", "BERT + fine-tuning: We follow the procedure for BERT sentence pair classification. That is, we feed the query as sentence A and the hotel name as sentence B into BERT, separated by a [SEP] token, then take the output corresponding to the [CLS] token into a final linear layer to predict the label. We initialize the weights with the pretrained checkpoint and fine-tune all layers for 3 epochs (Figure FIGREF19).", "Many people book vacations using travel agents, so the idea of booking travel through conversation is already familiar. Thus, we emulate the role of a travel agent, who talks to the customer while performing searches on various supplier databases on his behalf.", "The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact. The result of the intent model is used to determine the bot's response, what further processing is necessary (in the case of search intent), and whether to direct the conversation to a human agent (in the case of unknown intent)."]}
{"question_id": "9da181ac8f2600eb19364c1b1e3cdeb569811a11", "predicted_answer": "", "predicted_evidence": ["Each of our three models is evaluated by internal cross-validation using the metrics described above; however, the conversational AI system as a whole is validated using external metrics: agent handoff rate and booking completion rate. The agent handoff rate is the proportion of conversations that involve a customer support agent; the booking completion rate is the proportion of conversations that lead to a completed hotel booking. Both are updated on a daily basis.", "Many people book vacations using travel agents, so the idea of booking travel through conversation is already familiar. Thus, we emulate the role of a travel agent, who talks to the customer while performing searches on various supplier databases on his behalf.", "Deep learning has been applied to short text ranking, for example, using LSTMs BIBREF13, or CNN-based architectures BIBREF14, BIBREF15. We experiment with several neural architectures, which take in the user query as one input and the hotel or city name as the second input. The model is trained to classify the match as relevant or irrelevant to the query. We compare the following models:", "Averaged GloVe + feedforward: We use 100-dimensional, trainable GloVe embeddings BIBREF16 trained on Common Crawl, and produce sentence embeddings for each of the two inputs by averaging across all tokens. The sentence embeddings are then given to a feedforward neural network to predict the label.", "The combined NER model achieves the best accuracy, significantly better than the model with separate entity types. This is expected, since it only needs to identify entities as either hotel or location, without needing to distinguish them. The model is ineffective at differentiating between hotel and location names, likely because this is not always possible using syntactic properties alone; sometimes, world knowledge is required that is not available to the model."]}
{"question_id": "67f1b8a9f72e62cd74ec42e9631ef763a9b098c7", "predicted_answer": "", "predicted_evidence": ["The information retrieval (IR) system takes a user search query and matches it with the best location or hotel entry in our database. It is invoked when the intent model detects a search intent, and the NER model recognizes a hotel or location named entity. This is a non-trivial problem because the official name of a hotel often differs significantly from what a user typically searches. For example, a user looking for the hotel \u201cHyatt Regency Atlanta Downtown\u201d might search for \u201chyatt hotel atlanta\u201d.", "We use SpaCy to train custom NER models. The model initialized with SpaCy's English NER model, then fine-tuned using our data, consisting of 21K messages labelled with hotel and location entities. Our first model treats hotels and locations as separate entities, while our second model merges them and considers both hotels and locations as a single combined entity type. All models are evaluated by their precision, recall, and F1 scores for each entity type. The results are shown in Table TABREF14.", "The automated component of the chatbot is also closely integrated with human support agents: when the NLP system is unable to understand a customer's intentions, customer support agents are notified and take over the conversation. The agents' feedback is then used to improve the AI, providing valuable training data (Figure FIGREF5). In this paper, we describe our conversational AI systems, datasets, and models.", "External metrics serve as a proxy for our NLP system's performance, since users are more likely to request an agent and less likely to complete their booking when the bot fails. Thus, an improvement in these metrics after a model deployment validates that the model functions as intended in the real world. However, both metrics are noisy and are affected by factors unrelated to NLP, such as seasonality and changes in the hotel supply chain.", "The models are trained on 9K search messages, with up to 10 results from ElasticSearch and annotations for which results are valid matches. Each training row is expanded into multiple message-result pairs, which are fed as instances to the network. For the BERT model, we use the uncased BERT-base, which requires significantly less memory than BERT-large. All models are trained end-to-end and implemented using AllenNLP BIBREF8."]}
{"question_id": "9a6bf1d481e6896eef9f8fed835d9d29658ede36", "predicted_answer": "", "predicted_evidence": ["Note that we weigh each ancestor node here by its height; our rationale is that ancestor nodes that are closer to the root are more important. The formulation of these shallow features (nuclearity and relation scores) are inspired by ono1994abstract, who propose a number of ways to score an EDU based on the RST tree structure.", "In addition to the shallow features, we also extract latent features from the RST parser.", "In Figure FIGREF8, we present the feature extraction pipeline. Given an input document, we use Stanford CoreNLP to tokenize words and sentences, and obtain the POS tags. We then parse the processed input with the bi-affine parser BIBREF8 to get the syntax features.", "where $r_j$ is a discourse relation (one of 18 in total).", "Our summarization model is based on the pointer\u2013generator network BIBREF2. We present the architecture in Figure FIGREF13, and summarize it as follows:"]}
{"question_id": "4999da863ecbd40378505bfb1f4e395061a3f559", "predicted_answer": "", "predicted_evidence": ["where $\\lbrace h_i\\rbrace $ are the encoder hidden states, $\\lbrace w_i\\rbrace $ are the embedded encoder input words, $s_t$ is the decoder hidden state, and $x_t$ is the embedded decoder input word.", "Abstractive summarization is the task of creating a concise version of a document that encapsulates its core content. Unlike extractive summarization, abstractive summarization has the ability to create new sentences that are not in the original document; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner.", "We train the models for approximately 240,000-270,000 iterations (13 epochs). When we include the coverage mechanism (second baseline), we train for an additional 3,000\u20133,500 iterations using the coverage penalty, following see2017get.", "where $\\oplus $ denotes the concatenate operation.", "where $r_j$ is a discourse relation (one of 18 in total)."]}
{"question_id": "3098793595252039f363ee1150d4ea956f2504b8", "predicted_answer": "", "predicted_evidence": ["where $e$ is an EDU; $h(\\text{x})$ gives the height from node $x$; and $\\mathbb {1}_{\\text{nucleus}}(x)$ is an indicator function, i.e. it returns 1 when node $x$ is a nucleus and 0 otherwise.", "Intuitively, the coverage loss works by first summing the attention weights over all words from previous decoding steps ($c^t$), using that information as part of the attention computation ($e_i^t$), and then penalising the model if previously attended words receive attention again (covloss$_t$). see2017get train the model for an additional 3K steps with the coverage penalty after it is trained with cross-entropy loss.", "A similar idea was proposed by BIBREF16, where a recursive neural network is used to learn a discourse-aware representation. Here, DPLP is utilized to obtain discourse structures, and a recursive neural network is applied to the doc2vec BIBREF17 representations for each EDU. The proposed approach is evaluated over sentiment analysis and sarcasm detection tasks, but found to not be competitive with benchmark methods.", "Our summarization model is based on the pointer\u2013generator network BIBREF2. We present the architecture in Figure FIGREF13, and summarize it as follows:", "Given a discourse tree produced by the RST parser BIBREF1, we compute several shallow features for an EDU: (1) the nuclearity score; (2) the relation score for each relation; and (3) the node type and that of its sibling."]}
{"question_id": "99f898eb91538cb82bc9a00892d54ae2a740961e", "predicted_answer": "", "predicted_evidence": ["3. Adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed sightly improved the performance on the three downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the minor performance gain.", "Inspired by learning to exploit the contextual information present in adjacent sentences, we proposed an asymmetric encoder-decoder model with a suite of techniques for improving context-based unsupervised sentence representation learning. Since we believe that a simple model will be faster in training and easier to analyse, we opt to use simple techniques in our proposed model, including 1) an RNN as the encoder, and a predict-all-words CNN as the decoder, 2) learning by inferring subsequent contiguous words, 3) mean+max pooling, and 4) tying word vectors with word prediction. With thorough discussion and extensive evaluation, we justify our decision making for each component in our RNN-CNN model. In terms of the performance and the efficiency of training, we justify that our model is a fast and simple algorithm for learning generic sentence representations from unlabelled corpora. Further research will focus on how to maximise the utility of the context information, and how to design simple architectures to best make use of it.", "With Finding I, we conducted an experiment to test whether the model needs an autoregressive decoder at all. In this experiment, the goal is to compare the performance of the predict-all-words decoders and that of the autoregressive decoders separate from the RNN/CNN distinction, thus we designed a predict-all-words CNN decoder and RNN decoder. The predict-all-words CNN decoder is described in Section SECREF2 , which is a stack of three convolutional layers, and all words are predicted once at the output layer of the decoder. The predict-all-words RNN decoder is built based on our CNN decoder. To keep the number of parameters of the two predict-all-words decoder roughly the same, we replaced the last two convolutional layers with a bidirectional GRU.", "Learning distributed representations of sentences is an important and hard topic in both the deep learning and natural language processing communities, since it requires machines to encode a sentence with rich language content into a fixed-dimension vector filled with real numbers. Our goal is to build a distributed sentence encoder learnt in an unsupervised fashion by exploiting the structure and relationships in a large unlabelled corpus.", "The results are presented in Table TABREF10 (top two subparts). As we can see, the three decoding settings do not differ significantly in terms of the performance on selected downstream tasks, with RNN or CNN as the decoder. The results show that, in terms of learning good sentence representations, the autoregressive decoder doesn't require the correct ground-truth words as the inputs."]}
{"question_id": "cf68906b7d96ca0c13952a6597d1f23e5184c304", "predicted_answer": "", "predicted_evidence": ["Clearly, we can tell from the comparison between rows 1, 9 and 12 in Table TABREF21 , increasing the dimensionality of the RNN encoder leads to better transferability of the model.", "In other words, an encoder with larger size will result in a representation with higher dimensionality, and generally, it will augment the expressiveness of the vector representation, and the transferability of the model.", "4. Increasing the dimensionality of the RNN encoder improved the model performance, and the additional training time required was less than needed for increasing the complexity in the CNN decoder. We report results from both smallest and largest models in Table TABREF16 .", "Table TABREF16 presents the results on 9 evaluation tasks of our proposed RNN-CNN models, and related work. The \u201csmall RNN-CNN\u201d refers to the model with the dimension of representation as 1200, and the \u201clarge RNN-CNN\u201d refers to that as 4800. The results of our \u201clarge RNN-CNN\u201d model on SNLI is presented in Table TABREF19 .", "Suppose that the target sequence INLINEFORM0 has INLINEFORM1 words, which are INLINEFORM2 , the first layer of deconvolution will expand INLINEFORM3 , into a feature map with INLINEFORM4 elements. It can be easily implemented as a concatenation of outputs from INLINEFORM5 linear transformations in parallel. Then the second and third layer are 1D-convolution layers. The output feature map is INLINEFORM6 , where INLINEFORM7 is the dimension of the word vectors."]}
{"question_id": "3e5162e6399c7d03ecc7007efd21d06c04cf2843", "predicted_answer": "", "predicted_evidence": ["[noitemsep]", "Background and Warm Up", "All participants: Must be involved in politics in Canada and must be engaged on Twitter - i.e. have an account and follow political accounts and/or issues", "Method 15- to 30-minute interviews via telephone", "Round 2: Find people who've interacted with the bot on Twitter who we don't know, send them a DM, and ask if we can get their feedback over a 15- to 30-minute phone call"]}
{"question_id": "bd255aadf099854541d06997f83a0e478f526120", "predicted_answer": "", "predicted_evidence": ["Round 3: Use contacts in Canadian politics to recruit participants who have no prior awareness of ParityBOT", "Obtain feedback from Twitter users who've interacted with the bot", "[noitemsep]", "nolistsep", "Gain feedback and initial impressions from people who haven't interacted with the Bot, but are potential audience"]}
{"question_id": "a9ff35f77615b3a4e7fd7b3a53d0b288a46f06ce", "predicted_answer": "", "predicted_evidence": ["What do you think it's purpose is?", "nolistsep", "With your permission, we'd like to record our conversation. The recording will only be used to help us capture notes from the session and figure out how to improve the project, and it won't be seen by anyone except the people working on this project. We may use some quotes in an academic paper, You'll be anonymous and we won't identify you personally by name.", "If you have any concerns at time, we can stop the interview and the recording. Do we have your permission to do this? (Wait for verbal \u201cyes\u201d).", "When you were thinking about running for politics what were your major considerations? For example, barriers, concerns?"]}
{"question_id": "69a46a227269c3aac9bf9d7c3d698c787642f806", "predicted_answer": "", "predicted_evidence": ["What advice would you give to women in politics experiencing online harassment?", "[noitemsep]", "If you have any concerns at time, we can stop the interview and the recording. Do we have your permission to do this? (Wait for verbal \u201cyes\u201d).", "[Author]: Hey! Thanks for doing this. This shouldn't take longer than 20 minutes. [Author] is a UX researcher and is working with us. They'll take it from here and explain our process, get your consent and conduct the interview. I'll be taking notes. Over to [Author]!", "Introduction"]}
{"question_id": "ebe6b8ec141172f7fea66f0a896b3124276d4884", "predicted_answer": "", "predicted_evidence": ["Thank you very much for your time! If you have any questions, or further comments, feel free to text or email [Author].", "If you have any concerns at time, we can stop the interview and the recording. Do we have your permission to do this? (Wait for verbal \u201cyes\u201d).", "nolistsep", "Introduction to PartyBOT Thanks very much, now, more specifically about the ParityBOT:", "Next Steps If you could build on this idea of mitigating online harassment for women in politics, what ideas or suggestions would you have?"]}
{"question_id": "946d7c877d363f549f84e9500c852dce70ae5d36", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF28 presents the distribution of contribution scores over different layers for each example of SST-2 dataset. The number on the ordinate axis denotes the index of the example. We observe that even though there are subtle differences among these example, they follow certain same patterns when calculating the complementary representation, for example, layer 21 and 22 contribute the most for almost all the examples and also the layers around them. But the figure shows also that for some examples, all layers contribute almost equally.", "where $\\theta $ is the set of all parameters in the model, $N$ is the number of examples in the dataset, $p_{i,c}$ is the predicted probability of class $c$ for example $i$ and $y$ is the binary indicator defined as below:", "Given the simplicity between RTE, WNLI and MNLI, and the large-scale nature of MNLI dataset (393k), we also initialize RTRHI with the weights of MNLI single-task model before fine-tuning on RTE and WNLI. We submitted the ensemble-model results to the leaderboard. The results show that RTRHI still boosts the strong RoBERTa baseline model on the test set. To be specific, RTRHI outperforms RoBERTa over CoLA, SST-2, MRPC, SST-B, MNLI-mm with an improvement of 0.8 points, 0.4 points, 0.7/0.9 points, 0.2/0.1 points and 0.2 points respectively. In the meantime, RTRHI gets the same results as RoBERTa on QQP and WNLI. By category, RTRHI has better performance than RoBERTa on the single sentence tasks, similarity and paraphrase tasks.", "GLUE benchmark contains two types of tasks: 1. classification; 2. regression. For classification tasks, given the input text's contextualized representation $F$, following BIBREF2, we take the first row $C \\in \\mathbb {R}^{2d}$ of $F$ corresponding to the first input token ([CLS]) as the aggregate representation. Let $m$ be the number of labels in the datasets, we pass $C$ through a feed-forward network(FFN):", "We compare the contribution score's distribution of different NLU tasks. For each task, we run our best single model over the development set and the results are calculated by averaging the values across all the examples within each dataset. The results are showed in Figure FIGREF27. From the top to the bottom of the headmap, the results are placed in the following order: single-sentence tasks, similarity and paraphrase tasks and natural language inference tasks. From figure FIGREF27, we find that the distribution differs among the different tasks, which demonstrates RTRHI's dynamic ability to adapt for distinct task when computing the complementary representation. The most important contribution occurs below the final layer for all the tasks except MRPC and RTE. All layers have a close contribution for MRPC and RTE task."]}
{"question_id": "26e32f24fe0c31ef25de78935daa479534b9dd58", "predicted_answer": "", "predicted_evidence": ["Given the simplicity between RTE, WNLI and MNLI, and the large-scale nature of MNLI dataset (393k), we also initialize RTRHI with the weights of MNLI single-task model before fine-tuning on RTE and WNLI. We submitted the ensemble-model results to the leaderboard. The results show that RTRHI still boosts the strong RoBERTa baseline model on the test set. To be specific, RTRHI outperforms RoBERTa over CoLA, SST-2, MRPC, SST-B, MNLI-mm with an improvement of 0.8 points, 0.4 points, 0.7/0.9 points, 0.2/0.1 points and 0.2 points respectively. In the meantime, RTRHI gets the same results as RoBERTa on QQP and WNLI. By category, RTRHI has better performance than RoBERTa on the single sentence tasks, similarity and paraphrase tasks.", "Single-sentence tasks: The Corpus of Linguistic Acceptability (CoLA) BIBREF20 requires the model to determine whether a sentence is grammatically acceptable; the Stanford Sentiment Treebank (SST-2) BIBREF10 is to predict the sentiment of movie reviews with label of positive or negative.", "With the same shape as the output of Transformer-based encoder's final layer, HIRE's output $A$ is expected to contain the additional useful information from the encoder's hidden-states which is helpful for a better understanding of the input text and we call it complementary representation.", "with $W_1 \\in \\mathbb {R}^{2d\\times d}$, $W_2 \\in \\mathbb {R}^{d\\times m}$, $b_1\\in \\mathbb {R}^{d}$ and $b_2\\in \\mathbb {R}^{m}$ the only parameters that we introduce in output layer. Finally, the probability distribution of predicted label is computed as:", ""]}
{"question_id": "22375aac4cbafd252436b756bdf492a05f97eed8", "predicted_answer": "", "predicted_evidence": ["BIBREF28 combined FNNLM with cache model to enhance the performance of FNNLM in speech recognition, and the cache model was formed based on the previous context as following: INLINEFORM0", "where, the sum of all words' sqrt frequencies INLINEFORM0 .", "BiRNN was introduced to speech recognition by BIBREF35 , and then was evaluated in other NLP tasks, like NMT BIBREF36 , BIBREF3 . In these studies, BiRNN showed more excellent performance than unidirectional RNN. Nevertheless, BiRNN cannot be evaluated in LM directly as unidirectional RNN, because statistical language modeling is based on the chain rule which assumes that word in a word sequence only statistically depends on one side context. BiRNN can be applied in NLP tasks, like speech recognition and machine translation, because the input word sequences in these tasks are treated as a whole and usually encoded as a single vector. The architecture for encoding input word sequences using BiRNN is showed in Figure FIGREF18 . The facts that better performance can be achieved using BiRNN in speech recognition or machine translation indicate that a word in a word sequence is statistically determined by the words of its both side, and it is not a suitable way to deal with word sequence in a natural language word by word in an order.", "As is well known, ANN is designed by imitating biological neural system, but biological neural system does not share the same limit with ANN. In fact, the strong power of biological neural system is original from the enormous number of neurons and various connections among neurons, including gathering, scattering, lateral and recurrent connections BIBREF37 . In biological neural system, the features of signals are detected by different receptors, and encoded by low-level central neural system (CNS) which is changeless. The encoded signals are integrated by high-level CNS. Inspired by this, an improvement scheme for the architecture of ANN is proposed, as illustrated in Figure FIGREF19 . The features of signal are extracted according to the knowledge in certain field, and every feature is encoded using changeless neural network with careful designed structure. Then, the encoded features are integrated using a trainable neural network which may share the same architecture as existing ones.", "As is well known, ANN is designed by imitating biological neural system, but biological neural system does not share the same limit with ANN. In fact, the strong power of biological neural system is original from the enormous number of neurons and various connections among neurons, including gathering, scattering, lateral and recurrent connections BIBREF37 . In biological neural system, the features of signals are detected by different receptors, and encoded by low-level central neural system (CNS) which is changeless. The encoded signals are integrated by high-level CNS. Inspired by this, an improvement scheme for the architecture of ANN is proposed, as illustrated in Figure FIGREF19 . The features of signal are extracted according to the knowledge in certain field, and every feature is encoded using changeless neural network with careful designed structure. Then, the encoded features are integrated using a trainable neural network which may share the same architecture as existing ones. Because the model for encoding does not need to be trained, the size of this model can be much huge and the structure can be very complexity."]}
{"question_id": "d2f91303cec132750a416192f67c8ac1d3cf6fc0", "predicted_answer": "", "predicted_evidence": ["The INLINEFORM0 -th element of output vector INLINEFORM1 is the unnormalized conditional probability of the word with index INLINEFORM2 in the vocabulary. In order to guarantee all the conditional probabilities of words positive and summing to one, a softmax layer is always adopted following the output layer of FNN: INLINEFORM3", "Bidirectional recurrent neural network (BiRNN) BIBREF34 was designed to process data in both directions with two separate hidden layers, so better performance can be expected by using BiRNN. BiRNN was introduced to speech recognition by BIBREF35 , and then was evaluated in other NLP tasks, like NMT BIBREF36 , BIBREF3 . In these studies, BiRNN showed more excellent performance than unidirectional RNN. Nevertheless, BiRNN cannot be evaluated in LM directly as unidirectional RNN, because statistical language modeling is based on the chain rule which assumes that word in a word sequence only statistically depends on one side context. BiRNN can be applied in NLP tasks, like speech recognition and machine translation, because the input word sequences in these tasks are treated as a whole and usually encoded as a single vector. The architecture for encoding input word sequences using BiRNN is showed in Figure FIGREF18 .", "Where, INLINEFORM0 , INLINEFORM1 are weight matrixes, INLINEFORM2 is the size of hidden layer, INLINEFORM3 is the size of output layer, weight matrix INLINEFORM4 is for the direct connections between input layer and output layer, INLINEFORM5 and INLINEFORM6 are vectors for bias terms in hidden layer and output layer respectively, INLINEFORM7 is output vector, and INLINEFORM8 is activation function.", "Maybe more data is needed to train RNNLM and LSTM-RNNLM because longer dependencies are taken into account by RNNLM and LSTM-RNNLM when predicting next word. LSTM-RNNLM with bias terms or direct connections was also evaluated here. When the direct connections between input layer and output layer of LSTM-RNN are enabled, a slightly higher perplexity but shorter training time were obtained. An explanation given for this phenomenon by BIBREF10 is that direct connections provide a bit more capacity and faster learning of the \"linear\" part of mapping from inputs to outputs but impose a negative effect on generalization. For bias terms, no significant improvement on performance was gained by adding bias terms which was also observed on RNNLM by BIBREF16 . In the rest of this paper, all studies will be performed on LSTM-RNNLM with neither direct connections nor bias terms, and the result of this model in Table TABREF9 will be used as the baseline for the rest studies.", "Where, INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are input gate, forget gate and output gate, respectively. INLINEFORM3 is the internal memory of unit. INLINEFORM4 , INLINEFORM5 , INLINEFORM6 , INLINEFORM7 , INLINEFORM8 , INLINEFORM9 , INLINEFORM10 , INLINEFORM11 , INLINEFORM12 , INLINEFORM13 , INLINEFORM14 , INLINEFORM15 are all weight matrixes. INLINEFORM16 , INLINEFORM17 , INLINEFORM18 , INLINEFORM19 , and INLINEFORM20 are vectors for bias terms. INLINEFORM21 is the activation function in hidden layer and INLINEFORM22 is the activation function for gates."]}
{"question_id": "9f065e787a0d40bb4550be1e0d64796925459005", "predicted_answer": "", "predicted_evidence": ["Except for the probabilistic distribution of word sequences, the feature vectors of words in vocabulary are also formed by neural network during training. Because of the classification function of neural network, the similarities between words can be observed using these feature vectors. However, the similarities between words are evaluated in a multiple dimensional space by feature vectors and it is hard to know which features of words are taken into account when these vectors are formed, which means words cannot be grouped according to any single feature by the feature vectors. In summary, the knowledge represented by neural network language model is the probabilistic distribution of word sequences from certain training data set and feature vectors for words in vocabulary formed in multiple dimensional space. Neither the knowledge of language itself, like grammar, nor the knowledge conveyed by a language can be gained from neural network language models. Therefore, NNLM can be a good choice for NLP tasks in some special fields where language understanding is not necessary. Language understanding cannot be achieved just with the probabilistic distribution of word sequences in a natural language, and new kind of knowledge representation should be raised for language understanding.", "In most language models including neural network language models, words are predicated one by one according to their previous context or following one which is believed to simulate the way human deal with natural languages, and, according to common sense, human actually speak or write word by word in a certain order. However, the intrinsic mechanism in human mind of processing natural languages cannot like this way. As mentioned above, it is not always true that words in a word sequence only depend on their previous or following context. In fact, before human speaking or writing, they know what they want to express and map their ideas into word sequence, and the word sequence is already cached in memory when human speaking or writing. In most case, the cached word sequence may be not a complete sentence but at least most part of it. On the other hand, for reading or listening, it is better to know both side context of a word when predicting the meaning of the word or define the grammar properties of the word.", "As a word in word sequence statistically depends on its both previous and following context, it is better to predict a word using context from its both side. Bidirectional recurrent neural network (BiRNN) BIBREF34 was designed to process data in both directions with two separate hidden layers, so better performance can be expected by using BiRNN. BiRNN was introduced to speech recognition by BIBREF35 , and then was evaluated in other NLP tasks, like NMT BIBREF36 , BIBREF3 . In these studies, BiRNN showed more excellent performance than unidirectional RNN. Nevertheless, BiRNN cannot be evaluated in LM directly as unidirectional RNN, because statistical language modeling is based on the chain rule which assumes that word in a word sequence only statistically depends on one side context. BiRNN can be applied in NLP tasks, like speech recognition and machine translation, because the input word sequences in these tasks are treated as a whole and usually encoded as a single vector.", "Various architectures of neural network language models are described and a number of improvement techniques are evaluated in this paper, but there are still something more should be included, like gate recurrent unit (GRU) RNNLM, dropout strategy for addressing overfitting, character level neural network language model and ect. In addition, the experiments in this paper are all performed on Brown Corpus which is a small corpus, and different results may be obtained when the size of corpus becomes larger. Therefore, all the experiments in this paper should be repeated on a much larger corpus.", "However, it maybe not a proper way to deal with natural languages. Natural languages are not natural but man-made, and linguistical knowledge are also created by human long after natural language appeared. Liguistical knowledge only covers the \"right\" word sequences in a natural language, but it is common to deal with \"wrong\" ones in real world. In nature, every natural language is a mechanism of linking voices or signs with objects, both concrete and abstract. Therefore, the proper way to deal with natural languages is to find the relations between special voices or signs and objects, and the features of voices or signs can be defined easier than a natural language itself. Every voice or sign can be encoded as a unique code, vector or matrix, according to its features, and the similarities among voices or signs are indeed can be recognized from their codes. It is really difficult to model the relation between voices or signs and objects at once, and this work should be split into several steps."]}
{"question_id": "e6f5444b7c08d79d4349e35d5298a63bb30e7004", "predicted_answer": "", "predicted_evidence": ["Like word classes, caching is also a common used optimization technique in LM. The cache language models are based on the assumption that the word in recent history are more likely to appear again. In cache language model, the conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching, like: INLINEFORM0", "The recommended learning algorithm for neural network language models is stochastic gradient descent (SGD) method using backpropagation (BP) algorithm. A common choice for the loss function is the cross entroy loss which equals to negative log-likelihood here. The parameters are usually updated as: INLINEFORM0", "The results show that the knowledge represented by a neural network language model is the probabilistic distribution of word sequences from training data set which varies from field to field. Except for the probabilistic distribution of word sequences, the feature vectors of words in vocabulary are also formed by neural network during training. Because of the classification function of neural network, the similarities between words can be observed using these feature vectors. However, the similarities between words are evaluated in a multiple dimensional space by feature vectors and it is hard to know which features of words are taken into account when these vectors are formed, which means words cannot be grouped according to any single feature by the feature vectors. In summary, the knowledge represented by neural network language model is the probabilistic distribution of word sequences from certain training data set and feature vectors for words in vocabulary formed in multiple dimensional space. Neither the knowledge of language itself, like grammar, nor the knowledge conveyed by a language can be gained from neural network language models.", "As is well known, ANN is designed by imitating biological neural system, but biological neural system does not share the same limit with ANN. In fact, the strong power of biological neural system is original from the enormous number of neurons and various connections among neurons, including gathering, scattering, lateral and recurrent connections BIBREF37 . In biological neural system, the features of signals are detected by different receptors, and encoded by low-level central neural system (CNS) which is changeless. The encoded signals are integrated by high-level CNS. Inspired by this, an improvement scheme for the architecture of ANN is proposed, as illustrated in Figure FIGREF19 . The features of signal are extracted according to the knowledge in certain field, and every feature is encoded using changeless neural network with careful designed structure. Then, the encoded features are integrated using a trainable neural network which may share the same architecture as existing ones. Because the model for encoding does not need to be trained, the size of this model can be much huge and the structure can be very complexity.", "What's more, word sequences are commonly taken as signals for LM, and it is easy to take linguistical properties of words or sentences as the features of signals. However, it maybe not a proper way to deal with natural languages. Natural languages are not natural but man-made, and linguistical knowledge are also created by human long after natural language appeared. Liguistical knowledge only covers the \"right\" word sequences in a natural language, but it is common to deal with \"wrong\" ones in real world. In nature, every natural language is a mechanism of linking voices or signs with objects, both concrete and abstract. Therefore, the proper way to deal with natural languages is to find the relations between special voices or signs and objects, and the features of voices or signs can be defined easier than a natural language itself. Every voice or sign can be encoded as a unique code, vector or matrix, according to its features, and the similarities among voices or signs are indeed can be recognized from their codes."]}
{"question_id": "59f41306383dd6e201bded0f1c7c959ec4f61c5a", "predicted_answer": "", "predicted_evidence": ["We present our results in [tab:elmo]Table tab:elmo.", "We average the scores across all users for each sentence. Sentences with a score in the range $(x, 1]$ are marked as positive (where $x\\in [0.5,1)$ ), sentences in $[0, 1-x)$ marked as negative, and sentences in $[1-x, x]$ are marked as neutral. For instance, \u201cflat , but with a revelatory performance by michelle williams\u201d (score=0.56) is neutral when $x=0.6$ . We present statistics of our dataset in [tab:crowdall]Table tab:crowdall. Inter-annotator agreement was computed using Fleiss' Kappa ( $\\kappa $ ). As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences). According to landis1977measurement, $\\kappa \\in (0.2, 0.4]$ corresponds to \u201cfair agreement\u201d, whereas $\\kappa \\in (0.4, 0.6]$ corresponds to \u201cmoderate agreement\u201d.", "The next two columns, which show the results of repeating the above analysis after averaging over 100 random seeds, contradict this claim. The averaged figures show lower overall accuracy increases, and, more importantly, they attribute these improvements almost entirely to the projection component rather than the distillation component. To confirm this result, we repeat our averaged analysis restricted to only \u201cA-but-B\u201d sentences targeted by the rule (shown in the last two columns). We again observe that the effect of projection is pronounced, while distillation offers little or no advantage in comparison.", "Here we briefly review background from hu2016harnessing to provide a foundation for our reanalysis in the next section. We focus on a logic rule for sentences containing an \u201cA-but-B\u201d structure (the only rule for which hu2016harnessing provide experimental results). Intuitively, the logic rule for such sentences is that the sentiment associated with the whole sentence should be the same as the sentiment associated with phrase \u201cB\u201d.", "We conduct a crowdsourced analysis that reveals that SST2 data has significant levels of ambiguity even for human labelers. We discover that ELMo's performance improvements over the baseline are robust across varying levels of ambiguity, whereas the advantage of hu2016harnessing is reversed in sentences of low ambiguity (restricting to A-but-B style sentences)."]}
{"question_id": "b3432f52af0b95929e6723dd1f01ce029d90a268", "predicted_answer": "", "predicted_evidence": ["Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these discourse relations ([tab:sst2]Table tab:sst2). As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.", "We average the scores across all users for each sentence. Sentences with a score in the range $(x, 1]$ are marked as positive (where $x\\in [0.5,1)$ ), sentences in $[0, 1-x)$ marked as negative, and sentences in $[1-x, x]$ are marked as neutral. For instance, \u201cflat , but with a revelatory performance by michelle williams\u201d (score=0.56) is neutral when $x=0.6$ . We present statistics of our dataset in [tab:crowdall]Table tab:crowdall. Inter-annotator agreement was computed using Fleiss' Kappa ( $\\kappa $ ). As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences).", "We present our results in [tab:elmo]Table tab:elmo.", "Here we briefly review background from hu2016harnessing to provide a foundation for our reanalysis in the next section. We focus on a logic rule for sentences containing an \u201cA-but-B\u201d structure (the only rule for which hu2016harnessing provide experimental results). Intuitively, the logic rule for such sentences is that the sentiment associated with the whole sentence should be the same as the sentiment associated with phrase \u201cB\u201d.", "The next two columns, which show the results of repeating the above analysis after averaging over 100 random seeds, contradict this claim. The averaged figures show lower overall accuracy increases, and, more importantly, they attribute these improvements almost entirely to the projection component rather than the distillation component. To confirm this result, we repeat our averaged analysis restricted to only \u201cA-but-B\u201d sentences targeted by the rule (shown in the last two columns). We again observe that the effect of projection is pronounced, while distillation offers little or no advantage in comparison."]}
{"question_id": "6b1a6517b343fdb79f246955091ff25e440b9511", "predicted_answer": "", "predicted_evidence": ["We consider several evaluation metrics to estimate the quality and diversity of the generations.", "Let $X=(x_1, \\ldots , x_T)$ be a sequence of random variables $x_i$ 's. Each random variable is categorical in that it can take one of $M$ items from a vocabulary $V=\\left\\lbrace  v_1, \\ldots , v_{M} \\right\\rbrace $ . These random variables form a fully-connected graph with undirected edges, indicating that each variable $x_i$ is dependent on all the other variables.", "Following BIBREF22 , we compute self-BLEU: for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences. Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity.", "We also evaluate the percentage of $n$ -grams that are unique, when compared to the original data distribution and within the corpus of generations. We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique $n$ -grams implies higher BLEU.", "BERT is trained on a masked language modeling objective. Unlike a traditional language modeling objective of predicting the next word in a sequence given the history, masked language modeling predicts a word given its left and right context. Because the model expects context from both directions, it is not immediately obvious how to efficiently evaluate BERT as a language model (i.e., use it to evaluate the probability of a text sequence) or how to sample from it."]}
{"question_id": "5f25b57a1765682331e90a46c592a4cea9e3a336", "predicted_answer": "", "predicted_evidence": ["Meeting transcription and analytics would be a key to enhancing productivity as well as improving accessibility in the workplace. It can also be used for conversation transcription in other domains such as healthcare BIBREF0. Research in this space was promoted in the 2000s by NIST Rich Transcription Evaluation series and public release of relevant corpora BIBREF1, BIBREF2, BIBREF3. While systems developed in the early days yielded high error rates, advances have been made in individual component technology fields, including conversational speech recognition BIBREF4, BIBREF5, far-field speech processing BIBREF6, BIBREF7, BIBREF8, and speaker identification and diarization BIBREF9, BIBREF10, BIBREF11. When cameras are used in addition to microphones to capture the meeting conversations, speaker identification quality could be further improved thanks to the computer vision technology. These trends motivated us to build an end-to-end audio-visual meeting transcription system to identify and address unsolved challenges.", "We augment an input feature vector with the cosine similarity score between the input and a face signature, which results in a classification function of the form of $\\langle \\mathbf {x},\\mathbf {w}^h_{1:d} \\rangle + w^h_{d+1}\\cos \\big (\\mathbf {x}, \\mathbf {q}_h\\big )-b^h,$ where $\\mathbf {x}\\in {\\mathbb {R}^d}$, $\\mathbf {q}_h$ is $h$'s face signature obtained as the mean of the gallery face features of $h$, $\\text{cos}(\\cdot )$ is the cosine similarity, and $\\big (\\mathbf {w}^h,b^h\\big )$ are linear weights and bias. We note that more complex rules tend to overfit due to the small size of enrollment, which typically consists of no more than 10 images.", "Table TABREF22 shows SA-WERs for two different diarization configurations and two different experiment setups. In the first setup, we assumed all attendees were invited to the meetings and therefore their face and voice signatures were available in advance. In the second setup, we used precomputed face and voice signatures for 50% of the attendees and the other speakers were treated as `guests'. A diarization system using only face identification and SSL may be regarded as a baseline as this approach was widely used in previous audio-visual diarization studies BIBREF33, BIBREF34, BIBREF35. The results show that the use of speaker identification substantially improved the speaker attribution accuracy. The SA-WERs were improved by 11.6% and 6.0% when the invited/guest ratios were 100/0 and 50/50, respectively. The small differences between the SA-WERs from Table TABREF22 and the WER from Table TABREF22 indicate very accurate speaker attribution.", "Two test sets were created: a gold standard test set and an extended test set. They were manually transcribed in different ways. The gold standard test set consisted of seven meetings and was 4.0 hours long in total. Those meetings were recorded both with the device described above and headset microphones. Professional transcribers were asked to provide initial transcriptions by using the headset and far-field audio recordings as well as the video. Then, automatic segmentation was performed with forced alignment. Finally, the segment boundaries and transcriptions were reviewed and corrected. Significant effort was made to fine-tune timestamps of the segmentation boundaries. While being very accurate, this transcription process requires headset recordings and therefore is not scalable. The extended test set contained 19 meetings totaling 6.4 hours. It covered a wider variety of conditions.", "We follow the matched background similarity (MBGS) approach of BIBREF53 and make crucial adaptations to it that increase accuracy significantly for our problem. As with MBGS, we train a discriminative classifier for each identity $h$ in $\\mathcal {H}$. The gallery of $h$ is used as positive examples, while a separate fixed background set $B$ is used as negative examples. This approach has two important benefits. First, it allows us to train a classifier adapted to a specific person. Second, the use of a background set $B$ lets us account for misleading sources of variation e.g. if a blurry or poorly lit face from $B$ is similar to one of the positive examples, the classifier's decision boundary can be chosen accordingly. During meeting initialization, an support vector machine (SVM) classifier is trained to distinguish between the positive and negative sets for each invitee. At test time, we are given a tracklet $T=\\big \\lbrace \\mathbf {t}_1,...,\\mathbf {t}_N\\big \\rbrace $ represented as a set of face feature vectors $\\mathbf {t}_i\\in {\\mathbb {R}^d}$, and we classify each member $\\mathbf {t}_i$ with the classifier of each identity $h$ and obtain a set of classification confidences $\\big \\lbrace s\\big (T\\big )_{i,h}\\big \\rbrace $."]}
{"question_id": "2ba2ff6c21a16bd295b07af1ef635b3b4c5bd17e", "predicted_answer": "", "predicted_evidence": ["where $\\omega $ is a discrete-valued latent variable representing the sound direction. It should be noted that the strongest sound direction may be mismatched with the face direction to a varying degree due to sound reflections on tables, diffraction on obstacles, face orientation variability, and so on. $P(\\omega | r)$ is introduced to represent this mismatch and modeled as a uniform distribution with a width of 25 degrees centered at the face position for $r$. The likelihood term, $p(A_s | \\omega ; M)$, is modeled with the CACGM and the log likelihood reduces to the following form BIBREF24: $ \\log p(A_s | \\omega ;M) = -\\sum _{t,f} m_{t,f} \\log (1 - || \\mathbf {z}_{t,f}^H \\mathbf {h}_{f,\\omega } ||^2 / (1 + \\epsilon ) ), $ where $\\mathbf {z}_{t,f}$ is a magnitude-normalized multi-channel observation vector constituting $A_s$, $m_{t,f}$ a TF mask, $\\mathbf {h}_{f, \\omega }$ a steering vector corresponding to sound direction $\\omega $, and $\\epsilon $ a small flooring constant.", "Meeting transcription and analytics would be a key to enhancing productivity as well as improving accessibility in the workplace. It can also be used for conversation transcription in other domains such as healthcare BIBREF0. Research in this space was promoted in the 2000s by NIST Rich Transcription Evaluation series and public release of relevant corpora BIBREF1, BIBREF2, BIBREF3. While systems developed in the early days yielded high error rates, advances have been made in individual component technology fields, including conversational speech recognition BIBREF4, BIBREF5, far-field speech processing BIBREF6, BIBREF7, BIBREF8, and speaker identification and diarization BIBREF9, BIBREF10, BIBREF11. When cameras are used in addition to microphones to capture the meeting conversations, speaker identification quality could be further improved thanks to the computer vision technology. These trends motivated us to build an end-to-end audio-visual meeting transcription system to identify and address unsolved challenges.", "", "where $\\mathcal {R}$ includes all face position trajectories detected by the face tracking module within the input period. We call a face position trajectory a tracklet. The joint posterior probability on the right hand side (RHS) can be factorized as", "Finally, Table TABREF22 shows the WER and SA-WER of the proposed system on the extended test set. For this experiment, we introduced approximations to the vision processing module to keep the real time factor smaller than one regardless of the number of faces detected. We can still observe similar WER and SA-WER numbers to those seen in the previous experiments, indicating the robustness of our proposed system."]}
{"question_id": "74acaa205a5998af4ad7edbed66837a6f2b5c58b", "predicted_answer": "", "predicted_evidence": ["The query vector $\\mathbf {q}$ captures information from question-answer context. During the training process, the query vector $\\mathbf {q}$ initializes an episodic memory vector $\\mathbf {m}^{(0)}$ as $\\mathbf {m}^{(0)}=\\mathbf {q}$ . A iterative attention process is then triggered, which gradually refines the episodic memory $\\mathbf {m}$ until the maximum number of iterations steps $\\mathbf {b}_{1}$0 is reached. By the $\\mathbf {b}_{1}$1 iteration, the episodic memory $\\mathbf {b}_{1}$2 will memorize useful visual and external information to answer the question. Attention component. At the $\\mathbf {b}_{1}$3 iteration, we concatenate each knowledge embedding $\\mathbf {b}_{1}$4 with last iteration episodic memory $\\mathbf {b}_{1}$5 and query vector $\\mathbf {b}_{1}$6 , then apply the basic soft attention procedure to obtain the $\\mathbf {b}_{1}$7 context vector $\\mathbf {b}_{1}$8 as", "The DNN-based approaches BIBREF2 , BIBREF5 , BIBREF7 are therefore not sufficient, since they can only capture information present in the training images. Recent advances witness several attempts to link the knowledge to VQA methods BIBREF14 , BIBREF15 , which make use of structured knowledge graphs and reason about an image on the supporting facts. Most of these algorithms first extract the visual concepts from a given image, and implement reasoning over the structured knowledge bases explicitly. However, it is non-trivial to extract sufficient visual attributes, since an image lacks the structure and grammatical rules as language. To address this issue, we propose to retrieve a bath of candidate knowledge corresponding to the given image and related questions, and feed them to the deep neural network implicitly. The proposed approach provides a general pipeline that simultaneously preserves the advantages of DNN-based approaches BIBREF2 , BIBREF5 , BIBREF7 and knowledge-based techniques BIBREF14 , BIBREF15 .", "The memory networks BIBREF20 , BIBREF21 , BIBREF22 offer an opportunity to address these challenges by reading from and writing to the external memory module, which is modeled by the actions of neural networks. Recently, it has demonstrated the state-of-the-art performance in numerous NLP applications, including the reading comprehension BIBREF23 and textual question answering BIBREF24 , BIBREF22 . Some seminal efforts are also made to implement VQA based on dynamic memory networks BIBREF25 , but it does not involve the mechanism to incorporate the external knowledge, making it incapable of answering open-domain visual questions. Nevertheless, the attractive characteristics motivate us to leverage the memory structures to encode the large-scale structured knowledge and fuse it with the image features, which offers an approach to answer open domain visual questions.", "Different from the text-based QA problem, it is unfavourable to conduct the open-domain VQA based on the knowledge-based reasoning, since it is inevitably incomplete to describe an image with structured forms BIBREF16 . The recent availability of large training datasets BIBREF13 makes it feasible to train a complex model in an end-to-end fashion by leveraging the recent advances in deep neural networks (DNN) BIBREF2 , BIBREF5 , BIBREF7 , BIBREF10 , BIBREF12 . Nevertheless, it is non-trivial to integrate knowledge into DNN-based methods, since the knowledge are usually represented in a symbol-based or graph-based manner (e.g., Freebase BIBREF17 , DBPedia BIBREF18 ), which is intrinsically different from the DNN-based features. A few attempts are made in this direction BIBREF19 , but it may involve much irrelevant information and fail to implement multi-hop reasoning over several facts.", "To fulfill VQA tasks, it requires to endow the responder to understand intention of the question, reason over visual elements of the image, and sometimes have general knowledge about the world. Most of the present methods solve VQA by jointly learning interactions and performing inference over the question and image contents based on the recent success of deep learning BIBREF3 , BIBREF2 , BIBREF4 , BIBREF5 , BIBREF6 , which can be further improved by introducing the attention mechanisms BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . However, most of questions in the current VQA dataset are quite simple, which are answerable by analyzing the question and image alone BIBREF2 , BIBREF13 . It can be debated whether the system can answer questions that require prior knowledge ranging common sense to subject-specific and even expert-level knowledge."]}
{"question_id": "cfcf94b81589e7da215b4f743a3f8de92a6dda7a", "predicted_answer": "", "predicted_evidence": ["In this section, we report the quantitative performance of open-domain VQA in Table 2 along with the sample results in Fig. 4 . Since most of the alternative methods do not provide the results in the open-domain scenario, we make comprehensive comparison with our ablative models. As expected, we observe that a significant improvement ( $12.7\\%$ ) of our full KDMN model over the KDMN-NoKG model, where $6.8\\%$ attributes to the involvement of external knowledge and $5.9\\%$ attributes to the usage of memory network. Examples in Fig. 4 further provide some intuitive understanding of our algorithm. It is difficult or even impossible for a system to answer the open domain question when comprehensive reasoning beyond image content is required, e.g., the background knowledge for prices of stuff is essential for a machine when inferring the expensive ones.", "KDMN: our full model. External knowledge triples are incorporated in Dynamic Memory Network.", "4 . Since most of the alternative methods do not provide the results in the open-domain scenario, we make comprehensive comparison with our ablative models. As expected, we observe that a significant improvement ( $12.7\\%$ ) of our full KDMN model over the KDMN-NoKG model, where $6.8\\%$ attributes to the involvement of external knowledge and $5.9\\%$ attributes to the usage of memory network. Examples in Fig. 4 further provide some intuitive understanding of our algorithm. It is difficult or even impossible for a system to answer the open domain question when comprehensive reasoning beyond image content is required, e.g., the background knowledge for prices of stuff is essential for a machine when inferring the expensive ones. The larger performance improvement on open-domain dataset supports our belief that background knowledge is essential to answer general visual questions. Note that the performance can be further improved if the technique of ensemble is allowed. We fused the results of several KDMN models which are trained from different initializations.", "$$\\mathbf {m}^{(t)}=ReLU\\left(\\mathbf {W}_{3}\n\\left[\\mathbf {m}^{(t-1)};\\mathbf {c}^{(t)};\\mathbf {q}\\right]+\\mathbf {b}_{3}\\right),$$   (Eq. 16)", "where $\\mathbf {W}_1$ and $\\mathbf {b}_{1}$ are the weight matrix and bias vector, respectively; and, $\\mathbf {f}^{(I)}$ , $\\mathbf {f}^{(Q)}$ and $\\mathbf {f}^{(A)}$ are denoted as DNN features corresponding to the images, questions and multi-choice answers, respectively. The query vector $\\mathbf {q}$ captures information from question-answer context. During the training process, the query vector $\\mathbf {q}$ initializes an episodic memory vector $\\mathbf {m}^{(0)}$ as $\\mathbf {m}^{(0)}=\\mathbf {q}$ . A iterative attention process is then triggered, which gradually refines the episodic memory $\\mathbf {m}$ until the maximum number of iterations steps $\\mathbf {b}_{1}$0 is reached."]}
{"question_id": "d147117ef24217c43252d917d45dff6e66ff807c", "predicted_answer": "", "predicted_evidence": ["Finally, we generate a predicted answer by reasoning over the facts in the memory along with the image contents. In this paper, we focus on the task of multi-choice setting, where several multi-choice candidate answers are provided along with a question and a corresponding image. For each question, we treat every multi-choice answer as input, and predict whether the image-question-answer triplet is correct. The proposed model tries to choose one candidate answer with the highest probability by inferring the cross entropy error on the answers through the entire network.", "In our experiments, we fix the joint-embedding common space dimension as 1024, word-embedding dimension as 300 and the dimension of LSTM internal states as 512. We use a pre-trained ResNet-101 BIBREF32 model to extract image feature, and select 20 candidate knowledge triples for each QA pair through the experiments. Empirical study demonstrates it is sufficient in our task although more knowledge triples are also allowed. The iteration number of a dynamic memory network update is set to 2, and the dimension of episodic memory is set to 2048, which is equal to the dimension of memory slots.", "Our training objective is to learn parameters based on a cross-entropy loss function as", "Episodic memory updating component. We apply the memory update mechanism BIBREF21 , BIBREF25 as", "In this section, we report the quantitative performance of open-domain VQA in Table 2 along with the sample results in Fig. 4 . Since most of the alternative methods do not provide the results in the open-domain scenario, we make comprehensive comparison with our ablative models. As expected, we observe that a significant improvement ( $12.7\\%$ ) of our full KDMN model over the KDMN-NoKG model, where $6.8\\%$ attributes to the involvement of external knowledge and $5.9\\%$ attributes to the usage of memory network. Examples in Fig. 4 further provide some intuitive understanding of our algorithm. It is difficult or even impossible for a system to answer the open domain question when comprehensive reasoning beyond image content is required, e.g., the background knowledge for prices of stuff is essential for a machine when inferring the expensive ones."]}
{"question_id": "1a2b69dfa81dfeadd67b133229476086f2cc74a8", "predicted_answer": "", "predicted_evidence": ["In this section, we report the quantitative evaluation along with representative samples of our method, compared with our ablative models and the state-of-the-art method for both the conventional (close-domain) VQA task and open-domain VQA.", "Finally, we generate a predicted answer by reasoning over the facts in the memory along with the image contents. In this paper, we focus on the task of multi-choice setting, where several multi-choice candidate answers are provided along with a question and a corresponding image. For each question, we treat every multi-choice answer as input, and predict whether the image-question-answer triplet is correct. The proposed model tries to choose one candidate answer with the highest probability by inferring the cross entropy error on the answers through the entire network.", "where $ans$ represents the index of multi-choice candidate answers; the supported knowledge triples are stored in $\\mathbf {m}^{(T)}_{ans}$ ; and, $\\mathbf {W}_{4}$ and $\\mathbf {b}_{4}$ are the parameters to be optimized in the DNNs. The final choice are consequentially obtained once we have $ans^\\ast $ .", "To fulfill VQA tasks, it requires to endow the responder to understand intention of the question, reason over visual elements of the image, and sometimes have general knowledge about the world. Most of the present methods solve VQA by jointly learning interactions and performing inference over the question and image contents based on the recent success of deep learning BIBREF3 , BIBREF2 , BIBREF4 , BIBREF5 , BIBREF6 , which can be further improved by introducing the attention mechanisms BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . However, most of questions in the current VQA dataset are quite simple, which are answerable by analyzing the question and image alone BIBREF2 , BIBREF13 . It can be debated whether the system can answer questions that require prior knowledge ranging common sense to subject-specific and even expert-level knowledge.", "In this section, we elaborate on the details and formulations of our proposed model for answering open-domain visual questions. We first retrieve an appropriate amount of candidate knowledge from the large-scale ConceptNet by analyzing the image content and the corresponding questions; afterward, we propose a novel framework based on dynamic memory network to embed these symbolic knowledge triples into a continuous vector space and store it in a memory bank; finally, we exploit these information to implement the open-domain VQA by fusing the knowledge with image representation."]}
{"question_id": "6d6a9b855ec70f170b854baab6d8f7e94d3b5614", "predicted_answer": "", "predicted_evidence": ["Table TABREF10 presents the Precision, Recall and INLINEFORM0 -measure scores obtained on the Abuse class, for both baselines (Content-based BIBREF14 and Graph-based BIBREF10 ) and all three proposed fusion strategies (Early Fusion, Late Fusion and Hybrid Fusion). It also shows the number of features used to perform the classification, the time required to compute the features and perform the cross validation (Total Runtime) and to compute one message in average (Average Runtime). Note that Late Fusion has only 2 direct inputs (content- and graph-based SVMs), but these in turn have their own inputs, which explains the values displayed in the table.", "Another limitation of our work is the small size of our dataset. We must find some other corpora to test our methods at a much higher scale. However, all the available datasets are composed of isolated messages, when we need threads to make the most of our approach. A solution could be to start from datasets such as the Wikipedia-based corpus proposed by BIBREF4 , and complete them by reconstructing the original conversations containing the annotated messages. This could also be the opportunity to test our methods on an other language than French. Our content-based method may be impacted by this change, but this should not be the case for the graph-based method, as it is independent from the content (and therefore the language). Besides language, a different online community is likely to behave differently from the one we studied before. In particular, its members could react differently differently to abuse. The Wikipedia dataset would therefore allow assessing how such cultural differences affect our classifiers, and identifying which observations made for Space Origin still apply to Wikipedia.", "A mobile window is slid over the whole period, one message at a time. At each step, the network is updated either by creating new links, or by updating the weights of existing ones. This sliding window has a fixed length expressed in number of messages, which is derived from ergonomic constraints relative to the online conversation platform studied in Section SECREF5 . It allows focusing on a smaller part of the context period. At a given time, the last message of the window (in blue in Figure FIGREF4 ) is called current message and its author current author. The weight update method assumes that the current message is aimed at the authors of the other messages present in the window, and therefore connects the current author to them (or strengthens their weights if the edge already exists). It also takes chronology into account by favoring the most recent authors in the window. Three different variants of the conversational network are extracted for one given targeted message: the Before network is based on the messages posted before the targeted message, the After network on those posted after, and the Full network on the whole context period.", "These approaches rely on a mix of standard NLP features and manually crafted application-specific resources (e.g. linguistic rules). We also proposed a content-based method BIBREF3 using a wide array of language features (Bag-of-Words, INLINEFORM0 - INLINEFORM1 scores, sentiment scores). Other approaches are more machine learning intensive, but require larger amounts of data. Recently, BIBREF4 created three datasets containing individual messages collected from Wikipedia discussion pages, annotated for toxicity, personal attacks and aggression, respectively. They have been leveraged in recent works to train Recursive Neural Network operating on word embeddings and character INLINEFORM2 -gram features BIBREF5 , BIBREF6 . However, the quality of these direct content-based approaches is very often related to the training data used to learn abuse detection models. In the case of online social networks, the great variety of users, including very different language registers, spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have models robust enough to be applied in all cases.", "Another limitation of our work is the small size of our dataset. We must find some other corpora to test our methods at a much higher scale. However, all the available datasets are composed of isolated messages, when we need threads to make the most of our approach. A solution could be to start from datasets such as the Wikipedia-based corpus proposed by BIBREF4 , and complete them by reconstructing the original conversations containing the annotated messages. This could also be the opportunity to test our methods on an other language than French. Our content-based method may be impacted by this change, but this should not be the case for the graph-based method, as it is independent from the content (and therefore the language). Besides language, a different online community is likely to behave differently from the one we studied before. In particular, its members could react differently differently to abuse."]}
{"question_id": "870358f28a520cb4f01e7f5f780d599dfec510b4", "predicted_answer": "", "predicted_evidence": ["In this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose a new method to perform abuse detection while leveraging both sources. For this purpose, we take advantage of the content- BIBREF14 and graph-based BIBREF10 methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game. We then perform a feature study, finding the most informative ones and discussing their role. Our contribution is twofold: the exploration of fusion methods, and more importantly the identification of discriminative features for this problem.", "In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show that the content of the messages, but also of their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final INLINEFORM0 -measure of 93.26%.", "Finally, the third fusion strategy can be considered as Hybrid Fusion, as it seeks to combine both previous proposed ones. We create a feature set containing the content- and graph-based features, like with Early Fusion, but also both scores used in Late Fusion. This whole set is used to train a new SVM. The idea is to check whether the scores do not convey certain useful information present in the raw features, in which case combining scores and features should lead to better results.", "A number of works have tackled this problem, or related ones, in the literature. Most of them focus only on the content of the targeted message to detect abuse or similar properties. For instance, BIBREF0 applies this principle to detect hostility, BIBREF1 for cyberbullying, and BIBREF2 for offensive language. These approaches rely on a mix of standard NLP features and manually crafted application-specific resources (e.g. linguistic rules). We also proposed a content-based method BIBREF3 using a wide array of language features (Bag-of-Words, INLINEFORM0 - INLINEFORM1 scores, sentiment scores). Other approaches are more machine learning intensive, but require larger amounts of data. Recently, BIBREF4 created three datasets containing individual messages collected from Wikipedia discussion pages, annotated for toxicity, personal attacks and aggression, respectively.", "Because the reactions of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only on the targeted message itself. For instance, BIBREF8 use features derived from the sentences neighboring a given message to detect harassment on the Web. BIBREF9 take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work BIBREF10 , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation. This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection. Our conversational network extraction process is inspired from other works leveraging such graphs for other purposes: chat logs BIBREF11 or online forums BIBREF12 interaction modeling, user group detection BIBREF13 ."]}
{"question_id": "98aa86ee948096d6fe16c02c1e49920da00e32d4", "predicted_answer": "", "predicted_evidence": ["Because the reactions of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only on the targeted message itself. For instance, BIBREF8 use features derived from the sentences neighboring a given message to detect harassment on the Web. BIBREF9 take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work BIBREF10 , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation. This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection.", "The extraction process is restricted to a so-called context period, i.e. a sub-sequence of messages including the message of interest, itself called targeted message and represented in red in Figure FIGREF4 . Each participant posting at least one message during this period is modeled by a vertex in the produced conversational graph. A mobile window is slid over the whole period, one message at a time. At each step, the network is updated either by creating new links, or by updating the weights of existing ones. This sliding window has a fixed length expressed in number of messages, which is derived from ergonomic constraints relative to the online conversation platform studied in Section SECREF5 . It allows focusing on a smaller part of the context period. At a given time, the last message of the window (in blue in Figure FIGREF4 ) is called current message and its author current author. The weight update method assumes that the current message is aimed at the authors of the other messages present in the window, and therefore connects the current author to them (or strengthens their weights if the edge already exists).", "In this section, we summarize the content-based method from BIBREF14 (Section SECREF2 ) and the graph-based method from BIBREF10 (Section SECREF3 ). We then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information (Section SECREF6 ). Figure FIGREF1 shows the whole process, and is discussed through this section.", "A number of works have tackled this problem, or related ones, in the literature. Most of them focus only on the content of the targeted message to detect abuse or similar properties. For instance, BIBREF0 applies this principle to detect hostility, BIBREF1 for cyberbullying, and BIBREF2 for offensive language. These approaches rely on a mix of standard NLP features and manually crafted application-specific resources (e.g. linguistic rules). We also proposed a content-based method BIBREF3 using a wide array of language features (Bag-of-Words, INLINEFORM0 - INLINEFORM1 scores, sentiment scores). Other approaches are more machine learning intensive, but require larger amounts of data. Recently, BIBREF4 created three datasets containing individual messages collected from Wikipedia discussion pages, annotated for toxicity, personal attacks and aggression, respectively.", "The graph extraction is based on a number of concepts illustrated in Figure FIGREF4 , in which each rectangle represents a message. The extraction process is restricted to a so-called context period, i.e. a sub-sequence of messages including the message of interest, itself called targeted message and represented in red in Figure FIGREF4 . Each participant posting at least one message during this period is modeled by a vertex in the produced conversational graph. A mobile window is slid over the whole period, one message at a time. At each step, the network is updated either by creating new links, or by updating the weights of existing ones. This sliding window has a fixed length expressed in number of messages, which is derived from ergonomic constraints relative to the online conversation platform studied in Section SECREF5 . It allows focusing on a smaller part of the context period. At a given time, the last message of the window (in blue in Figure FIGREF4 ) is called current message and its author current author."]}
{"question_id": "c463136ba9a312a096034c872b5c74b9d58cef95", "predicted_answer": "", "predicted_evidence": ["We also use language features. We count the number of words, unique words and bad words in the message. For the latter, we use a predefined list of insults and symbols considered as abusive, and we also count them in the collapsed message. We compute two overall INLINEFORM0 \u2013 INLINEFORM1 scores corresponding to the sums of the standard INLINEFORM2 \u2013 INLINEFORM3 scores of each individual word in the message. One is processed relatively to the Abuse class, and the other to the Non-abuse class. We proceed similarly with the collapsed message. Finally, we lower-case the text and strip punctuation, in order to represent the message as a basic Bag-of-Words (BoW). We then train a Naive Bayes classifier to detect abuse using this sparse binary vector (as represented in the very bottom part of Figure FIGREF1 ). The output of this simple classifier is then used as an input feature for the SVM classifier.", "These approaches rely on a mix of standard NLP features and manually crafted application-specific resources (e.g. linguistic rules). We also proposed a content-based method BIBREF3 using a wide array of language features (Bag-of-Words, INLINEFORM0 - INLINEFORM1 scores, sentiment scores). Other approaches are more machine learning intensive, but require larger amounts of data. Recently, BIBREF4 created three datasets containing individual messages collected from Wikipedia discussion pages, annotated for toxicity, personal attacks and aggression, respectively. They have been leveraged in recent works to train Recursive Neural Network operating on word embeddings and character INLINEFORM2 -gram features BIBREF5 , BIBREF6 . However, the quality of these direct content-based approaches is very often related to the training data used to learn abuse detection models. In the case of online social networks, the great variety of users, including very different language registers, spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have models robust enough to be applied in all cases.", "We use the message length, average word length, and maximal word length, all expressed in number of characters. We count the number of unique characters in the message. We distinguish between six classes of characters (letters, digits, punctuation, spaces, and others) and compute two features for each one: number of occurrences, and proportion of characters in the message. We proceed similarly with capital letters. Abusive messages often contain a lot of copy/paste. To deal with such redundancy, we apply the Lempel\u2013Ziv\u2013Welch (LZW) compression algorithm BIBREF15 to the message and take the ratio of its raw to compress lengths, expressed in characters. Abusive messages also often contain extra-long words, which can be identified by collapsing the message: extra occurrences of letters repeated more than two times consecutively are removed. For instance, \u201clooooooool\u201d would be collapsed to \u201clool\u201d. We compute the difference between the raw and collapsed message lengths.", "The second strategy is Late Fusion, and we proceed in two steps. First, we apply separately both methods described in Sections SECREF2 and SECREF3 , in order to obtain two scores corresponding to the output probability of each message to be abusive given by the content- and graph-based methods, respectively. Second, we fetch these two scores to a third SVM, trained to determine if a message is abusive or not. This approach relies on the assumption that these scores contain all the information the final classifier needs, and not the noise present in the raw features.", "The Top Features obtained for each method are listed in Table TABREF12 . The last 4 columns precise which variants of the graph-based features are concerned. Indeed, as explained in Section SECREF3 , most of these topological measures can handle/ignore edge weights and/or edge directions, can be vertex- or graph-focused, and can be computed for each of the three types of networks (Before, After and Full)."]}
{"question_id": "81e101b2c803257492d67a00e8a1d9a07cbab136", "predicted_answer": "", "predicted_evidence": ["For each training set, we calculate the relative test quality change (percentage change in F1 or accuracy) of with-BERT over without-BERT. In Figure FIGREF24b, almost all percentage changes are within a narrow 2% band of no-change (i.e., 100%). This suggests that sometimes pre-trained language models have a limited impact on downstream tasks\u2013when weak supervision is used. Pretrained models do have higher quality at smaller training dataset sizes\u2013the Set task here shows an improvement at small scale, but this advantage vanishes at larger (weak) training set sizes in these workloads. This highlights a potentially interesting set of tradeoffs among weak supervision, pretraining, and the complexity of models.", "(3) Weak Supervision Applications have access to supervision of varying quality and combining this contradictory and incomplete supervision is a major challenge. Overton uses techniques from Snorkel BIBREF1 and Google's Snorkel DryBell BIBREF0, which have studied how to combine supervision in theory and in software. Here, we describe two novel observations from building production applications: (1) we describe the shift to applications which are constructed almost entirely with weakly supervised data due to cost, privacy, and cold-start issues, and (2) we observe that weak supervision may obviate the need for popular methods like transfer learning from massive pretrained models, e.g., BERT BIBREF8\u2013on some production workloads, which suggests that a deeper trade-off study may be illuminating.", "Overton is used in both near-real-time and backend production applications. However, for concreteness, our running example is a product that answers factoid queries, such as \u201chow tall is the president of the united states?\u201d In our experience, the engineers who maintain such machine learning products face several challenges on which they spend the bulk of their time.", "The area of Neural Architecture Search (NAS) BIBREF28 is booming: the goal of this area is to perform search (typically reinforcement learning but also increasingly random search BIBREF29). This has led to exciting architectures like EfficientNet BIBREF30. This is a tremendously exciting area with regular workshops at all major machine learning conferences. Overton is inspired by this area. On a technical level, the search used in Overton is a coarser-grained search than what is typically done in NAS. In particular, Overton searches over relatively limited large blocks, e.g., should we use an LSTM or CNN, not at a fine-grained level of connections. In preliminary experiments, NAS methods seemed to have diminishing returns and be quite expensive. More sophisticated search could only improve Overton, and we are excited to continue to apply advances in this area to Overton. Speed of developer iteration and the ability to ship production models seems was a higher priority than exploring fine details of architecture in Overton.", "This paper presented Overton, a system to help engineers manage the lifecycle of production machine learning systems. A key idea is to use a schema to separate the model from the supervision data, which allows developers to focus on supervision as their primary interaction method. A major direction of on-going work are the systems that build on Overton to aid in managing data augmentation, programmatic supervision, and collaboration."]}
{"question_id": "b942d94e4187e4fdc706cfdf92e3a869fc294911", "predicted_answer": "", "predicted_evidence": ["This paper presented Overton, a system to help engineers manage the lifecycle of production machine learning systems. A key idea is to use a schema to separate the model from the supervision data, which allows developers to focus on supervision as their primary interaction method. A major direction of on-going work are the systems that build on Overton to aid in managing data augmentation, programmatic supervision, and collaboration.", "The last few years have seen an unbelievable amount of change in the machine learning software landscape. TensorFlow, PyTorch, CoreML and MXNet have changed the way people write machine learning code to build models. Increasingly, there is a trend toward higher level interfaces. The pioneering work on higher level domain specific languages like Keras began in this direction. Popular libraries like Fast.ai, which created a set of libraries and training materials, have dramatically improved engineer productivity. These resources have made it easier to build models but equally important to train model developers. Enabled in part by this trend, Overton takes a different stance: model development is in some cases not the key to product success. Given a fixed budget of time to run a long-lived ML model, Overton is based on the idea that success or failure depends on engineers being able to iterate quickly and maintain the supervision\u2013not change the model.", "Acknowledgments This work was made possible by Pablo Mendes, Seb Dery, and many others. We thank many teams in Siri Search, Knowledge, and Platform and Turi for support and feedback. We thank Mike Cafarella, Arun Kumar, Monica Lam, Megan Leszczynski, Avner May, Alex Ratner, Paroma Varma, Ming-Chuan Wu, Sen Wu, and Steve Young for feedback.", "A slice also indicates to Overton that it should increase its representation capacity (slightly) to learn a \u201cper slice\u201d representation for a task. In this sense, a slice is akin to defining a \u201cmicro-task\u201d that performs the task just on the subset defined by the slice. Intuitively, this slice should be able to better predict as the data in a slice typically has less variability than the overall data. At inference time, Overton makes only one prediction per task, and so the first challenge is that Overton needs to combine these overlapping slice-specific predictions into a single prediction. A second challenge is that slices heuristically (and so imperfectly) define subsets of data. To improve the coverage of these slices, Overton learns a representation of when one is \u201cin the slice\u201d which allows a slice to generalize to new examples.", "Overton is used in both near-real-time and backend production applications. However, for concreteness, our running example is a product that answers factoid queries, such as \u201chow tall is the president of the united states?\u201d In our experience, the engineers who maintain such machine learning products face several challenges on which they spend the bulk of their time."]}
{"question_id": "8ffae517bc0efa453b7e316d41bd9f1b6679b158", "predicted_answer": "", "predicted_evidence": ["An Overton schema has two components: the tasks, which capture the tasks the model needs to accomplish, and payloads, which represent sources of data, such as tokens or entity embeddings. Every example in the data file conforms to this schema. Overton uses a schema both as a guide to compile a TensorFlow model and to describe its output for downstream use. Although Overton supports more types of tasks, we focus on classification tasks for simplicity. An example schema and its corresponding data file are shown in Figure FIGREF8. The schema file also provides schema information in a traditional database sense: it is used to define a memory-mapped row-store for example.", "Support for Multi-component Pipelines Even simple machine learning products comprise myriad individual tasks. Answering even a simple factoid query, such as \u201chow tall is the president of the united states?\u201d requires tackling many tasks including (1) find the named entities (`united states', and `president'), (2) find the database ids for named entities, (3) find the intent of the question, e.g., the height of the topic entity, (4) determine the topic entity, e.g., neither president nor united states, but the person Donald J. Trump, who is not explicitly mentioned, and (5) decide the appropriate UI to render it on a particular device. Any of these tasks can go wrong. Traditionally, systems are constructed as pipelines, and so determining which task is the culprit is challenging.", "For each training set, we calculate the relative test quality change (percentage change in F1 or accuracy) of with-BERT over without-BERT. In Figure FIGREF24b, almost all percentage changes are within a narrow 2% band of no-change (i.e., 100%). This suggests that sometimes pre-trained language models have a limited impact on downstream tasks\u2013when weak supervision is used. Pretrained models do have higher quality at smaller training dataset sizes\u2013the Set task here shows an improvement at small scale, but this advantage vanishes at larger (weak) training set sizes in these workloads. This highlights a potentially interesting set of tradeoffs among weak supervision, pretraining, and the complexity of models.", "The area of Neural Architecture Search (NAS) BIBREF28 is booming: the goal of this area is to perform search (typically reinforcement learning but also increasingly random search BIBREF29). This has led to exciting architectures like EfficientNet BIBREF30. This is a tremendously exciting area with regular workshops at all major machine learning conferences. Overton is inspired by this area. On a technical level, the search used in Overton is a coarser-grained search than what is typically done in NAS. In particular, Overton searches over relatively limited large blocks, e.g., should we use an LSTM or CNN, not at a fine-grained level of connections. In preliminary experiments, NAS methods seemed to have diminishing returns and be quite expensive. More sophisticated search could only improve Overton, and we are excited to continue to apply advances in this area to Overton. Speed of developer iteration and the ability to ship production models seems was a higher priority than exploring fine details of architecture in Overton.", "Overton's use of a relational schema to abstract statistical reasoning is inspired by Statistical Relational Learning (SRL), such as Markov Logic BIBREF31. DeepDive BIBREF27, which is based on Markov Logic, allows one to wrap deep learning as relational predicates, which could then be composed. This inspired Overton's design of compositional payloads. In the terminology of SRL BIBREF32, Overton takes a knowledge compilation approach (Overton does not have a distinct querying phase). Supporting more complex, application-level constraints seems ideally suited to an SRL approach, and is future work for Overton."]}
{"question_id": "0fd2854dd8d8191f00c8d12483b5a81a04de859f", "predicted_answer": "", "predicted_evidence": ["Updating Supervision When new features are created or quality bugs are identified, engineers provide additional supervision. Traditionally, supervision is provided by annotators (of varying skill levels), but increasingly programmatic supervision is the dominant form of supervision BIBREF0, BIBREF1, which includes labeling, data augmentation, and creating synthetic data. For both privacy and cost reasons, many applications are constructed using programmatic supervision as a primary source. An ideal system can accept supervision at multiple granularities and resolve conflicting supervision for those tasks.", "Support for Multi-component Pipelines Even simple machine learning products comprise myriad individual tasks. Answering even a simple factoid query, such as \u201chow tall is the president of the united states?\u201d requires tackling many tasks including (1) find the named entities (`united states', and `president'), (2) find the database ids for named entities, (3) find the intent of the question, e.g., the height of the topic entity, (4) determine the topic entity, e.g., neither president nor united states, but the person Donald J. Trump, who is not explicitly mentioned, and (5) decide the appropriate UI to render it on a particular device. Any of these tasks can go wrong. Traditionally, systems are constructed as pipelines, and so determining which task is the culprit is challenging.", "Weak supervision is the dominant form of supervision in all applications. Even annotator labels (when used) are filtered and altered by privacy and programmatic quality control steps. Note that validation is still done manually, but this requires orders of magnitude less data than training.", "The ideas above led naturally to what we now recognize as zero-code deep learning, a term we borrow from Ludwig. It is directly related to previous work on multitask learning as a key building block of software development BIBREF26 and inspired by Software 2.0 ideas articulated by Karpathy. The world of software engineering for machine learning is fascinating and nascent. In this spirit, Uber's Ludwig shares a great deal with Overton's design. Ludwig is very sophisticated and has supported complex tasks on vision and others. These methods were controversial two years ago, but seem to be gaining acceptance among production engineers. For us, these ideas began as an extension of joint inference and learning in DeepDive BIBREF27.", "This paper presented Overton, a system to help engineers manage the lifecycle of production machine learning systems. A key idea is to use a schema to separate the model from the supervision data, which allows developers to focus on supervision as their primary interaction method. A major direction of on-going work are the systems that build on Overton to aid in managing data augmentation, programmatic supervision, and collaboration."]}
{"question_id": "742d5e182b57bfa5f589fde645717ed0ac3f49c2", "predicted_answer": "", "predicted_evidence": ["Having produced a representation INLINEFORM0 for each mention INLINEFORM1 , a slot-specific attention mechanism produces INLINEFORM2 , representing the compatibility of mention INLINEFORM3 with slot INLINEFORM4 . Let INLINEFORM5 be the representation matrix composed of all INLINEFORM6 , and INLINEFORM7 is the index of INLINEFORM8 into INLINEFORM9 . We create a separate embedding, INLINEFORM10 , for each slot INLINEFORM11 , and utilize it to attend over all mentions in the cluster as follows: DISPLAYFORM0", "The mention-level scores reflect an interpretation of the value's encoding with respect to the slot. The softmax normalizes the scores over each slot, supplying the attention, and creating competition between mentions. This encourages the model to attend over mentions with the most characteristic contexts for each slot.", "If the most clearly expressed mentions correspond to correct values, max aggregation can be an effective strategy BIBREF10 . If the data set is noisy with numerous spurious mentions, a sum aggregation favoring values which are expressed both clearly and frequently may be the more appropriate choice.", "Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference BIBREF12 , BIBREF13 , we present a recurrent neural network (RNN) which implements inference under this constraint.", ""]}
{"question_id": "726c5c1b6951287f4bae22978f9a91d22d9bef61", "predicted_answer": "", "predicted_evidence": ["The exact words contained in the cluster also have an effect. For example, clusters with numerous mentions of killed, died, dead, will have a higher INLINEFORM0 Fatalities INLINEFORM1 , requiring the model to be more confident in its answer for that slot during training. Additionally, this provides a mechanism for driving down INLINEFORM2 when INLINEFORM3 is not strongly associated with INLINEFORM4 .", "Two natural choices for this aggregation are max and sum. Formally, under max aggregation the value-level scores for a value INLINEFORM0 and slot INLINEFORM1 are computed as: DISPLAYFORM0", "All subsequent LBP iterations compute variable messages as in Eq. EQREF24 , incorporating the out-going factor beliefs of the previous iteration.", "", "[leftmargin=*]"]}
{"question_id": "dfdd309e56b71589b25412ba85b0a5d79a467ceb", "predicted_answer": "", "predicted_evidence": ["In all experiments we train using adaptive online gradient updates (Adam, see kingma2014). Model architecture and parameter values were tuned on the development set, and are as follows (chosen values in bold):", "For each mention INLINEFORM0 we construct a representation INLINEFORM1 of the mention in its context. This representation functions as a general \u201creading\u201d or encoding of the mention, irrespective of the type of slots for which it will later be considered. This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query BIBREF0 . As in previous work, an embedding of a mention's context serves as its representation. We construct an embedding matrix INLINEFORM2 , using pre-trained word embeddings, where INLINEFORM3 is the dimensionality of the embeddings and INLINEFORM4 the number of words in the cluster. These are held fixed during training. All mentions are masked and receive the same one-hot vector in place of a pretrained embedding. From this matrix we embed the context using a two-layer convolutional neural network (CNN), with a detailed discussion of the architecture parameters provided in Section SECREF4 .", "The exact words contained in the cluster also have an effect. For example, clusters with numerous mentions of killed, died, dead, will have a higher INLINEFORM0 Fatalities INLINEFORM1 , requiring the model to be more confident in its answer for that slot during training. Additionally, this provides a mechanism for driving down INLINEFORM2 when INLINEFORM3 is not strongly associated with INLINEFORM4 .", "We assign aggregation weights heuristically with respect to a simple model of discourse. We assume every document begins on topic, and remains so until a sentence mentions a nontopical flight number. This and all successive sentences are considered nontopical, until a sentence reintroduces the topical flight. Mentions in topical sentences receive aggregation weights of INLINEFORM0 , and those in non-topical sentences receive weights of INLINEFORM1 , removing them from consideration completely.", "A model which focuses on a single high-scoring mention, at the expense of breadth, will make an incorrect prediction. In comparison, a model which learns to correctly accumulate evidence for each value across multiple mentions over the entire cluster can identify the correct information, circumventing this problem. Figure FIGREF1 (bottom) shows how this pooling of evidence can produce the correct cluster-level prediction."]}
{"question_id": "7ae95716977d39d96e871e552c35ca0753115229", "predicted_answer": "", "predicted_evidence": ["Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference BIBREF12 , BIBREF13 , we present a recurrent neural network (RNN) which implements inference under this constraint.", "There are two types of messages: messages from variables to factors and messages from factors to variables. The message that a variable INLINEFORM0 sends to a factor INLINEFORM1 (denoted INLINEFORM2 ) is defined recursively w.r.t. to incoming messages from its neighbors INLINEFORM3 as follows: DISPLAYFORM0", "We constrain model output by applying a factor graph model to the INLINEFORM0 scores it produces. The slot INLINEFORM1 taking the value INLINEFORM2 is represented in the factor graph by a Boolean variable INLINEFORM3 . Each INLINEFORM4 is connected to a local factor INLINEFORM5 whose initial potential is derived from INLINEFORM6 . A combinatorial logic factor, Exactly-1 BIBREF14 , is (1) created for each slot, connected across all values, and (2) created for each value, connected across all slots. This is illustrated in Figure FIGREF22 . Each Exactly-1 factor provides a hard constraint over neighboring Boolean variables requiring exactly one variable's value to be true, therefore diminishing the possibility of duplicate predictions during inference.", "and conveys the information \u201cMy other neighbors jointly suggest I have the posterior distribution INLINEFORM0 over my values.\u201d In our RNN formulation of message passing the initial outgoing message for a variable INLINEFORM1 to its neighboring Exactly-1 factors is: DISPLAYFORM0", ""]}
{"question_id": "ff3e93b9b5f08775ebd1a7408d7f0ed2f6942dde", "predicted_answer": "", "predicted_evidence": ["Back-translation enables the PBSMT models to be trained in a supervised way by providing pseudo-parallel data from the translation in the reverse direction, which indicates that the PBSMT models need to be trained in dual directions so that the two models trained in the opposite directions can promote each other's performance.", "We would like to thank our colleagues Jamin Shin, Andrea Madotto, and Peng Xu for insightful discussions. This work has been partially funded by ITF/319/16FP and MRP/055/18 of the Innovation Technology Commission, the Hong Kong SAR Government.", "If several words in the PBSMT translation match a context word, the word that is closest to the position of the context word in the PBSMT translation will be selected and put into the candidate list to replace the corresponding <UNK> in the translation from the word-level NMT model.", "Ensemble methods have been shown very effective in many natural language processing tasks BIBREF20, BIBREF21. We apply an ensemble method by taking the top five translations from word-level and subword-level NMT, and rescore all translations using our pre-trained Czech language model mentioned in \u00a7SECREF18. Then, we select the best translation with the lowest perplexity.", "where $v^*(x)$ denotes sentences in the target language translated from source language sentences $S$, $u^*(y)$ similarly denotes sentences in the source language translated from the target language sentences $T$ and $P_{t \\rightarrow s}$, and $P_{s \\rightarrow t}$ denote the translation direction from target to source and from source to target respectively."]}
{"question_id": "59a3d4cdd1c3797962bf8d72c226c847e06e1d44", "predicted_answer": "", "predicted_evidence": ["The BLEU (cased) score of the initialized phrase table and models after training at different iterations are shown in Table TABREF33. From comparing the results, we observe that back-translation can improve the quality of the phrase table significantly, but after five iterations, the phrase table has hardly improved. The PBSMT model at the sixth iteration is selected as the final PBSMT model.", "Step 2 and Step 3 are repeated until all the context words have been searched. After removing all the punctuation and the context words in the candidate list, the replacement word is the one that most frequently appears in the candidate list. If no candidate word is found, we just remove the <UNK> without adding a word.", "Back-translation enables the PBSMT models to be trained in a supervised way by providing pseudo-parallel data from the translation in the reverse direction, which indicates that the PBSMT models need to be trained in dual directions so that the two models trained in the opposite directions can promote each other's performance.", "The performances of our final model and other baseline models are illustrated in Table TABREF34. In the baseline unsupervised NMT models, subword-level NMT outperforms word-level NMT by around a 1.5 BLEU score. Although the unsupervised PBSMT model is worse than the subword-level NMT model, leveraging generated pseudo-parallel data from the PBSMT model to fine-tune the subword-level NMT model can still boost its performance. However, this pseudo-parallel data from the PBSMT model can not improve the word-level NMT model since the large percentage of OOV words limits its performance. After applying unknown words replacement to the word-level NMT model, the performance improves by a BLEU score of around 2. Using the Czech language model to re-score helps the model improve by around a 0.3 BLEU score each time. We also use this language model to create an ensemble of the best word-level and subword-level NMT model and achieve the best performance.", "Cross-lingual word embeddings can provide a good initialization for both the NMT and SMT models. In the unsupervised senario, BIBREF12 independently trained embeddings in different languages using monolingual corpora, and then learned a linear mapping to align them in a shared space based on a bilingual dictionary of a negligibly small size. BIBREF0 proposed a fully unsupervised learning method to build a bilingual dictionary without using any foregone word pairs, but by considering words from two languages that are near each other as pseudo word pairs. BIBREF24 showed that cross-lingual language model pre-training can learn a better cross-lingual embeddings to initialize an unsupervised machine translation model."]}
{"question_id": "49474a3047fa3f35e1bcd63991e6f15e012ac10b", "predicted_answer": "", "predicted_evidence": ["We use the recaser model provided in Moses and train the model with the two million latest sentences in the Czech monolingual dataset. After the training procedure, the recaser can restore words to the form in which the maximum probability occurs.", "We would like to thank our colleagues Jamin Shin, Andrea Madotto, and Peng Xu for insightful discussions. This work has been partially funded by ITF/319/16FP and MRP/055/18 of the Innovation Technology Commission, the Hong Kong SAR Government.", "The performances of our final model and other baseline models are illustrated in Table TABREF34. In the baseline unsupervised NMT models, subword-level NMT outperforms word-level NMT by around a 1.5 BLEU score. Although the unsupervised PBSMT model is worse than the subword-level NMT model, leveraging generated pseudo-parallel data from the PBSMT model to fine-tune the subword-level NMT model can still boost its performance. However, this pseudo-parallel data from the PBSMT model can not improve the word-level NMT model since the large percentage of OOV words limits its performance. After applying unknown words replacement to the word-level NMT model, the performance improves by a BLEU score of around 2. Using the Czech language model to re-score helps the model improve by around a 0.3 BLEU score each time. We also use this language model to create an ensemble of the best word-level and subword-level NMT model and achieve the best performance.", "We note that both German and Czech BIBREF14 are morphologically rich languages, which leads to a very large vocabulary size for both languages, but especially for Czech (more than one million unique words for German, but three million unique words for Czech). To overcome OOV issues, we leverage subword information, which can lead to better performance.", "In this paper, we propose to combine word-level and subword-level input representation in unsupervised NMT training on a morphologically rich language pair, German-Czech, without using any parallel data. Our results show the effectiveness of using language model rescoring to choose more fluent translation candidates. A series of pre-processing and post-processing approaches improve the quality of final translations, particularly to replace unknown words with possible relevant target words."]}
{"question_id": "63279ecb2ba4e51c1225e63b81cb021abc10d0d1", "predicted_answer": "", "predicted_evidence": ["We follow the unsupervised NMT in BIBREF10 by leveraging initialization, language modeling and back-translation. However, instead of using BPE, we use MUSE BIBREF0 to align word-level embeddings of German and Czech, which are trained by FastText BIBREF15 separately. We leverage the aligned word embeddings to initialize our unsupervised NMT model.", "For all the models mentioned above that work under a lower-case setting, a recaser implemented with Moses BIBREF22 is applied to convert the translations to the real cases.", "The BLEU (cased) score of the initialized phrase table and models after training at different iterations are shown in Table TABREF33. From comparing the results, we observe that back-translation can improve the quality of the phrase table significantly, but after five iterations, the phrase table has hardly improved. The PBSMT model at the sixth iteration is selected as the final PBSMT model.", "In this work, the systems we implement for the German-Czech language pair are built based on the previously proposed unsupervised MT systems, with some adaptations made to accommodate the morphologically rich characteristics of German and Czech BIBREF14. Both word-level and subword-level neural machine translation (NMT) models are applied in this task and further tuned by pseudo-parallel data generated from a phrase-based statistical machine translation (PBSMT) model, which is trained following the steps proposed in BIBREF10 without using any parallel data. We propose to train BPE embeddings for German and Czech separately and align those trained embeddings into a shared space with MUSE BIBREF0 to reduce the combinatorial explosion of word forms for both languages. To ensure the fluency and consistency of translations, an additional Czech language model is trained to select the translation candidates generated through beam search by rescoring them. Besides the above, a series of post-processing steps are applied to improve the quality of final translations.", "where $v^*(x)$ denotes sentences in the target language translated from source language sentences $S$, $u^*(y)$ similarly denotes sentences in the source language translated from the target language sentences $T$ and $P_{t \\rightarrow s}$, and $P_{s \\rightarrow t}$ denote the translation direction from target to source and from source to target respectively."]}
{"question_id": "f1a50f88898556ecdba8e9cac13ae54c11835945", "predicted_answer": "", "predicted_evidence": ["This analysis illustrates the richness of linguistic and semantic phenomena in QuaRTz.", "Numerical property values ($\\approx $11%) require numeric comparison to identify the qualitative relationship, e.g., that \u201c60 years\u201d is older than \u201c30 years\u201d.", "As the QuaRTz task involves using a general corpus $K$ of textual qualitative knowledge, a high-performing system would be close to a fully general system where $K$ was much larger (e.g., the Web or a filtered subset), encompassing many more qualitative relationships, and able to answer arbitrary questions of this kind. Scaling further would thus require more sophisticated retrieval over a larger corpus, and (sometimes) chaining across influences, when a direct connection was not described in the corpus. QuaRTz thus provides a dataset towards this end, allowing controlled experiments while still covering a substantial number of textual relations in an open setting. QuaRTz is available at http://data.allenai.org/quartz/.", "We are grateful to the AllenNLP and Beaker teams at AI2, and for the insightful discussions with other Aristo team members. Computations on beaker.org were supported in part by credits from Google Cloud.", "QuaRTz includes a rich set of annotations on all the knowledge sentences and questions, marking the properties being compared, and the linguistic and semantic comparatives employed (Figure FIGREF1). This provides a laboratory for exploring semantic parsing approaches, e.g., BIBREF13, BIBREF14, where the underlying qualitative comparisons are extracted and can be reasoned about."]}
{"question_id": "ef6304512652ba56bd13dbe282a5ce1d41a4f171", "predicted_answer": "", "predicted_evidence": ["\u201cincreased altitude\u201d $\\leftrightarrow $ \u201chigher\u201d", "\u201chigher temperatures\u201d $\\leftrightarrow $ \u201cA/C unit broken\u201d", "Examples of QuaRTz questions $Q_{i}$ are shown in Table TABREF3, along with a sentence $K_{i}$ expressing the relevant qualitative relationship. The QuaRTz task is to answer the questions given a small (400 sentence) corpus $K$ of general qualitative relationship sentences. Questions are crowdsourced, and the sentences $K_{i}$ were collected from a larger corpus, described shortly.", "Questions marked by workers as poor quality were reviewed by the authors and rejected/modified as appropriate. The dataset was then split into train/dev/test partitions such that questions from the same seed $K_i$ were all in the same partition. Statistics are in Table TABREF9.", "Despite rapid progress in general question-answering (QA), e.g., BIBREF1, and formal models for qualitative reasoning (QR), e.g., BIBREF2, BIBREF3, there has been little work on reasoning with textual qualitative knowledge, and no dataset available in this area. Although many datasets include a few qualitative questions, e.g., BIBREF4, BIBREF5, the only one directly probing QR is QuaRel BIBREF0. However, although QuaRel contains 2700 qualitative questions, its underlying qualitative knowledge was specified formally, using a small, fixed ontology of 19 properties. As a result, systems trained on QuaRel are limited to only questions about those properties. Likewise, although the QR community has performed some work on extracting qualitative models from text, e.g., BIBREF6, BIBREF7, and interpreting questions about identifying qualitative processes, e.g., BIBREF8, there is no dataset available for the NLP community to study textual qualitative reasoning."]}
{"question_id": "72dbdd11b655b25b2b254e39689a7d912f334b71", "predicted_answer": "", "predicted_evidence": ["Differing comparative expressions ($\\approx $68%) between $K_i$ and $Q_i$ occur in the majority of questions, e.g.,", "3. BERT (IR): This model performs the full task. First, a sentence $K_i$ is retrieved from $K$ using $Q_i$ as a search query. This is then supplied to BERT as [CLS] $K_i$ [SEP] question-stem [SEP] answer-option [SEP] for each option. The [CLS] output token is projected to a single logit and fed through a softmax layer across answer options, using cross entropy loss, the highest being selected. This model is fine-tuned using QuaRTz (only).", "Stories ($\\approx $15%): 15% of the questions are 3 or more sentences long, making comprehension more challenging.", "Understanding and applying textual qualitative knowledge is an important skill for question-answering, but has received limited attention, in part due the lack of a broad-coverage dataset to study the task. QuaRTz aims to fill this gap by providing the first open-domain dataset of qualitative relationship questions, along with the requisite qualitative knowledge and a rich set of annotations. Specifically, QuaRTz removes the requirement, present in all previous qualitative reasoning work, that a fixed set of qualitative relationships be formally pre-specified. Instead, QuaRTz tests the ability of a system to find and apply an arbitrary relationship on the fly to answer a question, including when simple reasoning (arguments, polarities) is required.", "2. A general science-trained QA system has not learned this style of reasoning. BERT-Sci only scores 54.6, just a little above random (50.0)."]}
{"question_id": "9b6339e24f58b576143d2adf599cfc4a31fd3b0c", "predicted_answer": "", "predicted_evidence": ["The order in which clinically relevant events are described is often important in constructing a comprehensible HPI; for example, chest pain followed by shortness of breath and lightheadedness has different implications than shortness of breath followed by lightheadedness and chest pain. However, capturing this level of detail required expanding the ontology which was found to be cognitively burdensome for labelers and was left out to be addressed in future work.", "One might expect that the task of extracting clinical concepts from audio faces challenges similar to the domain of unstructured clinical texts. In that domain, one of the earliest public-domain tasks is the i2b2 relations challenge, defined on a small corpus of written discharge summaries consisting of 394 reports for training, 477 for test, and 877 for evaluation BIBREF8. Given the small amount of training data, not surprisingly, a disproportionately large number of teams fielded rule-based systems. Conditional random field-based (CRF) systems BIBREF9 however did better even with the limited amount of training data BIBREF10. Other i2b2/n2c2 challenges focused on coreference resolution BIBREF11, temporal relation extraction BIBREF12, drug event extraction BIBREF13 on medical records, and extracting family history BIBREF14.", "A shared difficulty across all tasks was striking a balance between making the annotations clinically useful (i.e. developing a path to use the annotations to construct a complete HPI) while also reducing the cognitive burden for annotators. Through multiple experiments we uncovered significant challenges that could or could not be resolved with the addition, deletion, or substitution of tags in our ontologies.", "Labelers who scored highly with respect to the reference were chosen as reviewers.", "The annotators themselves are not always consistent in the span of text they label. E.g., \u201cpain medication\u201d vs. \u201cthe pain medication\u201d. To allow a certain degree of tolerance in the extracted span, the model performance was evaluated using a weighted F-score."]}
{"question_id": "55e3daecaf8030ed627e037992402dd0a7dd67ff", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 in the agent's network is a multilayer perceptron (MLP) with a single hidden layer and a INLINEFORM1 activation function over all possible system actions.", "Belief Tracking Belief tracker, or dialog state tracker BIBREF36 , BIBREF37 , continuously tracks the user's goal by accumulating evidence in the conversation. We represent the user's goal using a list of slot values. The belief tracker maintains and updates a probability distribution INLINEFORM0 over candidate values for each slot type INLINEFORM1 at each turn INLINEFORM2 : DISPLAYFORM0", "In this work, we propose a reinforcement learning framework for dialog policy optimization in end-to-end task-oriented dialog systems. The proposed method addresses the challenge of lacking a reliable user simulator for policy learning in task-oriented dialog systems. We present an iterative policy learning method that jointly optimizes the dialog agent and the user simulator with deep RL by simulating dialogs between the two agents. Both the dialog agent and the user simulator are designed with neural network models that can be trained end-to-end. Experiment results show that our proposed method leads to promising improvements on task success rate and task reward over the supervised training and single-agent RL training baseline models.", "Task-oriented dialog system is playing an increasingly important role in enabling human-computer interactions via natural spoken language. Different from chatbot type of conversational agents BIBREF0 , BIBREF1 , BIBREF2 , task-oriented dialog systems assist users to complete everyday tasks, which usually involves aggregating information from external resources and planning over multiple dialog turns. Conventional task-oriented dialog systems are designed with complex pipelines BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , and there are usually separately developed modules for natural language understanding (NLU) BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , dialog state tracking (DST) BIBREF11 , BIBREF12 , and dialog management (DM) BIBREF13 , BIBREF14 , BIBREF15 . Such pipeline approach inherently make it hard to scale a dialog system to new domains, as each of these modules has to be redesigned separately with domain expertise.", "RL policy optimization is performed on top of the supervised pre-trained networks. The system architecture is shown in Figure FIGREF10 . We defines the state, action, and reward in our RL training setting and present the training details."]}
{"question_id": "5522a9eeb06221722052e3c38f9b0d0dbe7c13e6", "predicted_answer": "", "predicted_evidence": ["Task-oriented dialog system is playing an increasingly important role in enabling human-computer interactions via natural spoken language. Different from chatbot type of conversational agents BIBREF0 , BIBREF1 , BIBREF2 , task-oriented dialog systems assist users to complete everyday tasks, which usually involves aggregating information from external resources and planning over multiple dialog turns. Conventional task-oriented dialog systems are designed with complex pipelines BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , and there are usually separately developed modules for natural language understanding (NLU) BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , dialog state tracking (DST) BIBREF11 , BIBREF12 , and dialog management (DM) BIBREF13 , BIBREF14 , BIBREF15 . Such pipeline approach inherently make it hard to scale a dialog system to new domains, as each of these modules has to be redesigned separately with domain expertise.", "In the supervised pre-training stage, we train the dialog agent and the user simulator separately using task-oriented dialog corpora. In the RL training stage, we simulate dialogs between the two agents. The user simulator starts the conversation based on a sampled user goal. The dialog agent attempts to estimate the user's goal and complete the task with the user simulator by conducting multi-turn conversation. At the end of each simulated dialog, a reward is generated based on the level of task completion. This reward is used to further optimize the dialog policies of the two agents with RL.", "This last expression above gives us an unbiased gradient estimator. We sample agent action and user action at each dialog turn and compute the policy gradient. Similarly, gradient on the user simulator side can be derived as: DISPLAYFORM0", "In this section, we first provide a high level description of our proposed framework. We then discuss each module component and the training methods in detail.", "Average Success Turn Size The average turn size of success dialogs tends to decrease along the episode of RL policy learning. This is in line with our expectation as both the dialog agent and the user simulator improve their policies for more efficient and coherent strategies with the RL training."]}
{"question_id": "30870a962cf88ac8c8e6b7b795936fd62214f507", "predicted_answer": "", "predicted_evidence": ["Utterance Generation We use a template-based NLG module to convert the user simulator's outputs (action and slot values) to the natural language surface form.", "In the supervised pre-training stage, we train the dialog agent and the user simulator separately using task-oriented dialog corpora. In the RL training stage, we simulate dialogs between the two agents. The user simulator starts the conversation based on a sampled user goal. The dialog agent attempts to estimate the user's goal and complete the task with the user simulator by conducting multi-turn conversation. At the end of each simulated dialog, a reward is generated based on the level of task completion. This reward is used to further optimize the dialog policies of the two agents with RL.", "Jointly optimizing policies for dialog agent and user simulator with RL has also been studied in literature. Chandramohan et al. BIBREF32 proposed a co-adaptation framework for dialog systems by jointly optimizing the policies for multiple agents. Georgila et al. BIBREF33 discussed applying multi-agent RL for policy learning in a resource allocation negotiation scenario. Barlier et al. BIBREF34 modeled non-cooperative task dialog as a stochastic game and learned jointly the strategies of both agents. Comparing to these previous work, our proposed framework focuses on task-oriented dialogs where the user and the agent positively collaborate to achieve the user's goal. More importantly, we work towards building end-to-end models for task-oriented dialogs that can handle noises and ambiguities in natural language understanding and belief tracking, which is not taken into account in previous work.", "Policy Gradient RL For policy optimization with RL, policy gradient method is preferred over Q-learning in our system as the policy network parameters can be initialized with the INLINEFORM0 parameters learnied during supervised pre-training stage. With REINFORCE BIBREF39 , the objective function can be written as INLINEFORM1 , with INLINEFORM2 being the discount factor. We optimize parameter sets INLINEFORM3 and INLINEFORM4 for the dialog agent and the user simulator to maximize INLINEFORM5 . For the dialog agent, with likelihood ratio gradient estimator, gradient of INLINEFORM6 can be derived as: DISPLAYFORM0", "User Goal We define a user's goal INLINEFORM0 using a list of informable and requestable slots BIBREF38 . Informable slots are the slots that users can provide a value for to describe their goal (e.g. slots for food type, area, etc.). Requestable slots are the slots that users want to request the value for, such as requesting the address of a restaurant. We treat informable slots as discrete type of inputs that can take multiple values, and treat requestable slots as inputs that take binary values (i.e. a slot is either requested or not). In this work, once the a goal is sampled at the beginning of the conversation, we fix the user goal and do not change it during the conversation."]}
{"question_id": "7ece07a84635269bb19796497847e4517d1e3e61", "predicted_answer": "", "predicted_evidence": ["Success Rate As shown in Table 2, the SL model achieves the lowest task success rate. Model trained with SL on dialog corpus has limited capabilities in capturing the change in state, and thus may not be able to generalize well to unseen dialog situations during simulation. RL efficiently improves the dialog task success rate, as it enables the dialog agent to explore strategies that are not in the training corpus. The agent-update-only models using REINFORCE and A2C achieve similar results, outperforming the baseline model by 14.9% and 15.3% respectively. The jointly optimized models improved the performance further over the agent-update-only models. Model using A2C for joint policy optimization achieves the best task success rate.", "Response Generation We use a template-based NLG module to convert the agent outputs (system action, slot values, and KB entity values) to natural language format.", "Table 2 shows the evaluation results. The baseline model uses the SL trained agents. REINFORCE-agent and A2C-agent apply RL training on the dialog agent only, without updating the user simulator. REINFORCE-joint and A2C-joint apply RL on both the dialog agent and user simulator over the SL pre-trained models. Figure 4, 5, and 6 show the learning curves of these five models during RL training on dialog success rate, average reward, and average success dialog length.", "Utterance Encoding For natural language format inputs at turn INLINEFORM0 , we use bi-directional LSTM to encode the utterance to a continuous vector INLINEFORM1 . With INLINEFORM2 representing the utterance at the INLINEFORM3 th turn with INLINEFORM4 words, the utterance vector INLINEFORM5 is produced by concatenating the last forward and backward LSTM states: INLINEFORM6 .", "Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance."]}
{"question_id": "f94cea545f745994800c1fb4654d64d1384f2c26", "predicted_answer": "", "predicted_evidence": ["The Food Model : It recognizes more than 1,000 food items in images down to the ingredient level. This model is used to identify the ingredients in a food image.", "In this paper, we present an effortless method to build a personal cuisine preference model. From images of food taken by each user, the data pipeline takes over, resulting in a visual representation of the user's preference. With more focus on preprocessing and natural text processing, it becomes important to realize the difficulty presented by the problem. We present a simple process to extract maximum useful information from the image. We observe that there is significant overlap between the ingredients from different cuisines and the identified unique ingredients might not always be picked up from the image. Although, this similarity is what helps when classifying using the KNN model. For the single user data used, we see that the 338 images are classified as food images. It is observed that Italian and Mexican are the most preferred cuisines. It is also seen that as K value increases, the number of food images classified into Italian increases significantly.", "Future Directions : The cuisine preferences determined for a user can be combined with the weather and physical activity of the user to build a more specific suggestive model. For example, if the meta data of the image were to be extracted and combined with the weather conditions for that date and time then we would be able to predict the type of food the user prefers during a particular weather. This would lead to a sophisticated recommendation system.", "H Holste et al's work BIBREF5 predicts the cuisine of a recipe given the list of ingredients. They eliminate distribution of ingredients per recipe as a weak feature. They focus on showing the difference in performance of models with and without tf-idf scoring. Their custom tf-idf scoring model performs well on the Yummly Dataset but is considerably naive.", "The sophistication of smart-phone cameras allows users to capture high quality images on their hand held device. Paired with the increasing popularity of social media platforms such as Facebook and Instagram, it makes sharing of photographs much easier than with the use of a standalone camera. Thus, each individual knowingly or unknowingly creates a food log."]}
{"question_id": "f3b851c9063192c86a3cc33b2328c02efa41b668", "predicted_answer": "", "predicted_evidence": ["Limitations : The quality of the image and presentation of food can drastically affect the system. Items which look similar in shape and colour can throw the system off track. However, with a large database this should not matter much.", "Future Directions : The cuisine preferences determined for a user can be combined with the weather and physical activity of the user to build a more specific suggestive model. For example, if the meta data of the image were to be extracted and combined with the weather conditions for that date and time then we would be able to predict the type of food the user prefers during a particular weather. This would lead to a sophisticated recommendation system.", "R M Kumar et al BIBREF6 use Tree Boosting algorithms(Extreme Boost and Random Forest) to predict cuisine based on ingredients. It is seen from their work that Extreme Boost performs better than Random Forest.", "A more sophisticated approach to classify based on the ingredients was adopted by using the K Nearest Neighbors Model. The Yummly dataset from Kaggle is used to train the model. The ingredients extracted from the images are used as a test set. The model was run successfully for k-values ranging from 1-25. The radar charts for some of the k values are shown in Fig 7, 8 and 9.", "A cuisine can often be identified by some distinctive ingredientsBIBREF10. Therefore, we performed a frequency test to find the most occurring ingredients in each cuisine. Ingredients such as salt and water tend to show up at the top of these lists quite often but they are not distinctive ingredients. Hence, identification of unique ingredients is an issue that is overcome by individual inspection. For example:"]}
{"question_id": "54b25223ab32bf8d9205eaa8a570e99c683f0077", "predicted_answer": "", "predicted_evidence": ["We have carried out an ample range of experiments to probe the performance of the proposed regularization approaches. This section describes the datasets, the models and the hyper-parameters used, and presents and discusses all results.", "Given the context vector, $\\textbf {c}_j$, the decoder output at the previous step, $\\textbf {s}_{j-1}$, and the word embedding of the previous word in the target sentence, $\\textbf {y}^{e}_{j}$ (Eq. DISPLAY_FORM18), the decoder generates vector $\\textbf {s}_j$ (Eq. DISPLAY_FORM19). This vector is later transformed into a larger vector of the same size as the target vocabulary via learned parameters $\\textbf {W}$, $\\textbf {b}$ and a softmax layer (Eq. DISPLAY_FORM20). The resulting vector, $\\textbf {p}_j$, is the inferred probability distribution over the target vocabulary at decoding step $j$. Fig. FIGREF12 depicts the full architecture of the baseline model.", "Extensive experimentation over four language pairs of different dataset sizes (from small to large) with both word and sentence regularization. We show that using both ReWE and ReSE can outperform strong state-of-the-art baselines based on long short-term memory networks (LSTMs) and transformers.", "Similarly to ReWE, a loss function computes the cosine distance between the predicted sentence vector, $\\textbf {r}$, and the sentence vector inferred with the off-the-shelf sentence embedder, $\\textbf {y}^r$ (Eq. DISPLAY_FORM31). This loss is added to the previous objective as an extra term with an additional, tunable hyper-parameter, $\\beta $:", "Both these limitations can be mitigated with sufficient training data. In theory, MLE could achieve optimal performance with infinite training data, but in practice this is impossible as the available resources are always limited. In particular, when the training data are scarce such as in low-resource language pairs or specific translation domains, NMT models display a modest performance, and other traditional approaches (e.g., PBSMT)BIBREF7 often obtain better accuracies. As such, generalization of NMT systems still calls for significant improvement."]}
{"question_id": "e5be900e70ea86c019efb06438ba200e11773a7c", "predicted_answer": "", "predicted_evidence": ["Yet, it would be very interesting to understand how do they influence the training to achieve improved models. For that purpose, we have conducted an exploration of the values of the hidden vectors on the decoder end ($\\textbf {s}_j$, Eq. DISPLAY_FORM19). These values are the \u201cfeature space\u201d used by the final classification block (a linear transformation and a softmax) to generate the class probabilities and can provide insights on the model. For this reason, we have considered the cs-en test set and stored all the $\\textbf {s}_j$ vectors with their respective word predictions. Then, we have used t-SNE BIBREF47 to reduce the dimensionality of the $\\textbf {s}_j$ vectors to a visualizable 2d. Finally, we have chosen a particular word (architecture) as the center of the visualization, and plotted all the vectors within a chosen neighborhood of this center word (Fig. FIGREF58).", "Such models learn to translate by only using monolingual corpora, and even though their accuracy is still well below that of their supervised counterparts, they have started to reach interesting levels. The architecture of unsupervised NMT systems differs from that of supervised systems in that it combines translation in both directions (source-to-target and target-to-source). Typically, a single encoder is used to encode sentences from both languages, and a separate decoder generates the translations in each language. The training of such systems follows three stages: 1) building a bilingual dictionary and word embedding space, 2) training two monolingual language models as denoising autoencoders BIBREF39, and 3) converting the unsupervised problem into a weakly-supervised one by use of back-translations BIBREF40. For more details on unsupervised NMT systems, we refer the reader to the original papers BIBREF36, BIBREF37, BIBREF38.", "Extensive experimentation over four language pairs of different dataset sizes (from small to large) with both word and sentence regularization. We show that using both ReWE and ReSE can outperform strong state-of-the-art baselines based on long short-term memory networks (LSTMs) and transformers.", "In this section, we describe the NMT model that has been used as the basis for the proposed regularizer. It is a neural encoder-decoder architecture with attention BIBREF1 that can be regarded as a strong baseline as it incorporates both LSTMs and transformers as modules. Let us assume that $\\textbf {x}:\\lbrace x_1 \\dots x_n\\rbrace $ is the source sentence with $n$ tokens and $\\textbf {y}:\\lbrace y_1 \\dots y_m\\rbrace $ is the target translated sentence with $m$ tokens. First, the words in the source sentence are encoded into their word embeddings by an embedding layer:", "The proposal of a new regularization technique for NMT based on sentence embeddings (ReSE)."]}
{"question_id": "b36a8a73b3457a94203eed43f063cb684a8366b7", "predicted_answer": "", "predicted_evidence": ["where $\\textbf {W}_1$, $\\textbf {W}_2$, $\\textbf {b}_1$ and $\\textbf {b}_2$ are the learnable parameters of a two-layer feed-forward network with a Rectified Linear Unit (ReLU) as activation function between the layers. Vector $\\textbf {e}_j$ aims to reproduce the word embedding of the target word, and thus the distributional properties (or co-occurrences) of its contexts. During training, the model is guided to regress the predicted vector, $\\textbf {e}_j$, towards the word embedding of the ground-truth word, $\\textbf {y}^{e}_j$. This is achieved by using a loss function that computes the distance between $\\textbf {e}_j$ and $\\textbf {y}^{e}_j$ (Eq. DISPLAY_FORM24).", "For its hyper-parameters, we have used the default values set by the developers of OpenNMT. Both the encoder and the decoder are formed by a 6-layer network. The sizes of the word embeddings, the hidden vectors and the attention network have all been set to either 300d or 512d, depending on the best results over the validation set. The head count has been set correspondingly to either 6 or 8, and the dropout rate to $0.2$ as for the LSTM. The model was also optimized using Adam, but with a much higher learning rate of 1 (OpenAI default). For this model, we have not used simulated annealing since some preliminary experiments showed that it did penalize performance. The batch size used was $4,096$ and $1,024$ words, again selected based on the accuracy over the validation set. Training was stopped upon convergence in perplexity over the validation set, which was evaluated at every epoch.", "The quantitative experiments have proven that ReWE and ReSE can act as effective regularizers for low- and medium-resource NMT. Yet, it would be very interesting to understand how do they influence the training to achieve improved models. For that purpose, we have conducted an exploration of the values of the hidden vectors on the decoder end ($\\textbf {s}_j$, Eq. DISPLAY_FORM19). These values are the \u201cfeature space\u201d used by the final classification block (a linear transformation and a softmax) to generate the class probabilities and can provide insights on the model. For this reason, we have considered the cs-en test set and stored all the $\\textbf {s}_j$ vectors with their respective word predictions. Then, we have used t-SNE BIBREF47 to reduce the dimensionality of the $\\textbf {s}_j$ vectors to a visualizable 2d.", "In addition, the word embeddings for both models were initialized with pre-trained fastText embeddings BIBREF26. For the 300d word embeddings, we have used the word embeddings available on the official fastText website. For the 512d embeddings and the subword units, we have trained our own pre-trained vectors using the fastText embedder with a large monolingual corpora from Wikipedia and the training data. Both models have used the same sentence embeddings which have been computed with the Universal Sentence Encoder (USE). However, the USE is only available for English, so we have only been able to use ReSE with the datasets where English is the target language (i.e., de-en, cs-en and eu-en). When using BPE, the subwords of every sentence have been merged back into words before passing them to the USE.", "For the 300d word embeddings, we have used the word embeddings available on the official fastText website. For the 512d embeddings and the subword units, we have trained our own pre-trained vectors using the fastText embedder with a large monolingual corpora from Wikipedia and the training data. Both models have used the same sentence embeddings which have been computed with the Universal Sentence Encoder (USE). However, the USE is only available for English, so we have only been able to use ReSE with the datasets where English is the target language (i.e., de-en, cs-en and eu-en). When using BPE, the subwords of every sentence have been merged back into words before passing them to the USE. The BLEU score for the BPE models has also been computed after post-processing the subwords back into words. Finally, hyper-parameters $\\lambda $ and $\\beta $ have been tuned only once for all datasets by using the en-fr validation set."]}
{"question_id": "3d73cb92d866448ec72a571331967da5d34dfbb1", "predicted_answer": "", "predicted_evidence": ["Eventually this process resulted in 2,774 texts with on average 4.5 annotations each. The distribution for the different factors is shown in Table FIGREF24, where multiple annotations of the same factor for one text were compounded into a single average value.", "Avpixlat and Nordfront have a larger proportion of annotated texts with factors below zero, while Flashback and especially Familjeliv have a larger proportion in the positive interval. The annotators had no information about the source of the data while they were annotating.", "Table TABREF22 shows how many texts were annotated for each factor, and Figure FIGREF25 shows how the different sources span over the factor values.", "In BIBREF7 the accuracy of works on Big Five personality inference as a function of the size of the input text was studied. The authors showed that using Word Embedding with Gaussian Processes provided the best results when building a classifier for predicting the personality from tweets. The data consisted of self-reported personality ratings as well as tweets from a set of 1,323 participants.", "Deep convolutional neural networks were used in BIBREF8 as classifiers on the Pennebaker & King dataset of 2,469 Big Five annotated stream-of-consciousness essays BIBREF9. The authors filtered the essays, discarding all sentences that did not contain any words from a list of emotionally charged words. One classifier was then trained for each trait, with each trait classified only as either yes (high) or no (low). The trait classifiers achieved their respective best accuracies using different configurations. Averaging these best results yielded an overall best accuracy of 58.83 percent."]}
{"question_id": "708f5f83a3c356b23b27a9175f5c35ac00cdf5db", "predicted_answer": "", "predicted_evidence": ["The texts were annotated by 18 psychology students, each of whom had studied at least 15 credits of personality psychology. The annotation was carried out using a web-based tool. A student working with this tool would be shown a text randomly picked from one of the sources, as well as instructions to annotate one of the Big Five traits by selecting a number from the discrete integer interval -3 to 3. Initially the students were allowed to choose which of the five traits to annotate, but at times they would be instructed to annotate a specific trait, to ensure a more even distribution of annotations. The tool kept the samples at a sufficiently meaningful yet comfortable size by picking only texts with at least two sentences, and truncating them if they exceeded five sentences or 160 words.", "Eventually this process resulted in 2,774 texts with on average 4.5 annotations each. The distribution for the different factors is shown in Table FIGREF24, where multiple annotations of the same factor for one text were compounded into a single average value.", "However, in the aftermath of Cambridge Analytica scandal, where the privacy of millions of Facebook users was violated, this line of research has been met with skepticism and suspicion. More recent research focuses on text from a variety of sources, including twitter data (e.g. BIBREF7, BIBREF8). Recent development in text analysis, machine learning, and natural language models, have move the field into an era of optimism, like never before. Importantly, the basic idea in this research is that personality is reflected in the way people write and that written communication includes information about the author\u2019s personality characteristics BIBREF9. Nevertheless, while a number of attempts has been made to extract personality from text (see below), the research is standing remarkably far from reality. There are, to our knowledge, very few attempts to test machine learning models \u201cin the wild\u201d. The present paper aims to deal with this concern.", "Flashback: an online forum with the tagline \u201cfreedom of speech - for real\u201d, and in 2018 the fourth most visited social media in SwedenBIBREF18. The discussions on Flashback cover virtually any topic, from computer and relationship problems to sex, drugs and ongoing crimes.", "Finally, we aimed to measure model performance in the wild, on data from two domains that differ from the training data. The results of our experiments showed that we were able to create models with reasonable performance (compared to a dummy classifier). These models exhibit a mean absolute error and accuracy in line with state-or-the-art models presented in previous research, with the caveat that comparisons over different datasets are fraught with difficulties. We also found that using a smaller amount of high-quality training data with multi-annotator assessments resulted in models that outperformed models based on a large amount of solo-annotated data. Finally, testing our best model ($\\textrm {\\textit {LM}}(D_\\textrm {\\textit {HR}})$) in the wild and found that the model could not, reliably, extract people\u2019s personality from their text. These findings reveal the importance of the quality of the data, but most importantly, the necessity of examining models in the wild."]}
{"question_id": "9240ee584d4354349601aeca333f1bc92de2165e", "predicted_answer": "", "predicted_evidence": ["Nordfront: the Swedish news site of the Nordic Resistance Movement (NMR - Nordiska motst\u00e5ndsr\u00f6relsen). NMR is a nordic national socialist party. The site features editorial articles, each with a section for reader comments.", "Since the introduction of the personality concept, psychologists have worked to formulate theories and create models describing human personality and reliable measure to accordingly. The filed has been successful to bring forth a number of robust models with corresponding measures. One of the most widely accepted and used is the Five Factor Model BIBREF0. The model describes human personality by five traits/factors, popularly referred to as the Big Five or OCEAN: Openness to experience, Conscientiousness, Extraversion, Agreeableness, and emotional stability (henceforth Stability). There is now an extensive body of research showing that these factors matter in a large number of domains of people\u2019s life. Specifically, the Big Five factors have been found to predict life outcomes such as health, longevity, work performance, interpersonal relations, migration and social attitudes, just to mention some domains (e.g. BIBREF1, BIBREF2, BIBREF3, BIBREF4).To date, the most common assessment of personality is by self-report questionnaires BIBREF5.", "Eventually this process resulted in 2,774 texts with on average 4.5 annotations each. The distribution for the different factors is shown in Table FIGREF24, where multiple annotations of the same factor for one text were compounded into a single average value.", "In BIBREF7 the accuracy of works on Big Five personality inference as a function of the size of the input text was studied. The authors showed that using Word Embedding with Gaussian Processes provided the best results when building a classifier for predicting the personality from tweets. The data consisted of self-reported personality ratings as well as tweets from a set of 1,323 participants.", "For $D_\\textrm {\\textit {HR}}$, the smaller set with higher annotation reliability, we therefore modified the process. Texts were now randomly selected from the subset of $D_\\textrm {\\textit {LR}}$ containing texts which had been annotated with -3 or 3. We reasoned that these annotations at the ends of the spectrum were indicative of texts where the authors had expressed their personalities more clearly. Thus this subset would be easier to annotate, and each text was potentially more suitable for the annotation of multiple factors."]}
{"question_id": "9133a85730c4090fe8b8d08eb3d9146efe7d7037", "predicted_answer": "", "predicted_evidence": ["We compare six estimation procedures in terms of different types of errors they incur. The error is defined as the difference to the gold standard. First, the magnitude and sign of the errors show whether a method tends to underestimate or overestimate the performance, and by how much (subsection sec:median-errors). Second, relative errors give fractions of small, moderate, and large errors that each procedure incurs (subsection sec:rel-errors). Third, we rank the estimation procedures in terms of increasing absolute errors, and estimate the significance of the overall ranking by the Friedman-Nemenyi test (subsection sec:friedman). Finally, selected pairs of estimation procedures are compared by the Wilcoxon signed-rank test (subsection sec:wilcoxon).", "Besides sequential estimation methods, some variants of INLINEFORM0 -fold cross-validation were proposed in the literature that are specially designed to cope with dependency in the data and enable the application of cross-validation to time-ordered data. For example, blocked cross-validation (the name is adopted from Bergmeir BIBREF11 ) was proposed by Snijders BIBREF16 . The method derives from a standard INLINEFORM1 -fold cross-validation, but there is no initial random shuffling of observations. This renders INLINEFORM2 blocks of contiguous observations.", "Our contribution to the state-of-the-art is a large scale empirical comparison of several estimation procedures on Twitter sentiment data. We focus on the differences between the cross-validation and sequential validation methods, to see how important is the violation of data independence in the case of Twitter posts. We consider longer-term time-dependence between the training and test sets, and completely ignore finer-scale dependence at the level of individual tweets (e.g., retweets and replies). To the best of our knowledge, there is no settled approach yet regarding proper validation of models for Twitter time-ordered data. This work provides some results which contribute to bridging that gap.", "xval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.", "The complexity of Twitter data raises some challenges on how to perform such estimations, as, to the best of our knowledge, there is currently no settled approach to this. Sentiment classes are typically ordered and unbalanced, and the data itself is time-ordered. Taking these properties into account is important for the selection of appropriate estimation procedures."]}
{"question_id": "42279c3a202a93cfb4aef49212ccaf401a3f8761", "predicted_answer": "", "predicted_evidence": ["The median errors of the six estimation procedures are in Tables TABREF22 and TABREF23 , measured by INLINEFORM0 and INLINEFORM1 , respectively.", "Igor Mozeti\u010d and Jasmina Smailovi\u0107 acknowledge financial support from the H2020 FET project DOLFINS (grant no. 640772), and the Slovenian Research Agency (research core funding no. P2-0103).", "DISPLAYFORM0", "The paper is structured as follows. sec:relatedWork provides an overview of the state-of-the-art in estimation methods. In section sec:methods we describe the experimental setting for an empirical comparison of estimation procedures for sentiment classification of time-ordered Twitter data. We describe the Twitter sentiment datasets, a machine learning algorithm we employ, performance measures, and how the gold standard and estimation results are produced. In section sec:results we present and discuss the results of comparisons of the estimation procedures along several dimensions. sec-conclusions provide the limitations of our work and give directions for the future.", "All the performance results, for gold standard and the six estimation procedures, are provided in a form which allows for easy reproduction of the presented results. The R code and data files needed to reproduce all the figures and tables in the paper are available at http://ltorgo.github.io/TwitterDS/."]}
{"question_id": "9ca85242ebeeafa88a0246986aa760014f6094f2", "predicted_answer": "", "predicted_evidence": ["All Twitter data were collected through the public Twitter API and are subject to the Twitter terms and conditions. The Twitter language datasets are available in a public language resource repository clarin.si at http://hdl.handle.net/11356/1054, and are described in BIBREF22 . There are 15 language files, where the Serbian/Croatian/Bosnian dataset is provided as three separate files for the constituent languages. For each language and each labeled tweet, there is the tweet ID (as provided by Twitter), the sentiment label (negative, neutral, or positive), and the annotator ID (anonymized). Note that Twitter terms do not allow to openly publish the original tweets, they have to be fetched through the Twitter API. Precise details how to fetch the tweets, given tweet IDs, are provided in Twitter API documentation https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-statuses-lookup.", "A coincidence matrix tabulates all pairable values of INLINEFORM0 from two models. In our case, we have a 3-by-3 coincidence matrix, and compare a model to the gold standard. The coincidence matrix is then the sum of the confusion matrix and its transpose. Each labeled tweet is entered twice, once as a INLINEFORM1 pair, and once as a INLINEFORM2 pair. INLINEFORM3 is the number of tweets labeled by the values INLINEFORM4 and INLINEFORM5 by different models, INLINEFORM6 and INLINEFORM7 are the totals for each value, and INLINEFORM8 is the grand total.", "Social media are becoming an increasingly important source of information about the public mood regarding issues such as elections, Brexit, stock market, etc. In this paper we focus on sentiment classification of Twitter data. Construction of sentiment classifiers is a standard text mining task, but here we address the question of how to properly evaluate them as there is no settled way to do so. Sentiment classes are ordered and unbalanced, and Twitter produces a stream of time-ordered data. The problem we address concerns the procedures used to obtain reliable estimates of performance measures, and whether the temporal ordering of the training and test data matters. We collected a large set of 1.5 million tweets in 13 European languages. We created 138 sentiment models and out-of-sample datasets, which are used as a gold standard for evaluations. The corresponding 138 in-sample datasets are used to empirically compare six different estimation procedures: three variants of cross-validation, and three variants of sequential validation (where test set always follows the training set).", "There are many supervised machine learning algorithms suitable for training sentiment classification models from labeled tweets. In this study we use a variant of Support Vector Machine (SVM) BIBREF23 . The basic SVM is a two-class, binary classifier. In the training phase, SVM constructs a hyperplane in a high-dimensional vector space that separates one class from the other. In the classification phase, the side of the hyperplane determines the class. A two-class SVM can be extended into a multi-class classifier which takes the ordering of sentiment values into account, and implements ordinal classification BIBREF24 . Such an extension consists of two SVM classifiers: one classifier is trained to separate the negative examples from the neutral-or-positives; the other separates the negative-or-neutrals from the positives. The result is a classifier with two hyperplanes, which partitions the vector space into three subspaces: negative, neutral, and positive. During classification, the distances from both hyperplanes determine the predicted class.", "In this section we briefly review typical estimation methods used in sentiment classification of Twitter data. In general, for time-ordered data, the estimation methods used are variants of cross-validation, or are derived from the methods used to analyze time series data. We examine the state-of-the-art of these estimation methods, pointing out their advantages and drawbacks."]}
{"question_id": "8641156c4d67e143ebbabbd79860349242a11451", "predicted_answer": "", "predicted_evidence": ["All Twitter data were collected through the public Twitter API and are subject to the Twitter terms and conditions. The Twitter language datasets are available in a public language resource repository clarin.si at http://hdl.handle.net/11356/1054, and are described in BIBREF22 . There are 15 language files, where the Serbian/Croatian/Bosnian dataset is provided as three separate files for the constituent languages. For each language and each labeled tweet, there is the tweet ID (as provided by Twitter), the sentiment label (negative, neutral, or positive), and the annotator ID (anonymized). Note that Twitter terms do not allow to openly publish the original tweets, they have to be fetched through the Twitter API. Precise details how to fetch the tweets, given tweet IDs, are provided in Twitter API documentation https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-statuses-lookup.", "The TwoPlaneSVMbin classifier and several other machine learning algorithms are implemented in an open source LATINO library BIBREF36 . LATINO is a light-weight set of software components for building text mining applications, openly available at https://github.com/latinolib.", "Figure FIGREF28 shows the proportion of the three types of errors, measured by INLINEFORM0 , for individual language datasets. Again, we observe a higher proportion of large errors for languages with poor annotations (alb, spa), annotations of different quality (scb), and different topics (por).", "The main result of the study is that standard, random cross-validation should not be used when dealing with time-ordered data. Instead, one should use blocked cross-validation, a conclusion already corroborated by Bergmeir et al. BIBREF19 , BIBREF11 . Another result is that we find no significant differences between the blocked cross-validation and the best sequential validation. However, we do find that cross-validations typically overestimate the performance, while sequential validations underestimate it.", "The differences between the estimation procedures are easier to detect when we aggregate the errors over all language datasets. The results are in Figures FIGREF25 and FIGREF26 , for INLINEFORM0 and INLINEFORM1 , respectively. In both cases we observe that the cross-validation procedures (xval) consistently overestimate the performance, while the sequential validations (seq) underestimate it. The largest overestimation errors are incurred by the random cross-validation, and the largest underestimations by the sequential validation with the training:test set ratio 2:1. We also observe high variability of errors, with many outliers. The conclusions are consistent for both measures, INLINEFORM2 and INLINEFORM3 ."]}
{"question_id": "2a120f358f50c377b5b63fb32633223fa4ee2149", "predicted_answer": "", "predicted_evidence": ["All the performance results, for gold standard and the six estimation procedures, are provided in a form which allows for easy reproduction of the presented results. The R code and data files needed to reproduce all the figures and tables in the paper are available at http://ltorgo.github.io/TwitterDS/.", "There are some biased decisions in our creation of the gold standard though, which limit the generality of the results reported, and should be addressed in the future work. An out-set always consists of 10,000 tweets, and immediately follows the in-sets. We do not consider how the performance drops over longer out-sets, nor how frequently should a model be updated. More importantly, we intentionally ignore the issue of dependent observations, between the in- and out-sets, and between the training and test sets. In the case of tweets, short-term dependencies are demonstrated in the form of retweets and replies. Medium- and long-term dependencies are shaped by periodic events, influential users and communities, or individual user's habits. When this is ignored, the model performance is likely overestimated. Since we do this consistently, our comparative results still hold.", "We thank Miha Gr\u010dar and Sa\u0161o Rutar for valuable discussions and implementation of the LATINO library.", "First, we apply 10-fold cross-validation where the training:test set ratio is always 9:1. Cross-validation is stratified when the fold partitioning is not completely random, but each fold has roughly the same class distribution. We also compare standard random selection of examples to the blocked form of cross-validation BIBREF16 , BIBREF11 , where each fold is a block of consecutive tweets. We use the following abbreviations for cross-validations:", "On the other hand, there are also approaches that use methods typical for time series data. For example, Bifet and Frank BIBREF7 use the prequential (predictive sequential) method to evaluate a sentiment classifier on a stream of Twitter posts. Moniz et al. BIBREF8 present a method for predicting the popularity of news from Twitter data and sentiment scores, and estimate its performance using a sequential approach in multiple testing periods."]}
